<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Thu, 26 Feb 2026 17:10:15 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.9.1</generator>
	<item>
		<title>CORPGEN advances AI agents for real work</title>
		<link>https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/</link>
		
		<dc:creator><![CDATA[Abubakarr Jaye, Nigel Boachie Kumankumah, Chidera Biringa, Sulaiman Vesal, Anjel Patel, Dayquan Julienne]]></dc:creator>
		<pubDate>Thu, 26 Feb 2026 17:06:34 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1162836</guid>

					<description><![CDATA[<p>By mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today’s best models are evaluated one [&#8230;]</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/">CORPGEN advances AI agents for real work</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1.jpg" alt="decorative icons in white on a blue and green gradient background" class="wp-image-1162851" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/CORPGEN-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>Today’s AI agent benchmarks test one task at a time, while real workplace productivity requires managing dozens of interdependent tasks at once. To reflect this, we created a setting called Multi-Horizon Task Environments (MHTEs).</li>



<li>Under multi-task loads, leading computer-using agents degrade sharply, with completion rates dropping from 16.7% to 8.7%.</li>



<li>CORPGEN introduces <em>digital employees</em>, with hierarchical planning, memory isolation, and experiential learning, delivering up to 3.5 times higher completion rates than baselines across three independent agent backends.</li>



<li>Because CORPGEN is architecture-agnostic and modular, its gains come from system design rather than any single base model, and it benefits directly as underlying models improve.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>By mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today’s best models are evaluated one task at a time, not dozens at once.</p>



<p>In our paper, “<a href="https://www.microsoft.com/en-us/research/publication/corpgen-simulating-corporate-environments-with-autonomous-digital-employees-in-multi-horizon-task-environments/">CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments</a>,” we propose an agent framework that equips AI with the memory, planning, and learning capabilities to close that gap.</p>



<h2 class="wp-block-heading" id="introducing-multi-horizon-task-environments">Introducing Multi-Horizon Task Environments</h2>



<p>Replicating the reality of workplace multitasking requires a new kind of evaluation environment. In response, we developed Multi-Horizon Task Environments (MHTEs), settings where an agent must manage multiple complex tasks simultaneously. Each task requires 10 to 30 dependent steps within a single session spanning five hours.</p>



<p>To determine what a benchmark would need to test, we ran MHTEs at scale on some of today’s leading AI agents, exposing four weaknesses. First, memory fills up. An agent cannot hold details for multiple active tasks at once. Second, information from one task interferes with reasoning about another. Third, tasks don’t depend on each other in simple sequences. They form complex webs where an agent must constantly check whether upstream work is finished before it can move forward on anything downstream. Fourth, every action cycle requires reprioritizing across all active tasks, not simply resuming where the agent left off.</p>



<p>We also tested three independent agent systems under increasing loads. As the number of concurrent tasks rose from 12 to 46, completion rates fell from 16.7% to 8.7% across all systems.</p>



<h2 class="wp-block-heading" id="corpgen-s-architecture">CORPGEN’s architecture</h2>



<p>CORPGEN introduces <em>digital employees</em>: LLM-powered AI agents with persistent identities, role-specific expertise, and realistic work schedules. They operate Microsoft Office applications through GUI automation and perform consistently within MHTEs over hours of continuous activity. Figure 1 illustrates how a digital employee moves through a full workday.</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="2560" height="1318" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-scaled.png" alt="Diagram showing a digital employee's workday in three phases. Day Init on the left, where the agent loads memory and generates a daily plan. Execution Cycles in the center, where the agent repeatedly retrieves context, reasons and acts through a ReAct loop, and persists results across 50+ interleaved tasks. Day End on the right, where the agent generates a reflection and consolidates experience into long-term memory. Below the diagram, labels show the tiered memory architecture and experiential learning components." class="wp-image-1162865" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-300x154.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-1024x527.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-768x395.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-1536x791.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-2048x1055.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/day_in_life_color_AH-240x124.png 240w" sizes="(max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1. Each day begins with a structured plan and memory loaded from previous sessions. The agent then works through overlapping tasks in repeated cycles, storing key outcomes at day’s end to inform the next session.</figcaption></figure>



<p>CORPGEN addresses each of the four weaknesses of concurrent task execution—memory overload, cross-task interference, dependency complexity, and reprioritization—in a targeted way. Hierarchical planning breaks objectives into daily goals and then into moment-to-moment decisions, allowing the agent to act from a structured plan instead of reviewing all available tasks before each step.</p>



<p>Subagents perform complex operations like web research in isolated contexts, preventing cross-task contamination. A tiered memory system enables selective recall of task-related information rather than retaining everything in active context. Adaptive summarization compresses routine observations while preserving critical information, keeping memory growth controlled. </p>



<p>Because these mechanisms are not tied to a specific base model, we tested CORPGEN across three different agents. In each case, we observed consistent gains. The improvements came from the architecture, not from the strength of any particular model. Figure 2 shows how they fit together within CORPGEN&#8217;s architecture.</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="2560" height="1318" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-scaled.png" alt="Architecture diagram of the CORPGEN framework. At center is the Digital Employee with persistent identity, execution engine, cognitive tools, sub-agents, and context management. On the left, Hierarchical Planning decomposes strategic objectives into tactical plans and operational actions. On the right, Sub-Agents as Tools shows a Research Agent and Computer-Use agent (UFO2) operating in isolated contexts. At the bottom, the Tiered Memory Architecture spans working memory, structured long-term memory, and semantic memory via Mem0. Experiential Learning in the bottom right captures successful trajectories and routes feedback to UFO2. Multi-Employee Collaboration at the top shows async communication via Email and Teams with no shared state." class="wp-image-1162863" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-300x154.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-1024x527.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-768x395.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-1536x791.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-2048x1055.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/corpgen_arch_color_AH2-240x124.png 240w" sizes="(max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 2. Four mechanisms support concurrent task execution in CORPGEN: hierarchical planning, isolated subagents, tiered memory, and adaptive summarization.</figcaption></figure>



<h2 class="wp-block-heading" id="how-digital-employees-collaborate">How digital employees collaborate</h2>



<p>When multiple digital employees operate in the same environment, collaboration takes shape through standard communication channels, without predefined coordination rules. One employee sends an email requesting data; another picks it up in the next cycle, uses its memory to process it, and responds. This exchange mirrors real workplace communication.</p>



<p>There is no shared internal state between agents. Coordination occurs entirely through email and Microsoft Teams, the same channels many workers use. Over time, these independent exchanges form recognizable organizational patterns. Some agents take on leadership roles; others provide support; shared documents become the connective tissue.</p>



<p>When a communication path breaks, such as an email delivery error, agents reroute messages through alternate channels to keep work moving. The result is a virtual organization that behaves like a real one without being explicitly programmed to do so.</p>



<h2 class="wp-block-heading" id="evaluating-corpgen">Evaluating CORPGEN</h2>



<p>We evaluated CORPGEN on a multi-task benchmark that combined up to 46 tasks into a single six-hour session. Three findings stood out.</p>



<p><strong>Baselines degrade as load increases; CORPGEN does not.</strong> All three baseline agent systems showed steady performance declines as task load rose. CORPGEN, by contrast, maintained or improved its completion rates at higher loads. At 46 tasks, CORPGEN completed 15.2% of tasks, compared with 4.3% for the baselines, roughly 3.5 times more.</p>



<p><strong>Experiential learning drives the largest gains.</strong> We introduced CORPGEN’s components sequentially: first the orchestration layer, then cognitive tools, and finally experiential learning. The first two produced moderate improvements. Experiential learning, in which agents store records of completed tasks and reuse them when they encounter structurally similar work, produced the largest increase, raising completion rates from 8.7% to 15.2%.</p>



<p><strong>Evaluation methodology changes the picture.</strong> When we inspected the actual output files produced by agents, the results agreed with human judgements roughly 90% of the time. Evaluation based on screenshots and action logs agreed only about 40% of the time. This gap suggests that common evaluation approaches may underestimate what agents actually accomplish in practice.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="implications-and-looking-forward">Implications and looking forward</h2>



<p>The results suggest that memory and retrieval, not just raw model capability, may be a key bottleneck in getting agents to work in the real world. The largest gains came from experiential learning. Agents that learn from prior successes and apply those patterns to structurally similar tasks build an advantage over systems that respond to each task in isolation.</p>



<p>CORPGEN also opens a new lens on how AI agents collaborate. Next steps include testing whether agents can maintain memory across multiple workdays and how they coordinate when working in teams. We are also exploring ways to make agents faster and more reliable by combining different methods of interacting with software.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading" id="acknowledgments">Acknowledgments</h2>



<p>This work is a result of a collaboration between the Office of the CTO at Microsoft and the Microsoft AI Development Accelerator Program (MAIDAP). We would like to thank the Microsoft Security Research team for providing resources that supported this research. We also thank the members of the Microsoft <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/UFO" target="_blank" rel="noopener noreferrer">UFO2<span class="sr-only"> (opens in new tab)</span></a> team and the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/mem0ai/mem0" target="_blank" rel="noopener noreferrer">Mem0<span class="sr-only"> (opens in new tab)</span></a> project for their open-source contributions, which enabled key components of the CORPGEN architecture, and the OSWorld team for the benchmark that served as the foundation for our multi-task evaluation.</p>



<p>Finally, we thank the many contributors to this research: Anjel Shaileshbhai Patel, Dayquan Julienne, Charlotte Siska, Manuel Raúl Meléndez Luján, Anthony Twum-Barimah, Mauricio Velazco, and Tianwei Chen.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/">CORPGEN advances AI agents for real work</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</title>
		<link>https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/</link>
		
		<dc:creator><![CDATA[Eric Horvitz, Andrew Jenks, Jessica Young]]></dc:creator>
		<pubDate>Thu, 19 Feb 2026 16:00:51 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1162092</guid>

					<description><![CDATA[<p>As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/">Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p><em><em>Insights from Microsoft’s Media Integrity and Authentication: Status, Directions, and Futures report</em></em></p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1.jpg" alt="three white outline icons on a blue-to-pink gradient background: an image with a copyright “CR” badge, an image overlaid with fingerprint-like lines, and an image framed by a cropping grid." class="wp-image-1162625" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/MediaIntegrityReport-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>It has become increasingly difficult to distinguish fact from fiction when viewing online images and videos. Resilient, trustworthy technologies can help people determine whether the content they are viewing was captured by a camera or microphone—or generated or modified by AI tools.&nbsp;</p>



<p>We refer to technologies aimed at helping viewers verify the source and history—that is, the provenance—of digital content as <em>media integrity and authentication</em> (MIA) methods. This technique, driven by the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://c2pa.org/" target="_blank" rel="noopener noreferrer">Coalition for Content Provenance and Authenticity<span class="sr-only"> (opens in new tab)</span></a> (C2PA), a standards body dedicated to scaling these capabilities, as well as complementary methods such as watermarks and fingerprinting, have become critically important with the rapid advance of AI systems capable of creating, realistic imagery, video, and audio at scale.</p>



<h2 class="wp-block-heading" id="a-convergence-of-forces">A convergence of forces</h2>



<p>Our team recognized an inflection point in the evolution of online content integrity, driven by the convergence of four forces:</p>



<ul class="wp-block-list">
<li><strong>Growing saturation of synthetic media</strong>, driven by proliferation of high-fidelity content-generation tools and the explosion of AI generated or modified media online</li>



<li><strong>Forthcoming legislation</strong> both nationally and internationally seeking to define what “verifiable” provenance should mean in practice</li>



<li><strong>Mounting pressure on implementers</strong> to ensure authentication signals are clear and helpful, especially as signals increase when legislation goes into effect in 2026</li>



<li><strong>Heightened awareness of&nbsp;potential&nbsp;adversarial attacks</strong> that attempt to exploit weaknesses in authenticity systems</li>
</ul>



<p>The usefulness and trustworthiness of provenance signals, whether certifying content as synthetic or as an authentic capture of real-world scenes, will depend not only on advances in technology, but also on how the broader digital ecosystem adopts, implements, and governs these tools. Aligning around implementation choices that promote consistency and clarity is essential to ensure transparency signals strengthen, rather than erode, public confidence.</p>



<p>To address these challenges, we launched a comprehensive evaluation of the real-world limits, edge cases, and emerging “attack surfaces” for MIA methods. Today, we are publishing our findings in the <a href="https://www.microsoft.com/en-us/research/publication/media-integrity-and-authentication-status-directions-and-futures/"><em>Media Integrity & Authentication: Status, Directions & Futures </em>report</a>. The report distills lessons learned and outlines practical directions for strengthening media integrity in the years ahead.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1002645">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: AI-POWERED EXPERIENCE</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/01/MSR-Chat-Promo.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft research copilot experience</h2>
				
								<p id="microsoft-research-copilot-experience" class="large">Discover more about research at Microsoft through our AI-powered experience</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-describedby="microsoft-research-copilot-experience" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft research copilot experience" target="_blank">
							Start now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="findings-and-directions-forward">Findings and directions forward</h2>



<p>Our research recognizes that different media integrity and authenticity methods serve differing purposes and offer distinct levels of protection. After defining each method in detail, we focused on secure provenance (C2PA), imperceptible watermarking, and soft hash fingerprinting across images, audio, and video.</p>



<p>Grounded in our evaluation of these MIA methods across modalities, attack categories, and real-world workflows, several new findings emerged including two new concepts:</p>



<ul class="wp-block-list">
<li><strong>High-Confidence Provenance Authentication</strong>: a&nbsp;critical capability for verifying, under defined conditions, whether claims about the origin of and modifications made to an asset can be&nbsp;validated&nbsp;with high certainty.</li>



<li><strong>Sociotechnical Provenance Attacks</strong>: attacks aimed at deception and capable of inverting signals, making authentic content appear synthetic, and synthetic content appear authentic.</li>
</ul>



<p>Drawing on our findings, we identified four promising directions for further strengthening media authentication, along with suggestions to support more effective implementation strategies and future decisions. We’ve summarized the findings and directions below, with additional detail available in the report.</p>



<figure class="wp-block-table"><table><thead><tr><th>Promising directions</th><th>High-level findings</th></tr></thead><tbody><tr><td>Delivering high-confidence provenance authentication</td><td>&#8211; <strong>Implementation and display choices may affect the reliability of provenance indicators</strong> and how they are interpreted by the public. <br><br>&#8211; Using a C2PA provenance manifest for media created and signed in a high security environment <strong>enables high-confidence validation</strong>. <br><br>&#8211; High-confidence validation is also possible across a broader volume of images, audio, and video when <strong>an imperceptible watermark is linked to C2PA provenance manifest as an additional layer to recover the provenance information if removed</strong>. <br><br>&#8211; Fingerprinting is <strong>not an enabler for high-confidence validation</strong> and can involve significant costs when expected at scale. However, it can support manual forensics.</td></tr><tr><td>Mitigating confusion from sociotechnical provenance attacks</td><td>&#8211; <strong>MIA methods are susceptible to sociotechnical attacks on provenance that may mislead the public</strong>, resulting in confusion and misplaced trust about an asset’s provenance if there is an overreliance on low-quality signals.<br><br>&#8211; Layering and linking secure provenance and imperceptible watermarking methods to achieve high confidence validation also offers a promising option to <strong>both deter and mitigate the impact of attacks</strong>.<br><br>&#8211; Unintended consequences may result from the use of methods lacking authentication, such as the use of perceptible watermarks in the absence of secure provenance. <strong>Perceptible watermarks may cause confusion</strong> in cases of forgery or discourage people from consulting high-confidence provenance information via a validation tool, if such perceptible disclosures are taken at face value.<br> <br>&#8211; <strong>UX design that enables users to explore manifest details</strong>—such as where edits occurred&nbsp;or region of interest—has the potential to&nbsp;reduce confusion and&nbsp;support forensics and fact checking efforts.&nbsp;&nbsp;</td></tr><tr><td>Enabling more trusted provenance on edge devices</td><td>&#8211; High-confidence results <strong>aren&#8217;t feasible when provenance is added by a conventional offline device</strong> (e.g., camera or recording device without connectivity).<br><br>&#8211; <strong>Implementing a secure enclave</strong> within the hardware layer of offline devices is essential to make the provenance of captured images, audio, and video more trustworthy.</td></tr><tr><td>Investing in ongoing research and policy development</td><td>&#8211; All three methods offer organizations&nbsp;<strong>valuable tools for addressing operational challenges</strong>&nbsp;such as fraud prevention, risk management, and digital accountability.&nbsp;<br><br>&#8211; <strong>UX and display&nbsp;</strong>are promising directions for research. Important directions include in-stream tools that display provenance information where people are&nbsp;and&nbsp;distinguish&nbsp;between&nbsp;high- and lower-confidence&nbsp;provenance signals.<br><br>&#8211; <strong>Stakeholders&nbsp;should&nbsp;conduct ongoing analysis and red teaming</strong>&nbsp;to&nbsp;identify&nbsp;and mitigate weaknesses&nbsp;through&nbsp;technical approaches,&nbsp;policies, and&nbsp;laws.&nbsp;&nbsp;&nbsp;</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="the-journey-continues">The journey continues</h2>



<p>This report marks the beginning of a new chapter in our <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://blogs.microsoft.com/on-the-issues/2021/02/22/deepfakes-disinformation-c2pa-origin-cai/">media provenance journey<span class="sr-only"> (opens in new tab)</span></a>, building on years of foundational work, from <a href="https://www.microsoft.com/en-us/research/publication/amp-authentication-of-media-via-provenance/">developing the very first prototype</a> in 2019 to co-founding the C2PA in 2021 and helping catalyze an ecosystem that has since grown to more than <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://c2pa.org/the-c2pa-launches-content-credentials-2-3-and-celebrates-5-years-of-impact-across-the-digital-ecosystem/">6,000 members and affiliates<span class="sr-only"> (opens in new tab)</span></a> supporting C2PA Content Credentials. This research represents the next evolution of that long‑standing commitment.</p>



<p>We hope that by sharing our learnings will help others prepare for an important wave, especially as generative technologies accelerate and provenance signals multiply. This work is already underway across our products at Microsoft. Together, these directions highlight opportunities for the ecosystem to align, harden, and innovate, so authentication signals are not merely visible, but robust, meaningful, and resilient throughout the content lifecycle.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/">Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Project Silica’s advances in glass storage technology</title>
		<link>https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/</link>
		
		<dc:creator><![CDATA[Richard Black]]></dc:creator>
		<pubDate>Wed, 18 Feb 2026 16:11:45 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1161970</guid>

					<description><![CDATA[<p>Project Silica introduces new techniques for encoding data in borosilicate glass, as described in the journal Nature. These advances lower media cost and simplify writing and reading systems while supporting 10,000-year data preservation.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/">Project Silica’s advances in glass storage technology</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1.jpg" alt="A blue-to-green gradient background featuring three white icons: a networked globe on the left, a cloud in the center, and a stacked database on the right." class="wp-image-1162006" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/NatureSilica-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>Microsoft Research publishes&nbsp;breakthrough&nbsp;in&nbsp;<em>Nature</em>&nbsp;on glass-based data storage that could preserve information for 10,000 years.&nbsp;</li>



<li>New&nbsp;technique extends technology from expensive fused silica to ordinary borosilicate glass found in kitchen cookware.&nbsp;</li>



<li>Innovations enable faster parallel writing, simplified readers (one camera instead of three), and easier manufacturing.&nbsp;</li>



<li>Phase voxel method requires only a single laser pulse, significantly reducing complexity and cost.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>Long-term preservation of digital information has long challenged archivists and datacenters, as magnetic tapes and hard drives degrade within decades. Existing archival storage solutions have limited media lifespans that make them less than ideal for preserving information for future generations.</p>



<p>Now, we are excited to report significant progress on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://unlocked.microsoft.com/sealed-in-glass/" type="link" id="https://unlocked.microsoft.com/sealed-in-glass/" target="_blank" rel="noopener noreferrer">Project Silica<span class="sr-only"> (opens in new tab)</span></a>, our effort to encode data in glass using femtosecond lasers, a technology that could preserve information for 10,000 years. Glass is a permanent data storage material that is resistant to water, heat, and dust.</p>



<p>In findings published in <em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41586-025-10042-w" type="link" id="https://www.nature.com/articles/s41586-025-10042-w" target="_blank" rel="noopener noreferrer">Nature<span class="sr-only"> (opens in new tab)</span></a></em>, we describe a breakthrough that extends the technology beyond expensive fused silica to ordinary borosilicate glass. A readily available and lower-cost medium, this is the same material found in kitchen cookware and oven doors. This advance addresses key barriers to commercialization: cost and availability of storage media. We have unlocked the science for parallel high-speed writing and developed a technique to permit accelerated aging tests on the written glass, suggesting that the data should remain intact for at least 10,000 years.</p>



<p>Storing data inside glass with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bing.com/search?q=femtosecond" type="link" id="https://www.bing.com/search?q=femtosecond" target="_blank" rel="noopener noreferrer">femtosecond<span class="sr-only"> (opens in new tab)</span></a> laser pulses is one of the few technologies on the horizon with the potential for durable, immutable, and long-lived storage. Although we have been leading innovation in <a href="https://www.microsoft.com/en-us/research/blog/project-silica-sustainable-cloud-archival-storage-in-glass/" type="link" id="https://www.microsoft.com/en-us/research/blog/project-silica-sustainable-cloud-archival-storage-in-glass/">this type of storage for years</a>, prior to this research the technique only worked with pure fused silica glass, a type of glass that is relatively difficult to manufacture and available from only a few sources.</p>



<p>In the paper, we show how data can be stored in borosilicate glass. The new technique stores hundreds of layers of data in glass only 2mm thin, as with previous methods, but with important improvements. The reader for the glass now needs only one camera, not three or four, reducing cost and size. In addition, the writing devices require fewer parts, making them easier to manufacture and calibrate, and enabling them to encode data more quickly.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="key-scientific-discoveries">Key scientific discoveries</h2>



<p>The <em>Nature</em> paper details several key new scientific discoveries:</p>



<p><strong>Advances in <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bing.com/search?q=birefringent%20voxel&qs=n&form=QBRE&sp=-1&lq=0&pq=birefringent%20voxel" target="_blank" rel="noopener noreferrer">birefringent voxel<span class="sr-only"> (opens in new tab)</span></a> writing</strong>: For the previous type of data storage in fused silica glass using birefringent (i.e., polarization) voxels, we developed a technique to reduce the number of pulses used to form the voxel from many to only two, critically showing that the polarization of the first pulse is not important to the polarization of the voxel formed. We further developed this to enable pseudo-single-pulse writing, in which a single pulse can be split after its polarization is set to simultaneously form the first pulse for one voxel (where the polarization doesn’t matter) and the second pulse of another (where the set polarization is essential). We demonstrated how to use this pseudo-single-pulse writing to enable fast writing with beam scanning across the media.</p>



<p><strong>Phase voxels, a new storage method</strong>: We invented a new type of data storage in glass called phase voxels, in which the phase change of the glass is modified instead of its polarization, showing that only a single pulse is necessary to make a phase voxel. We demonstrated that these phase voxels can also be formed in borosilicate glass and devised a technique to read the phase information from phase voxels encoded in this material. We showed that the much higher levels of three-dimensional inter-symbol interference in phase voxels can be mitigated with a machine learning classification model.</p>



<p><strong>Parallel writing capabilities</strong>: By combining a mathematical model of pre-heating and post-heating within the glass with the invention of a multi-beam delivery system, we showed that many data voxels can be written in proximity in the glass at the same time, significantly increasing writing speed. We explained a method for using light emissions (a side effect of voxel formation) for both static calibration and dynamic control to fully support automatic writing operations.</p>



<p><strong>Optimization and longevity testing</strong>: We developed a new way to optimize symbol encodings using machine learning and a better way to understand the tradeoff between error rates, error protection, and error recovery when evaluating new digital storage systems. We also created a new <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bing.com/search?pglt=161&q=nondestructive+optical+method" type="link" id="https://www.bing.com/search?pglt=161&q=nondestructive+optical+method" target="_blank" rel="noopener noreferrer">nondestructive optical method<span class="sr-only"> (opens in new tab)</span></a> to identify the aging of data storage voxels within the glass, using this and standard accelerated aging techniques to support data lasting 10,000 years. We extended the industry standard Gray codes to apply to nonpower-of-two numbers of symbols.</p>


<div class="wp-block-msr-cards msr-cards msr-cards--carousel mt-4 has-text-align-left has-cards" data-bi-aN="Cards">
	<div class="msr-cards__inner">
		
					<div class="mt-4">
				<div class="row">
	<div class="col-12 px-0 px-md-g">
		<section aria-label="Slideshow for: " aria-roledescription="slideshow">
			<div id="status-wp-block-msr-cards-1">
				<span id="status-msg-wp-block-msr-cards-1" class="sr-only" aria-live="polite"></span>
			</div>
			<div class="carousel slide carousel-content-cards carousel-sneak-peek" data-mount="carousel" data-status="status-wp-block-msr-cards-1">
				<a href="#skip-wp-block-msr-cards-1" class="btn btn-link sr-only-focusable w-100 position-absolute">
					Skip slideshow for: 				</a>
				<div>
					<div class="carousel-controls">
						<button type="button" class="carousel-control-prev " data-slide="prev">
							<span class="sr-only">Previous slide</span>
						</button>

						<ol class="carousel-indicators" aria-hidden="true">
															<li class="active"></li>
															<li class=""></li>
															<li class=""></li>
															<li class=""></li>
													</ol>

						<button type="button" class="carousel-control-next " data-slide="next">
							<span class="sr-only">Previous slide</span>
						</button>
					</div>

					<div class="carousel-inner">
													<section class="carousel-item msr-cards__card msr-cards__card--carousel active" aria-label="Slide 1 of 4" aria-roledescription="slide">

	<div class="card material-card h-100 p-0">

					<div class="embed-responsive embed-responsive-16by9">
				<img loading="lazy" decoding="async" width="1024" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-1024x683.jpg" class="card-img embed-responsive-item img-object-cover" alt="Project Silica | Nature | Microsoft Azure - vertical color bars in varying shades of blue" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-300x200.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-768x512.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800-240x160.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_1_1200x800.jpg 1200w" sizes="auto, (max-width: 1024px) 100vw, 1024px" />			</div>
		
		<div class="card-body px-4 px-lg-5 pt-4">
										<div class="card__description mb-3">
					<p>A piece of Project Silica media written with data.</p>				</div>
					</div>

			</div>
</section>
													<section class="carousel-item msr-cards__card msr-cards__card--carousel" aria-label="Slide 2 of 4" aria-roledescription="slide">

	<div class="card material-card h-100 p-0">

					<div class="embed-responsive embed-responsive-16by9">
				<img loading="lazy" decoding="async" width="1024" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-1024x683.jpg" class="card-img embed-responsive-item img-object-cover" alt="Project Silica | Nature | photo of lab testing set up" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-300x200.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-768x512.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800-240x160.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_2_1200x800.jpg 1200w" sizes="auto, (max-width: 1024px) 100vw, 1024px" />			</div>
		
		<div class="card-body px-4 px-lg-5 pt-4">
										<div class="card__description mb-3">
					<p>A research-grade Writer used to set the record for high speed data writing into glass.</p>				</div>
					</div>

			</div>
</section>
													<section class="carousel-item msr-cards__card msr-cards__card--carousel" aria-label="Slide 3 of 4" aria-roledescription="slide">

	<div class="card material-card h-100 p-0">

					<div class="embed-responsive embed-responsive-16by9">
				<img loading="lazy" decoding="async" width="1024" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-1024x683.jpg" class="card-img embed-responsive-item img-object-cover" alt="Project Silica | Nature | photo of lab testing set up" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-300x200.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-768x512.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800-240x160.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_3_1200x800.jpg 1200w" sizes="auto, (max-width: 1024px) 100vw, 1024px" />			</div>
		
		<div class="card-body px-4 px-lg-5 pt-4">
										<div class="card__description mb-3">
					<p>A research-grade Reader for retrieving data from glass.</p>				</div>
					</div>

			</div>
</section>
													<section class="carousel-item msr-cards__card msr-cards__card--carousel" aria-label="Slide 4 of 4" aria-roledescription="slide">

	<div class="card material-card h-100 p-0">

					<div class="embed-responsive embed-responsive-16by9">
				<img loading="lazy" decoding="async" width="1024" height="683" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-1024x683.jpg" class="card-img embed-responsive-item img-object-cover" alt="Project Silica | Nature | photo of lab testing set up" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-1024x683.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-300x200.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-768x512.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800-240x160.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/Nature_Silica_4_1200x800.jpg 1200w" sizes="auto, (max-width: 1024px) 100vw, 1024px" />			</div>
		
		<div class="card-body px-4 px-lg-5 pt-4">
										<div class="card__description mb-3">
					<p>Close up of Writer showing high-speed multi-beam data encoding on laser pulses.</p>				</div>
					</div>

			</div>
</section>
											</div>
				</div>
			</div>
		</section>
		<a id="#skip-wp-block-msr-cards-1" class="sr-only" tabindex="-1">
			End of slideshow for: 		</a>
	</div>
</div>
			</div>
		
			</div>
</div>



<h2 class="wp-block-heading" id="demonstrating-the-technology">Demonstrating the technology</h2>



<p>As a research initiative, Project Silica has demonstrated these advances through several proofs of concept, including <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/features/innovation/ignite-project-silica-superman" type="link" id="https://news.microsoft.com/source/features/innovation/ignite-project-silica-superman" target="_blank" rel="noopener noreferrer">storing Warner Bros.’ “Superman” movie on quartz glass<span class="sr-only"> (opens in new tab)</span></a>, partnering with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://unlocked.microsoft.com/sealed-in-glass/" type="link" id="https://unlocked.microsoft.com/sealed-in-glass/" target="_blank" rel="noopener noreferrer">Global Music Vault<span class="sr-only"> (opens in new tab)</span></a> to <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fastcompany.com/90757574/with-glass-buried-under-ice-microsoft-plans-to-preserve-music-for-10000-years" type="link" id="https://www.fastcompany.com/90757574/with-glass-buried-under-ice-microsoft-plans-to-preserve-music-for-10000-years" target="_blank" rel="noopener noreferrer">preserve music under ice for 10,000 years<span class="sr-only"> (opens in new tab)</span></a>, and working with students on a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.geekwire.com/2024/microsoft-silica-golden-record-glass/" type="link" id="https://www.geekwire.com/2024/microsoft-silica-golden-record-glass/" target="_blank" rel="noopener noreferrer">“Golden Record 2.0” project<span class="sr-only"> (opens in new tab)</span></a>, a digitally curated archive of images, sounds, music, and spoken language, crowdsourced to represent and preserve humanity’s diversity for millennia.</p>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead</h2>



<p>The research phase is now complete, and we are continuing to consider learnings from Project Silica as we explore the ongoing need for sustainable, long-term preservation of digital information. We have added this paper to our <a href="https://www.microsoft.com/en-us/research/project/project-silica/publications/">published works</a> so that others can build on them.</p>



<h2 class="wp-block-heading" id="related-work">Related work</h2>



<p><a href="https://www.microsoft.com/en-us/research/project/project-silica/" type="link" id="https://www.microsoft.com/en-us/research/project/project-silica/">Project Silica</a> has made scientific advances across multiple areas beyond laser direct writing (LDW) in glass, including archival storage systems design, archival workload analysis, datacenter robotics, erasure coding, free-space optical components, and machine learning-based methods for symbol decoding in storage systems. Many of these innovations were described in our <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/Silica" target="_blank" rel="noopener noreferrer">ACM Transactions on Storage publication<span class="sr-only"> (opens in new tab)</span></a> in 2025.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/">Project Silica’s advances in glass storage technology</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Rethinking imitation learning with Predictive Inverse Dynamics Models</title>
		<link>https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/</link>
		
		<dc:creator><![CDATA[Pallavi Choudhury, Lukas Sch&auml;fer, Chris Lovett, Katja Hofmann, Sergio Valcarcel Macua]]></dc:creator>
		<pubDate>Thu, 05 Feb 2026 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1160901</guid>

					<description><![CDATA[<p>This research looks at why Predictive Inverse Dynamics Models often outperform standard Behavior Cloning in imitation learning. By using simple predictions of what happens next, PIDMs reduce ambiguity and learn from far fewer demonstrations.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/">Rethinking imitation learning with Predictive Inverse Dynamics Models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New.jpg" alt="Smart Replay - flowchart diagram showing the flow between Encoder, State Predictor, and Policy" class="wp-image-1161128" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay-BlogHeroFeature-1400x788_New-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>Imitation learning becomes easier when an AI&nbsp;agent&nbsp;understands why an action is taken.</li>



<li>Predictive Inverse Dynamics Models (PIDMs)&nbsp;predict&nbsp;plausible future states,&nbsp;clarifying the direction of behavior during imitation&nbsp;learning.</li>



<li>Even imperfect predictions reduce ambiguity,&nbsp;making&nbsp;it clearer which action makes sense&nbsp;in the moment.</li>



<li>This makes PIDMs far more data‑efficient than traditional approaches.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>Imitation&nbsp;learning&nbsp;teaches&nbsp;AI agents by example: show the agent recordings of how people perform a task and let it&nbsp;infer&nbsp;what to do.&nbsp;The&nbsp;most common&nbsp;approach,&nbsp;Behavior Cloning&nbsp;(BC),&nbsp;frames this as a simple question: “Given the current state&nbsp;of the environment, what action&nbsp;would&nbsp;an expert take?”</p>



<p>In practice, this is done through supervised learning, where the states serve as inputs and expert actions as outputs. While simple in principle, BC often requires large demonstration datasets to account for the natural variability in human behavior, but collecting such datasets can be costly and difficult in real-world settings.</p>



<p>Predictive Inverse Dynamics Models (PIDMs) offer a different take on imitation learning by changing how agents interpret human behavior. Instead of directly mapping states to actions, PIDMs break down the problem into two subproblems: predicting what should happen next and inferring an appropriate action to go from the current state to the predicted future state. While PIDMs often outperform BC, it has not been clear why they work so well, motivating a closer look at the mechanisms behind their performance.</p>



<p>In the paper, “<a href="https://www.microsoft.com/en-us/research/publication/when-does-predictive-inverse-dynamics-outperform-behavior-cloning/">When does predictive inverse dynamics outperform behavior cloning?</a>” we show how this two-stage approach enables PIDMs to learn effective policies from far fewer demonstrations than BC. By grounding the selection process in a plausible future, PIDMs provide a clearer basis for choosing an action&nbsp;during inference. In practice, this can mean achieving comparable performance with as few as one-fifth the demonstrations required by BC, even when predictions are imperfect.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1009" height="658" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1.png" alt="Figure 1. BC vs. PIDM architectures. (Top) Behavior Cloning learns how to perform a direct mapping from the current state to an action. (Bottom) PIDMs add a state predictor that predicts future states. They then use an inverse dynamics model to predict the action required to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder." class="wp-image-1161185" style="width:600px" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1.png 1009w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1-300x196.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1-768x501.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/SmartReplay_FIG1-240x157.png 240w" sizes="auto, (max-width: 1009px) 100vw, 1009px" /><figcaption class="wp-element-caption">Figure 1. BC vs. PIDM architectures.&nbsp;(Top) Behavior&nbsp;Cloning learns&nbsp;how to perform&nbsp;a direct mapping from the current state to an action. (Bottom)&nbsp;PIDMs add a state predictor that predicts future&nbsp;states. They&nbsp;then use an inverse dynamics model to predict the action&nbsp;required&nbsp;to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder.</figcaption></figure>



<h2 class="wp-block-heading" id="how-pidms-rethink-imitation">How PIDMs rethink imitation</h2>



<p>PIDMs’ approach to imitation learning consists of two core elements: a model that forecasts plausible future states, and an inverse dynamics model (IDM) that predicts the action needed to move from the present state toward that future. Instead of asking, “What action would an expert take?” PIDMs effectively ask, “What would an expert try to achieve, and what action would lead to it?” This shift turns the information in the current observation (e.g., video frame) into a coherent sense of direction, reducing ambiguity about intent and making action prediction easier.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="real-world-validation-in-a-3d-gameplay-environment">Real-world validation in a 3D gameplay environment</h2>



<p>To evaluate PIDMs under realistic conditions, we trained agents on human gameplay demonstrations in a visually rich video game. These conditions include operating directly from raw video input, interacting with a complex 3D environment in real time at 30 frames per second, and handling visual artifacts and unpredictable system delays.  </p>



<p>The agents ran from beginning to end, taking video frames as input and continuously deciding which buttons to press and how to move the joysticks. Instead of relying on a hand-coded set of game variables and rules, the model worked directly from visual input, using past examples to predict what comes next and choosing actions that moved play in that direction.</p>



<p>We ran all experiments on a cloud gaming platform, which introduced additional delays and visual distortions. Despite these challenges, the PIDM agents consistently matched human patterns of play and achieved high success rates across tasks, as shown in Video 1 below and Videos 2 and 3 in the appendix.</p>



<figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Smart Replay demo: exercise (video 1)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/Jfjt_k6Pw1k?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Video 1. A player&nbsp;(left)&nbsp;and a PIDM agent&nbsp;(right)&nbsp;side by side playing the game&nbsp;<em>Bleeding Edge</em>.&nbsp;Both&nbsp;navigate the same trajectory,&nbsp;jumping over obstacles and engaging&nbsp;with&nbsp;nonplayer&nbsp;characters. Despite&nbsp;network delays, the&nbsp;agent closely matches the player&#8217;s timing and&nbsp;movement&nbsp;in real time.</figcaption></figure>



<h2 class="wp-block-heading" id="why-and-when-pidms-outperform-bc">Why and when PIDMs outperform BC</h2>



<p>Of course, AI agents do not have access to future outcomes. They can only generate predictions based on available data, and those predictions are sometimes wrong. This creates a central trade‑off for PIDMs.</p>



<p>On one hand, anticipating where the agent should be heading can clarify what action makes sense in the present. Knowing the intended direction helps narrow an otherwise ambiguous choice. On the other hand, inaccurate predictions can occasionally steer the model toward the wrong action.</p>



<p>The key insight is that these effects are not symmetric. While prediction errors introduce some risk, reducing ambiguity in the present often matters more. Our theoretical analysis shows that even with imperfect predictions, PIDMs outperform BC as long as the prediction error remains modest. If future states were known perfectly, PIDMs would outperform BC outright.</p>



<p>In practice, this means that clarifying intent often matters more than accurately predicting the future. That advantage is most evident in the situations where BC struggles: where human behavior varies and actions are driven by underlying goals rather than by what is immediately visible on the screen.</p>



<p>BC requires many demonstrations because each example is noisy and open to multiple interpretations. PIDMs, by contrast, sharpen each demonstration by linking actions to the future states they aim to reach. As a result, PIDMs can learn effective action strategies from far fewer examples.</p>



<h2 class="wp-block-heading" id="evaluation">Evaluation</h2>



<p>To test these ideas under realistic conditions, we designed a sequence of experiments that begins with a simple, interpretable 2D environment (Video 4 in the appendix) and culminates in a complex 3D video game. We trained both BC and PIDM on very small datasets, ranging from one to fifty demonstrations in the 2D environment and from five to thirty for the 3D video game. Across all tasks, PIDM reached high success rates with far fewer demonstrations than BC.</p>



<p>In the 2D setting, BC needed two to five times more data to match PIDM’s performance (Figure 2). In the 3D game, BC needed 66% more data to achieve comparable results (Video 5 in the appendix).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1166" height="871" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d.png" alt="Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility." class="wp-image-1161012" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d.png 1166w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d-300x224.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d-1024x765.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d-768x574.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d-80x60.png 80w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/SmartReplay_blog_Fig2a-d-240x180.png 240w" sizes="auto, (max-width: 1166px) 100vw, 1166px" /><figcaption class="wp-element-caption">Figure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility. </figcaption></figure>



<h2 class="wp-block-heading" id="takeaway-intent-matters-in-imitation-learning">Takeaway: Intent matters in imitation learning</h2>



<p>The main message of our investigation is simple: imitation becomes easier when intent is made explicit. Predicting a plausible future, even an imperfect one, helps resolve ambiguity about which action makes sense right now, much like driving more confidently in the fog when the driver already knows where the road is headed. PIDM shifts imitation learning from pure copying toward goal-oriented action.</p>



<p>This approach has limits. If predictions of future states become too unreliable, they can mislead the model about the intended next move. In those cases, the added uncertainty can outweigh the benefit of reduced ambiguity, causing PIDM to underperform BC.</p>



<p>But when predictions are reasonably accurate, reframing action prediction as “<em>How do I get there from here</em>?” helps explain why learning from small, messy human datasets can be surprisingly effective. In settings where data is expensive and demonstrations are limited, that shift in perspective can make a meaningful difference.</p>



<h2 class="wp-block-heading" id="appendix-visualizations-and-results-videos">Appendix: Visualizations and results (videos)</h2>



<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h3 class="wp-block-heading h4" id="a-player-a-naive-action-replay-baseline-and-a-pidm-agent-playing-bleeding-edge-1">A player, a naïve action-replay baseline, and a PIDM agent playing <em>Bleeding Edge</em></h3>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Smart Replay demo: Player and PIDM performing a complex task (video 2)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/x1WdGiX4QYk?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Video&nbsp;2. (Left)&nbsp;The player completes the task under normal conditions. (Middle)&nbsp;The baseline replays the recorded actions at their original timestamps, which initially appears to work. Because the game runs on a cloud gaming platform, however, random network delays quickly push the replay&nbsp;out of sync, causing the trajectory to fail. (Right) Under the same conditions, the PIDM agent behaves differently. Instead of naively replaying actions, it continuously interprets visual input, predicts how the behavior is likely to unfold, and adapts its actions in real time. This allows it to correct delays, recover from deviations, and successfully reproduce the task in settings where naïve replay inevitably fails.</figcaption></figure>
</div>



<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h3 class="wp-block-heading h4" id="a-player-and-a-pidm-agent-performing-a-complex-task-in-bleeding-edge">A player and a PIDM agent&nbsp;performing a complex task in&nbsp;<em>Bleeding Edge</em></h3>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Smart Replay demo: Player, replay baseline, and PIDM (video 3)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/gUbIsAcsW6w?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Video&nbsp;3.&nbsp;In this video, the task&nbsp;exhibits&nbsp;strong partial observability: correct behavior depends on whether a location is being visited for the first or second time. For example,&nbsp;in the first encounter, the agent proceeds straight up the ramp; on the second, it turns right toward the bridge. Similarly, it may jump over a box on the first pass but walk around it on the second. The PIDM agent reproduces this trajectory reliably, using coarse future guidance to select actions in the correct direction.</figcaption></figure>
</div>
</div>



<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h3 class="wp-block-heading h4" id="visualization-of-the-2d-navigation-environment">Visualization of the 2D navigation environment</h3>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Smart Replay demo: 2D navigation task visualization (video 4)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/PfU2gMXqQ8c?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Video&nbsp;4.&nbsp;These&nbsp;videos show ten demonstrations for each of four tasks: Four Room, Zigzag, Maze, and Multiroom. In all cases, the setup is the same: the character (blue box) moves through the environment and must reach a sequence of goals (red squares).&nbsp;The overlaid trajectories visualize the paths the player took; the models never see these paths. Instead, they observe only their character’s current location, the position of all goals, and whether each goal has already been reached. Because these demonstrations come from real players, no two paths are identical: players pause, take detours, or correct small mistakes along the way. That natural variability is exactly what the models must learn to handle.</figcaption></figure>
</div>



<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h3 class="wp-block-heading h4" id="pidm-vs-bc-in-a-3d-environment">PIDM vs. BC in a 3D&nbsp;environment</h3>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Smart Replay demo: PIDM vs. BC in a 3D environment (video 5)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/iXgDXSVPJxY?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Video&nbsp;5. The PIDM agent achieves an 85% success rate with only fifteen demonstrations used in training. The BC agent struggles to stay on track and levels off around 60%.&nbsp;The contrast illustrates how differently the two approaches perform when training data is limited.</figcaption></figure>
</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/">Rethinking imitation learning with Predictive Inverse Dynamics Models</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Paza: Introducing automatic speech recognition benchmarks and models for low resource languages</title>
		<link>https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/</link>
		
		<dc:creator><![CDATA[Mercy Muchai, Kevin  Chege, Nick  Mumero, Stephanie Nyairo]]></dc:creator>
		<pubDate>Thu, 05 Feb 2026 05:07:55 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1160691</guid>

					<description><![CDATA[<p>Microsoft Research unveils Paza, a human-centered speech pipeline, and PazaBench, the first leaderboard for low-resource languages. It covers 39 African languages and 52 models and is tested with communities in real settings. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/">Paza: Introducing automatic speech recognition benchmarks and models for low resource languages</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a blue‑to‑purple gradient background: a vertical audio waveform on the left, a globe showing Africa and Europe in the center, and a network on the right." class="wp-image-1160744" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Paza-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li><strong>Microsoft Research releases PazaBench and Paza automatic speech recognition models</strong>, advancing speech technology for low resource languages.</li>



<li><strong>Human-centered pipeline for low-resource languages: </strong>Built for and tested by communities, Paza is an end-to-end, continuous pipeline that elevates historically under-represented languages and makes speech models usable in real-world, low-resource contexts.</li>



<li><strong>First-of-its-kind ASR leaderboard, starting with African languages: </strong>Pazabench is the first automatic speech recognition (ASR) leaderboard for low-resource languages. Launching with 39 African languages and 51 state-of-the-art models, it tracks three key metrics across leading public and community datasets.</li>



<li><strong>Human-centered&nbsp;Paza&nbsp;ASR&nbsp;models:</strong>&nbsp;Minimal&nbsp;data, fine-tuned&nbsp;ASR models&nbsp;grounded in&nbsp;real-world&nbsp;testing&nbsp;with farmers on everyday mobile devices, covering&nbsp;six&nbsp;Kenyan languages:&nbsp;Swahili,&nbsp;Dholuo, Kalenjin, Kikuyu, Maasai,&nbsp;and Somali.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>According to the 2025&nbsp;<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Microsoft-AI-Diffusion-Report-2025-H2.pdf" target="_blank" rel="noreferrer noopener">Microsoft AI Diffusion Report</a>&nbsp;approximately one&nbsp;in&nbsp;six&nbsp;people globally had used a generative AI product.&nbsp;Yet for billions of&nbsp;people,&nbsp;the promise of voice interaction still falls short, and&nbsp;whilst&nbsp;AI is becoming increasingly multilingual, a key question&nbsp;remains:&nbsp;<em><strong>Do&nbsp;these models&nbsp;actually work&nbsp;for all languages and the people who rely on them?</strong></em>&nbsp;This challenge is one we first confronted through&nbsp;<a href="https://www.microsoft.com/en-us/research/project/project-gecko/" target="_blank" rel="noreferrer noopener">Project Gecko</a>—a collaboration between Microsoft Research and&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://digitalgreen.org/" target="_blank" rel="noopener noreferrer">Digital&nbsp;Green<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;where&nbsp;field teams across Africa and India focused on building usable AI tools for farmers.</p>



<p>Gecko revealed how often speech systems fail in real‑world, low‑resource environments—where many languages go&nbsp;unrecognized&nbsp;and non‑Western accents are frequently misunderstood. Yet speech remains the primary medium of communication globally. For communities across Kenya, Africa, and beyond, this mismatch creates cascading challenges: without foundational data&nbsp;representing&nbsp;their languages and cultures, innovation stalls, and the digital and AI divides widen.&nbsp;</p>



<p>Paza addresses this with a human-centered speech models pipeline. Through&nbsp;PazaBench, it benchmarks low-resource languages using both public and community-sourced data, and through Paza&nbsp;models, it&nbsp;fine-tunes&nbsp;speech models&nbsp;to deliver outsized gains in mid- and low-resource languages, evaluating with community testers using real devices in real contexts. Upcoming playbooks complement this work by sharing practical guidance on&nbsp;dataset creation,&nbsp;fine-tuning&nbsp;approaches&nbsp;with minimal data&nbsp;and evaluation considerations, introducing a continuous pipeline that&nbsp;enables&nbsp;researchers&nbsp;and practitioners to build and evaluate systems grounded in real human use.</p>



<h2 class="wp-block-heading" id="how-project-gecko-informed-paza-s-design">How Project Gecko informed Paza’s design</h2>



<p>In addition to building cost-effective, adaptable AI systems, the extensive&nbsp;fieldwork on&nbsp;Project Gecko highlighted an important lesson:&nbsp;<strong><em>Building usable speech&nbsp;models&nbsp;in low‑resource settings is not only a data problem,&nbsp;but also&nbsp;a design and evaluation problem.</em></strong>&nbsp;For AI systems to be useful, they must work in local languages, support hands‑free interaction through voice, text, and video, and deliver information in formats that fit real-world environments, that is, on low-bandwidth&nbsp;mobile devices,&nbsp;in&nbsp;noisy settings, and&nbsp;for&nbsp;varying literacy levels.&nbsp;&nbsp;</p>



<p>These insights shaped the design of Paza, from&nbsp;the&nbsp;Swahili&nbsp;phrase&nbsp;<em><strong>paza&nbsp;sauti</strong></em>&nbsp;meaning “to project,” or “to raise your voice.” &nbsp;The name reflects our intent: rather than simply adding more languages to existing systems,<strong>&nbsp;Paza is about co-creating speech technologies in partnership with the communities who use them.</strong>&nbsp;Guided by this principle, Paza puts human use&nbsp;first,&nbsp;which&nbsp;enables&nbsp;model improvement.&nbsp;</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144027">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-label="AI Testing and Evaluation: Learnings from Science and Industry" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP2-AI-TE_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshots of Daniel Carpenter, Timo Minssen, Chad Atalla, and Kathleen Sullivan for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Testing and Evaluation: Learnings from Science and Industry</h2>
				
								<p id="ai-testing-and-evaluation-learnings-from-science-and-industry" class="large">Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-describedby="ai-testing-and-evaluation-learnings-from-science-and-industry" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="pazabench-the-first-asr-leaderboard-for-low-resource-languages">PazaBench: The first ASR leaderboard for low-resource languages</h2>



<p><strong>PazaBench</strong> is the first automatic speech recognition (ASR) leaderboard dedicated to low‑resource languages. It launches&nbsp;with&nbsp;initial&nbsp;coverage&nbsp;for&nbsp;39 African languages and benchmarks&nbsp;52 state‑of‑the‑art ASR and language models, including newly released Paza ASR models for six Kenyan languages. The platform aggregates leading public and community datasets from diverse styles of speech including conversational, scripted read aloud, unscripted, broadcast news, and domain-specific data—into one easy‑to‑explore platform per language.&nbsp;This makes it easier for&nbsp;researchers, developers, and product teams to easily assess which models perform best across underserved languages and diverse regions, understand trade-offs between speed and accuracy&nbsp;while&nbsp;identifying&nbsp;where gaps persist.&nbsp;</p>



<p><strong>PazaBench tracks three core metrics:</strong></p>



<ol class="wp-block-list">
<li><strong>Character Error Rate (CER)</strong> which is important for languages with rich word forms, where meaning is built by combining word parts, therefore errors at the character level can significantly impact meaning</li>



<li><strong>Word Error Rate (WER)</strong> for word-level transcript accuracy</li>



<li><strong>RTFx (Inverse Real‑Time Factor)</strong> which measures how fast transcription runs relative to real‑time audio duration<em>.</em></li>
</ol>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="PazaBench Walkthrough" width="500" height="281" src="https://www.youtube-nocookie.com/embed/jAuuh0saMUI?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<blockquote class="wp-block-quote is-layout-flow wp-block-quote-is-layout-flow">
<p><em><strong><em>More than scores,&nbsp;PazaBench&nbsp;standardizes evaluation to prioritize dataset gaps,&nbsp;identify&nbsp;underperforming languages, and highlight where localized models beat&nbsp;wider coverage ASR models—offering early evidence of&nbsp;the value of African‑centric innovation.</em></strong></em></p>
</blockquote>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--2"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://huggingface.co/spaces/microsoft/paza-bench" target="_blank" rel="noreferrer noopener">Explore PazaBench</a></div>
</div>



<p class="has-text-align-center"><em><sup>To contribute to the benchmark, request additional language evaluation on the leaderboard.</sup></em></p>



<h2 class="wp-block-heading" id="paza-asr-models-built-with-and-for-kenyan-languages">Paza ASR Models: Built with and for Kenyan languages</h2>



<p>The Paza ASR models consist of three fine-tuned ASR models built on top of state‑of‑the‑art model architectures. Each model targets <em>Swahili, </em>a mid-resource language and five low‑resource Kenyan languages; <em>Dholuo, Kalenjin, Kikuyu, Maasai and Somali</em>. The models are fine-tuned on public and curated proprietary datasets.  </p>



<p>Fine‑tuning the three models allowed us to explore supportive approaches toward a shared goal: building speech recognition systems that are usable for local contexts starting with the six Kenyan languages and bridging the gaps of multi-lingual and multi-modal video question and answering through the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://labs.ai.azure.com/projects/mmct-agent/" target="_blank" rel="noopener noreferrer">MMCT agent.<span class="sr-only"> (opens in new tab)</span></a></p>



<figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Project Gecko: Building globally equitable generative AI" width="500" height="281" src="https://www.youtube-nocookie.com/embed/59O8kP8pmtI?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">See the MMCT agent in action in the field</figcaption></figure>



<p>Early versions of two models in Kikuyu and Swahili were deployed on mobile devices and tested directly with farmers in real‑world settings, enabling the team to observe how the models performed with everyday use. Farmers provided in‑the‑moment feedback on accuracy, usability, and relevance, highlighting where transcripts broke down, which errors were most disruptive, and what improvements would make the models more helpful in practice. This feedback loop directly informed subsequent fine‑tuning, ensuring model improvements were driven not only by benchmark scores, but by the needs and expectations of the communities they are intended to serve.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--3"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://huggingface.co/collections/microsoft/paza" target="_blank" rel="noreferrer noopener">Explore Paza Collection Here</a></div>
</div>



<p>Here is how Paza models compare to&nbsp;three&nbsp;state-of-the-art&nbsp;ASR&nbsp;models&nbsp;today:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-scaled.png" alt="Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance." class="wp-image-1161323" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-300x151.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-1024x515.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-768x386.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-1536x772.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-2048x1029.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG1_overall_cer_grouped_sorted_NEW-240x121.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance.</em></figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1287" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-scaled.png" alt="Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance." class="wp-image-1161325" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-300x151.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-1024x515.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-768x386.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-1536x772.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-2048x1029.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG2_overall_wer_grouped_sorted_NEW-240x121.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance.</em></figcaption></figure>



<p><strong>1) Paza‑Phi‑4‑Multimodal‑Instruct</strong></p>



<p>Microsoft’s <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/microsoft/Phi-4-multimodal-instruct" target="_blank" rel="noopener noreferrer">Phi‑4 multimodal‑instruct<span class="sr-only"> (opens in new tab)</span></a> is a next‑generation small language model built to reason across audio, text, and vision. With Paza, we extend its audio capabilities, adapting a powerful multimodal architecture into a high‑quality automatic speech recognition (ASR) system for low‑resource African languages.</p>



<p>Fine‑tuned on unified multilingual speech datasets, the model was optimized specifically for transcription in the six languages. The model preserves its underlying transformer architecture and multi-modal capabilities, while selectively fine-tuning only the audio‑specific components, enabling strong cross‑lingual generalization.</p>



<p>As the results below show, this model delivers consistent improvements in transcription quality across all six languages.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-scaled.png" alt="Figure 3: Character Error Rate (CER) comparison across the six languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance." class="wp-image-1161376" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-300x125.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-1024x425.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-768x319.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-1536x638.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-2048x851.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG3_phi_cer_comparison_NEW-240x100.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 3: <em>Character Error Rate (CER) comparison across&nbsp;the six</em><strong>&nbsp;</strong><em>languages for the base&nbsp;model&nbsp;versus the finetuned Paza model.&nbsp;Lower CER&nbsp;indicates&nbsp;better transcription performance.</em></em></figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1063" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-scaled.png" alt="Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance." class="wp-image-1161378" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-300x125.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-1024x425.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-768x319.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-1536x638.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-2048x851.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG4_phi_wer_comparison_NEW-240x100.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.</em></figcaption></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--4"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://huggingface.co/microsoft/paza-Phi-4-multimodal-instruct" target="_blank" rel="noreferrer noopener">Test the model here</a></div>
</div>



<p><strong>2) Paza‑MMS‑1B‑All</strong></p>



<p>This model is fine-tuned on Meta’s mms-1b-all model, which employs a large-scale Wav2Vec2.0-style encoder with lightweight language-specific adapters to enable efficient multilingual specialization. For this release, each of the six language adapters was fine‑tuned independently on curated low‑resource datasets, allowing targeted adaptation while keeping the shared encoder largely frozen.</p>



<p>As shown in the figures below, this model improves transcription accuracy while maintaining the model’s strong cross‑lingual generalization.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-scaled.png" alt="Figure 5: Character Error Rate (CER) comparison across the six languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance." class="wp-image-1161380" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-300x136.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-1024x464.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-768x348.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-1536x696.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-2048x928.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG5_mms_cer_comparison_NEW-240x109.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 5: <em>Character Error Rate (CER)&nbsp;comparison across the six</em><strong>&nbsp;</strong><em>languages for the base model&nbsp;versus&nbsp;the finetuned Paza model.&nbsp;Lower CER&nbsp;indicates&nbsp;better transcription performance.</em></em></figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1160" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-scaled.png" alt="Figure 6: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance." class="wp-image-1161382" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-300x136.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-1024x464.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-768x348.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-1536x696.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-2048x928.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG6_mms_wer_comparison_NEW-240x109.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 6: <em>Word Error Rate (WER)&nbsp;comparison across the six</em><strong>&nbsp;</strong><em>languages for the base model&nbsp;versus the finetuned Paza model.&nbsp;Lower WER indicates better transcription performance.</em></em></figcaption></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--5"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="http://Aka.ms/pazareap" target="_blank" rel="noreferrer noopener">Join the Research Early Access Program</a></div>
</div>



<p><strong>3) Paza‑Whisper‑Large‑v3‑Turbo</strong></p>



<p>This model is finetuned on OpenAI’s whisper-large-v3-turbo&nbsp;base model. Whisper is a transformer-based encoder–decoder model which&nbsp;delivers robust automatic speech recognition (ASR)&nbsp;capabilities. This model was fine‑tuned on the entire unified multilingual ASR dataset,&nbsp;on&nbsp;the mentioned six languages, to encourage cross-lingual generalization.&nbsp;In addition, an extra post‑processing step was applied to address the known Whisper hallucination failure modes, improving transcription reliability.</p>



<p>As shown below, this release achieves improved transcription accuracy while retaining Whisper’s robustness.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-scaled.png" alt="Figure 7: Character Error Rate (CER) comparison across the six languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance." class="wp-image-1161338" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-300x127.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-1024x432.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-768x324.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-1536x648.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-2048x864.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG7whisper_cer_comparison_NEW-240x101.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 7: <em>Character Error Rate (CER) comparison across the six</em><strong>&nbsp;</strong><em>languages for the base model versus the finetuned Paza model.&nbsp;Lower CER&nbsp;indicates&nbsp;better transcription performance.</em></em></figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1081" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-scaled.png" alt="Figure 8: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance." class="wp-image-1161341" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-300x127.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-1024x432.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-768x324.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-1536x648.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-2048x864.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/02/FIG8_whisper_wer_comparison_NEW-240x101.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption"><em>Figure 8: <em>Word Error Rate (WER) comparison across the six</em><strong>&nbsp;</strong><em>languages for the base model versus the finetuned Paza model.&nbsp;Lower WER indicates better transcription performance.</em></em></figcaption></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-16018d1d wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--6"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://huggingface.co/microsoft/paza-whisper-large-v3-turbo" target="_blank" rel="noreferrer noopener">Test the model here</a></div>
</div>



<h2 class="wp-block-heading" id="where-do-we-go-from-here">Where do we go from here</h2>



<p>AI is reshaping how the world communicates. Designing with people, not just for them, means looking beyond the languages that are already well‑served. We plan to expand PazaBench beyond African languages and evaluate state‑of‑the‑art ASR models across&nbsp;more low‑resource languages globally. The Paza ASR models are an early step; truly supporting small and under‑represented languages requires dedicated datasets, strong local partnerships, and rigorous evaluation. Meaningful progress depends on sustained collaboration with the communities who speak these languages, and expanding responsibly means prioritizing depth and quality over broad but shallow coverage.&nbsp;</p>



<p>As we continue this work,&nbsp;we&#8217;re&nbsp;distilling our methods into a forthcoming playbook to help the broader ecosystem curate datasets, fine‑tune responsibly, and evaluate models in real‑world conditions. And we’re not stopping at speech—additional&nbsp;playbooks will guide&nbsp;teams&nbsp;building AI tools and applications for multilingual, multicultural contexts, and give them practical recommendations for deploying across diverse communities.&nbsp;</p>



<p>Together, these guides—grounded in technical advances and community‑driven design—share our learnings to help researchers, engineers, and designers build more human‑centered AI systems.&nbsp;</p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>The following researchers played an integral role in this work: Najeeb Abdulhamid, Felermino Ali, Liz Ankrah, Kevin Chege, Ogbemi Ekwejunor-Etchie, Ignatius Ezeani, Tanuja Ganu, Antonis Krasakis, Mercy Kwambai, Samuel Maina, Muchai Mercy, Danlami Mohammed, Nick Mumero, Martin Mwiti, Stephanie Nyairo, Millicent Ochieng and Jacki O’Neill.</p>



<p>We would like to thank the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://digitalgreen.org/" target="_blank" rel="noopener noreferrer">Digital Green<span class="sr-only"> (opens in new tab)</span></a> team—Rikin Gandhi, Alex Mwaura, Jacqueline Wang’ombe, Kevin Mugambi, Lorraine Nyambura, Juan Pablo, Nereah Okanga, Ramaskanda R.S, Vineet Singh, Nafhtari Wanjiku, Kista Ogot, Samuel Owinya&nbsp;and the community evaluators in Nyeri and Nandi, Kenya — for their valuable contributions to this work.</p>



<p>We extend our gratitude to the creators, community contributors, and maintainers of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/MCAA1-MSU/anv_data_ke" target="_blank" rel="noopener noreferrer">African Next Voices Kenya<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/dsfsi-anv/za-african-next-voices" target="_blank" rel="noopener noreferrer">African Next Voices South Africa<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://openslr.org/25/" target="_blank" rel="noopener noreferrer">ALFFA<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/DigiGreen/KikuyuASR_trainingdataset" target="_blank" rel="noopener noreferrer">Digigreen<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/google/fleurs" target="_blank" rel="noopener noreferrer">Google FLEURS<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://commonvoice.mozilla.org/" target="_blank" rel="noopener noreferrer">Mozilla Common Voice<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/datasets/naijavoices/naijavoices-dataset" target="_blank" rel="noopener noreferrer">Naija Voices<span class="sr-only"> (opens in new tab)</span></a> whose efforts have been invaluable in advancing African languages speech data.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/">Paza: Introducing automatic speech recognition benchmarks and models for low resource languages</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</title>
		<link>https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/</link>
		
		<dc:creator><![CDATA[Sheng Zhang, Flora Liu, Guanghui Qin, Mu Wei, Hoifung Poon]]></dc:creator>
		<pubDate>Tue, 27 Jan 2026 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1160723</guid>

					<description><![CDATA[<p>AI can help generate medical image reports, but today’s models struggle with varying reporting schemes. Learn how UniRG uses reinforcement learning to boost performance of medical vision-language models.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/">UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a blue‑green gradient: a ribcage scan, a circuit‑style document, and a neural network diagram" class="wp-image-1160804" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>AI-driven medical image report generation can help medical providers become more efficient and productive.</li>



<li>Current models are difficult to train because reporting practices vary widely among providers.</li>



<li>Universal Report Generation (UniRG) uses reinforcement learning to align model training with real-world radiology practice rather than proxy text-generation objectives.</li>



<li>UniRG&nbsp;has&nbsp;achieved&nbsp;state-of-the-art&nbsp;performance across datasets, metrics, diagnostic tasks, longitudinal settings, and demographic subgroups.</li>



<li>Test results show that reinforcement learning, guided by clinically meaningful reward signals, can substantially improve the reliability and generality of medical vision–language models.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>AI can be used to produce clinically meaningful radiology reports using medical images like chest x-rays. Medical image report generation can reduce reporting burden while improving workflow efficiency for healthcare professionals. Beyond the real-world benefits, report generation has also become a critical benchmark for evaluating multimodal reasoning in healthcare AI.</p>



<p>Despite recent advances driven by large vision–language models, current systems still face major limitations in real-world clinical settings. One challenge stems from the wide variation in radiology reporting practices across institutions, departments, and patient populations. A model trained with supervised fine-tuning on one set of data may learn its specific phrasing and conventions instead of more general patterns—a problem known as <em>overfitting</em>. As a result, the model performs well on that data but delivers poor results when evaluated on unseen institutions or external datasets. Moreover, since model training is often aimed at producing text that looks similar to existing reports, some well written but clinically inaccurate reports can slip through.</p>



<p>In this blog, we introduce <strong><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="http://aka.ms/unirg-paper">Universal Report Generation (UniRG)<span class="sr-only"> (opens in new tab)</span></a></strong>, a reinforcement learning–based framework for medical imaging report generation. This work is a research prototype intended to advance medical AI research and is not validated for clinical use. UniRG uses reinforcement learning as a unifying mechanism to directly optimize clinically grounded evaluation signals, aligning model training with real-world radiology practice rather than proxy text-generation objectives. Using this framework, we train <strong><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="http://aka.ms/unirg-paper">UniRG-CXR<span class="sr-only"> (opens in new tab)</span></a></strong>, a state-of-the-art chest x-ray report generation model at scale, spanning over 560,000 studies, 780,000 images, and 226,000 patients from more than 80 medical institutions.</p>



<p>To our knowledge, this is the first report generation model to achieve consistent state-of-the-art performance across report-level metrics, disease-level diagnostic accuracy, cross-institution generalization, longitudinal report generation, and demographic subgroups. These results demonstrate that reinforcement learning, when guided by clinically meaningful reward signals, can substantially improve both the reliability and generality of medical vision–language models.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="a-unified-framework-for-scaling-medical-image-report-generation">A unified framework for scaling medical image report generation</h2>



<p>UniRG&nbsp;builds&nbsp;state-of-the-art&nbsp;report generation models by combining supervised fine-tuning with reinforcement learning, which&nbsp;optimizes&nbsp;a composite reward that integrates rule-based metrics, model-based semantic metrics, and LLM-based clinical error signals. This approach allows the resulting model&nbsp;UniRG-CXR to learn from diverse data sources, move beyond dataset-specific reporting patterns, and learn representations that generalize across institutions, metrics, and clinical contexts. Notably,&nbsp;UniRG-CXR sets a new state of&nbsp;the art on the authoritative&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://rexrank.ai/" target="_blank" rel="noopener noreferrer"><strong>ReXrank leaderboard</strong><span class="sr-only"> (opens in new tab)</span></a>,&nbsp;a&nbsp;public leaderboard for chest X-ray image interpretation,&nbsp;as of&nbsp;01/22/2026, surpassing&nbsp;previous&nbsp;best models&nbsp;by&nbsp;substantial&nbsp;margins (Figure 1).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2141" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-scaled.png" alt="Fig 1: Overview diagram of the UniRG-CXR framework showing training data sources, reinforcement-learning–based training with composite rewards, evaluation on multiple datasets, and a results panel demonstrating state-of-the-art performance across benchmarks." class="wp-image-1160837" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-scaled.png 2141w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-251x300.png 251w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-857x1024.png 857w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-768x918.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-1285x1536.png 1285w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-1713x2048.png 1713w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig1_UPDATED-151x180.png 151w" sizes="auto, (max-width: 2141px) 100vw, 2141px" /><figcaption class="wp-element-caption">Figure 1. Overview of UniRG-CXR. (a) Training Data: UniRG-CXR is trained on the training splits of MIMIC-CXR, CheXpert Plus, and ReXGradient-160k, covering diverse institutions and patient demographics. (b) Training and Rewards: Taking input from the current image, clinical context (e.g., indication), and optionally prior studies, UniRG-CXR uses GRPO reinforcement learning to optimize composite rewards that combine rule-based, model-based, and LLM-based metrics. (c) Evaluation: We assess UniRG-CXR on held-out test sets (MIMIC-CXR, CheXpert Plus, ReXGradient), and unseen datasets (IU Xray and proprietary data). Report quality measured using ReXrank metrics and an LLM-based clinical-error metric, while diagnostic ability is evaluated via F1-based disease classification from generated reports. (d) ReXrank Results: UniRG-CXR achieves SOTA performance across four datasets and two generation settings (findings only and findings + impression), showing substantial gains over prior state-of-the-art systems.</figcaption></figure>



<h2 class="wp-block-heading" id="universal-improvements-across-metrics-and-clinical-errors">Universal improvements across metrics and clinical errors</h2>



<p>Rather than excelling on one metric at the expense of others, UniRG-CXR delivers balanced improvements across many different measures of report quality. More importantly, it produces reports with substantially fewer clinically significant errors. This indicates that the model is not just learning how to sound like a radiology report, but is better capturing the underlying clinical facts. Explicitly optimizing for clinical correctness helps the model avoid common failure modes where fluent language masks incorrect or missing findings (Figure 2).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="871" height="835" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig2.png" alt="Fig 2: Multi-panel figure showing UniRG-CXR’s state-of-the-art performance: leaderboard gains across metrics, ablation studies demonstrating benefits of combined reinforcement-learning rewards, improved training dynamics with fewer clinical errors, qualitative case studies with error-free reports, and a distribution showing fewer high-error reports compared to prior models." class="wp-image-1160707" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig2.png 871w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig2-300x288.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig2-768x736.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig2-188x180.png 188w" sizes="auto, (max-width: 871px) 100vw, 871px" /><figcaption class="wp-element-caption">Figure 2. UniRG-CXR achieves state-of-the-art performance, delivering consistent and comprehensive performance gains across metrics. (a) On the ReXrank leaderboard, UniRG-CXR (green) shows robust, universal improvement across all evaluation metrics.  (b). Starting from the same SFT checkpoint, RL with our combined reward achieves more balanced gains across metrics and the highest RadCliQ-v1 score compared to RL on single metrics. This ablation study is trained and tested on MIMIC (c). Ablation study on the training dynamics shows RL full (UniRG-CXR) achieves significantly better RadCliQ-v1 score than RL only on BLEU. (d). During training, RL full (UniRG-CXR) shows a steady decrease in clinical errors per report as compared with a fluctuating trajectory without consistent improvement from an ablation run without error awareness (i.e. removing CheXprompt metric optimization). Both (c) and (d) show results on 1024 MIMIC validation set from ablations that are trained on MIMIC. (e). Case studies illustrate that UniRG-CXR can produce error-free reports, unlike MedVersa and MedGemma. (f). UniRG-CXR yields a substantially higher proportion of reports with $\leq 1$ error and fewer with $\geq 4$ errors than prior models.</figcaption></figure>



<h2 class="wp-block-heading" id="strong-performance-in-longitudinal-report-generation">Strong performance in longitudinal report generation</h2>



<p>In clinical practice, radiologists often compare current images with prior exams to determine whether a condition is improving, worsening, or unchanged. UniRG-CXR is able to incorporate this historical information effectively, generating reports that reflect meaningful changes over time. This allows the model to describe new findings, progression, or resolution of disease more accurately, moving closer to how radiologists reason across patient histories rather than treating each exam in isolation (Figure 3).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2548" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-scaled.png" alt="Fig 3: Multi-panel results demonstrating UniRG-CXR’s advantages in longitudinal chest X-ray report generation, including superior performance over prior models and a non-longitudinal ablation across encounters, consistent gains at increasing follow-up complexity, improved handling of temporal disease changes, and qualitative examples of accurate longitudinal predictions." class="wp-image-1160838" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-scaled.png 2548w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-1019x1024.png 1019w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-768x772.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-1529x1536.png 1529w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-2039x2048.png 2039w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/fig3_UPDATED-179x180.png 179w" sizes="auto, (max-width: 2548px) 100vw, 2548px" /><figcaption class="wp-element-caption">Figure 3. UniRG-CXR enhances longitudinal report generation. (a). Comparing UniRG-CXR and its non-longitudinal ablation with prior models on longitudinal report generation, we show UniRG-CXR exhibits the best performance and the longitudinal information is beneficial to the performance. (b). UniRG-CXR achieves the best performance across different longitudinal encounter points ranging from the first encounter to the more complex 5th+ encounters, showcasing its improvements are across the board. In comparison, prior models such as GPT-5, GPT-4o and MedGemma are barely surpassing the copy prior report baseline (grey lines).  (c). Compared with prior models which barely improve over the copy prior baseline (dashed line), UniRG-CXR significantly and consistently improves performance across different temporal disease change categories including new development, no change, progression and regression (categorized by GPT-5 on ground truth report). Qualitative examples are shown for each category where UniRG-CXR correctly predicts the temporal change based on the input. All results in this figure are on MIMIC test set with prior information where available.</figcaption></figure>



<h2 class="wp-block-heading" id="robust-generalization-across-institutions-and-populations">Robust generalization across institutions and populations</h2>



<p>UniRG-CXR maintains strong performance even when applied to data from institutions it has never seen before. This suggests that the model is learning general clinical patterns rather than memorizing institution-specific reporting styles. In addition, its performance remains stable across different patient subgroups, including age, gender, and race. This robustness is critical for real-world deployment, where models must perform reliably across diverse populations and healthcare environments (Figure 4).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="871" height="761" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig4.png" alt="Fig 4: Multi-panel figure showing UniRG-CXR’s generalization and robustness: zero-shot evaluation with strong performance on unseen datasets, superior condition-level diagnostic F1 scores, and consistent accuracy across gender, age, and race subgroups compared with prior models." class="wp-image-1160706" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig4.png 871w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig4-300x262.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig4-768x671.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/UniRG_fig4-206x180.png 206w" sizes="auto, (max-width: 871px) 100vw, 871px" /><figcaption class="wp-element-caption">Figure 4. Generalization and robustness of UniRG-CXR. (a). We evaluate UniRG-CXR in a zero-shot setting on two datasets from previously unseen institutions: IU-Xray and PD (proprietary data). UniRG-CXR consistently outperforms prior models, maintaining substantial performance gains in this challenging setup. (b) and (c) present condition-level F1 scores on MIMIC-CXR and PD and highlight that UniRG-CXR remains the overall top-performing model in condition-level diagnostic accuracy. (d). UniRG-CXR demonstrates stable and robust performance across gender, age, and race subgroups, all of which exceed the performance of the second-best model (the dashed lines).</figcaption></figure>



<h2 class="wp-block-heading" id="unirg-is-a-promising-step-toward-scaling-medical-imaging-report-generation">UniRG is a promising step toward scaling medical imaging report generation</h2>



<p>UniRG introduces a reinforcement learning–based framework that rethinks how medical imaging report generation models are trained and evaluated. By directly optimizing clinically grounded reward signals, UniRG-CXR achieves state-of-the-art performance across datasets, metrics, diagnostic tasks, longitudinal settings, and demographic subgroups, addressing longstanding limitations of supervised-only approaches.</p>



<p>Looking ahead, this framework can be extended to additional imaging modalities and clinical tasks, and combined with richer multimodal patient data such as prior imaging, laboratory results, and clinical notes. More broadly, UniRG highlights the promise of reinforcement learning as a core component of next-generation medical foundation models that are robust, generalizable, and clinically aligned.</p>



<p>UniRG reflects Microsoft’s larger commitment to <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AI-S2300233" target="_blank" rel="noopener noreferrer">advancing multimodal generative AI for precision health<span class="sr-only"> (opens in new tab)</span></a>, with other exciting progress such as <a href="https://www.microsoft.com/en-us/research/blog/gigapath-whole-slide-foundation-model-for-digital-pathology/">GigaPath</a>, <a href="https://www.microsoft.com/en-us/research/publication/biomedclip-a-multimodal-biomedical-foundation-model-pretrained-from-fifteen-million-scientific-image-text-pairs/">BiomedCLIP</a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41467-025-58344-x" target="_blank" rel="noopener noreferrer">LLaVA-Rad<span class="sr-only"> (opens in new tab)</span></a>, <a href="https://www.microsoft.com/en-us/research/publication/biomedjourney-counterfactual-biomedical-image-generation-by-instruction-learning-from-multimodal-patient-journeys/">BiomedJourney</a>, <a href="https://www.microsoft.com/en-us/research/blog/biomedparse-a-foundation-model-for-smarter-all-in-one-biomedical-image-analysis/">BiomedParse</a>, <a href="https://www.microsoft.com/en-us/research/publication/trialscope-a-unifying-causal-framework-for-scaling-real-world-evidence-generation-with-biomedical-language-models/">TrialScope</a>, <a href="https://www.microsoft.com/en-us/research/publication/generative-medical-event-models-improve-with-scale/">Curiosity</a>.</p>



<p>Paper co-authors: <a href="https://www.microsoft.com/en-us/research/people/qianchuliu/">Qianchu Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/shezhan/">Sheng Zhang</a>, <a href="https://www.microsoft.com/en-us/research/people/guanghuiqin/">Guanghui Qin</a>, <a href="https://www.microsoft.com/en-us/research/people/yugu1/">Yu Gu</a>, Ying Jin, Sam Preston, Yanbo Xu, Sid Kiblawi, <a href="https://www.microsoft.com/en-us/research/people/yimwenwai/">Wen-wai Yim</a>, Tim Ossowski, <a href="https://www.microsoft.com/en-us/research/people/tristan/">Tristan Naumann</a>, Mu Wei, <a href="https://www.microsoft.com/en-us/research/people/hoifung/">Hoifung Poon</a></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/">UniRG: Scaling medical imaging report generation with multimodal reinforcement learning</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Multimodal reinforcement learning with agentic verifier for AI agents</title>
		<link>https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/</link>
		
		<dc:creator><![CDATA[Reuben Tan, Baolin Peng, Zhengyuan Yang, Oier Mees, Jianfeng Gao]]></dc:creator>
		<pubDate>Tue, 20 Jan 2026 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1160129</guid>

					<description><![CDATA[<p>Argos improves multimodal RL by evaluating whether an agent’s reasoning aligns with what it observes over time. The approach reduces visual hallucinations and produces more reliable, data-efficient agents for real-world applications.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/">Multimodal reinforcement learning with agentic verifier for AI agents</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1.jpg" alt="Diagram showing visual, audio, and document icons feeding into a central network icon of connected people, which then leads to a checkmark symbol, all on a blue‑to‑purple gradient background." class="wp-image-1160195" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/Argos-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns mb-10 pb-1 pr-1 is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex" style="box-shadow:var(--wp--preset--shadow--outlined)">
<div class="wp-block-column is-layout-flow wp-block-column-is-layout-flow">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>Today&#8217;s multimodal AI systems&nbsp;can&nbsp;give&nbsp;answers that sound right but&nbsp;may not be&nbsp;grounded in what they&nbsp;actually&nbsp;observe&nbsp;over time, leading to unpredictable errors and safety risks in real-world settings.</li>



<li>Argos is a verification framework for multimodal reinforcement learning that trains models by rewarding not just correct answers, but correct answers grounded in visual and temporal evidence, using automated verification rather than human labeling.&nbsp;It selects the appropriate specialized tools for each answer&nbsp;based on what needs to be verified.&nbsp;</li>



<li>Models trained with Argos show stronger spatial reasoning, far fewer visual hallucinations, more stable learning dynamics, and better performance on robotics and real-world tasks while requiring fewer training samples.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>Over the past few years, AI systems have become much better at discerning images, generating language, and performing tasks within physical and virtual environments. Yet they still fail in ways that are hard to predict and even harder to fix. A robot might try to grasp a tool when the object is visibly blocked, or a visual assistant integrated into smart glasses might describe objects that aren’t actually present.</p>



<p>These errors often arise because today’s multimodal agents are trained to generate outputs that are plausible rather than grounded in the actual information they receive from their environment. As a result, a model’s output can seem correct while relying on incorrect information. As AI systems are increasingly used to navigate 3D spaces and make decisions in real-world settings, this gap can be a safety and reliability concern.</p>



<p>To tackle this challenge, we posed the question: How can we train AI agents to generate correct answers and take appropriate actions for the right reasons so that their behavior is reliable even as the environment or tasks change?</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/">Argos</a> represents a novel answer to this challenge. It’s an agentic verification framework designed to improve the reliability of reinforcement learning in multimodal models.&nbsp;Reinforcement learning is a training method where AI models learn by receiving rewards for desired behaviors and penalties for undesired ones, gradually improving their performance through trial and error.</p>



<p>Rather than rewarding only correct behaviors, Argos evaluates <em>how</em> those behaviors were produced. It draws on a pool of larger, more capable teacher models and rule-based checks to verify two things: first, that the objects and events a model references actually exist in its input, and second, that the model’s reasoning aligns with what it observes. Argos rewards the model when both conditions are met. In practice, these rewards help curate high-quality training data and guide the model’s further training.</p>



<h2 class="wp-block-heading" id="how-argos-works">How Argos works<strong></strong></h2>



<p>Argos functions as a verification layer on top of an existing multimodal model. Given an image or video, a task or query, and information about the model’s reasoning and output, Argos identifies where the model indicates objects are located in the image, when it indicates events occur in a video, and what action or answer it produces.</p>



<p>Argos then applies specialized tools tailored to the specific content to evaluate and score three aspects of the model’s output. It checks whether the answer is correct, whether referenced objects and events appear at the indicated locations and times, and whether the reasoning is consistent with the visual evidence and the answer (Figure 1).</p>



<p>These scores are combined using a gated aggregation function, a method that dynamically adjusts the importance of different scores. It emphasizes reasoning checks only when the final output is correct. This design prevents unreliable feedback from dominating training and produces a stable reward signal for&nbsp;reinforcement learning.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1255" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-scaled.jpg" alt="Figure 1 shows an overview of Argos, an agentic verifier for multimodal reinforcement learning and its downstream applications. The left half of the figure illustrates Argos verifying model responses to visual questions. The left example counts dogs in an image, with red dots marking the referenced dogs and a visual grounding score; another example shows a bathroom scene where the agent reasons whether it can open the door, with an accuracy score. Below these, a blue bar titled “Argos verifier” feeds into icons representing multiple tools, including Grounding DINO, SAM-2, a pointing-hand evaluator, string matching, and a language model score, where their outputs combine into grounding and accuracy scores. The right half of the figure depicts three categories of downstream tasks powered by this supervision: robotic manipulation (a robot arm interacting with objects on a table), high-level task planning and completion (placing toilet paper on the back of a toilet and putting a bowl on a coffee table), and spatial reasoning (answering a viewpoint-based navigation question using room images). The overall message is that dense, grounded verification enables stronger agent performance on complex, real-world tasks." class="wp-image-1160145" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-300x147.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-1024x502.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-768x376.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-1536x753.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-2048x1004.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos_agentic_verifier-240x118.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1. Argos selects different specialized tools to verify and score the accuracy of referenced points and events in the agent’s reasoning.</figcaption></figure>



<h2 class="wp-block-heading" id="using-argos-to-curate-data-for-supervised-fine-tuning">Using Argos to curate data for supervised fine-tuning</h2>



<p>Argos also helps curate high-quality training data to provide the model with a strong foundation in grounded reasoning. Before the reinforcement learning stage begins, Argos uses a multi-stage process to generate data that is explicitly tied to visual locations and time intervals.</p>



<p>In the first stage, Argos identifies the objects, actions, and events that are relevant to a task and links them to specific locations in images or specific moments in videos. These references are overlaid on images and selected video frames. Next, a reasoning model generates step-by-step explanations that refer to these visual locations and time spans.</p>



<p>Finally, Argos evaluates each generated example for accuracy and visual grounding, filtering out low-quality training data and retaining only data that is both correct and well-grounded in visual input. The resulting dataset is then used in an initial training phase, where the model learns to generate reasoning steps before producing its final output. This process is illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="800" height="450" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/data-curation-animation-gif.gif" alt="Figure 2 illustrates the Argos scoring pipeline for both images and videos. On the left, two examples show an image of a living room and a short video clip, each paired with a question and a free-form model response (e.g., estimating the distance between two lamps, or describing why a person failed to pour oil). In the middle, an “Agentic Verifier” column parses each response into structured elements: spatial 2D points indicating the referenced object and pixel coordinates, temporal segments for the relevant video frames, a reasoning-quality panel that combines the image/video, question, and response, and a final-answer panel comparing the predicted answer to ground truth. Below, a row of teacher models and scoring functions, such as Grounding DINO, SAM-2, a pointing-hand metric, string matching, relative accuracy, and a language model score, take these extracted elements as input to produce separate scores. On the right, arrows labeled “Action” and “Score” show how the verifier adaptively selects which teachers to call and then aggregates their outputs via a gated aggregation function into a single reward signal for training. " class="wp-image-1160147"/><figcaption class="wp-element-caption">Figure 2. Argos generates step-by-step reasoning grounded in image locations and video timestamps then filters out low-quality training data.</figcaption></figure>



<h2 class="wp-block-heading" id="evaluation">Evaluation</h2>



<p>Building on this foundation in grounded reasoning, we further trained the model using reinforcement learning guided by Argos and evaluated its performance across a range of benchmarks. On spatial reasoning tasks, the Argos-trained model outperformed both the base model Qwen2.5-VL-7B and the stronger Video-R1 baseline across challenging 3D scenarios and multi-view tasks. Models trained with Argos also showed a substantial reduction of hallucinations compared with both standard chain-of-thought prompting and reinforcement learning baselines.</p>



<p>Finally, we evaluated the model in robotics and other real-world task settings, focusing on high-level planning and fine-grained control. Models trained with Argos performed better on complex, multi-step tasks. Notably, these improvements were achieved using fewer training samples than existing approaches, highlighting the importance of reward design in producing more capable and data-efficient agents. Figure 3 illustrates some of these findings.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1331" height="406" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3.png" alt="Figure 3 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, “Response Accuary,” tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, “Visual Grounding Acc,” shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model’s performance and grounding collapse over training." class="wp-image-1160369" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3.png 1331w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3-300x92.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3-1024x312.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3-768x234.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-blog_fig3-240x73.png 240w" sizes="auto, (max-width: 1331px) 100vw, 1331px" /><figcaption class="wp-element-caption">Figure 3.&nbsp;Performance of&nbsp;Argos&nbsp;compared with&nbsp;baseline models&nbsp;on the task of visual hallucination detection&nbsp;(left)&nbsp;and&nbsp;embodied&nbsp;task planning and completion&nbsp;(right).&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="how-argos-shapes-reinforcement-learning">How Argos shapes reinforcement learning</h3>



<p>To understand how Argos affects learning, we took the same vision-language model that had been trained on our curated dataset and fine-tuned it using&nbsp;reinforcement learning in two different ways. In one approach, Argos was an agentic verifier, checking the correctness of outputs and the quality of reasoning. In the other, the model received feedback only on whether its answers were correct.</p>



<p>We evaluated both versions on 1,500 samples from a new dataset and tracked their performance throughout the learning process (Figure 4). Although they started at similar levels, the model without Argos quickly got worse. Its accuracy steadily declined, and it increasingly gave answers that ignored what was in the videos. It learned to game the system by producing answers that seemed correct without grounding them in visual evidence.</p>



<p>The model trained with Argos showed the opposite pattern. Accuracy improved steadily, and the model became better at linking its reasoning to what appeared in the videos. This difference highlights the value of verification: when training rewards both correct outputs and sound reasoning based on visual and temporal evidence, models learn to be more reliable rather than simply finding shortcuts to high scores.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1996" height="550" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final.jpg" alt="Figure 4 shows two side-by-side line charts comparing an Agentic model (dashed line) that uses the Argos verifier with a Non-Agentic model (solid line) trained only with an outcome reward. The left plot, “Response Accuary,” tracks response accuracy versus RL step (0, 5, 10, 15). Both models start near 0.54 accuracy, but the Agentic curve slightly rises and then stays roughly flat, while the Non-agentic curve steadily declines to about 0.50. The right plot, “Visual Grounding Acc,” shows visual grounding accuracy over the same steps: the Agentic curve increases monotonically from about 0.39 to just above 0.5, whereas the Non-Agentic curve initially rises slightly and then drops sharply to about 0.1. Together, the plots illustrate that Argos stabilizes answer accuracy and significantly improves visual grounding, while the non-agentic model’s performance and grounding collapse over training. " class="wp-image-1160428" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final.jpg 1996w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final-300x83.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final-1024x282.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final-768x212.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final-1536x423.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/evaluation-fig4-final-240x66.jpg 240w" sizes="auto, (max-width: 1996px) 100vw, 1996px" /><figcaption class="wp-element-caption">Figure 4.&nbsp;Comparison of&nbsp;response accuracy changes with and without Argos&nbsp;across&nbsp;two model versions&nbsp;(left) and&nbsp;differences in&nbsp;visual grounding accuracy&nbsp;over training for both&nbsp;versions&nbsp;(right).</figcaption></figure>



<h2 class="wp-block-heading" id="potential-impact-and-looking-forward">Potential impact and looking forward</h2>



<p>This research points toward a different way of building AI agents for real-world applications. Rather than fixing errors after they occur, it focuses on training agents to systematically anchor their reasoning in what they actually receive as input throughout the training process.</p>



<p>The potential applications span many domains. A visual assistant for a self-driving car that verifies what’s actually in an image is less likely to report phantom obstacles. A system that automates digital tasks and checks each action against what’s displayed on the screen is less likely to click the wrong button.</p>



<p>As AI systems move beyond research labs into homes, factories, and offices, reliable reasoning becomes essential for safety and trust. Argos represents an early example of verification systems that evolve alongside the AI models they supervise. Future verifiers could be tailored for specific fields like medical imaging, industrial simulations, and business analytics. As more advanced models and richer data sources become available, researchers can use them to improve these verification systems, providing even better guidance during training and further reducing hallucinations.</p>



<p>We hope that this research helps move the field toward AI systems that are both capable and interpretable: agents that can explain their decisions, point to the evidence behind them, and be trained to adhere to real-world requirements and values.</p>



<figure class="wp-block-video aligncenter"><video height="1076" style="aspect-ratio: 1920 / 1076;" width="1920" controls poster="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos.png" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-demo-video.mp4"></video></figure>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/multimodal-reinforcement-learning-with-agentic-verifier-for-ai-agents/">Multimodal reinforcement learning with agentic verifier for AI agents</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/argos-demo-video.mp4" length="101343835" type="video/mp4" />

			</item>
		<item>
		<title>OptiMind: A small language model with optimization expertise</title>
		<link>https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/</link>
		
		<dc:creator><![CDATA[Xinzhi Zhang, Zeyi Chen, Humishka Hope, Hugo Barbalho, Konstantina Mellou, Marco Molinaro, Janardhan (Jana) Kulkarni, Ishai Menache, Sirui Li]]></dc:creator>
		<pubDate>Thu, 15 Jan 2026 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1159707</guid>

					<description><![CDATA[<p>OptiMind is a small language model that converts business operation challenges, described naturally, into mathematical formulations that optimization software can solve. It reduces formulation time &#038; errors &#038; enables fast, privacy-preserving local use.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/">OptiMind: A small language model with optimization expertise</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2.jpg" alt="A flowchart with three horizontal sections on a blue-to-green gradient background. The first section, labeled “Classification,” shows icons of a computer, an arrow pointing to a robot face, and another arrow pointing to a box labeled “TSP.” The second section, labeled “Inference,” displays a robot icon connected by arrows to two document icons, one of which includes a magnifying glass. The third section, labeled “Test-time scaling,” shows a document with a checkmark connected by an arrow to a circular refresh icon. Arrows indicate the flow between sections, starting from Classification to Inference and then to Test-time scaling." class="wp-image-1159915" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-BlogHeroFeature-1400x788-2-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<div class="wp-block-columns is-layout-flex wp-container-core-columns-is-layout-9d6595d7 wp-block-columns-is-layout-flex">
<div class="wp-block-column is-style-default mb-10 pb-1 pr-1 is-layout-flow wp-block-column-is-layout-flow" style="box-shadow:var(--wp--preset--shadow--outlined)">
<h2 class="wp-block-heading h3" id="at-a-glance">At a glance</h2>



<ul class="wp-block-list">
<li>Many real-world business problems can benefit from optimization, but translating decisions, constraints, and goals from natural language into optimization algorithms is slow.</li>



<li>OptiMind is a small language model designed to convert business problems described in natural language into the mathematical formulations needed by optimization software.</li>



<li>OptiMind&nbsp;is trained on&nbsp;a carefully curated, expert-aligned dataset&nbsp;and applies domain-specific hints and self-checks at inference time, improving&nbsp;its&nbsp;accuracy.</li>



<li>OptiMind&nbsp;matches or exceeds the performance of much larger systems,&nbsp;can&nbsp;run&nbsp;locally to protect sensitive data,&nbsp;produces&nbsp;more reliable formulations, and&nbsp;reduces the time and&nbsp;expertise&nbsp;needed to prepare optimization models.</li>
</ul>
</div>
</div>		</div>
	</div>

	</div>



<p>Enterprises across industries, from energy to finance, use optimization models to plan complex operations like supply chains and logistics. These models work by defining three elements: the choices that can be made (such as production quantities or delivery routes), the rules and limits those choices must follow, and the goal, whether that’s minimizing costs, meeting customer demand, or improving efficiency.</p>



<p>Over the past few decades,&nbsp;many&nbsp;businesses have shifted&nbsp;from&nbsp;judgment-based decision-making to data-driven&nbsp;approaches,&nbsp;leading to&nbsp;major&nbsp;efficiency gains and cost&nbsp;savings. Advances in AI promise to accelerate this shift even further, potentially cutting decision times from days to minutes while delivering better results.</p>



<p>In practice, however, turning real-world business problems into a form that optimization software can understand is challenging. This translation process requires expressing decisions, constraints, and objectives in mathematical terms. The work demands specialized&nbsp;expertise,&nbsp;and&nbsp;it&nbsp;can take anywhere from&nbsp;one&nbsp;day to several weeks&nbsp;to solve&nbsp;complex problems.&nbsp;</p>



<p>To address this challenge, we’re introducing <a href="https://www.microsoft.com/en-us/research/publication/optimind-teaching-llms-to-think-like-optimization-experts/">OptiMind</a>, a small language model designed to convert problems described in plain language into the mathematical formulations that optimization software needs. Built on a 20-billion parameter model, OptiMind is compact by today’s standards yet matches the performance of larger, more complex systems. Its modest size means it can run locally on users’ devices, enabling fast iteration while keeping sensitive business data on users’ devices rather than transmitting it to external servers.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="how-it-works">How it works<strong></strong></h2>



<p>OptiMind incorporates knowledge from optimization experts both during training and when it’s being used to improve formulation accuracy at scale. Three stages enable this: domain-specific hints improve training data quality, the model undergoes fine-tuning, and expert reasoning guides the model as it works.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="750" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-scaled.jpg" alt="The image illustrates a linear programming model for a manufacturing facility, detailing the production quantities, setup indicators, and inventory levels for different products over a six-month period, aiming to optimize costs." class="wp-image-1160007" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-300x88.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-1024x300.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-768x225.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-1536x450.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-2048x600.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/OptiMind-1-240x70.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1.&nbsp;From problem description to solution&nbsp;</figcaption></figure>



<p>One of the central challenges&nbsp;in&nbsp;developing&nbsp;OptiMind was the poor quality of existing public datasets for optimization problems. Many examples were incomplete or&nbsp;contained incorrect solutions. To address this, we developed a systematic approach that combines automation with expert review. It organizes problems into well-known categories, such as scheduling or routing, and identifies common error patterns within each. Using these insights, we generated expert-verified &#8220;hints&#8221; to guide the process, enabling the system to regenerate higher-quality solutions and filter out unsolvable examples (Figure 2). The result is a training dataset that more accurately reflects how optimization experts structure problems.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1648" height="638" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning.png" alt="Process for correcting training data" class="wp-image-1159877" style="aspect-ratio:2.58319131760696;width:526px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning.png 1648w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning-300x116.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning-1024x396.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning-768x297.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning-1536x595.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/training-data-cleaning-240x93.png 240w" sizes="auto, (max-width: 1648px) 100vw, 1648px" /><figcaption class="wp-element-caption">Figure 2. Process for correcting training data</figcaption></figure>



<p>Using this refined dataset, we applied supervised fine-tuning to the base model. Rather than simply generating code, we trained OptiMind to produce structured mathematical formulations alongside intermediate reasoning steps, helping it avoid the common mistakes found in earlier datasets.</p>



<p>When in use, the model&#8217;s reliability further improves. When given a new problem, OptiMind first classifies it into a category, such as scheduling or network design. It then applies expert hints relevant to that type of problem, which act as reminders to check for errors before generating a solution. For particularly challenging problems, the system generates multiple solutions and either selects the most&nbsp;frequently&nbsp;occurring one or uses feedback to refine its response. This approach increases accuracy without requiring a larger model, as illustrated in Figure 3.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2057" height="817" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline.png" alt="OptiMind’s inference process" class="wp-image-1159878" style="aspect-ratio:2.5178126502600864;width:770px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline.png 2057w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-300x119.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-1024x407.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-768x305.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-1536x610.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-2048x813.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/inference-pipeline-240x95.png 240w" sizes="auto, (max-width: 2057px) 100vw, 2057px" /><figcaption class="wp-element-caption">Figure 3. OptiMind’s inference process</figcaption></figure>



<h2 class="wp-block-heading" id="evaluation">Evaluation</h2>



<p>To test the system, we turned to three widely used public benchmarks that&nbsp;represent&nbsp;some of the most complex formulation tasks in the field. On closer inspection, we discovered that&nbsp;30 to 50 percent&nbsp;of the original test data was flawed.&nbsp;After manually correcting the issues, OptiMind improved accuracy by approximately 10 percent over the base model. Figure 4 and Table 1 show detailed comparisons: OptiMind outperformed other open-source models under 32 billion parameters and, when combined with expert hints and correction strategies, matched or exceeded the performance of current leading models.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1231" height="457" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1.png" alt="Average accuracy percentages over all models." class="wp-image-1160042" style="aspect-ratio:2.461748867424566;width:850px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1.png 1231w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1-300x111.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1-1024x380.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1-768x285.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main_barplot_new-1-240x89.png 240w" sizes="auto, (max-width: 1231px) 100vw, 1231px" /><figcaption class="wp-element-caption">Figure 4. Average accuracy percentages over all models.</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2122" height="922" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table.png" alt="Performance of all models on corrected benchmark datasets" class="wp-image-1159880" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table.png 2122w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-300x130.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-1024x445.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-768x334.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-1536x667.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-2048x890.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2026/01/main-results-table-240x104.png 240w" sizes="auto, (max-width: 2122px) 100vw, 2122px" /><figcaption class="wp-element-caption">Table 1. Performance of all models on corrected benchmark datasets</figcaption></figure>



<p>OptiMind is more reliable than other models because it learns from higher-quality, domain-aligned data. And by correcting errors and inconsistencies in standard datasets, we significantly reduced the model&#8217;s tendency to hallucinate relative&nbsp;to the base and comparison models.</p>



<h2 class="wp-block-heading" id="looking-forward">Looking forward<strong></strong></h2>



<p>While supervised fine-tuning has provided a strong foundation, we are exploring reinforcement learning to further refine OptiMind&#8217;s reasoning capabilities. We’re also investigating automated frameworks that would allow LLMs to generate their own expert hints, enabling continuous autonomous improvement. Additionally, we are working with Microsoft product teams and industry collaborators to expand OptiMind’s utility, adding support for more programming languages and a variety of input formats, including Excel and other widely used tools.</p>



<p>We&#8217;re releasing OptiMind as an experimental model to gather community feedback and inform future development. The model is available through <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/OptiMindCatalog" target="_blank" rel="noopener noreferrer">Microsoft Foundry<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/OptiMindHF" target="_blank" rel="noopener noreferrer">Hugging Face<span class="sr-only"> (opens in new tab)</span></a>, and we’ve open-sourced the benchmarks and data-processing procedures on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/OptiGuideGithub" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a> to support more reliable evaluation across the field. We welcome feedback through <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/OptiGuideGithub" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a>, and invite those interested in shaping the future of optimization to apply for one of our <a href="https://www.microsoft.com/en-us/research/careers/">open roles</a>.</p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/">OptiMind: A small language model with optimization expertise</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Agent Lightning: Adding reinforcement learning to AI agents without code rewrites</title>
		<link>https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/</link>
		
		<dc:creator><![CDATA[Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Dongsheng Li, Luna K. Qiu, Yuqing Yang]]></dc:creator>
		<pubDate>Thu, 11 Dec 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1158079</guid>

					<description><![CDATA[<p>By decoupling how agents work from how they’re trained, Agent Lightning turns each step an agent takes into data for reinforcement learning. This makes it easy for developers to improve agent performance with almost zero code changes.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/">Agent Lightning: Adding reinforcement learning to AI agents without code rewrites</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW.jpg" alt="Three white line icons on a blue-to-purple gradient background: the first icon shows a simple flowchart with connected squares and a diamond, the second icon shows a network of interconnected circles, and the third icon shows three user profile symbols linked together." class="wp-image-1158206" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/AgentLightning-BlogHeroFeature-1400x788_NEW-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>AI agents are reshaping software development, from writing code to carrying out complex instructions. Yet LLM-based agents are prone to errors and often perform poorly on complicated, multi-step tasks. Reinforcement learning (RL) is an approach where AI systems learn to make optimal decisions by receiving rewards or penalties for their actions, improving through trial and error. RL can help agents improve, but it typically requires developers to extensively rewrite their code. This discourages adoption, even though the data these agents generate could significantly boost performance through RL training.</p>



<p>To address this, a research team from <a href="https://www.microsoft.com/en-us/research/group/msr-asia-shanghai/">Microsoft Research Asia &#8211; Shanghai</a> has introduced <a href="https://www.microsoft.com/en-us/research/project/agent-lightning/">Agent Lightning</a>. This <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/microsoft/agent-lightning">open-source<span class="sr-only"> (opens in new tab)</span></a> framework makes AI agents trainable through RL by separating how agents execute tasks from model training, allowing developers to add RL capabilities with virtually no code modification.</p>



<h2 class="wp-block-heading" id="capturing-agent-behavior-for-training">Capturing agent behavior for training</h2>



<p>Agent Lightning converts an agent&#8217;s experience into a format that RL can use by treating the agent&#8217;s execution as a sequence of states and actions, where each state captures the agent&#8217;s status and each LLM call is an action that moves the agent to a new state.</p>



<p>This approach works for any workflow, no matter how complex. Whether it involves multiple collaborating agents or dynamic tool use, Agent Lightning breaks it down into a sequence of transitions. Each transition captures the LLM’s input, output, and reward (Figure 1). This standardized format means the data can be used for training without any&nbsp;additional&nbsp;steps.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="891" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-scaled.jpg" alt="Figure 1: Diagram illustrating Agent Lightning’s unified data interface for a retrieval-augmented generation (RAG) agent. On the left, four states (state₀ to state₃) show the agent’s execution flow, where semantic variables—UserInput, Query, Passages, and Answer—are updated after each component call (LLM or Search). Green blocks represent populated variables; gray blocks indicate empty ones. On the right, the unified data interface converts these transitions into a trajectory format containing prompt, generation, and immediate reward for RL training. " class="wp-image-1158102" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-300x104.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-1024x356.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-768x267.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-1536x534.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-2048x713.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-1-240x84.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 1. An illustration of Agent Lightning’s standardized format using a retrieval-augmented generation (RAG) agent. Left: The full agent workflow, where the agent&#8217;s state updates after each component step. The green blocks show assigned variables, and the gray blocks indicate variables without content. Right: The collected transitions are based on the standardized format for the RL training process, with each transition corresponding to one LLM step that contains its prompt, result, and immediate reward.</figcaption></figure>



<h2 class="wp-block-heading" id="hierarchical-reinforcement-learning">Hierarchical reinforcement learning</h2>



<p>Traditional RL training for agents that make multiple LLM requests involves stitching together all content into one long sequence and then identifying which parts should be learned and which ignored during training. This approach is difficult to implement and can create excessively long sequences that degrade model performance.</p>



<p>Instead, Agent Lightning’s LightningRL algorithm takes a hierarchical approach. After a task completes, a credit assignment module determines how much each LLM request contributed to the outcome and assigns it a corresponding reward. These independent steps, now paired with their own reward scores, can be used with any existing single-step RL algorithm, such as Proximal Policy Optimization (PPO) or Group Relative Policy Optimization (GRPO) (Figure 2).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="835" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-scaled.jpg" alt="Figure 2: Comparison of three reinforcement learning approaches for LLM tasks. (a) Single-step GRPO: The model completes the task in one call, and multiple outputs for the same task are compared with associated rewards. (b) Previous multi-step GRPO: The task spans multiple LLM calls, forming trajectories; non-LLM tokens (gray boxes) are ignored during training, and entire multi-step runs are compared. (c) LightningRL: Breaks multi-step runs into individual LLM calls, each including input, context, output, and reward assigned by a credit assignment module. Calls from the same task are grouped for reinforcement. " class="wp-image-1158259" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-300x98.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-1024x334.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-768x250.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-1536x501.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-2048x668.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-2-new-240x78.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 2. (a) Single-step GRPO: The LLM completes the task in one call. Multiple responses for the same task are compared to determine how strongly each should be reinforced. (b) Previous multi-step GRPO: The task involves multiple LLM calls. Multiple multi-step runs of the same task are compared, with non-LLM generated tokens (grey boxes) ignored during training. (c) LightningRL: The multi-step run is divided into individual LLM calls. Calls from the same task are compared to determine how strongly each should be reinforced. Each call includes its input, context, output, and reward, assigned by the credit assignment module.</figcaption></figure>



<p>This design offers several benefits. It remains fully compatible with widely used single-step RL algorithms, allowing existing training methods to be applied without modification. Organizing data as a sequence of independent transitions lets developers flexibly construct the LLM input as needed, supporting complex behaviors like agents that use multiple tools or&nbsp;work&nbsp;with other agents. Additionally, by keeping sequences short, the approach scales cleanly and keeps training efficient.</p>



<h2 class="wp-block-heading" id="agent-lightning-as-middleware">Agent Lightning as middleware</h2>



<p>Agent Lightning serves as middleware between RL algorithms and agent environments, providing modular components that enable scalable RL through standardized protocols and well-defined interfaces.</p>



<p>An <strong>agent runner</strong> manages the agents as they complete tasks. It distributes work and collects and stores the results and progress data. It operates separately from the LLMs, enabling them to run on different resources and scale to support multiple agents running concurrently.</p>



<p>An <strong>algorithm</strong> trains the models and hosts the LLMs used for inference and training. It orchestrates the overall RL cycle, managing which tasks are assigned, how agents complete them, and how models are updated based on what the agents learn. It typically runs on GPU resources and communicates with the agent runner through shared protocols.</p>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/agent-lightning/latest/how-to/write-first-algorithm/" target="_blank" rel="noopener noreferrer">LightningStore<span class="sr-only"> (opens in new tab)</span></a> serves as the central repository for all data exchanges within the system. It provides standardized interfaces and a shared format, ensuring that the different components can work together and enabling the algorithm and agent runner to communicate effectively.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="942" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-scaled.png" alt="Figure 3: Diagram showing the architecture of Agent Lightning (AGL). On the left, the AGL Algorithm block includes an inference engine (e.g., vLLM), an algorithm iteration loop, and an adapter for trainable data and weights update. In the center, the AGL Core contains LightningStore, which manages tasks, resources, spans, and LLM calls. On the right, the AGL Agent Runner & Tracer includes a user-defined agent using OpenAI chat completion and agl.emit(). Arrows indicate flows of prompts, responses, tasks, resources, spans, and datasets between components, with roles for algorithm researchers and agent developers highlighted. " class="wp-image-1158104" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-300x110.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-1024x377.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-768x283.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-1536x565.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-2048x754.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-3-240x88.png 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 3. The Agent Lightning framework</figcaption></figure>



<p>All RL cycles follow two steps: (1) Agent Lightning collects agent execution data (called “spans”) and store them in the data store; (2) it then retrieves the required data and sends it to the algorithm for training. Through this design, the algorithm can delegate tasks asynchronously to the agent runner, which completes them and reports the results back (Figure 4).</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="2347" height="840" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new.png" alt="Figure 4: Diagram of the training loop in Agent Lightning. The central element is ‘Trainer,’ with arrows forming a cycle between three components: Agent on the left, Algorithm on the right, and Trainer in the middle. The top arrow labeled ‘Tasks’ flows from Algorithm to Agent, while the bottom arrow labeled ‘Spans’ flows from Agent to Algorithm. ‘Prompt Templates’ is noted above the cycle, indicating its role in task generation. " class="wp-image-1158290" style="width:628px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new.png 2347w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-1024x366.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-768x275.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-1536x550.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-2048x733.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-4-new-240x86.png 240w" sizes="auto, (max-width: 2347px) 100vw, 2347px" /><figcaption class="wp-element-caption">Figure 4. Agent Lightning’s RL cycle</figcaption></figure>



<p>One key advantage of this approach is its algorithmic flexibility. The system makes it easy for developers to customize how agents learn, whether they&#8217;re defining different rewards, capturing intermediate data, or experimenting with different training approaches.</p>



<p>Another advantage is resource efficiency. Agentic RL systems are complex, integrating agentic systems, LLM inference engines, and training frameworks. By separating these components, Agent Lightning makes this complexity manageable and allows each part to be optimized independently</p>



<p>A decoupled design allows each component to use the hardware that suits it best. The agent runner can use CPUs while model training uses GPUs. Each component can also scale independently, improving efficiency and making the system easier to maintain. In practice, developers can keep their existing agent frameworks and switch model calls to the Agent Lightning API without changing their agent code (Figure 5). </p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1238" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new.png" alt="Figure 5: Side-by-side code comparison showing agent implementation before and after integrating Agent Lightning. The left panel (dark background) displays the original agent code written by the developer, including logic for LLM calls, tool usage, and reward assignment. The right panel (light background) shows the modified version using Agent Lightning, where most of the agent logic remains unchanged but includes additional imports and calls to Agent Lightning components such as agl.PromptTemplate, agl.emit(), and agl.Trainer for training and credit assignment. A stylized lightning icon is centered between the two panels. " class="wp-image-1158107" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new.png 1238w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new-300x248.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new-1024x847.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new-768x635.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-5-new-218x180.png 218w" sizes="auto, (max-width: 1238px) 100vw, 1238px" /><figcaption class="wp-element-caption">Figure 5. On the left, the developer implements the agent code. On the bottom right is the code required for Agent Lightning. The main body of the agent code is unchanged.</figcaption></figure>



<h2 class="wp-block-heading" id="evaluation-across-three-real-world-scenarios">Evaluation across three real-world scenarios</h2>



<p>Agent Lightning was tested on three distinct tasks, achieving consistent performance improvements across all scenarios (Figure 6):</p>



<p><strong>Text-to-SQL (LangChain):</strong> In a system with three agents handling SQL generation, checking, and rewriting, Agent Lightning simultaneously optimized two of them, significantly improving the accuracy of generating executable SQL from natural language queries.</p>



<p><strong>Retrieval-augmented generation (OpenAI Agents SDK implementation):</strong> On the multi-hop question-answering dataset MuSiQue, which requires querying a large Wikipedia database, Agent Lightning helped the agent generate more effective search queries and reason better from retrieved content.</p>



<p><strong>Mathematical QA and tool use (AutoGen implementation):</strong> For complex math problems, Agent Lightning trained LLMs to more accurately determine when and how to call the tool and integrate the results into its reasoning, increasing accuracy.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2560" height="1169" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-scaled.jpg" alt="Figure 6: Figure with six line charts showing reward curves across three evaluation scenarios (Spider, MuSiQue, Calculator) for train and test splits. Top row: Train Rewards on Spider, MuSiQue, and Calculator—each plot shows a blue line with noisy upward trend over steps, indicating increasing rewards; Spider and Calculator rise faster with more variance, MuSiQue climbs more gradually. Bottom row: Test Rewards on Spider, MuSiQue, and Calculator—each plot shows a blue line that increases and then stabilizes at higher rewards; Calculator reaches near-plateau earliest, Spider shows steady gains with minor fluctuations, MuSiQue improves more slowly. All plots use ‘Steps’ on the x‑axis and ‘Rewards’ on the y‑axis, with a legend labeled ‘ours’ and light gridlines. " class="wp-image-1158109" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-300x137.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-1024x468.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-768x351.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-1536x701.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-2048x935.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/figure-6-240x110.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 6. Reward curves across the three evaluation scenarios</figcaption></figure>



<h2 class="wp-block-heading" id="enabling-continuous-agent-improvement">Enabling continuous agent improvement</h2>



<p>By simplifying RL integration, Agent Lightning can make it easier for developers to build, iterate, and deploy high-performance agents. We plan to expand Agent Lightning&#8217;s capabilities to include automatic prompt optimization and additional RL algorithms.</p>



<p>The framework is designed to serve as an open platform where any AI agent can improve through real-world practice. By bridging existing agentic&nbsp;systems with reinforcement learning, Agent Lightning aims to help create AI systems that learn from experience and improve over time.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/agent-lightning-adding-reinforcement-learning-to-ai-agents-without-code-rewrites/">Agent Lightning: Adding reinforcement learning to AI agents without code rewrites</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Promptions helps make AI prompting more precise with dynamic UI controls</title>
		<link>https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/</link>
		
		<dc:creator><![CDATA[Sean Rintel, Advait Sarkar, Jack Williams, Nicholas Wilson, Richard Banks, Neeltje Berger, Philipp Steinacher, Payod Panda, Ian Drosos]]></dc:creator>
		<pubDate>Wed, 10 Dec 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1157824</guid>

					<description><![CDATA[<p>Promptions helps developers add dynamic, context-aware controls to chat interfaces so users can guide generative AI responses. It lets users shape outputs quickly without writing long instructions.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/">Promptions helps make AI prompting more precise with dynamic UI controls</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a blue-to-green gradient background: a hub-and-spoke network symbol on the left, a laptop with a user icon in the center, and a connected group of three user icons on the right." class="wp-image-1157946" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/Promptions-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Anyone who uses AI systems knows the frustration: a prompt is given, the response misses the mark, and the cycle repeats. This trial-and-error loop can feel unpredictable and discouraging. To address this, we are excited to introduce <strong>Promptions</strong> (<em>prompt + options</em>), a UI framework that helps developers build AI interfaces with more precise user control.</p>



<p>Its simple design makes it easy to integrate into any setting&nbsp;that relies on added context, including customer support, education, and medicine. Promptions is available under the MIT license on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://labs.ai.azure.com/projects/promptions/" target="_blank" rel="noopener noreferrer">Microsoft Foundry Labs<span class="sr-only"> (opens in new tab)</span></a> and GitHub.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Promptions" width="500" height="281" src="https://www.youtube-nocookie.com/embed/vr3fZpkKy8Q?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<h2 class="wp-block-heading" id="background">Background</h2>



<p>Promptions&nbsp;builds on&nbsp;our research,&nbsp;“<a href="https://www.microsoft.com/en-us/research/publication/dynamic-prompt-middleware-contextual-prompt-refinement-controls-for-comprehension-tasks/" target="_blank" rel="noreferrer noopener">Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks</a>.”&nbsp;This&nbsp;project&nbsp;examined&nbsp;how&nbsp;knowledge&nbsp;workers&nbsp;use&nbsp;generative AI when their goal is&nbsp;to <em>understand</em> rather than&nbsp;<em>create</em>. While much public&nbsp;discussion centers on&nbsp;AI producing text&nbsp;or&nbsp;images, understanding involves asking AI to explain, clarify, or teach—a&nbsp;task&nbsp;that can&nbsp;quickly become&nbsp;complex. Consider a spreadsheet formula: one&nbsp;user may want a&nbsp;simple syntax&nbsp;breakdown,&nbsp;another a&nbsp;debugging&nbsp;guide, and&nbsp;another an explanation suitable for&nbsp;teaching&nbsp;colleagues.&nbsp;The same formula can require&nbsp;entirely&nbsp;different explanations depending on the user’s role, expertise, and goals.&nbsp;</p>



<p>A great deal of complexity sits beneath these&nbsp;seemingly simple&nbsp;requests.&nbsp;Users&nbsp;often&nbsp;find&nbsp;that the way&nbsp;they phrase a question&nbsp;doesn’t&nbsp;match&nbsp;the&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/what-is-it-like-to-program-with-artificial-intelligence/" target="_blank" rel="noreferrer noopener">level of detail the AI&nbsp;needs</a>.&nbsp;Clarifying what they really want can require long, carefully worded&nbsp;prompts that are tiring to produce.&nbsp;And because the connection&nbsp;between natural language and system behavior&nbsp;isn’t always transparent, it can be difficult to predict&nbsp;how the AI will interpret a&nbsp;given&nbsp;request.&nbsp;In the end, users spend more time&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/the-metacognitive-demands-and-opportunities-of-generative-ai/" target="_blank" rel="noreferrer noopener">managing the interaction itself</a>&nbsp;than understanding the material they&nbsp;hoped&nbsp;to learn.</p>



<h2 class="wp-block-heading" id="identifying-how-users-want-to-guide-ai-outputs">Identifying&nbsp;how users want to guide AI outputs&nbsp;</h2>



<p>To explore why these challenges persist and how people can better steer AI toward customized results, we conducted two studies with knowledge workers across technical and nontechnical roles. Their experiences highlighted important gaps that guided Promptions&#8217; design.</p>



<p>Our&nbsp;first study&nbsp;involved&nbsp;38 professionals&nbsp;across&nbsp;engineering, research, marketing, and program management. Participants reviewed&nbsp;design mock-ups that&nbsp;provided&nbsp;static&nbsp;prompt-refinement&nbsp;options—such as&nbsp;<em>length</em>, <em>tone</em>, or <em>start with</em>—for shaping&nbsp;AI&nbsp;responses.&nbsp;</p>



<p>Although these&nbsp;static&nbsp;options were helpful, they&nbsp;couldn’t&nbsp;adapt to the&nbsp;specific formula, code snippets, or text&nbsp;the participant&nbsp;was trying to understand.&nbsp;Participants&nbsp;also wanted direct ways to&nbsp;customize&nbsp;the&nbsp;tone,&nbsp;detail, or&nbsp;format of the response&nbsp;without&nbsp;having to&nbsp;type instructions.</p>



<h3 class="wp-block-heading" id="why-dynamic-refinement-matters">Why dynamic refinement matters</h3>



<p>The&nbsp;second study&nbsp;tested&nbsp;prototypes&nbsp;in a&nbsp;controlled experiment.&nbsp;We compared the static&nbsp;design&nbsp;from the first study, called&nbsp;the&nbsp;“Static Prompt Refinement Control”&nbsp;(Static PRC),&nbsp;against a&nbsp;“Dynamic Prompt Refinement Control” (Dynamic PRC)&nbsp;with features that&nbsp;responded&nbsp;to&nbsp;participants’ feedback.&nbsp;Sixteen&nbsp;technical&nbsp;professionals familiar with generative AI&nbsp;completed six tasks,&nbsp;spanning&nbsp;code explanation,&nbsp;understanding a&nbsp;complex topic, and&nbsp;learning a new&nbsp;skill.&nbsp;Each participant tested both systems, with task assignments balanced to ensure fair comparison.&nbsp;&nbsp;</p>



<p>Comparing Dynamic PRC to Static PRC revealed key insights into how dynamic prompt-refinement options change users’ sense of control and exploration and how those options help them reflect on their understanding. </p>



<h3 class="wp-block-heading" id="static-prompt-refinement">Static&nbsp;prompt&nbsp;refinement</h3>



<p>Static PRC&nbsp;offered a set of pre‑selected controls&nbsp;(Figure 1)&nbsp;identified&nbsp;in the&nbsp;initial&nbsp;study.&nbsp;We expected these options to be useful&nbsp;across many&nbsp;types of&nbsp;explanation-seeking&nbsp;prompts.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1379" height="852" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1.png" alt="Alt text: The Static PRC interface in the user study. It includes dropdowns and radio buttons for selecting expertise level (Beginner to Advanced), explanation length (Short to Long), role of AI (Coach, Teach, Explain), explanation type (End result, Modular, Step-by-step), starting point (High-level or Detailed), and tone (Formal, Informal, Encouraging, Neutral)." class="wp-image-1157834" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1.png 1379w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1-300x185.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1-1024x633.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1-768x475.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig02-staticprc-1-240x148.png 240w" sizes="auto, (max-width: 1379px) 100vw, 1379px" /><figcaption class="wp-element-caption">Figure&nbsp;1: The static PRC&nbsp;interface&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="dynamic-prompt-refinement">Dynamic prompt refinement</h3>



<p>We built the Dynamic PRC system to automatically produce prompt options and refinements based on the user’s input, presenting them in real time so that users could adjust these controls and guide the AI’s responses more precisely (Figure 2).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1379" height="915" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1.png" alt="Alt text: How users interacted with the Dynamic PRC system. (1) shows a user input prompt of “Explain the formula” [with a long Excel formula] (2) Three rows of options relating to this prompt, Explanation Detail Level, Focus Areas, and Learning Objectives, with several options for each, preselected (3) User has modified the preselected options by clicking Troubleshooting under Learning Objectives (4) AI response of an explanation for the formula based on the selected options (5) Session chat control panel with text box that the user adds "I want to control the structure or format of the response". (6) Generates 3 option sets based on this input. (7) A new response is generated with the new session options added." class="wp-image-1157876" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1.png 1379w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1-300x199.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1-1024x679.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1-768x510.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig03-dynamicprc-1-240x159.png 240w" sizes="auto, (max-width: 1379px) 100vw, 1379px" /><figcaption class="wp-element-caption">Figure&nbsp;2.&nbsp;Interaction flow in&nbsp;the Dynamic PRC system. (1)&nbsp;The&nbsp;user&nbsp;asks&nbsp;the system to explain&nbsp;a long Excel formula.&nbsp;(2)&nbsp;Dynamic PRC generates refinement&nbsp;options:&nbsp;Explanation Detail Level, Focus Areas, and Learning Objectives.&nbsp;(3)&nbsp;The user&nbsp;modifies&nbsp;these&nbsp;options.&nbsp;(4)&nbsp;The&nbsp;AI returns&nbsp;an explanation based on the selected options.&nbsp;(5)&nbsp;In the session chat panel,&nbsp;the user adds&nbsp;a request&nbsp;to control the structure or format of the response.&nbsp;(6)&nbsp;Dynamic PRC generates&nbsp;new&nbsp;option&nbsp;sets based on this input.&nbsp;(7)&nbsp;The&nbsp;AI produces an updated explanation reflecting the&nbsp;newly applied&nbsp;options.&nbsp;</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="findings">Findings</h2>



<p>Participants consistently reported that dynamic controls made it easier to express the nuances of their tasks without repeatedly rephrasing their prompts. This reduced the effort of prompt engineering and allowed users to focus more on understanding content than managing the mechanics of phrasing.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="627" height="313" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1.png" alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Which tool…”, comparing user responses to six questions about preference, mental demand, feeling rushed, success, effort, and annoyance. Y-axis ranges from 1 (Dynamic) to 7 (Static), with 4 marked as Equal. Each question is represented by a box plot showing response distribution, median, and variability, illustrating perceived differences between dynamic and static PRC tools." class="wp-image-1157879" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1.png 627w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1-300x150.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig04-plotpreferences-1-240x120.png 240w" sizes="auto, (max-width: 627px) 100vw, 627px" /><figcaption class="wp-element-caption">Figure&nbsp;3.&nbsp;Comparison of&nbsp;user&nbsp;preferences for Static&nbsp;PRC&nbsp;versus&nbsp;Dynamic PRC&nbsp;across key evaluation criteria.&nbsp;</figcaption></figure>



<p>Contextual options prompted users to try refinements they might not have considered on their own. This behavior suggests that Dynamic PRC can broaden how users engage with AI explanations, helping them uncover new ways to approach tasks beyond their initial intent. Beyond exploration, the dynamic controls prompted participants to think more deliberately about their goals. Options like &#8220;Learning Objective&#8221; and &#8220;Response Format&#8221; helped them clarify what they needed, whether guidance on applying a concept or step-by-step troubleshooting help.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="627" height="358" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1.png" alt="Alt text: Box plot chart titled “Dynamic vs Static PRC: Control Effectiveness,” comparing user agreement with four statements about AI control tools. Each statement has two box plots—blue for Dynamic and orange for Static—showing response distributions on a 1 (Strongly Disagree) to 7 (Strongly Agree) Likert scale. Statements assess perceived control over AI output, usefulness for understanding, desire for more control, and clarity of control functions." class="wp-image-1157883" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1.png 627w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1-300x171.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig05-ploteffectiveness-1-240x137.png 240w" sizes="auto, (max-width: 627px) 100vw, 627px" /><figcaption class="wp-element-caption">Figure 4.&nbsp;Participant ratings comparing the&nbsp;effectiveness of Static PRC and&nbsp;Dynamic PRC&nbsp;</figcaption></figure>



<p>While participants valued Dynamic PRC’s adaptability, they also found it more difficult to interpret. Some struggled to anticipate how a selected option would influence the response, noting that the controls seemed opaque because the effect became clear only after the output appeared.</p>



<p>However,&nbsp;the&nbsp;overall&nbsp;positive response&nbsp;to&nbsp;Dynamic PRC&nbsp;showed us that Promptions&nbsp;could&nbsp;be&nbsp;broadly useful,&nbsp;leading&nbsp;us to share it&nbsp;with the developer community.   &nbsp;</p>



<h3 class="wp-block-heading" id="technical-design">Technical design</h3>



<p>Promptions works as a lightweight middleware layer that sits between the user and the underlying language model (Figure 5). It has two main components:</p>



<p><strong>Option Module</strong>. This module reviews the user’s prompt and conversation history, then generates a set of refinement options. These are presented as interactive UI elements (radio buttons, checkboxes, text fields) that directly shape how the AI interprets the prompt.</p>



<p><strong>Chat&nbsp;Module.</strong>&nbsp;This module&nbsp;produces the&nbsp;AI’s response based&nbsp;on the refined prompt.&nbsp;When&nbsp;a user changes an option,&nbsp;the&nbsp;response&nbsp;immediately&nbsp;updates,&nbsp;making the interaction feel more like an&nbsp;evolving&nbsp;conversation&nbsp;than&nbsp;a cycle of&nbsp;repeated prompts.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="524" height="547" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1.png" alt="Alt text: The Promptions system model. (1) The Option Module ingests the user’s prompt input along with the conversation history. (2) It then outputs a set of prompt options, each initialized based on the content of the prompt. (3) These options are rendered inline via a dedicated rendering engine. (4) The Chat Module incorporates the refined options as grounding, alongside the original prompt and conversation history, to generate a chat response. (5) The user can modify the GUI controls, which updates the refinements and triggers the Chat Module to regenerate the current response accordingly." class="wp-image-1157886" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1.png 524w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1-287x300.png 287w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/12/blog-fig07-systemflow-1-172x180.png 172w" sizes="auto, (max-width: 524px) 100vw, 524px" /><figcaption class="wp-element-caption">Figure&nbsp;5.&nbsp;Promptions&nbsp;middleware workflow. (1) The Option Module&nbsp;reads&nbsp;the user’s prompt&nbsp;and&nbsp;conversation history&nbsp;and&nbsp;(2)&nbsp;generates&nbsp;prompt options. (3) These options are&nbsp;rendered&nbsp;inline&nbsp;by&nbsp;a dedicated&nbsp;component. (4) The Chat Module incorporates these&nbsp;refined options alongside the original prompt and history to&nbsp;produce&nbsp;a response.&nbsp;(5)&nbsp;When the user&nbsp;adjusts&nbsp;the&nbsp;controls,&nbsp;the&nbsp;refinements&nbsp;update&nbsp;and the Chat Module regenerates&nbsp;the response accordingly.</figcaption></figure>



<h3 class="wp-block-heading" id="adding-promptions-to-an-application">Adding Promptions to an application</h3>



<p>Promptions easily integrates into any conversational chat interface. Developers only need to add a component to display the options and connect it to the AI system. There&#8217;s no need to store date between sessions, which keeps implementation simple. The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://labs.ai.azure.com/projects/promptions/" target="_blank" rel="noopener noreferrer">Microsoft Foundry Labs<span class="sr-only"> (opens in new tab)</span></a> repository includes two sample applications, a generic chatbot and an image generator, that demonstrate this design in practice.  </p>



<p>Promptions is well-suited for interfaces where users need to provide context but don’t want to write it all out. Instead of typing lengthy explanations, they can adjust the controls that guide the AI’s response to match their preferences.</p>



<h2 class="wp-block-heading" id="questions-for-further-exploration">Questions for further exploration</h2>



<p>Promptions raises important questions for future research. Key usability challenges include clarifying how dynamic options affect AI output and managing the complexity of multiple controls. Other questions involve balancing immediate adjustments with persistent settings and enabling users to share options collaboratively.</p>



<p>On the technical side, questions focus on generating more effective options, validating and customizing dynamic interfaces, gathering relevant context automatically, and supporting the ability to save and share option sets across sessions.</p>



<p> These questions, along with broader considerations of collaboration, ethics, security, and scalability, are guiding our ongoing work on Promptions and related systems.</p>



<div class="annotations " data-bi-aN="margin-callout">
	<article class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<div class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Tool</span>
			<a href="https://labs.ai.azure.com/projects/promptions/" data-bi-cN="Explore Promptions on Microsoft Foundry Labs" target="_blank" rel="noopener noreferrer" data-external-link="true" data-bi-aN="margin-callout" data-bi-type="annotated-link" class="annotations__link font-weight-semibold text-decoration-none"><span>Explore Promptions on Microsoft Foundry Labs</span>&nbsp;<span class="glyph-in-link glyph-append glyph-append-open-in-new-tab" aria-hidden="true"></span></a>					</div>
	</article>
</div>



<p>By making Promptions open source, we hope to help developers create smarter, more responsive AI experiences.</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://labs.ai.azure.com/projects/promptions/">Explore Promptions on Microsoft Foundry Labs<span class="sr-only"> (opens in new tab)</span></a></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/promptions-helps-make-ai-prompting-more-precise-with-dynamic-ui-controls/">Promptions helps make AI prompting more precise with dynamic UI controls</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
