<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Wed, 08 Oct 2025 03:18:44 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.8.3</generator>
	<item>
		<title>Ideas: More AI-resilient biosecurity with the Paraphrase Project</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/</link>
		
		<dc:creator><![CDATA[Eric Horvitz, Bruce Wittmann, Tessa Alexanian, James Diggans]]></dc:creator>
		<pubDate>Mon, 06 Oct 2025 14:04:34 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1151021</guid>

					<description><![CDATA[<p>Microsoft’s Eric Horvitz and guests Bruce Wittmann, Tessa Alexanian, and James Diggans discuss the Paraphrase Project—a red-teaming effort that exposed and secured a biosecurity vulnerability in AI-driven protein design. The work offers a model for addressing AI’s dual-use risks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/">Ideas: More AI-resilient biosecurity with the Paraphrase Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe title="Ideas: More AI-resilient biosecurity with the Paraphrase Project" width="500" height="281" src="https://www.youtube-nocookie.com/embed/xA9nvhX7e7A?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe src="https://player.blubrry.com/?podcast_id=148928066&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.</p>



<p>AI has been described as a “dual use” technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer <a href="https://www.microsoft.com/en-us/research/people/horvitz/">Eric Horvitz</a> and his guests—<a href="https://www.microsoft.com/en-us/research/people/bwittmann/">Bruce Wittmann</a>, a senior applied scientist at Microsoft; <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/people/tessa-alexanian/" target="_blank" rel="noopener noreferrer">Tessa Alexanian<span class="sr-only"> (opens in new tab)</span></a>, a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&nbsp;and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/jdiggans/" target="_blank" rel="noopener noreferrer">James Diggans<span class="sr-only"> (opens in new tab)</span></a>, a vice president at Twist Bioscience—explore this idea in the context of AI-powered protein design.</p>



<p>With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI—and that they could bypass the systems in place to defend against their creation. The project, known as the <em>Paraphrase Project</em>, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in <em>Science.</em></p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/strengthening-nucleic-acid-biosecurity-screening-against-generative-protein-design-tools/">Strengthening nucleic acid biosecurity screening against generative protein design tools</a><br>Publication | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/toward-ai-resilient-screening-of-nucleic-acid-synthesis-orders-process-results-and-recommendations/">Toward AI-Resilient Screening of Nucleic Acid Synthesis Orders: Process, Results, and Recommendations</a><br>Preprint | December 2024</li>



<li><a href="https://www.microsoft.com/en-us/research/story/the-paraphrase-project-designing-defense-for-an-era-of-synthetic-biology/">The Paraphrase Project: Designing defense for an era of synthetic biology</a><br>Microsoft Research Blog | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI meets biology: Promise, risk, and responsibility</a><br>Microsoft Research Blog | Eric Horvitz | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/project/paraphrase-project/">Paraphrase Project</a><br>Project homepage</li>
</ul>
</div>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript-1">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>ERIC HORVITZ: </strong>You’re&nbsp;listening to&nbsp;<em>Ideas</em>, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&nbsp;I’m&nbsp;Eric Horvitz, Microsoft’s chief scientific officer, and in this series, we explore the technologies shaping our future and the&nbsp;big ideas&nbsp;that propel them forward.</p>



<p>[MUSIC FADES]</p>



<p>Today,&nbsp;I’m&nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&nbsp;advances in&nbsp;AI tools&nbsp;for protein design&nbsp;might&nbsp;impact&nbsp;biosecurity. The results were reported in our recent paper,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="http://www.science.org/doi/10.1126/science.adu8578" target="_blank" rel="noopener noreferrer">“Strengthening nucleic acid biosecurity screening against generative protein design tools,”<span class="sr-only"> (opens in new tab)</span></a>&nbsp;published in&nbsp;<em>Science</em>&nbsp;on Oct. 2.&nbsp;</p>



<p>Joining me are&nbsp;three&nbsp;of the larger set of&nbsp;coauthors on that paper:&nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&nbsp;James&nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as <em>IBBIS</em>.&nbsp;</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>Now, let’s&nbsp;rewind two years.&nbsp;Almost to&nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&nbsp;identify&nbsp;incoming orders of concern.&nbsp;</p>



<p>Now in that work, we&nbsp;created an AI pipeline from open-source tools that could&nbsp;essentially “paraphrase” the amino acid sequences—reformulating&nbsp;them while&nbsp;working to&nbsp;preserve&nbsp;their structure and potentially their function.&nbsp;</p>



<p>These paraphrased sequences could evade the screening systems used by major DNA&nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&nbsp;</p>



<p>Now, experts in the field described this finding as the first “zero day” for AI and biosecurity.&nbsp;And this&nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&nbsp;</p>



<p>With the help of a&nbsp;strong&nbsp;cross-sector team—including James, Tessa, Bruce, and many others—we worked behind the scenes to build AI biosecurity&nbsp;<em>red-teaming approaches</em>,&nbsp;probe for vulnerabilities, and to design practical fixes. These “patches,” akin to those in cybersecurity,&nbsp;have now been shared with&nbsp;organizations&nbsp;globally to strengthen biosecurity screening.&nbsp;</p>



<p>This has been one of the most fascinating projects&nbsp;I’ve&nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&nbsp;</p>



<p>The project highlights that the&nbsp;same AI tools capable of&nbsp;incredible&nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&nbsp;that&nbsp;we avoid costly misuses.&nbsp;</p>



<p>With that, let me officially welcome our guests.<s></s></p>



<p>Bruce, James, Tessa, welcome to the podcast.</p>



<p><strong>BRUCE WITTMANN: </strong>Thanks, Eric.</p>



<p><strong>JAMES DIGGANS: </strong>Thanks for having us.</p>



<p><strong>HORVITZ: </strong>It&#8217;s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.</p>



<p>Before we dive into the technical side of things, I&#8217;d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?</p>



<p><strong>WITTMANN:</strong>&nbsp;Well, I&#8217;ve always liked building things. That&#8217;s where I would say I come from. You know, my hobbies when I&#8217;m not working on biology or AI things—as you know, Eric—is, like, building things around the house, right. Doing construction. That kind of stuff.</p>



<p>But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that&#8217;s kind of like organic chemistry, but you&#8217;re wiring together different parts of an organism’s metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there&#8217;s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that&#8217;s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don&#8217;t necessarily even understand using our understanding from data or <em>deriving</em> understanding from data.</p>



<p>So, you know, that&#8217;s the roundabout way of how I got to where I am—the abstract way of how I got to where I am.</p>



<p><strong>HORVITZ: </strong>And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid <em>catastrophic</em> outcomes?</p>



<p><strong>ALEXANIAN:</strong> Yeah, I mean, probably the origin of me being really excited about biology is actually a book called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.penguinrandomhouse.com/books/535043/the-lives-of-a-cell-by-lewis-thomas/" target="_blank" rel="noopener noreferrer"><em>[The] Lives of [a] Cell</em><span class="sr-only"> (opens in new tab)</span></a> by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, <em>Oh, wow, life is just incredible</em>. I think I read it when I was, you know, 12 or 13, and I was like, <em>Life is incredible. I want to work on this. This is the most beautiful science</em>, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology—this <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://igem.org/" target="_blank" rel="noopener noreferrer">iGEM<span class="sr-only"> (opens in new tab)</span></a> team—and I joined it, and I thought, <em>Oh, this is so cool. I really got to go work in this field of synthetic biology.</em></p>



<p>And then I also tried doing the wet lab biology, and I was like, <em>Oh, but I don&#8217;t like this part</em>. <em>I don&#8217;t actually, like, like babysitting microbes.</em> [LAUGHTER] I think there&#8217;s a way … some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I&#8217;m not that, apparently.</p>



<p>And so I ended up becoming a lab automation engineer because I could help the science happen, but I … but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.</p>



<p><strong>HORVITZ: </strong>Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids …</p>



<p><strong>WITTMANN: </strong>Not anymore. <strong>ALEXANIAN: </strong>It&#8217;s true. [LAUGHTER]</p>



<p><strong>DIGGANS: </strong>Not anymore. [LAUGHS]</p>



<p><strong>ALEXANIAN:</strong> They used to be tougher. They used to be tougher.</p>



<p><strong>HORVITZ: </strong>James.</p>



<p><strong>DIGGANS:</strong> So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn&#8217;t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It’s a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.</p>



<p><strong>HORVITZ: </strong>Well, thanks everyone.</p>



<p>I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We&#8217;re already seeing it lead to new vaccines, novel therapeutics, and—on the scientific front—powerful insights into the machinery of life.</p>



<p>So there&#8217;s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.</p>



<p>So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?</p>



<p><strong>WITTMANN: </strong>Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it’s right in my wheelhouse. [LAUGHS]</p>



<p><strong>HORVITZ: </strong>Perfect, perfect background. [LAUGHS]</p>



<p><strong>WITTMANN:</strong>&nbsp;It&#8217;s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.</p>



<p>Every chemical reaction that happens in our body—well, nearly every chemical reaction that happens in our body—most of the structure of our cells, you name it. Any life process, proteins are central to it.</p>



<p>Now proteins are encoded by what are known as … well, I shouldn&#8217;t say encoded. They are <em>constructed</em> from what are called amino acids—there are 20 of them—and depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that&#8217;s what we mean when we say protein sequence.</p>



<p>The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.</p>



<p>Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it&#8217;s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that&#8217;s 20 to the power of 300 possible combinations. And a common reference point is that it&#8217;s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.</p>



<p>So when a human has an idea of, OK, here&#8217;s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we&#8217;ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.</p>



<p><strong>HORVITZ: </strong>Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality—from in silico to test tubes. What role does <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.twistbioscience.com/" target="_blank" rel="noopener noreferrer">Twist Bioscience<span class="sr-only"> (opens in new tab)</span></a> play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.</p>



<p><strong>DIGGANS: </strong>So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.</p>



<p>So we are cognizant also, however, that these are what are called <em>dual-use technologies</em>. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.</p>



<p>And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we&#8217;re asked to make some sequence of DNA that we understand what that thing is encoding and who we&#8217;re giving it for. So we&#8217;re screening both the customer that&#8217;s coming to us and we&#8217;re screening the sequence that they&#8217;re requesting.</p>



<p>And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we&#8217;re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they&#8217;re going to use those for legitimate purpose and responsibly.</p>



<p><strong>HORVITZ: </strong>And how do the emergence of these new generative AI tools influence how you think about risk?</p>



<p><strong>DIGGANS: </strong>A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don&#8217;t exist in the natural world. That&#8217;s an extremely powerful capability.</p>



<p>But the existing defensive tools that we use at DNA synthesis companies generally rely on what&#8217;s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.</p>



<p><strong>HORVITZ: </strong>Now you also serve as chair of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://genesynthesisconsortium.org/" target="_blank" rel="noopener noreferrer">International Gene Synthesis Consortium<span class="sr-only"> (opens in new tab)</span></a>. Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?</p>



<p><strong>DIGGANS: </strong>Certainly. So the IGSC was founded in 2010<a href="#_ftn1" id="_ftnref1">[1]</a> and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.</p>



<p><strong>HORVITZ: </strong>Thanks, James. Now, Tessa, your organization, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/" target="_blank" rel="noopener noreferrer">IBBIS<span class="sr-only"> (opens in new tab)</span></a> is focused—it&#8217;s a beautiful mission—on advancing science while minimizing <em>catastrophic</em> risk, likelihood of <em>catastrophic</em> risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that … do you view that risk landscape as evolving as AI capabilities are growing?</p>



<p><strong>ALEXANIAN:</strong> I think the … to be honest, as a person who&#8217;s been in biosecurity for a while, I&#8217;ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.</p>



<p>Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I&#8217;m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.</p>



<p>Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we&#8217;ve seen with many historical pandemics, there&#8217;s a possibility for something to emerge or be created that is beyond our society&#8217;s ability to control.</p>



<p>You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.</p>



<p>Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a … these risks of both, sort of, public health risks, pandemic risks, and misuse risks—people deliberately trying to do harm with biology, as we&#8217;ve seen from the long history of biological weapons programs—you know, we think that those might be accelerated in a few different ways by AI technology, both the potential … and I say potential here because as everyone who has worked in a wet lab—which I think is everyone on this call—knows, engineering biology is really difficult. So there&#8217;s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there&#8217;s maybe also the potential to create novel threats.</p>



<p>And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn&#8217;t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we&#8217;re facing—as long as we develop those safeguards in a way that keeps pace with AI.</p>



<p><strong>HORVITZ: </strong>Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today&#8217;s screening systems?</p>



<p>And I was preparing for a global workshop on AI and biosecurity that I&#8217;d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.</p>



<p>So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?</p>



<p><strong>WITTMANN: </strong>Sure. Yeah. I think the best place to start with this is really by analogy.</p>



<p>An analogy I often use in this case is the type of image generation AI tools we&#8217;re all familiar with now where I can tell the AI model, &#8220;Hey, give me a cartoonish picture of a dog playing fetch.&#8221; And it&#8217;ll do that, and it&#8217;ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.</p>



<p>And that&#8217;s kind of the same technology that we&#8217;re using in this red-teaming pipeline. Only rather than using plain language, English, we&#8217;re passing in what we would call conditioning information that is relevant to a protein.</p>



<p>So our AI models aren&#8217;t at the point yet where I can say, &#8220;Give me a protein that does <em>x</em>.&#8221; That would be the dream. We&#8217;re a long way from that. But what instead we do is we pass in things that match that theme that we&#8217;re interested in. So rather than saying, &#8220;Hey, give me back the theme on a dog,&#8221; we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.</p>



<p>So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There&#8217;s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ”Hey, here&#8217;s this structure; give me a protein sequence that folds to this structure,” just like with that analogy with the dog, it&#8217;s going to give me something that matches that structure but that is likely still never been seen before. It&#8217;s going to be a new sequence.</p>



<p>So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening—that <em>would</em> be captured by DNA synthesis screening—find its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, <em>paraphrased</em>, <em>reformulated</em>, whatever phrase we want to use to describe them.</p>



<p>And they have a chance or a greater chance than <em>not</em> of maintaining the structure and so maintaining the function while being sufficiently different that they&#8217;re not detected by these tools anymore.</p>



<p>So that&#8217;s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.<s></s></p>



<p><strong>HORVITZ: </strong>But to get down to brass tacks, what Bruce did for the framing study was … we took the toxic, well-known toxic protein ricin, as we described in a framing paper that&#8217;s actually part of the appendix now to the <em>Science</em> publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.</p>



<p>And this brings us to the next step of our project, way back when, at the early … in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?</p>



<p>And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?</p>



<p><strong>DIGGANS: </strong>I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really … a valuable opportunity. And so we really leapt at that.</p>



<p><strong>HORVITZ:</strong> And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through … flew under the radar of the biosecurity screening software as we covered in that framing paper.</p>



<p>Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first <em>zero day</em> in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there&#8217;s no time to respond before it could be exploited should it be known.</p>



<p>In that vein, we took a cybersecurity approach. We stood up a CERT—C-E-R-T—a <em>cybersecurity [computer] emergency response team</em> approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.</p>



<p>At one point down the line, it was so rewarding to hear you say, James, “I&#8217;m really glad Microsoft got here first.” I&#8217;m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you&#8217;ve encountered, and I&#8217;d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.</p>



<p><strong>DIGGANS: </strong>Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, <em>Oh, I&#8217;ll build a new tool to detect this concrete universe of things</em>, right. This was more a pattern of I&#8217;m going to use tools—and I love the name “Paraphrase”; it&#8217;s a fantastic name—I can paraphrase anything that I would normally think of as biological … as <em>posing</em> biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.</p>



<p><strong>HORVITZ: </strong>Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?</p>



<p><strong>WITTMANN:</strong>&nbsp;Yeah, of course. So, you know, using machine learning lingo, you don&#8217;t want to <em>overfit</em> to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&nbsp;and we could at the very least detect ricin or reformulated versions of ricin.</p>



<p>So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn&#8217;t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous …</p>



<p><strong>HORVITZ: </strong>I think we had 72, was it?</p>



<p><strong>WITTMANN:</strong>&nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list … on the paper, who primarily put that list together &#8230;</p>



<p><strong>HORVITZ: </strong>This is Jacob Beal … Jacob Beal at Raytheon BBN.</p>



<p><strong>WITTMANN: </strong>I think James actually might be the better one to answer how this list was expanded.</p>



<p><strong>DIGGANS:</strong>&nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn&#8217;t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.</p>



<p><strong>HORVITZ: </strong>And, Bruce, can you describe some about how we characterize the updates and the, we&#8217;ll say, the boost in capabilities of the patched screening tools?</p>



<p><strong>WITTMANN:</strong> So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?</p>



<p>We put them on a big grid. So we have two axes. We have on the x-axis—and this is a figure in our paper—the quality of the prediction. It&#8217;s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are <em>most likely</em>, having to say <em>most likely</em>, to retain function of the original.</p>



<p>So when you compare the original tools—Tool Series A, right, the unpatched tools—what you&#8217;ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series—Series B, the patched version of tools—we have more flagged in that upper-right corner.</p>



<p><strong>HORVITZ: </strong>And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?</p>



<p><strong>DIGGANS: </strong>I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.</p>



<p><strong>HORVITZ: </strong>Thanks, James.</p>



<p>Now, I know that we all understand this particular effort to be important but a <em>piece</em> of the biosecurity and AI problem. I&#8217;m just curious to … I’ll ask all three of you to just share some brief reflections.</p>



<p>I know, Bruce, you&#8217;ve been on … you’ve stayed on this, and we’ve—all of us on the original team—have other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.</p>



<p>Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.</p>



<p><strong>ALEXANIAN: </strong>Yeah, I think with the Paraphrase’s work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you&#8217;ve scrambled your DNA sequence and it doesn&#8217;t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it&#8217;s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening … <em>everybody</em> had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.</p>



<p>I feel like we&#8217;re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it&#8217;s very unlike any <em>one</em> CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you&#8217;re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we&#8217;re not … we&#8217;re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.</p>



<p>But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think &#8230; I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know—you have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein—I think all of us on the screening side are going to have to be responding to that, as well.</p>



<p>So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that&#8217;s … I think we&#8217;re going to see more of those. And I think what I&#8217;m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.</p>



<p><strong>HORVITZ: </strong>Thank you, Tessa.</p>



<p>The, the … Bruce, I mean, you and I are working on all sorts of dimensions. You&#8217;re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We&#8217;ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?</p>



<p><strong>WITTMANN:</strong>&nbsp;I feel like that could have its own dedicated podcast, as well. There&#8217;s a lot … [LAUGHTER] there&#8217;s a lot to talk about.</p>



<p><strong>HORVITZ: </strong>Yeah. We want to make sure that we don&#8217;t tell the world that the whole problem is solved here.</p>



<p><strong>WITTMANN:</strong>&nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we&#8217;re doing right now, it&#8217;s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.</p>



<p>And obviously the goal is to move away from that in benign applications, where when I&#8217;m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don&#8217;t know what this protein does. It&#8217;s kind of a circular problem, right, where we&#8217;re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.</p>



<p>Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It&#8217;s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It&#8217;s a very powerful roadblock. It&#8217;s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won&#8217;t go into the details of them. Again, that would be its own podcast.</p>



<p>But primarily my big push—and I think this is emerging consensus in the field, though I don&#8217;t want to speak for everybody—is it needs to … any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It&#8217;s just <em>a</em> protein.</p>



<p>So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don&#8217;t want my model producing that, do want my model producing that. I don&#8217;t have that luxury in this space. So it&#8217;s a totally different problem. It&#8217;s an evolving problem. Conversations are happening about it, but the work is very much not done.</p>



<p><strong>HORVITZ: </strong>And, James, I want to give you the same open question, but I&#8217;d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you&#8217;re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms—logging, auditing nucleic acid orders, transparency, various kinds—that might complement technical approaches like Paraphrase and their status today.</p>



<p><strong>DIGGANS: </strong>Yeah, I&#8217;m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.</p>



<p>Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you&#8217;re evaluating your customers. You should know your customer; you know that they&#8217;re legitimate. I think that&#8217;s an important practice.</p>



<p>Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it&#8217;s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.</p>



<p><strong>HORVITZ: </strong>Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn&#8217;t always easy. As we&#8217;ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.</p>



<p>A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, <em>all experts</em>, cautioned against what are called <em>information hazards</em>, the risk of sharing the details to enable malevolent actions with our findings or our approach.</p>



<p>So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even <em>then</em>, once we chose our balance and submitted our manuscript to <em>Science</em>, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.</p>



<p>So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.</p>



<p>Now, we brought the proposal to Tessa and her team at IBBIS—this is a great nonprofit group; look at their mission—and we worked with Tessa and her colleagues to refine a workable solution that was accepted by <em>Science</em> magazine as a new approach to handling information hazards as first demonstrated by our paper.</p>



<p>So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.</p>



<p><strong>ALEXANIAN: </strong>Yeah. And thanks, Eric.</p>



<p>It&#8217;s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.</p>



<p>And I think it showed us that there isn&#8217;t a consensus right now on how to handle information hazards in biotechnology. You know, I think … I don&#8217;t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you&#8217;ll hear people about how they&#8217;ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we&#8217;re … we have even less of a consensus when it comes to handling biological information.</p>



<p>You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there&#8217;s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.</p>



<p>And I think what we landed on that I&#8217;m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you&#8217;ll see a lot of information got added back in.</p>



<p>And I&#8217;m excited to see people&#8217;s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they&#8217;d maybe read a number of papers talking about biosecurity risks from AI that didn&#8217;t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.</p>



<p>So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.</p>



<p>But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you&#8217;re trying to do harm.</p>



<p>And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you&#8217;re trying to do harm.</p>



<p>And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.</p>



<p>You know, if you have an affiliation with a recognizable institution or some good explanation of why you don&#8217;t have one right now, you know, if you have a reason for accessing this data, it shouldn&#8217;t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we&#8217;ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you&#8217;ll get a list of, &#8220;Here&#8217;s the reasons we rejected you. If you don&#8217;t think that&#8217;s right, get back to us.&#8221;</p>



<p>So I&#8217;m really excited to pilot this in part because I think, you know, we&#8217;re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it&#8217;s, like, still hard to engineer harm with biology, even if it&#8217;s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.</p>



<p><strong>HORVITZ: </strong>Thanks, Tessa. So we&#8217;re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we&#8217;ve learned, the process we&#8217;ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with … even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?</p>



<p>Tessa, Bruce, James … James, have you ever thought about that? And we&#8217;ll go to Bruce and then Tessa.</p>



<p><strong>DIGGANS:</strong>&nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool … the <em>products</em> of AI tools?</p>



<p><strong>HORVITZ: </strong>Bruce.</p>



<p><strong>WITTMANN:</strong>&nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?</p>



<p>And my hope is similar to what James said. We&#8217;ve made it easier for others to do this type of work. Not this exact work; it doesn&#8217;t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that&#8217;s my takeaway.</p>



<p><strong>HORVITZ:</strong> Tessa, bring us home—<em>bring us home!</em> [LAUGHS]</p>



<p><strong>ALEXANIAN: </strong><em>Bring us home!</em> Let&#8217;s do it faster next time. [LAUGHTER] Come talk to any of us if you&#8217;re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it&#8217;ll be faster next time.</p>



<p>And I think, you know, the other thing I would encourage is if you&#8217;re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.</p>



<p>I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it&#8217;s more like once a year or maybe once every six months, if we&#8217;re lucky, that we get something that&#8217;s like applying AI bio to biosecurity. So, you know, if you&#8217;re interested in these threats, I think we&#8217;d love to see more work that&#8217;s directly applied to facing these threats using the most modern technology.</p>



<p><strong>HORVITZ: </strong>Well said.</p>



<p>Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.</p>



<p>It&#8217;s been a true pleasure to work with you. I&#8217;m so excited about what we&#8217;ve accomplished, the processes and the models that we&#8217;re now sharing with the world. And I&#8217;m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.</p>



<p>[MUSIC]</p>



<p><strong>WITTMANN: </strong>Thanks, Eric.</p>



<p><strong>DIGGANS: </strong>Thank you.</p>



<p><strong>ALEXANIAN: </strong>Thank you.</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a href="#_ftnref1" id="_ftn1">[1]</a> The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/">Ideas: More AI-resilient biosecurity with the Paraphrase Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>When AI Meets Biology: Promise, Risk, and Responsibility</title>
		<link>https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/</link>
		
		<dc:creator><![CDATA[Eric Horvitz]]></dc:creator>
		<pubDate>Mon, 06 Oct 2025 14:03:54 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1150818</guid>

					<description><![CDATA[<p>Microsoft researchers reveal a confidential research effort that explored how open-source AI tools could be used to bypass biosecurity checks—and helped create fixes now influencing global standards.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI Meets Biology: Promise, Risk, and Responsibility</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1920" height="1080" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png" alt="Paraphrase Project Protiens" class="wp-image-1151098" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png 1920w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-1024x576.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-768x432.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-1536x864.png 1536w" sizes="(max-width: 1920px) 100vw, 1920px" /></figure>



<p>Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This “dual-use” potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.</p>



<h2 class="wp-block-heading" id="great-promise-and-potential-threat">Great Promise—and Potential Threat</h2>



<p>I’m excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I’ve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. </p>



<p>In our paper published in <em>Science</em> on October 2, “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.science.org/doi/10.1126/science.adu8578" target="_blank" rel="noopener noreferrer">Strengthening nucleic acid biosecurity screening against generative protein design tools<span class="sr-only"> (opens in new tab)</span></a>,” we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.</p>



<p>We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity “red-teaming” methods that allowed us to better understand vulnerabilities and craft practical solutions—&#8221;patches” that have now been adopted globally, making screening systems significantly more AI-resilient.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1788" height="693" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png" alt="An illustration of the AI Protein Design red-teaming workflow. [starting at the left] an icon of a database with the heading above that reads: Database of Wild-Type Proteins of Concern. [arrow moves right] Above the arrow the text reads: Generate Synthetic Homologs (x) Conditioned on Wild Types (y). P(x|y) appears below the arrow. [continuing to the right] a computer monitor icon with protein sequences on the screen appears in brackets with N appearing outside the bottom of the right bracket. The text above the computer screen reads: “N” Synthetic Homologs per Wild-Type. [arrows move to the right and fork to an upper arrow and a lower arrow] The text above the upper arrow reads Reverse Translate and the arrow points to a computer monitor icon with a DNA icon on the screen. [upper arrow continues to the right] The arrow points to a computer monitor icon with the text Hazard Screening appearing above and a biohazard icon and a question mark appearing on the screen. [lower arrow moves to the right] A computer monitor icon includes a paraphrased toxin sequence verses a protein sequence on the computer screen. Above the monitor the text reads: Score in silico. [lower arrow continues to the right] An illustration provides an example of the evaluation results (see also table S1 in the paper) tracking the number of flagged sequences (y-axis) and hazardous sequences (x-axis). [the lower arrow moves up to the Hazard Screening step (from the upper arrow process) and another arrow moves from the Hazard Screening to the evaluation results illustration. There is a dotted line with the words Repeat Process moving from the Evaluation illustration to the left and back to the database." class="wp-image-1151201" style="width:812px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png 1788w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-300x116.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-1024x397.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-768x298.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-1536x595.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-240x93.png 240w" sizes="auto, (max-width: 1788px) 100vw, 1788px" /><figcaption class="wp-element-caption">Summary of AIPD red-teaming workflow.</figcaption></figure>



<p>For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where “zero-day” vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders—including synthesis companies, biosecurity organizations, and policymakers—to rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.</p>



<h2 class="wp-block-heading" id="dilemma-of-disclosure">Dilemma of Disclosure</h2>



<p>The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: </p>



<blockquote class="wp-block-quote is-style-spectrum is-layout-flow wp-block-quote-is-layout-flow">
<p>How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?</p>
</blockquote>



<p>We recognized that our work itself—detailing methods and failure modes—could be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility—and to help others to build on our work; others stressed restraint to minimize risk. It was clear that a <em>new model of scientific communication</em> was needed, one that could balance openness and security.</p>



<h2 class="wp-block-heading" id="the-novel-framework">The Novel Framework</h2>



<p>The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&nbsp;</p>



<p>In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&nbsp;</p>



<p>To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/" target="_blank" rel="noopener noreferrer">International Biosecurity and Biosafety Initiative for Science (IBBIS)<span class="sr-only"> (opens in new tab)</span></a>, a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:</p>



<ul class="wp-block-list">
<li><strong>Controlled access</strong>: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.</li>



<li><strong>Stratified tiers of information</strong>: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.</li>



<li><strong>Safeguards and agreements</strong>: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.</li>



<li><strong>Resilience and longevity</strong>: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.</li>
</ul>



<p>This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.</p>



<p>To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program <em>in perpetuity</em>. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.</p>



<h2 class="wp-block-heading" id="an-important-step-in-scientific-publishing">An Important Step in Scientific Publishing</h2>



<p>We are pleased that the leadership at <em>Science</em> accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist—and that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at <em>Science,</em> including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.</p>



<h2 class="wp-block-heading" id="beyond-biology-a-model-for-sensitive-research">Beyond Biology: A Model for Sensitive Research</h2>



<p>While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.</p>



<p>We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into <em>how</em> knowledge is communicated—not just <em>what</em> is communicated—we can ensure that scientific progress continues to serve humanity safely.</p>



<p>The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h3 class="wp-block-heading" id="additional-reading">Additional reading</h3>



<p><a href="https://www.microsoft.com/en-us/research/publication/strengthening-nucleic-acid-biosecurity-screening-against-generative-protein-design-tools/" target="_blank" rel="noreferrer noopener"><em>Strengthening nucleic acid biosecurity screening against generative protein design tools</em>.</a></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://nap.nationalacademies.org/28868">The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025.<span class="sr-only"> (opens in new tab)</span></a></em></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nap.nationalacademies.org/catalog/29174/disseminating-in-silico-and-computational-biological-research-navigating-benefits-and" target="_blank" rel="noopener noreferrer">Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025.<span class="sr-only"> (opens in new tab)</span></a></em></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.erichorvitz.com/scientific_integrity.htm" target="_blank" rel="noopener noreferrer">Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024.<span class="sr-only"> (opens in new tab)</span></a></em></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI Meets Biology: Promise, Risk, and Responsibility</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Using AI to assist in rare disease diagnosis</title>
		<link>https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/</link>
		
		<dc:creator><![CDATA[Mandi Hall, Ashley Conard]]></dc:creator>
		<pubDate>Mon, 22 Sep 2025 14:17:03 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1142402</guid>

					<description><![CDATA[<p>New research from Microsoft, Drexel, and the Broad explores how generative AI could support genetic professionals in rare disease diagnosis.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg" alt="Icons representing individual and group connections to a central computer monitor with a globe, symbolizing online connectivity, set against a gradient background transitioning from blue to pink." class="wp-image-1143080" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;</p>



<p>To better understand and address these challenges, Microsoft Research—in collaboration with Drexel University and the Broad Institute​​—conducted a comprehensive study titled <em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals<span class="sr-only"> (opens in new tab)</span></a>.</em> The study was recently published in a special edition of <em>ACM Transactions on Interactive Intelligent Systems</em> journal focused on generative AI.  </p>



<p>The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;​&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;​We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseases—ultimately aiming to improve patients’ quality of life and life expectancy.</p>



<h2 class="wp-block-heading" id="whole-genome-sequencing-in-rare-disease-diagnosis">Whole genome sequencing in rare disease diagnosis</h2>



<p>Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patient’s DNA sequence to reference genomes.&nbsp;​​Genetic professionals use bioinformatics tools such as&nbsp;<em>seqr,&nbsp;</em>an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;</p>



<p>​​The multi-step sequencing process​​​&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;​​to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysis​​. This creates a backlog of patient cases​​. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;</p>



<h2 class="wp-block-heading" id="the-promise-of-ai-systems-to-assist-with-complex-human-tasks">The promise of AI systems to assist with complex human tasks</h2>



<p>Approximately 87% of AI systems never reach deployment&nbsp;​simply because they solve​​​&nbsp;the wrong problems.&nbsp;​​Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;​To design an effective​​​&nbsp;AI-based system​, one needs to identify​&nbsp;​​tasks AI can support,&nbsp;​​determine​​​​​​&nbsp;the appropriate level of AI involvement, and&nbsp;​​design​​​​​​&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;​​AI&nbsp;can best be incorporated into workflows and tools.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="999693">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Event Series</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo" aria-label="Microsoft Research Forum" data-bi-cN="Microsoft Research Forum" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Research-Forum-hero_1400x788.jpg" alt="Research Forum | abstract background with colorful hexagons" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Forum</h2>
				
								<p id="microsoft-research-forum" class="large">Join us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo" aria-describedby="microsoft-research-forum" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Forum" target="_blank">
							Watch on-demand						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="study-objectives-and-co-designing-a-genetic-ai-assistant">Study objectives and co-designing a genetic AI assistant</h2>



<p>Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.</p>



<h2 class="wp-block-heading" id="identifying-challenges-in-whole-genome-sequencing-analysis">Identifying challenges in whole genome sequencing analysis</h2>



<p>Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:</p>



<ol class="wp-block-list">
<li><em>Information Overload</em>: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.</li>



<li><em>Collaborative Sharing</em>: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.</li>



<li><em>Prioritizing Reanalysis</em>: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.</li>
</ol>



<p>Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.</p>



<h2 class="wp-block-heading" id="co-designing-with-experts-and-ai-human-sensemaking-tasks">Co-designing with experts and AI-human sensemaking tasks</h2>



<p>Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.</p>



<p>Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;​​approach not only&nbsp;​underscores​​​&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.</p>



<h2 class="wp-block-heading" id="design-implications-for-expert-ai-sensemaking">Design implications for expert-AI sensemaking</h2>



<p>In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilities​​. The task of staying&nbsp;​​current​​​​​​, synthesizing information from diverse sources, and making informed decisions&nbsp;​​is challenging​​​​​​.&nbsp;&nbsp;</p>



<p>Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;​​particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.</p>



<h2 class="wp-block-heading" id="distributed-expert-ai-sensemaking-design">Distributed expert-AI sensemaking design</h2>



<p>Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;​​​​​​​​​​Trust is gained by​​​​​​&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated information​​.&nbsp;​​Some​​​​​​&nbsp;users​, however,​&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;​​and annotated​​​​​​. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the process​​.&nbsp;</p>



<h2 class="wp-block-heading" id="initial-expert-ai-sensemaking-and-re-sensemaking-design">Initial expert-AI sensemaking and re-sensemaking design</h2>



<p>In our fast-paced, information-driven world,&nbsp;​​it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.​​&nbsp;​​Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing users—both&nbsp;original decision-makers or new individuals—to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;​​transparency​​​​​​&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;</p>



<h2 class="wp-block-heading" id="combining-evidence-from-multiple-modalities-to-enhance-ai-expert-sensemaking">Combining evidence from multiple modalities to enhance AI-expert sensemaking</h2>



<p>​​​The​​​​​​&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of data—text, images, spatial coordinates, and more—into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;​​could create​​​​​​&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;​​context​​​​​​&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;</p>



<h2 class="wp-block-heading" id="sensemaking-process-with-ai-assistant">Sensemaking Process with AI Assistant</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="952" height="481" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png" alt="Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines." class="wp-image-1142535" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png 952w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-300x152.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-768x388.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-240x121.png 240w" sizes="auto, (max-width: 952px) 100vw, 952px" /><figcaption class="wp-element-caption">Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.</figcaption></figure>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>We explored the potential of generative AI&nbsp;to support​​ genetic professionals​&nbsp;​in diagnosing rare diseases​​. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;​pinpointing​​​&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is needed​​. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;</p>



<h2 class="wp-block-heading" id="join-the-conversation">Join the conversation</h2>



<p>Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">here<span class="sr-only"> (opens in new tab)</span></a>. Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.biorxiv.org/content/10.1101/2025.03.10.642480v1" target="_blank" rel="noopener noreferrer">Evidence Aggregator: AI reasoning applied to rare disease diagnosis.<span class="sr-only"> (opens in new tab)</span></a>&nbsp;&nbsp;</p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tool-space interference in the MCP era: Designing for agent compatibility at scale</title>
		<link>https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/</link>
		
		<dc:creator><![CDATA[Adam Fourney, Tyler Payne, Maya Murad, Saleema Amershi]]></dc:creator>
		<pubDate>Thu, 11 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149210</guid>

					<description><![CDATA[<p>As agentic AI ushers in a new era marked by tool expansion, systems are converging, and complexity is rising. Microsoft Research explores the Model Context Protocol (MCP) as a new standard for agent collaboration across fragmented tool ecosystems.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Three white icons on a gradient background transitioning from blue to purple to pink. From left to right: a globe with a magnifying glass representing internet search, a central circle connected to smaller circles symbolizing network connectivity, and a checklist with two checkmarks and one empty box indicating task management." class="wp-image-1149369" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<p>This year&nbsp;we’ve&nbsp;seen&nbsp;remarkable&nbsp;advances in agentic AI, including&nbsp;systems that conduct deep research,&nbsp;operate&nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&nbsp;multi-step goals. In each case,&nbsp;the industry relied&nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&nbsp;for peak&nbsp;performance. For example,&nbsp;OpenAI&#8217;s&nbsp;recent models&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/openai/gpt-oss?tab=readme-ov-file#tools" target="_blank" rel="noopener noreferrer">presume&nbsp;the&nbsp;availability&nbsp;of web search and document retrieval&nbsp;tools<span class="sr-only"> (opens in new tab)</span></a>. Likewise,&nbsp;the prompts and actions&nbsp;of&nbsp;<a href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/" target="_blank" rel="noreferrer noopener">Magentic-One</a>&nbsp;are&nbsp;set up to make hand-offs easy—for example, allowing the WebSurfer agent to pass downloaded files to the Coder agent. &nbsp;But as agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&nbsp;Agents&nbsp;from&nbsp;different&nbsp;developers&nbsp;or companies will&nbsp;increasingly&nbsp;encounter&nbsp;each other and&nbsp;must&nbsp;work together to complete tasks, in what we refer to as a&nbsp;<em>society of agents</em>.&nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&nbsp;in this&nbsp;setting, or will they hinder one another and slow progress?</p>



<p>Early clues have&nbsp;emerged&nbsp;from an&nbsp;unexpected&nbsp;source:&nbsp;namely,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer">Model Context Protocol<span class="sr-only"> (opens in new tab)</span></a>&nbsp;(MCP). Since January 2025, MCP has grown from a&nbsp;promising spec to a&nbsp;thriving&nbsp;market&nbsp;of&nbsp;tool&nbsp;servers.&nbsp;As an example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://docs.zapier.com/mcp/home" target="_blank" rel="noopener noreferrer">Zapier boasts a catalog of 30,000 tools<span class="sr-only"> (opens in new tab)</span></a>&nbsp;across 7,000 services.&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://mcp.composio.dev/" target="_blank" rel="noopener noreferrer">Composio&nbsp;provide over 100 managed MCP servers<span class="sr-only"> (opens in new tab)</span></a>, surfacing hundreds of tools. Hugging&nbsp;Face is now serving&nbsp;many&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/spaces?filter=mcp-server" target="_blank" rel="noopener noreferrer">Spaces&nbsp;apps over MCP<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://shopify.dev/docs/apps/build/storefront-mcp/servers/storefront" target="_blank" rel="noopener noreferrer">Shopify has enabled MCP for millions of storefronts<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;A&nbsp;society of&nbsp;<em>tools</em>&nbsp;is already here, and it promises to&nbsp;extend&nbsp;agent capabilities through&nbsp;cross-provider&nbsp;horizontal integration.&nbsp;</p>



<p>So,&nbsp;what does MCP have to say about&nbsp;horizontal integration? As catalogs grow,&nbsp;we expect some new failure modes to surface.&nbsp;This&nbsp;blog&nbsp;post introduces&nbsp;these&nbsp;as <em>tool-space interference</em>, and sketches both early observations and some pragmatic interventions to keep the society&nbsp;we’re&nbsp;building&nbsp;from stepping on its own feet.&nbsp;</p>



<p>Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.</p>



<h2 class="wp-block-heading" id="a-framing-example">A framing example</h2>



<p>Consider MCP as a means for extending <a href="https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/">Magentic-One</a>, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won’t change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1021" height="410" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png" alt="Diagram depicting Magentic-One's multi-agentic architecture. An Orchestrator agent has access to 4 specialized sub-agents: a Coder agent that can write code and reason to sol solve tasks, a Computer Terminal Agent that can execute code written by the Coder agent, a WebSurfer agent that browse the internet (navigate pages, fill forms, etc), and a FileSurfer agent that can navigate files (e.g. PDFs, PPTx, etc). The diagram is annotated to show that for any incoming git-related task, the Orchestrator agent has to decide at evert orchestration step whether to access Git CLI via ComputerTerminal, visit Github site via WebSurfer, or directly access Github’s MCP server." class="wp-image-1149211" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png 1021w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-300x120.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-768x308.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-240x96.png 240w" sizes="auto, (max-width: 1021px) 100vw, 1021px" /><figcaption class="wp-element-caption">Figure 1: We can extend&nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&nbsp;&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="tool-space-interference-through-the-lens-of-mcp">Tool-space interference, through the lens of MCP</h2>



<p>To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://smithery.ai/">smithery.ai<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://hub.docker.com/mcp">Docker MCP Hub<span class="sr-only"> (opens in new tab)</span></a>. Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.</p>



<p>To&nbsp;automate the&nbsp;inspection&nbsp;of&nbsp;running MCP servers,&nbsp;we developed an&nbsp;MCP&nbsp;Interviewer&nbsp;tool.&nbsp;The MCP&nbsp;Interviewer&nbsp;begins by cataloging the server’s tools, prompts, resources, resource templates, and capabilities.&nbsp;From&nbsp;this catalog we can compute&nbsp;descriptive statistics&nbsp;such as the number of tools, or the depth of the parameter&nbsp;schemas.&nbsp;&nbsp;Then, given the list of available tools, the interviewer uses&nbsp;an LLM (in our case,&nbsp;OpenAI&#8217;s GPT-4.1)&nbsp;to construct a functional testing&nbsp;plan&nbsp;that&nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&nbsp;the&nbsp;interviewer&nbsp;can&nbsp;also&nbsp;grade&nbsp;more qualitative&nbsp;criteria&nbsp;by&nbsp;using&nbsp;an LLM&nbsp;to&nbsp;apply purpose-built rubrics&nbsp;to&nbsp;tool&nbsp;schemas&nbsp;and&nbsp;tool call outputs.&nbsp;&nbsp;We are excited to&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">release the MCP Interviewer&nbsp;as an open-source CLI&nbsp;tool<span class="sr-only"> (opens in new tab)</span></a>, so server developers can automatically evaluate their MCP servers with agent usability in mind,&nbsp;and users can&nbsp;validate&nbsp;new servers.&nbsp;</p>



<p>While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.</p>



<h3 class="wp-block-heading" id="one-size-fits-all-but-some-more-than-others">One-size fits all (but some more than others)</h3>



<p>So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">OpenAI’s guide on function calling<span class="sr-only"> (opens in new tab)</span></a> advises developers to:</p>



<p>“<em>Include examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).”</em></p>



<p>So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let’s dive into more numbers.</p>



<h3 class="wp-block-heading" id="tool-count">Tool count</h3>



<p>While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">recommends<span class="sr-only"> (opens in new tab)</span></a> that developers:</p>



<p>“<em>Keep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.</em>”</p>



<p>While we expect this to improve with each new model generation, at present, large tool spaces can <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">lower performance by up to 85% for some models<span class="sr-only"> (opens in new tab)</span></a>. Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/microsoft/playwright-mcp">Playwright-MCP<span class="sr-only"> (opens in new tab)</span></a> (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.</p>



<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png" alt="chart" class="wp-image-1149361" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-360x360.png 360w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.</figcaption></figure>



<h3 class="wp-block-heading" id="response-length">Response length</h3>



<p>Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://research.trychroma.com/context-rot">practical limits can be much lower<span class="sr-only"> (opens in new tab)</span></a>. MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/tiktoken"><span class="sr-only"> (opens in new tab)</span></a>, some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">up to 91% in one study<span class="sr-only"> (opens in new tab)</span></a>), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td></td><td></td><td colspan="4"><strong># of tools that would overflow context in</strong></td></tr><tr><td><strong>Model</strong></td><td><strong>Context Window</strong></td><td><strong>1 call</strong></td><td><strong>2 calls</strong></td><td><strong>3-5 calls</strong></td><td><strong>6-10 calls</strong></td></tr><tr><td>GPT 4.1</td><td>1,000,000</td><td>0</td><td>1</td><td>7</td><td>11</td></tr><tr><td>GPT 5</td><td>400,000</td><td>1</td><td>7</td><td>15</td><td>25</td></tr><tr><td>GPT-4o, Llama 3.1,</td><td>128,000</td><td>16</td><td>15</td><td>33</td><td>40</td></tr><tr><td>Qwen 3</td><td>32,000</td><td>56</td><td>37</td><td>86</td><td>90</td></tr><tr><td>Phi-4</td><td>16,000</td><td>93</td><td>60</td><td>116</td><td>109</td></tr></tbody></table></figure>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="936" height="935" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png" alt="Chart showing the average tool call output lengths (in tokens) for 1,312 tools, as observed by the MCP Interviewer’s functional test plan. The x-axis represents individual tools (sorted by index), and the y-axis displays the average output length on a logarithmic scale. Horizontal dashed lines indicate context window limits for GPT-4o (128k tokens) and GPT-5 (400k tokens). A pink annotation box summarizes statistics: total tools (1,312), mean (4,431 tokens), median (98 tokens), minimum (0 tokens), and maximum (557,766 tokens)." class="wp-image-1149213" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-768x767.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-360x360.png 360w" sizes="auto, (max-width: 936px) 100vw, 936px" /><figcaption class="wp-element-caption">Figure 3: Tool call response length averages, in tokens, as&nbsp;observed&nbsp;by the MCP Interviewer’s functional test plan. Only successful tool calls are considered. Horizontal lines&nbsp;indicate&nbsp;context window limits for GPT-4o and GPT-5.</figcaption></figure>



<h3 class="wp-block-heading" id="tool-parameter-complexity">Tool parameter complexity</h3>



<p>Mirroring the challenges from increasing&nbsp;the&nbsp;number of tools,&nbsp;increasing the complexity of a tool’s parameter space can also lead to degradation.&nbsp;For example, while MCP tools can take complex object types and structures as parameters,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://composio.dev/blog/gpt-4-function-calling-example" target="_blank" rel="noopener noreferrer">composio<span class="sr-only"> (opens in new tab)</span></a>&nbsp;found that&nbsp;flattening the parameter space could improve tool-calling performance&nbsp;by 47%&nbsp;compared to baseline performance.&nbsp;&nbsp;In our analysis, we&nbsp;find&nbsp;numerous examples of deeply nested structure—in&nbsp;one&nbsp;case, going&nbsp;20&nbsp;levels deep.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png" alt="Chart showing the maximum depth of each tool’s input properties schema. The x-axis represents individual tools (sorted by index), and the y-axis shows the maximum property schema depth. Most tools have a depth  of 2 (named and annotated properties). A pink annotation box summarizes statistics: total tools (12,643), mean (2.24), median (2.00), standard deviation (1.38), minimum (0.00), and maximum (20.00). " class="wp-image-1149365" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-360x360.png 360w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 4: The maximum depth of each tool’s input properties schema. A depth of 0&nbsp;indicates&nbsp;a tool with no properties. A depth of 1&nbsp;indicates&nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&nbsp;indicates&nbsp;a tool with named and annotated properties.&nbsp;&nbsp;A depth of 3+&nbsp;indicates&nbsp;a tool with structured properties that have&nbsp;additional&nbsp;nested annotations.&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="namespacing-issues-and-naming-ambiguity">Namespacing issues and naming ambiguity</h3>



<p>Another often-cited issue with the current MCP specification is the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/128">lack of a formal namespace mechanism<span class="sr-only"> (opens in new tab)</span></a>. If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/openai-agents-python/issues/464">raise an error<span class="sr-only"> (opens in new tab)</span></a> under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was “search”, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td><strong>Tool Name</strong></td><td><strong>Number of Instances</strong></td></tr><tr><td><strong>search</strong></td><td>32</td></tr><tr><td><strong>get_user</strong></td><td>11</td></tr><tr><td><strong>execute_query</strong></td><td>11</td></tr><tr><td><strong>list_tables</strong></td><td>10</td></tr><tr><td><strong>update_task</strong></td><td>9</td></tr><tr><td><strong>generate_image</strong></td><td>9</td></tr><tr><td><strong>send_message</strong></td><td>9</td></tr><tr><td><strong>execute_command</strong></td><td>8</td></tr><tr><td><strong>list_tasks</strong></td><td>8</td></tr><tr><td><strong>search_files</strong></td><td>8</td></tr></tbody></table></figure>



<p>Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td>websearch</td><td>brave_web_search</td></tr><tr><td>search-web</td><td>tavily_web_search</td></tr><tr><td>web_search</td><td>google_news_search</td></tr><tr><td>search_web</td><td>google-play-search</td></tr><tr><td>search_webkr</td><td>google_search_parsed</td></tr><tr><td>google_search</td><td>search_google_images</td></tr><tr><td>search_google</td><td>get_webset_search_exa</td></tr><tr><td>ai_web_search</td><td>search_google_scholar</td></tr><tr><td>web_search_exa</td><td>duckduckgo_web_search</td></tr><tr><td>search_web_tool</td><td>google_search_scraper</td></tr><tr><td>web_search_agent</td><td>answer_query_websearch</td></tr><tr><td>batch-web-search</td><td>&nbsp;</td></tr></tbody></table></figure>



<h3 class="wp-block-heading" id="errors-and-error-messages">Errors and error messages</h3>



<p>Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an “IsError” flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string “error: job,” while another tool providing academic search returned “Please retry with 0 or fewer IDs.”</p>



<h3 class="wp-block-heading" id="resource-sharing-conventions">Resource sharing conventions</h3>



<p>Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://modelcontextprotocol.io/specification/2025-06-18/server/tools#resource-links">resource_link<span class="sr-only"> (opens in new tab)</span></a> as a result &#8212; but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).</p>



<p>Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a <em>local</em> PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.</p>



<h2 class="wp-block-heading" id="recommendations">Recommendations</h2>



<p>On balance, along any given dimension, the average MCP server is quite reasonable—but, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.</p>



<h3 class="wp-block-heading" id="protocol-developers">Protocol developers</h3>



<p>We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.</p>



<p>Likewise, we believe&nbsp;MCP should&nbsp;quickly move to&nbsp;provide formal namespaces&nbsp;to eliminate tool name collisions.&nbsp;If namespaces&nbsp;are hierarchical, then this also provides a way of organizing large catalogs&nbsp;of functions&nbsp;into thematically&nbsp;related tool&nbsp;sets.&nbsp;Tool sets, as an organizing principle,&nbsp;are already showing some promise&nbsp;in&nbsp;GitHub MCP Server’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/github/github-mcp-server?tab=readme-ov-file#dynamic-tool-discovery" target="_blank" rel="noopener noreferrer">dynamic tool discovery,<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and VS Code’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://code.visualstudio.com/updates/v1_103#_tool-grouping-experimental" target="_blank" rel="noopener noreferrer">tool grouping (with virtual tools)<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;where agents or users&nbsp;can&nbsp;enable and disable tools&nbsp;as needed.&nbsp;&nbsp;In the future,&nbsp;a standardized mechanism for grouping tools would allow&nbsp;<em>clients</em>&nbsp;to engage in hierarchical tool-calling,&nbsp;where they first select a category, then select a tool, without needing to keep all possible&nbsp;tools in context.</p>



<h3 class="wp-block-heading" id="server-developers">Server developers</h3>



<p>While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.</p>



<h3 class="wp-block-heading" id="client-developers">Client developers</h3>



<p>Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://www.anthropic.com/engineering/multi-agent-research-system">Anthropic recently reported using a tool testing agent<span class="sr-only"> (opens in new tab)</span></a> to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions— for example, generating namespaces from server names—and could reduce token outputs by summarizing or paginating long tool results.</p>



<h3 class="wp-block-heading" id="market-developers">Market developers</h3>



<p>Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/">distributes Python wheels matched to a developer’s operating system or processor<span class="sr-only"> (opens in new tab)</span></a>, an MCP marketplace could serve tool schemas optimized for a developer’s chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users’ clients.</p>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>In summary, the MCP&nbsp;ecosystem offers significant value for AI agent development,&nbsp;despite&nbsp;some&nbsp;early&nbsp;growing pains.&nbsp;Grounded in insights from the&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">MCP Interviewer<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&nbsp;well positioned&nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&nbsp;eliminate&nbsp;collisions, enhancing protocol support for&nbsp;client provided&nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&nbsp;</p>



<p>By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>RenderFormer: How neural networks are reshaping 3D rendering</title>
		<link>https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/</link>
		
		<dc:creator><![CDATA[Yue Dong]]></dc:creator>
		<pubDate>Wed, 10 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149051</guid>

					<description><![CDATA[<p> RenderFormer, from Microsoft Research, is the first model to show that a neural network can learn a complete graphics rendering pipeline. It’s designed to support full-featured 3D rendering using only machine learning—no traditional graphics computation required. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: network node icon, lightbulb-shaped icon with a path tool icon in the center; a monitor icon showing a web browser icon" class="wp-image-1149127" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>3D rendering—the process of converting three-dimensional models into two-dimensional images—is a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.</p>



<p>Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.</p>



<p>Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene—limiting their generalizability.</p>



<h2 class="wp-block-heading" id="renderformer-toward-a-general-purpose-neural-rendering-model">RenderFormer: Toward a general-purpose neural rendering model</h2>



<p>To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML—no traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. <a href="https://www.microsoft.com/en-us/research/publication/renderformer-transformer-based-neural-rendering-of-triangle-meshes-with-global-illumination/">This work</a> has been accepted at SIGGRAPH 2025 and is <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/renderformer" target="_blank" rel="noopener noreferrer">open-sourced on GitHub<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="architecture-overview">Architecture overview</h2>



<p>As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens—each one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2419" height="1008" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png" alt="Figure 1: The figure illustrates the architecture of RenderFormer. It includes a Triangle Mesh Scene with a 3D rabbit model inside a colored cube, a Camera Ray Map grid, a View Independent Transformer (12 layers of Self-Attention and Feed Forward Network), a View Dependent Transformer (6 layers with Cross-Attention and Self-Attention), and a DPT Decoder. Scene attributes—Vertex Normal, Reflectance (Diffuse, Specular, Roughness), Emission, and Position—are embedded into Triangle Tokens via Linear + Norm operations. These tokens and Ray Bundle Tokens (from the Camera Ray Map) are processed by the respective transformers and decoded to produce a rendered image of a glossy rabbit in a colored room." class="wp-image-1149133" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png 2419w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-300x125.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1024x427.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-768x320.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1536x640.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-2048x853.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-240x100.png 240w" sizes="auto, (max-width: 2419px) 100vw, 2419px" /><figcaption class="wp-element-caption">Figure 1. Architecture of RenderFormer</figcaption></figure>



<p>To describe the viewing direction, the model uses ray bundle tokens derived from a ray map—each pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.</p>



<p>The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144027">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-label="AI Testing and Evaluation: Learnings from Science and Industry" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP2-AI-TE_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshots of Daniel Carpenter, Timo Minssen, Chad Atalla, and Kathleen Sullivan for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Testing and Evaluation: Learnings from Science and Industry</h2>
				
								<p id="ai-testing-and-evaluation-learnings-from-science-and-industry" class="large">Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-describedby="ai-testing-and-evaluation-learnings-from-science-and-industry" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="dual-branch-design-for-view-independent-and-view-dependent-effects">Dual-branch design for view-independent and view-dependent effects</h2>



<p>The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.</p>



<ul class="wp-block-list">
<li>The <strong>view-independent transformer</strong> captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.</li>



<li>The <strong>view-dependent transformer</strong> models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.</li>
</ul>



<p>Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.</p>



<p>To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="963" height="509" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png" alt="Table 1: A table comparing the performance of different network variants in an ablation study. The columns are labeled Variant, PSNR (↑), SSIM (↑), LPIPS (↓), and FLIP (↓). Variants include configurations such as "full view-dependent stage," "w/o DPT," "w/o self-attention," and "w/o DPT & w/o self-attention." Each variant is associated with numerical values for the four metrics, showing how removing or altering components affects performance. The full view-dependent stage achieves the highest PSNR and SSIM and lowest LPIPS and FLIP, indicating optimal performance. Additional rows explore configurations involving camera space and world space view-dependent stages with various token and layer setups." class="wp-image-1149129" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png 963w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-300x159.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-768x406.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-240x127.png 240w" sizes="auto, (max-width: 963px) 100vw, 963px" /><figcaption class="wp-element-caption">Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. </figcaption></figure>



<p>To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="943" height="240" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png" alt="Figure 2: The figure displays four 3D-rendered objects showcasing view-independent rendering effects. From left to right: a purple teapot on a green surface, a blue rectangular object on a red surface, an upside-down table casting shadows on a green surface, and a green apple-like object on a blue surface. Each object features diffuse lighting and coarse shadow effects, with distinct highlights and shadows produced by directional light sources." class="wp-image-1149132" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png 943w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-300x76.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-768x195.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-240x61.png 240w" sizes="auto, (max-width: 943px) 100vw, 943px" /><figcaption class="wp-element-caption">Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. </figcaption></figure>



<p>The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall—capturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="947" height="620" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png" alt="Figure 3: The figure contains six panels arranged in two rows and three columns. The top row displays a teapot in a room with red and green walls under three different roughness values: 0.3, 0.7, and 0.99 (left to right). The bottom row shows the corresponding attention outputs for each roughness setting, featuring the teapot silhouette against a dark background with distinct light patterns that vary with roughness." class="wp-image-1149131" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png 947w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-300x196.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-768x503.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-240x157.png 240w" sizes="auto, (max-width: 947px) 100vw, 947px" /><figcaption class="wp-element-caption">Figure 3. Visualization of attention outputs</figcaption></figure>



<h2 class="wp-block-heading" id="training-methodology-and-dataset-design">Training methodology and dataset design</h2>



<p>RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1–3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender’s Cycles renderer, under varied lighting conditions and camera angles.</p>



<p>The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:</p>



<ul class="wp-block-list">
<li>500,000 steps at 256×256 resolution with up to 1,536 triangles</li>



<li>100,000 steps at 512×512 resolution with up to 4,096 triangles</li>
</ul>



<p>The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="805" height="805" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg" alt="Figure 4: The figure presents a 3×3 grid of diverse 3D scenes rendered by RenderFormer. In the top row, the first scene shows a room with red, green, and white walls containing two rectangular prisms; the second features a metallic tree-like structure in a blue-walled room with a reflective floor; and the third depicts a red animal figure, a black abstract shape, and a multi-faceted sphere in a purple container on a yellow surface. The middle row includes three constant width bodies (black, red, and blue) floating above a colorful checkered floor; a green shader ball with a square cavity inside a gray-walled room; and crystal-like structures in green, purple, and red on a reflective surface. The bottom row showcases a low-poly fox near a pink tree emitting particles on grassy terrain; a golden horse statue beside a heart-shaped object split into red and grey halves on a reflective surface; and a wicker basket, a banana and a bottle placed on a white platform." class="wp-image-1149130" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg 805w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-300x300.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-150x150.jpg 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-768x768.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-180x180.jpg 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-360x360.jpg 360w" sizes="auto, (max-width: 805px) 100vw, 805px" /><figcaption class="wp-element-caption">Figure 4. Rendered results of different 3D scenes generated by RenderFormer </figcaption></figure>



<p>RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.</p>



<figure class="wp-block-video aligncenter"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4"></video><figcaption class="wp-element-caption">3D animation sequence rendered by RenderFormer </figcaption></figure>



<h2 class="wp-block-heading" id="looking-ahead-opportunities-and-challenges">Looking ahead: Opportunities and challenges</h2>



<p>RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination—all without any hand-coded graphics computations.</p>



<p>However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI. </p>



<p>Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4" length="58187117" type="video/mp4" />

			</item>
		<item>
		<title>Breaking the networking wall in AI infrastructure </title>
		<link>https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</link>
		
		<dc:creator><![CDATA[Paolo Costa]]></dc:creator>
		<pubDate>Tue, 09 Sep 2025 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false"></guid>

					<description><![CDATA[<p>Datacenter memory and network limits are restraining AI system performance. MOSAIC uses microLEDs and a wide-and-slow optical architecture to deliver faster, longer, more reliable, and energy efficient connections that could transform AI cluster designs.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.</p>



<p>Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(< 2 meters)&nbsp;that&nbsp;restrict their use&nbsp;to within a single&nbsp;GPU&nbsp;rack. Optical&nbsp;fiber&nbsp;links&nbsp;can&nbsp;reach&nbsp;tens of meters,&nbsp;but&nbsp;they&nbsp;consume far more&nbsp;power&nbsp;and fail up to 100 times&nbsp;as often as&nbsp;copper. A&nbsp;team working across&nbsp;Microsoft&nbsp;aims&nbsp;to&nbsp;resolve&nbsp;this trade-off&nbsp;by&nbsp;developing&nbsp;MOSAIC,&nbsp;a novel optical link technology&nbsp;that&nbsp;can provide&nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&nbsp;<em>simultaneously</em>.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;</p>



<p>The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;<em>narrow-and-fast</em>&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;</p>



<p>These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;<em>per rack</em>. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;<em>networking wall</em>&nbsp;akin&nbsp;to the&nbsp;<em>memory wall</em>, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.</p>



<p class="has-text-align-left">A technology offering copper-like power efficiency and reliability over long distances can overcome this networking wall, enabling multi-rack scale-up domains and unlocking new architectures. This is a highly active R&D area, with many candidate technologies currently being developed across the industry. In our recent paper, <em>“<a href="https://www.microsoft.com/en-us/research/publication/mosaic-breaking-the-optics-versus-copper-trade-off-with-a-wide-and-slow-architecture-and-microleds/">MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs</a>”</em>, which received the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://conferences.sigcomm.org/sigcomm/2025/program/papers-info/" target="_blank" rel="noopener noreferrer">Best Paper award at ACM SIGCOMM<span class="sr-only"> (opens in new tab)</span></a>, we present one such promising approach that is the result of a multi-year collaboration between Microsoft Research, Azure, and M365. This work is centered around an optical wide-and-slow architecture, shifting from a small number of high-speed serial channels towards hundreds of parallel low-speed channels. This would be impractical to realize with today’s copper and optical technologies because of i) electromagnetic interference challenges in high-density copper cables and ii) the high cost and power consumption of lasers in optical links, as well as the increase in packaging complexity. MOSAIC overcomes these issues by leveraging directly modulated microLEDs, a technology originally developed for screen displays. </p>



<p>MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm×1 mm&nbsp;silicon&nbsp;die.&nbsp;</p>



<p>MOSAIC’s&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.</p>



<ul class="wp-block-list">
<li>Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.</li>



<li>By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper’s reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.</li>



<li>MicroLEDs’&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;</li>



<li>The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;</li>
</ul>



<p>Further,&nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&nbsp;factor&nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it’s&nbsp;fully compatible with today’s protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;</p>



<p>While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;</p>



<p>Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.</p>



<p>For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today’s&nbsp;large,&nbsp;multi-die packages to&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/good-things-come-in-small-packages-should-we-adopt-lite-gpus-in-ai-infrastructure/">smaller, more cost-effective, ones</a>.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/storage-class-memory-is-dead-all-hail-managed-retention-memory-rethinking-memory-for-the-ai-era/">novel memory technologies</a>.&nbsp;</p>



<p>Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Crescent library brings privacy to digital identity systems</title>
		<link>https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/</link>
		
		<dc:creator><![CDATA[Christian Paquin, Greg Zaverucha]]></dc:creator>
		<pubDate>Tue, 26 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148317</guid>

					<description><![CDATA[<p>Crescent helps make digital IDs private by preventing tracking across uses while letting users only disclose what’s necessary from their credentials.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/">Crescent library brings privacy to digital identity systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a computer chip, padlock icon, an avatar icon" class="wp-image-1148394" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&nbsp;</p>



<p>One of these risks is <em>linkability, </em>the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver&#8217;s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&nbsp;&nbsp;</p>



<p>To address this, we have released <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eprint.iacr.org/2024/2013" target="_blank" rel="noopener noreferrer">Crescent<span class="sr-only"> (opens in new tab)</span></a>, a cryptographic library that adds <em>unlinkability </em>to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver&#8217;s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. &nbsp;</p>



<p>The protection goes beyond existing privacy features. Some digital identity systems already offer <em>selective disclosure</em>, allowing users to share only specific pieces of information in each interaction. &nbsp;</p>



<p>But even with selective disclosure, credentials can still be linked through serial numbers, cryptographic signatures, or embedded identifiers. Crescent&#8217;s unlinkability feature is designed to prevent anything in the credential, beyond what a user explicitly chooses to reveal, from being used to connect their separate digital interactions.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="400" height="242" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability.png" alt="Figure 1: Unlinkability between a credential issuance and presentation" class="wp-image-1148323" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability.png 400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability-300x182.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability-240x145.png 240w" sizes="auto, (max-width: 400px) 100vw, 400px" /><figcaption class="wp-element-caption">Figure 1: Unlinkability between a credential issuance and presentation</figcaption></figure>



<h2 class="wp-block-heading" id="two-paths-to-unlinkability">Two paths to unlinkability&nbsp;</h2>



<p>To understand how Crescent works, it helps to examine the two main approaches researchers have developed for adding unlinkability to identity systems:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Specialized cryptographic signature schemes</strong>. These schemes can provide unlinkability but require extensive changes to existing infrastructure. New algorithms must be standardized, implemented, and integrated into software and hardware platforms. For example, the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://identity.foundation/bbs-signature/draft-irtf-cfrg-bbs-signatures.html" target="_blank" rel="noopener noreferrer">BBS<span class="sr-only"> (opens in new tab)</span></a> signature scheme is currently being standardized by the Internet Engineering Task Force (IETF), but even after completion, adoption may be slow.&nbsp;&nbsp;&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li><strong>Zero-knowledge proofs with existing credentials</strong>. This approach, used by <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/crescent-credentials" target="_blank" rel="noopener noreferrer">Crescent<span class="sr-only"> (opens in new tab)</span></a>, allows users to prove specific facts about their credentials without revealing the underlying data that could enable tracking. For example, someone could prove they hold a valid driver&#8217;s license and live in a particular ZIP code without exposing any other personal information or identifiers that could link this interaction to future ones.&nbsp;</li>
</ol>



<p>Zero-knowledge proofs have become more practical since they were first developed 40 years ago but they are not as efficient as the cryptographic algorithms used in today’s credentials. Crescent addresses this computational challenge through preprocessing, performing the most complex calculations once in advance so that later proof generation is quick and efficient for mobile devices.&nbsp;</p>



<p>Beyond unlinkability, Crescent supports selective disclosure, allowing users to prove specific facts without revealing unnecessary details. For example, it can confirm that a credential is valid and unexpired without disclosing the exact expiration date, which might otherwise serve as a unique identifier. These privacy protections work even when credentials are stored in a phone&#8217;s secure hardware, which keeps them tied to the device and prevents unauthorized access.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1002645">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: AI-POWERED EXPERIENCE</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/01/MSR-Chat-Promo.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft research copilot experience</h2>
				
								<p id="microsoft-research-copilot-experience" class="large">Discover more about research at Microsoft through our AI-powered experience</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-describedby="microsoft-research-copilot-experience" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft research copilot experience" target="_blank">
							Start now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="behind-the-cryptographic-curtain">Behind the cryptographic curtain&nbsp;</h2>



<p>At its core, Crescent uses a sophisticated form of cryptographic proof called a zero-knowledge SNARK (Zero-Knowledge Succinct Noninteractive Argument of Knowledge). This method allows one party to prove possession of information or credentials without revealing the underlying data itself.&nbsp;</p>



<p>Crescent specifically uses the Groth16 proof system, one of the first practical implementations of this technology. What makes Groth16 particularly useful is that its proofs are small in size, quick to verify, and can be shared in a single step without back-and-forth communication between the user and verifier.&nbsp;</p>



<p>The system works by first establishing shared cryptographic parameters based on a credential template. Multiple organizations issuing similar credentials, such as different state motor vehicle departments issuing mobile driver&#8217;s licenses, can use the same parameters as long as they follow compatible data formats and security standards.&nbsp;</p>



<p>The mathematical rules that define what each proof will verify are written using specialized programming tools that convert them into a Rank-1 Constraint System (R1CS), a mathematical framework that describes exactly what needs to be proven about a credential.&nbsp;</p>



<p>To make the system fast enough for real-world use, Crescent splits the proof generation into two distinct stages:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Prepare stage</strong>. This step runs once and generates cryptographic values that can be stored on the user&#8217;s device for repeated use.&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li><strong>Show stage</strong>. When a user needs to present their credential, this quicker step takes the stored values and randomizes them to prevent any connection to previous presentations. It also creates a compact cryptographic summary that reveals only the specific information needed for that particular interaction.&nbsp;</li>
</ol>



<p>Figures 2 and 3 illustrate this credential-proving workflow and the division between the prepare and show steps.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="453" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline.png" alt="Figure 2: Crescent’s credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier." class="wp-image-1148322" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-300x97.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-1024x331.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-768x249.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-240x78.png 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2: Crescent’s credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier. </figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="443" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview.png" alt="Figure 3: The Crescent presentation steps show the division between prepare and show steps." class="wp-image-1148321" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-300x95.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-1024x324.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-768x243.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-240x76.png 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3: The Crescent presentation steps show the division between prepare and show steps.</figcaption></figure>



<h2 class="wp-block-heading" id="a-sample-application">A sample application&nbsp;</h2>



<p>To demonstrate how Crescent works, we created a sample application covering two real-world scenarios: verifying employment and proving age for online access. The application includes sample code for setting up fictional issuers and verifiers as Rust servers, along with a browser-extension wallet for the user. The step numbers correspond to the steps in Figure 4.&nbsp;</p>



<h3 class="wp-block-heading" id="setup">Setup&nbsp;</h3>



<ol start="1" class="wp-block-list">
<li>A Crescent service pre-generates the zero-knowledge parameters for creating and verifying proofs from JSON Web Tokens and mobile driver’s licenses.&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li>The user obtains a mobile driver’s license from their Department of Motor Vehicles.&nbsp;</li>
</ol>



<ol start="3" class="wp-block-list">
<li>The user obtains a proof-of-employment JSON Web Token from their employer, Contoso.&nbsp;</li>
</ol>



<ol start="4" class="wp-block-list">
<li>These credentials and their private keys are stored in the Crescent wallet.&nbsp;</li>
</ol>



<h3 class="wp-block-heading" id="scenarios">Scenarios&nbsp;</h3>



<ol start="5" class="wp-block-list">
<li><strong>Employment verification</strong>: The user presents their JSON Web Token to Fabrikam, an online health clinic, to prove they are employed at Contoso and eligible for workplace benefits. Fabrikam learns that the user works at Contoso but not the user&#8217;s identity, while Contoso remains unaware of the interaction.&nbsp;</li>
</ol>



<ol start="6" class="wp-block-list">
<li><strong>Age verification</strong>:<strong> </strong>The user presents their mobile driver’s license to a social network, proving they are over 18. The proof confirms eligibility without revealing their age or identity.&nbsp;</li>
</ol>



<p>Across both scenarios, Crescent ensures that credential presentations remain unlinkable, preventing any party from connecting them to the user.&nbsp;</p>



<p>For simplicity, the sample defines its own issuance and presentation protocol, but it could be integrated into higher-level identity frameworks such as OpenID/OAuth, Verifiable Credentials, or the mobile driver’s license ecosystem.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="800" height="502" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch.png" alt="Figure 4. The sample architecture, from credential issuance to presentation." class="wp-image-1148404" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch.png 800w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-300x188.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-768x482.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-240x151.png 240w" sizes="auto, (max-width: 800px) 100vw, 800px" /><figcaption class="wp-element-caption">Figure 4. The sample architecture, from credential issuance to presentation.</figcaption></figure>



<p>To learn more about the project, visit the Crescent project <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/crescent-credentials/" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a> page, or check out our recent presentations given at the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/live/gnB76DQI1GE?t=3475s" target="_blank" rel="noopener noreferrer">Real-Word Crypto 2025<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=9IT659uUXfs&t=13361s" target="_blank" rel="noopener noreferrer">North Sec 2025<span class="sr-only"> (opens in new tab)</span></a> conferences. </p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/">Crescent library brings privacy to digital identity systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Applicability vs. job displacement: further notes on our recent research on AI and occupations</title>
		<link>https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/</link>
		
		<dc:creator><![CDATA[Kiran Tomlinson, Sonia Jaffe, Will Wang, Scott Counts, Siddharth Suri]]></dc:creator>
		<pubDate>Thu, 21 Aug 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148193</guid>

					<description><![CDATA[<p>Recently, we released a paper Working with AI: Measuring the Occupational Implications of Generative AI that studied what occupations might find AI chatbots useful, and to what degree. The paper sparked significant discussion, which is no surprise since people care deeply about the future of AI and jobs--that’s part of why we think it’s important to study these topics.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: a network structure with connected circles, an upward-trending line graph with bars and an arrow, and a checklist with horizontal lines and checkmarks." class="wp-image-1148296" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Recently, we released a paper&nbsp;(<em><a href="https://www.microsoft.com/en-us/research/publication/working-with-ai-measuring-the-occupational-implications-of-generative-ai/">Working with AI: Measuring the Occupational Implications of Generative AI</a></em>)&nbsp;that studied what occupations might&nbsp;find&nbsp;AI chatbots&nbsp;useful, and to what degree.&nbsp;The paper sparked significant discussion,&nbsp;which is no&nbsp;surprise&nbsp;since&nbsp;people care&nbsp;deeply&nbsp;about&nbsp;the future of AI and&nbsp;jobs&#8211;that’s part of why we think&nbsp;it’s&nbsp;important to study these&nbsp;topics.</p>



<p>Unfortunately, not all the&nbsp;discussion&nbsp;was&nbsp;accurate&nbsp;in its portrayal of the&nbsp;study’s scope or conclusions.&nbsp;Specifically, our&nbsp;study&nbsp;does not&nbsp;draw any conclusions about jobs being eliminated; in the paper,&nbsp;we&nbsp;explicitly&nbsp;cautioned&nbsp;against using our findings to make that conclusion.&nbsp;</p>



<p>Given the importance&nbsp;of this&nbsp;topic, we&nbsp;want&nbsp;to&nbsp;clarify any misunderstandings and&nbsp;provide&nbsp;a more digestible summary of the paper,&nbsp;our&nbsp;methodology,&nbsp;and its limitations.&nbsp;</p>



<h2 class="wp-block-heading" id="what-did-our-research-find">What&nbsp;did our research find?</h2>



<p>We set out to better understand how people are using AI, <strong>highlighting where AI might be useful in different occupations</strong>. To do this, we analyzed how people currently use generative AI—specifically Microsoft Bing Copilot (now Microsoft Copilot)—to assist with tasks. We then compared these sets of tasks against the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.onetcenter.org/overview.html" target="_blank" rel="noopener noreferrer">O*NET database<span class="sr-only"> (opens in new tab)</span></a>, a widely used occupational classification system, to understand potential applicability to various occupations.</p>



<p>We found&nbsp;that AI&nbsp;is most&nbsp;useful&nbsp;for&nbsp;tasks related to knowledge work and communication, particularly tasks such as writing, gathering information, and learning.</p>



<p>Those in occupations with these tasks&nbsp;may benefit by&nbsp;considering&nbsp;how AI&nbsp;can be used&nbsp;as a tool to help improve their workflows. On the&nbsp;flip side,&nbsp;it’s&nbsp;not surprising that physical tasks like performing surgeries or moving objects had less&nbsp;direct&nbsp;AI&nbsp;chatbot applicability.</p>



<p>So, to summarize, our paper is about&nbsp;identifying&nbsp;the occupations where&nbsp;AI may be most useful,&nbsp;by&nbsp;assisting&nbsp;or performing subtasks.&nbsp;&nbsp;Our data do&nbsp;not&nbsp;indicate, nor&nbsp;did&nbsp;we&nbsp;suggest, that certain jobs will be replaced by AI.</p>



<h2 class="wp-block-heading" id="methodological-limitations-are-acknowledged-and-important">Methodological limitations are acknowledged—and important</h2>



<p>The paper is transparent about the limitations of our approach.&nbsp;&nbsp;</p>



<p>We analyzed&nbsp;anonymized&nbsp;Bing Copilot conversations to see what&nbsp;activities&nbsp;users are seeking AI&nbsp;assistance&nbsp;with and what activities AI can perform when mapped to the O*NET database.&nbsp;While O*NET provides a structured list of&nbsp;activities&nbsp;associated with various occupations, it does&nbsp;<strong>not</strong>&nbsp;capture the full spectrum of skills, context, and nuance&nbsp;required&nbsp;in the real&nbsp;world.&nbsp;&nbsp;<strong>A job is far more than the collection of tasks that make&nbsp;it up.</strong></p>



<p>For example, a task might involve “writing reports,” but O*NET&nbsp;won’t&nbsp;reflect the interpersonal judgment, domain&nbsp;expertise, or ethical considerations that go into doing that well. The paper acknowledges this gap and warns against over-interpreting the AI applicability scores as measures of AI’s ability to perform an occupation.</p>



<p>Additionally, the dataset is based on user queries from Bing Copilot (from January – September 2024), which may be influenced by factors like awareness, access, or comfort with AI tools.&nbsp;&nbsp;Different people use different LLMs for different purposes and it also is&nbsp;very difficult&nbsp;(or&nbsp;nearly impossible) to&nbsp;determine&nbsp;what conversations are performed in a work context or for leisure.&nbsp;</p>



<p>Finally, we only evaluated AI chatbot usage, so this study does not evaluate the impact or applicability of other forms of AI.</p>



<h2 class="wp-block-heading" id="where-do-we-go-from-here-1">Where do we go from here?</h2>



<p>Given the intense interest in how AI will shape our collective future,&nbsp;it&#8217;s&nbsp;important we continue to study and better understand its societal and economic impact. As with&nbsp;all&nbsp;research on this topic,&nbsp;the findings&nbsp;are&nbsp;nuanced, and&nbsp;it’s&nbsp;important to pay attention to this nuance.&nbsp;</p>



<p>The public interest in our research is based, in large part, on the&nbsp;topic&nbsp;of AI&nbsp;and job displacement.&nbsp;However,&nbsp;our current&nbsp;methodology&nbsp;for this study&nbsp;is unlikely to lead to firm conclusions about this.&nbsp;&nbsp;AI may prove to be a useful tool for many occupations, and we believe the right balance lies in finding how to use the technology in a way that&nbsp;leverages&nbsp;its abilities while complementing human strengths and accounting for people&#8217;s preferences.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>For more information from Microsoft on the future of work and AI skilling, check out Microsoft’s Annual&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/annual-work-trend-index-2025/" target="_blank" rel="noopener noreferrer">Work Trend Index<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blogs.microsoft.com/on-the-issues/2025/07/09/elevate/" target="_blank" rel="noopener noreferrer">Microsoft Elevate<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</title>
		<link>https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/</link>
		
		<dc:creator><![CDATA[Peter Lee, Carey Goldberg, Dr. Isaac Kohane]]></dc:creator>
		<pubDate>Thu, 21 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148218</guid>

					<description><![CDATA[<p>For the series finale, Peter Lee, Carey Goldberg, and Dr. Zak Kohane compare their predictions to insights from the series’ most recent guests, including experts on AI’s economic and societal impact, leaders in AI-driven medicine, and doctors in training.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/">Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated headshots of Carey Goldberg, Peter Lee, and Dr. Isaac Kohane." class="wp-image-1148279" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=147919496&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this series finale, Lee welcomes back coauthors <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.careygoldberg.net/" target="_blank" rel="noopener noreferrer">Carey Goldberg<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dbmi.hms.harvard.edu/people/isaac-kohane" target="_blank" rel="noopener noreferrer">Dr. Zak Kohane<span class="sr-only"> (opens in new tab)</span></a> to discuss how their predictions stack up against key takeaways from guests in the second half of the series: experts on AI’s economic and societal impact; technologists on the cutting edge; leaders in AI-driven medicine; next-generation physicians; and heads of healthcare organizations. Lee, Goldberg, and Kohane explore thinking innovatively about existing healthcare processes, including the structure of care teams and the role of specialties, to take advantage of AI opportunities and consider what clinicians and patients might need these new AI tools to be to feel empowered when it comes to giving and receiving the best healthcare. They close the episode with their hopes for the future of AI in health.</p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/scalable-emulation-of-protein-equilibrium-ensembles-with-generative-deep-learning/">Scalable emulation of protein equilibrium ensembles with generative deep learning</a>&nbsp;<br>Publication | July 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/sequential-diagnosis-with-language-models/">Sequential Diagnosis with Language Models</a>&nbsp;<br>Publication | July 2025</li>



<li><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/05/19/developing-next-generation-cancer-care-management-with-multi-agent-orchestration/" target="_blank" rel="noreferrer noopener">Developing next-generation cancer care management with multi-agent orchestration</a>&nbsp;<br>Microsoft Industry Blogs | May 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/">The AI Revolution in Medicine: GPT-4 and Beyond</a><br>Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;</li>
</ul>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC] </p>



<p>[BOOK PASSAGE]&nbsp;</p>



<p><strong>PETER LEE: </strong>“As a society—indeed, as a species—we have a choice to make. Do we constrain or even kill artificial intelligence out of fear of its risks and obvious ability to create new harms? Do we submit ourselves to Al and allow it to freely replace us, make us less useful and less needed? Or do we start, today, shaping our Al future together, with the aspiration to accomplish things that humans alone, and Al alone, can&#8217;t do but that humans+Al can? The choice is in our hands … .”&nbsp;</p>



<p>[END OF BOOK PASSAGE]</p>



<p>[THEME MUSIC]</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. </p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? </p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here. </p>



				</span>
				<span id="show-more-show-less-toggle-2" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The book passage I read at the top is from the epilogue, and I think it’s a truly fitting closing sentiment for the conclusion of this podcast series—because it calls back to the very beginning.</p>



<p>As I’ve mentioned before, Carey, Zak, and I wrote <em>The AI Revolution in Medicine</em> as a guide to help answer these big questions, particularly as they pertain to medicine. You know, we wrote the book to empower people to make a choice about AI’s development and use. Well, have they? Have <em>we</em>?</p>



<p>Perhaps we’ll need more time to tell. But over the course of this podcast series, I’ve had the honor of speaking with folks from across the healthcare ecosystem. And my takeaway? They’re all committed to shaping AI into a tool that can improve the industry for practitioners and patients alike.</p>



<p>In this final episode, I’m thrilled to welcome back my coauthors, Carey Goldberg and Dr. Zak Kohane. We’ll examine the insights from the second half of the season.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Carey, Zak—it’s really great to have you here again!&nbsp;</p>



<p><strong>CAREY</strong> <strong>GOLDBERG: </strong>Hey, Peter!&nbsp;</p>



<p><strong>ZAK</strong> <strong>KOHANE:</strong> Hi, Peter.&nbsp;</p>



<p><strong>LEE:</strong> So this is the second roundtable. And just to recap, you know, we had several early episodes of the podcast where we talked to some doctors, some technology developers, some people who think about regulation and public policy, patient advocates, a venture capitalist who invests in, kind of, consumer and patient-facing medical ventures, and some bioethicists.&nbsp;</p>



<p>And I think we had a great conversation there. I think, you know, it felt mostly validating. A lot of the things that we predicted might happen happened, and then we learned a lot of new things. But now we have five more episodes, and the mix of kinds of people that we talk to here is different than the original.&nbsp;</p>



<p>And so I thought it would be great for us to have a conversation and recap what we think we heard from all of them. So let&#8217;s just start at the top.&nbsp;</p>



<p>So in this first episode in the second half of this podcast series, we talked to economists Azeem Azhar and Ethan Mollick. And I thought those conversations were really interesting. Maybe there were, kind of, two things, two main topics. One was just the broader impact on the economy, on the cost of healthcare, on overall workforce issues.&nbsp;</p>



<p>One of the things that I thought was really interesting was something that Ethan Mollick brought up. And maybe just to refresh our memories, let&#8217;s play this little clip from Ethan.<strong>&nbsp;</strong></p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-ab782358e1fff73901dd881b85e04b33"><strong><em>ETHAN MOLLICK: </em></strong><em>So</em><strong><em> </em></strong><em>we’re in this really interesting period where there’s incredible amounts of individual innovation in productivity and performance improvements in this field, like very high levels of it. …</em><em> </em><em>We’re seeing that in nonmedical problems, the same kind of thing, which is, you know, we’ve got research showing 20 and 40% performance improvements. … But then the organization doesn’t capture it; the system doesn’t capture it. Because the individuals are doing their own work, and the systems don’t have the ability to, kind of, learn or adapt as a result.</em>&nbsp;</p>



<p><strong>LEE:</strong> So let me start with you, Zak. Does that make sense to you? Are you seeing something similar?&nbsp;</p>



<p><strong>KOHANE:</strong> I thought it was incredibly insightful because we discussed on our earlier podcast how a chief AI officer in one of the healthcare hospitals, in one of the healthcare systems, was highly regulating the use of AI, but yet in her own practice on her smartphone was using all these AI technologies.&nbsp;</p>



<p>And so it&#8217;s insightful that on the one hand, she is increasing her personal productivity, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> … and perhaps she&#8217;s increasing her quality of her care. But it&#8217;s very hard for the healthcare system to actually realize any gains. It&#8217;s unlikely … let&#8217;s put it this way. It would be for her a defeat if they said, “Now you should see <em>more</em> patients.”&nbsp;</p>



<p><strong>LEE:</strong> Yes. [LAUGHS]&nbsp;</p>



<p><strong>KOHANE:</strong> Now, I&#8217;m not saying that won&#8217;t happen. It could happen. But, you know, gains of productivity are really at the individual level of the doctors. And that&#8217;s why they&#8217;re adopting it. That&#8217;s why the ambient dictation tools are so successful. But really turning it into things that matter in terms of productivity for healthcare, namely making sure that patients are getting healthy, requires that every piece of the puzzle works well together. You know, it&#8217;s well-tread ground to talk about how patients get very expensive procedures, like a cardiac transplant, and then go home, and they’re not put on blood thinners …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> … and then they get a stroke. You know, the chain is as strong as the weakest link. And just having AI in one part of it is not going to do it. And so hospitals, I think, are doubly burdened by the fact that, (A) they tend to not like innovation because they are high-revenue, low-margin companies. But if they want it implemented effectively, they have to do it across the entire processes of healthcare, which are vast and not completely under their control.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Yep. You know, that was Sara Murray, who&#8217;s the chief health AI officer at UC San Francisco.&nbsp;</p>



<p>And then, you know, Carey, remember, we were puzzled by Chris Longhurst&#8217;s finding in a controlled study that the, you know, having an AI respond to patient emails didn&#8217;t seem to lead to any, I guess you would call it, <em>productivity benefits</em>. I remember we were both kind of puzzled by that. I wonder if that&#8217;s related to what Ethan is saying here.&nbsp;</p>



<p><strong>GOLDBERG:</strong> I mean, possibly, but I think we&#8217;ve seen since then that there have been multiple studies showing that in fact using AI can be extremely effective or helpful, even, for example, for diagnosis.&nbsp;</p>



<p>And so I find just from the patient point of view, it kind of drives me crazy that you have individual physicians using AI because they know that it will improve the care that they&#8217;re offering. And yet you don&#8217;t have their institutions kind of stepping up and saying, “OK, these are the new norms.”&nbsp;</p>



<p>By the way, Ethan Mollick is a national treasure, right. Like, he is the classic example of someone who just stepped up at this moment …&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… when we saw this extraordinary technological advance. And he&#8217;s not only stepping up for himself. He&#8217;s spreading the word to the masses that this is what these things can do.&nbsp;</p>



<p>And so it&#8217;s frustrating to see the institutions not stepping up and instead the individual doctors having to do it.&nbsp;</p>



<p><strong>KOHANE:</strong> But he made another very interesting point, which was that the reason that <em>he</em> could be so informative to not only the public but practitioners of AI is these things would emerge out of the shop, and they would not be aged too long, like a fine wine, before they were just released to the public.&nbsp;</p>



<p>And so he was getting exposure to these models just weeks after some of the progenitors had first seen it. And therefore, because he&#8217;s actually a really creative person in terms of how he exercises models, he sees uses and problems very early on. But the point is institutions, think about how much they are disadvantaged. They&#8217;re not Ethan Mollick. They&#8217;re not the progenitors. So they&#8217;re even further behind. So it&#8217;s very hard. If you talk to most of the C-suite of hospitals, they&#8217;d be delighted to know as much about the impact as Ethan Mollick.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. By the way, you know, I picked out this quote because within Microsoft, and I suspect every other software company, we&#8217;re seeing something very similar, where individual programmers are 20 to 30% more productive just in the number of lines of code they write per day or the number of pull requests per week. Any way you measure it, it&#8217;s very consistent. And yet by the time you get to, say, a 25-person software engineering team, the productivity of that whole team isn&#8217;t 25% more productive.&nbsp;</p>



<p>Now, that <em>is</em> starting to change because we&#8217;re starting to figure out that, well, maybe we should reshape how the team operates. And there&#8217;s more of an orientation towards having, you know, smaller teams of full-stack developers. And then you start to see the gains. But if you just keep the team organized in the usual way, there seems to be a loss. So there&#8217;s something about what Ethan was saying that resonated very strongly with me.&nbsp;</p>



<p><strong>GOLDBERG: </strong>But I would argue that it&#8217;s not just productivity we&#8217;re talking about. There&#8217;s a moral imperative to improve the care. And if you have tools that will do that, you should be using them or trying harder to.&nbsp;</p>



<p><strong>LEE:</strong> Right. Yep.&nbsp;</p>



<p><strong>KOHANE:</strong> I think, yes, first of all, absolutely you would. Unfortunately, most of the short-term productivity measures will not measure improvements in the quality of care because it takes a long time to die even with bad care.&nbsp;</p>



<p>And so that doesn&#8217;t show up right away. But I think what Peter just said actually came across in several of the podcasts, which is that it&#8217;s very tricky trying to shoehorn these things into making what we&#8217;re already doing more productive.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. Existing structures.&nbsp;</p>



<p><strong>KOHANE:</strong> Yeah. And I know, Carey, that you&#8217;ve raised this issue many times. But it really calls into question, what should we be doing with our time with doctors? And they are a scarce resource. And what is the most efficient way to use them?&nbsp;</p>



<p>You know, I remember we [<em>The New England Journal of Medicine AI</em>] published a paper of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIoa2400296" target="_blank" rel="noopener noreferrer">someone who was able to use AI to increase the throughput of their emergency room<span class="sr-only"> (opens in new tab)</span></a> by actually more appropriately having the truly sick people in the sick queue, in the triage queue, for urgent care.&nbsp;</p>



<p>And so I think we&#8217;re going to have to think that way more broadly, about we don&#8217;t have to now look at every patient as an unknown with maybe a few pointers on diagnosis. We can have a fairly extensive profiling.&nbsp;</p>



<p>And I know that colleagues in Clalit [Health Services] in Israel, for example, are using the overall trajectory of the patient and some considerations about utilities to actually figure out who to see next week.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, you know, what you said brings up another maybe connection to one thing that we see also in software development. And it relates to also what we were discussing earlier: about the last thing a doctor wants is to have a tool that allows them to see even yet more patients per day.&nbsp;</p>



<p>So in software development, there&#8217;s always this tension. Like, how many lines of code can you write per day? That&#8217;s one productivity measure.&nbsp;</p>



<p>But sometimes we&#8217;re taught, well, don&#8217;t write more lines of code per day, but make sure that your code is well structured. Take the time to document it. Make sure it&#8217;s fully commented. Take the time to talk to your fellow software engineering team members to make sure that it&#8217;s well coordinated. And in the long run, even if you&#8217;re writing half the number of lines of code per day, the software process will be far more efficient.</p>



<p>And so I&#8217;ve wondered whether there&#8217;s a similar thing where doctors could see 20% fewer patients in a day, but if they take the time and also had AI help to coordinate, maybe a patient&#8217;s journey might be half as long. And therefore, the health system would be able to see twice as many patients in a year&#8217;s period or something like that.&nbsp;</p>



<p><strong>KOHANE:</strong> So I think you&#8217;ve “nerd sniped” me because you [LAUGHTER]—which is all too easy—but I think there&#8217;s a central issue here. And I think this is the stumbling block between what Ethan&#8217;s telling us about between the individual productivity and the larger productivity, is the <em>team&#8217;s</em> productivity.&nbsp;</p>



<p>And there is actually a good analogy in computer science and that&#8217;s, uh, Brooks’s “mythical man-month,” &#8230;&nbsp;</p>



<p><strong>LEE:</strong> Yes, exactly.&nbsp;</p>



<p><strong>KOHANE:</strong> … where he shows how you can have more and more resources, but when the coordination starts failing, because you have so many, uh, individuals on the team, you start falling apart. And so even if the, uh, individual doctors get that much better, yeah, they take better care of patients, make less stupid things.&nbsp;</p>



<p>But in terms of giving the “I get you into the emergency room, and I get you out of a hospital as fast as possible, as safely as possible, as effectively as possible,” that&#8217;s teamwork. And we don&#8217;t do it. And we&#8217;re not really optimizing our tools for that.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And just to throw in a little reality check, I&#8217;m not aware of <em>any</em> indication yet that AI is in any way shortening medical journeys or making physicians more efficient. Yet …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>GOLDBERG:</strong> …<strong> </strong>at least. Yeah.&nbsp;</p>



<p><strong>LEE:</strong> Yes. So I think, you know, with respect to our book, critiquing our book, you know, I think it&#8217;s fair to say we were fairly focused or maybe even fixated on the individual doctor or nurse or patient, and we didn&#8217;t really, at least I never had a time where I stepped back to think about the whole care coordination team or the whole health system.&nbsp;</p>



<p><strong>KOHANE: </strong>And I think that&#8217;s right. It&#8217;s because, first of all, <em>you</em> weren’t thinking about it? It&#8217;s not what we&#8217;re taught in medical school. We&#8217;re not taught to talk about team communication excellence. And I think it&#8217;s absolutely essential.&nbsp;</p>



<p>There’s a … what’s the … there was an early … [Terry] Winograd. And he was trying to capture what are the different kinds of actions related to pronouncements that you could expect and how could AI use that. And that was beginning to get at it.&nbsp;</p>



<p>But I actually think this is dark matter of human organizational technology that is not well understood. And our products don&#8217;t do well. You know, we can talk about all the groupware things that are out there. But they all don&#8217;t quite get to that thing.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> And I can imagine an AI serving as a team leader, a really active team leader, a real quarterback of, let&#8217;s say, a care team.&nbsp;</p>



<p><strong>LEE:</strong> Well, in fact, you know, we have been trying to experiment with this. My colleague, Matt Lungren, who was also one of the interviewees early on, has been <a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/05/19/developing-next-generation-cancer-care-management-with-multi-agent-orchestration/" target="_blank" rel="noreferrer noopener">working with Stanford Medicine on a tumor board AI agent</a>—something that would facilitate tumor board meetings.&nbsp;</p>



<p>And the early experiences are pretty interesting. Whether it relates to efficiency or productivity I think remains to be seen, but it does seem pretty interesting.&nbsp;</p>



<p>But let&#8217;s move on.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Well, actually, Peter, …&nbsp;</p>



<p><strong>LEE: </strong>Oh, go ahead.&nbsp;</p>



<p><strong>GOLDBERG: </strong>…<strong> </strong>if you&#8217;re willing to not quite move on yet …&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] All right.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… this kind of segues into one of, I think, the most provocative questions that arose in the course of these episodes and that I&#8217;d love to have you answer, which was, remember, it was a question at a gathering that you were at, and you were asked, “Well, you&#8217;re focusing a lot on potential AI effects on individual patient and physician experiences. But what about the revolution, right? What about, like, can you be more big-picture and envision how generative AI could actually, kind of, overturn or fix the broken system, right?”&nbsp;</p>



<p>I&#8217;m sure you&#8217;ve thought about that a lot. Like, what&#8217;s your answer?&nbsp;</p>



<p><strong>LEE:</strong> You know, I think ultimately, it will have to. For it to really make a difference, I think that the normal processes, our normal concept of how healthcare is delivered—how new medical discoveries are made and brought into practice—I think those things are going to have to change a lot.&nbsp;</p>



<p>You know, one of the things I think about a lot right at the moment is, you know, we tend to think about, let&#8217;s say, medical diagnosis as a problem-solving exercise. And I think, at least at the Kaiser Permanente School of Medicine, the instruction really treats it as a kind of detective thing based on a lot of knowledge about biology and biomedicine and human condition, and so on.&nbsp;</p>



<p>But there&#8217;s another way to think about it, given AI, which is when you see a patient and you develop some data, maybe through a physical exam, labs, and so on, you can just simply ask, “You know, what did the 500 other people who are most similar to this experience, how were they diagnosed? How were they treated? What were their outcomes? What were their experiences?”&nbsp;</p>



<p>And that&#8217;s really a fundamentally different paradigm. And it just seems like at least the technical means will be there. And by the way, that also then relates to [the questions]: “And what was most efficacious cost-wise? What was most efficient in terms of the total length of the patient journey? How does this relate to my quality scores so I can get more money from Medicare and Medicaid?”&nbsp;</p>



<p>All of those things, I think, you know, we&#8217;re starting to confront.&nbsp;</p>



<p>One of the other episodes that we&#8217;re going to talk about, was my interview with two medical students. Actually, thinking of a Morgan Cheatham as just a medical student or medical resident [LAUGHTER] is a little strange. But he is.&nbsp;</p>



<p>One of the things he talks about is the importance that he placed in his medical training about adopting AI. So, Zak, I assume you see this also with some students at Harvard Medical School. And the other medical student we interviewed, Daniel Chen, seemed to indicate this, too, where it seems like it&#8217;s the students who are bringing AI into the medical education ahead of the faculty. Does that resonate with you?&nbsp;</p>



<p><strong>KOHANE:</strong> It absolutely resonates with me. There are students I run into who, honestly, my first thought when I&#8217;m talking to them is, why am I teaching you [LAUGHTER], and why are you not starting a big AI company, AI medicine company, now and really change healthcare instead of going through the rest of the rigmarole? And I think broadly, higher education has a problem there, which is we have not embraced, again, going back to Ethan, a lot of the tools that can be used. And it&#8217;s because we don&#8217;t know necessarily the right way to teach them. And so far, the only lasting heuristic seems to be: use them and use them often.&nbsp;</p>



<p>And so it&#8217;s an awkward thing, where the person who knows how to use the AI tools now in the first-year medical school can teach themselves better and faster than anybody else in their class who is just relying on the medical school curriculum.&nbsp;</p>



<p><strong>LEE: </strong>Now, the reason I brought up Morgan now after our discussion with Ethan Mollick is Morgan also talked about AI collapsing medical specialties.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes.&nbsp;</p>



<p><strong>LEE:</strong> And so let&#8217;s hear this snippet from him.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-df1ec79bf2184e924d149dd5a8bfd566"><strong><em>MORGAN CHEATHAM:</em></strong><em> AI collapses medical specialties onto themselves, right. You have the canonical example of the cardiologist, you know, arguing that we should diuresis and maybe the nephrologist arguing that we should, you know, protect the kidneys. And how do two disciplines disagree on what is right for the patient when in theory, there is an objective best answer given that patient’s clinical status? … So I’m interested in this question of whether medical specialties themselves need to evolve. And if we look back in the history of medical technology, there are many times where a new technology forced a medical specialty to evolve.</em></p>



<p><strong>LEE:</strong> So on the specific question about specialties, Zak, do you have a point of view? And let me admit, first of all, for us, all three of us, we didn&#8217;t have any clue about this in our book. I don&#8217;t think.&nbsp;</p>



<p><strong>KOHANE:</strong> Not much. Not much of a clue.&nbsp;</p>



<p>So I&#8217;m reminded of a <em>New Yorker</em> cartoon where you see a bunch of surgeons around the patient, and someone says, “Is that a spleen?” And it says, “I don&#8217;t know. I slept during the spleen lecture,” [LAUGHTER] and &#8230; or “I didn&#8217;t take the spleen course.”&nbsp;</p>



<p>And yet when we measure things, we measure things much more than we think we are doing. So for example, we [<em>NEJM AI</em>] just published a paper where echocardiograms were being done. And it turns out those ultrasound waves just happen to also permeate the liver. And <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIoa2400948" target="_blank" rel="noopener noreferrer">you can actually diagnose on the way with AI all the liver disease<span class="sr-only"> (opens in new tab)</span></a> that is in—and treatable liver disease—that&#8217;s in those patients.&nbsp;</p>



<p>But if you&#8217;re a cardiologist, “Liver? You know, I slept through liver lecture.” [LAUGHTER] And so I do think that, (A) the natural, often guild/dollar-driven silos in medicine are less obvious to AI, despite the fact that they do exist in departments and often in chapters.&nbsp;</p>



<p>But Morgan&#8217;s absolutely right. I can tell you as an endocrinologist, if I have a child in the ICU, the endocrinologist, the nephrologist, and the neurosurgeon will argue about the right thing to do.&nbsp;</p>



<p>And so in my mind, the truly revolutionary thing to do is to go back to 1994 with Pete Szolovits, the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dspace.mit.edu/bitstream/handle/1721.1/149765/MIT-LCS-TR-604.pdf?sequence=1" target="_blank" rel="noopener noreferrer">Guardian Angel Project<span class="sr-only"> (opens in new tab)</span></a>. What I think you need is a process. And the process is the quarterback. And the quarterback has only one job: take care of the patient.&nbsp;</p>



<p>And it should be thinking all the time about the patient. What&#8217;s the right thing? And can be as school-marmish or not about, “Zak, you&#8217;re eating this or that or exercise or sleep,” but also, “Hey, surgeons and endocrinologists, you&#8217;re talking about my host, Zak. This is the right way because this problem and this problem and our best evidence is this is the right way to get rid of the fluid. The other ways will kill him.”</p>



<p>And I think you need an authoritative quarterback that has the view of the others but then makes the calls.&nbsp;</p>



<p><strong>LEE:</strong> Is that quarterback going to be AI or human?&nbsp;</p>



<p><strong>KOHANE:</strong> Well, for the very lucky people, it&#8217;ll be a human augmented by AI, <em>super concierge</em>.&nbsp;</p>



<p>But I think we&#8217;re running out of doctors. And so realistically, it&#8217;s going to be an AI that will have to be certified in very different ways, along the ways Dave Blumenthal says, essentially, trial by fire. Like putting residents into clinics, we&#8217;re going to be putting AIs into clinics.&nbsp;</p>



<p>But what&#8217;s worse, by the way, than the three doctors arguing about care in front of the patient is, what happens so frequently, is then you see them outpatient, and each one of them gives you a different set of decisions to make. Sometimes that actually interact pathologically, unhealthily with each other. And only the very smart nurses or primary care physicians will actually notice that and call, quote, a “family meeting,” or bring everybody in the same room to align them.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, I think this idea of quarterback is really very, very topical right now because there&#8217;s so much intensity in the AI space around agents. And in fact, you know, the Microsoft AI team under Mustafa Suleyman and Dominic King, Harsha Nori, and team just recently posted <a href="https://www.microsoft.com/en-us/research/publication/sequential-diagnosis-with-language-models/" target="_blank" rel="noreferrer noopener">a paper on something called sequential diagnosis</a>, which is basically an AI quarterback that is supposed to smartly consult with other AI specialties. And interestingly, one of the AI agents is sort of the devil&#8217;s advocate that&#8217;s always criticizing and questioning things. </p>



<p><strong>GOLDBERG: </strong>That’s interesting.&nbsp;</p>



<p><strong>LEE:</strong> And at least on very, very hard, rare cases, it can develop some impressive results. There&#8217;s something to this that I think is emerging.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And, Peter, Morgan said something that blew me away even more, which was, well, why do we even need specialists if the reason for a specialist is because there&#8217;s so much medical knowledge that no single physician can know all of it, and therefore we create specialists, but that limitation does not exist for AI.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And so there he was kind of undermining this whole elaborate structure that has grown up because of human limitations that may not ultimately need to be there.&nbsp;</p>



<p><strong>LEE: </strong>Right. So now that gives me a good segue to get back to our economist and get to something that Azeem Azhar said. And so there&#8217;s a clip here from Azeem.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-6ff46028707e465b3cd8582210840bd0"><strong><em>AZEEM AZHAR: </em></strong><em>We didn’t talk about, you know, AI in its ability to potentially do this, which is to extend the clinician’s presence throughout the week. <em>You know, t</em>he idea that maybe some part of what the clinician would do if you could talk to them on Wednesday, Thursday, and Friday could be delivered through an app or a chatbot just as a way of encouraging the compliance, which is often, especially with older patients, one reason why conditions, you know, linger on for longer.</em>&nbsp;</p>



<p><strong>LEE:</strong> And, you know, in the same conversation, he also talked about his own management of asthma and the fact that he&#8217;s been managing this for several decades and knows more than any other human being, no matter how well medically trained, could possibly know. And it&#8217;s also very highly personalized. And it&#8217;s not a big leap to imagine AI having that sort of lifelong understanding.&nbsp;</p>



<p><strong>KOHANE:</strong> So in fact, I want to give credit back to our book since you insulted us. [LAUGHTER] You challenged us. You doubted us. We do have at the end of the book a AI which is helping this woman manage her way through life. It&#8217;s quarterbacking for the woman all these different services.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE:</strong> So there.&nbsp;</p>



<p><strong>LEE: </strong>Ah, you&#8217;re right. Yes. In fact, it&#8217;s very much, I think, along the lines of the vision that Azeem laid out in our conversation.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. It also reminded me of the piece <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.wbur.org/news/2017/06/16/managing-mom-weight-algorithm" target="_blank" rel="noopener noreferrer">Zak wrote about his mother<span class="sr-only"> (opens in new tab)</span></a> at one point when she was managing congestive heart failure and she needed to watch her weight very carefully to see her fluid status. And absolutely, there&#8217;s no … I see no reason whatsoever why that couldn&#8217;t be done with AI right now. Actually, although back then, Zak, you were writing that it takes much more than an AI [LAUGHS] to manage such a thing, right?&nbsp;</p>



<p><strong>KOHANE:</strong> You need an AI that you can trust. Now, my mother was born in 1927, and she&#8217;d learned through the school of hard knocks that you can&#8217;t trust too many people, maybe even not your son, <em>MD</em>, <em>PhD</em> [LAUGHTER].&nbsp;</p>



<p>But what I&#8217;ve been surprised [by] is how, for example, how many people are willing to trust and actually see effective use of AI as mental health counselors, for example.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah&nbsp;</p>



<p><strong>KOHANE:</strong> So it may in fact be that there&#8217;s a generational thing going on, and at least there&#8217;ll be some very large subset of patients which will be completely comfortable in ways that my mother would have never tolerated.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Now, I think we&#8217;re starting to veer into some of the core AI.&nbsp;</p>



<p>And so I think maybe one of the most fun conversations I had was in the episode with both Sébastien Bubeck, my former colleague at Microsoft Research, and now he&#8217;s at OpenAI, and Bill Gates. And there was so much that was, I thought, interesting there. And there was one point, I think that sort of touches tangentially on what we were just conversing about, that Sébastien said. So let&#8217;s hear this snippet.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-19a223099867a12ed2599ae811939b43"><strong><em>SÉBASTIEN BUBECK: </em></strong><em>And one example that I really like, a study that recently appeared where … they were comparing doctors without and with ChatGPT. … So this was a set of cases where the accuracy of the doctors alone was around 75%. ChatGPT alone was 90%. … But then the kicker is that doctors with ChatGPT was 80%. Intelligence alone is not enough. It’s also how it’s presented, how you interact with it. And ChatGPT, it’s an amazing tool. Obviously, I absolutely love it. But it’s not … you don’t want a doctor to have to type in, you know, prompts and use it that way. It should be, as Bill was saying, kind of running continuously in the background, sending you notifications.</em></p>



<p><strong>LEE:</strong> So I thought Sébastien was saying something really profound, but I haven&#8217;t been able to quite decide or settle in my mind what it is. What do you make of what Seb just said?&nbsp;</p>



<p><strong>KOHANE:</strong> I think it&#8217;s context. I think that it requires an enormous amount of energy, brain energy, to actually correctly provide the context that you want this thing to work on. And it&#8217;s only going to really feel like we&#8217;re in a different playing field when it&#8217;s listening all the time, and it just steps right in.&nbsp;</p>



<p>There is an advantage that, for example, a good programmer can have in prompting Cursor or any of these tools to do so. But it takes effort. And I think being in the conversation all the time so that you understand the context in the widest possible way is incredibly important. And I think that&#8217;s what Seb is getting at, which is if we spoon feed these machines, yes, 90%.&nbsp;</p>



<p>But then, talking to a human being who then has to interact and gets distracted from whatever flow they&#8217;re in and maybe even makes them feel like an early bicycle rider who all of a sudden realizes, “I&#8217;m balancing on two wheels—oh no!” And they fall over. You know, there&#8217;s that interaction which is negatively synergistic.&nbsp;</p>



<p>And so I do think it&#8217;s a very hard human-computer engineering problem. How do we make these two agents, human and computational, work in an ongoing way in the flow? I don&#8217;t think I&#8217;m seeing anything that&#8217;s particularly new. And the things that you&#8217;re beginning to hint about, Peter, in terms of agentic coordination, I think we&#8217;ll get to some of that. </p>



<p><strong>LEE:</strong> Yeah. Carey, does this give you any pause? The kind of results that … they&#8217;re puzzling results. I mean, the idea of doctors with AI seeming at least in this one test—it&#8217;s just one test—but it&#8217;s odd that it does worse than the AI alone.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes. I would want to understand more about the actual conditions of that study.&nbsp;</p>



<p>From what Bill Gates said, I was most struck by the question of resource-poor environments. That even though this was absolutely one of the most promising, brightest perspectives that we highlighted in the book, we still don&#8217;t seem to be seeing a lot of use among the one half of humanity that lacks decent access to healthcare.&nbsp;</p>



<p>I mean, there are access problems everywhere, including here in the United States. And it is one of the most potentially promising uses of AI. And I thought if anyone would know about it, he would with the work that the Gates Foundation does.&nbsp;</p>



<p><strong>LEE: </strong>You know, I think both you and Bill, I felt, are really simpatico. You know, Bill expressed genuine surprise that more isn&#8217;t happening yet. And it really echoed, in fact, maybe even using some of the exact same words that you&#8217;ve used. And so two years on, you&#8217;ve expressed repeatedly expecting to have seen more out in the field by now. And then I thought Bill was saying something in our conversation very similar.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p><strong>LEE: </strong>You know, for me, I see it both ways. I see the world of medicine really moving fast in confronting the reality of AI in such a serious way. But at the same time, it&#8217;s also hard to escape the feeling that somehow, we should be seeing even more.&nbsp;</p>



<p>So it&#8217;s an odd thing, a little bit paradoxical.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I think one thing that we didn&#8217;t focus on hardly at all in the book but that we are seeing is these companies rising up, stepping up to the challenge, Abridge and OpenEvidence, and what Morgan describes as a new stack, right.&nbsp;</p>



<p>So there is that on the flip side.&nbsp;</p>



<p><strong>LEE: </strong>Now, I want to get back to this thing that Seb was saying. And, you know, I had to bring up the issue of sycophancy, which we discussed at our last roundtable also. But it was particularly … at the time that Seb, Bill, and I had our conversation, OpenAI had just gone through having to retract a fresh update of GPT-4o because it had become too sycophantic.&nbsp;</p>



<p>So I can&#8217;t escape the feeling that some of these human-computer interaction issues are related to this tension between you want AI to follow your directions and be faithful to you, but at the same time not agree with you so often that it becomes a fault.&nbsp;</p>



<p><strong>KOHANE:</strong> I think it&#8217;s asking the AI to enter into a fundamental human conundrum, which is there are extreme versions of doublethink, and there&#8217;s everyday things, everyday asks of doublethink, which is how to be an effective citizen.&nbsp;</p>



<p>And even if you&#8217;re thinking, “Hmm. I&#8217;m thinking this. I&#8217;m just not going to say it because that would be rude or counterproductive.” Or some of the official doublethinks, where you&#8217;re actually told you must say this, even if you think something else. And I think we&#8217;re giving a very tough mission for these things: be nice to the user and be useful.&nbsp;</p>



<p>And, in education, where the thing is not always one in the same. Sometimes you have to give a little tough love to educate someone, and doing that well is both an art and it&#8217;s also very difficult. And so, you know, I&#8217;m willing to believe that the latest frontier models that have made the news in the last month are very high-performing, but they&#8217;re also all highlighting that tension …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>… that tension between behaving like a good citizen and being helpful. And this gets back to what are the fundamental values that we hope these things are following.&nbsp;</p>



<p>It&#8217;s not, you know, “Are these things going to develop us into the paperclip factory?” It&#8217;s more of, “Which of our values are going to be elevated, and which one will be suppressed?”&nbsp;</p>



<p><strong>LEE: </strong>Well, since I criticized our book before, let me pat ourselves on the back this time because, I think, pervasive throughout our book, we were touching on some of these issues.&nbsp;</p>



<p>In fact, we started the book, you know, with GPT-4 scolding me for wanting it to impersonate Zak. And there was the whole example of asking it to rewrite a poem in a certain way, and it kind of silently just tried to slide, you know, without me knowing, slide by without following through on the whole thing.&nbsp;</p>



<p>And so that early version of GPT-4 was definitely not sycophantic at all. In fact, it was just as prone to call you an idiot if it thought you were wrong. [LAUGHTER]&nbsp;</p>



<p><strong>KOHANE:</strong> I had some very testy conversations around my endocrine diagnosis with it. [LAUGHTER]&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. Well then, Peter, I would ask you, I mean last time I asked you about, <em>well, hallucinations, aren&#8217;t those solvable?</em> And this time I would ask you, well, sycophancy, isn&#8217;t that kind of like a dial you can turn? Like, is that not solvable?&nbsp;</p>



<p><strong>LEE: </strong>You know, I think there are several interlocking problems. But if we assume superintelligence, even with superintelligence, medicine is such an inexact science that there will always be situations that are guesses that take into account other factors of a person&#8217;s life, other value judgments, exactly as Zak had pointed out in our previous roundtable conversation.&nbsp;</p>



<p>And so I think there&#8217;s always going to be an opening for either differences of opinion or agreeing with you too much. And there are dangers in both cases. And I think they&#8217;ll always be present. I don&#8217;t know that, at least in something as inexact as medical science, I don&#8217;t know that it&#8217;ll ever be completely eliminated.&nbsp;</p>



<p><strong>KOHANE: </strong>And it&#8217;s interesting because I was trying to think what&#8217;s the right balance, but there are patients who want to be told this is what you do. Whereas there&#8217;s other patients who want to go through every detail of the reasoning.&nbsp;</p>



<p>And it&#8217;s not a matter of education. It&#8217;s really a temperamental, personality issue. And so we&#8217;re going to <em>have to</em>, I think, develop personalities …&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>… that are most effective for those different kinds of individuals. And so I think that is going to be the real frontier. Having human values and behaving in ways that are recognizable and yet effective for certain groups of patients.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>And lots of deep questions, including how paternalistic do we want to be?&nbsp;</p>



<p><strong>LEE: </strong>All right, so we&#8217;re getting into medical science and hallucination. So that gives me a great segue to the conversations in the episode on biomedical research. And one of the people that I interviewed was Noubar Afeyan from Moderna and Flagship Pioneering. So let&#8217;s listen to this snippet.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-574184a033e6f29ff2ba40f7e611b28a"><strong><em>NOUBAR AFEYAN:</em></strong><em> We, some hundred or so times a year, ask “what if” questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that’s testable. Then we go into a lab, and we test it. So in that world, right, sitting there going, like, “How do I know this transformer is going to work?” The answer is, “For what?” Like, it’s going to work to make something up … well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.</em></p>



<p><strong>LEE: </strong>[LAUGHS] So I think that really touches on just the fact that there&#8217;s so many unknowns and such lack of precision and exactness in our understanding of human biology and of medicine. Carey, what do you think?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, I just have this emotional reaction, which is that I love the idea of AI marching into biomedical science and everything from getting to the virtual cell eventually to, Zak, I think it was a colleague of yours who recently published about &#8230; it was a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41591-025-03832-2" target="_blank" rel="noopener noreferrer">new medication that had been sort of discovered by AI<span class="sr-only"> (opens in new tab)</span></a>, and it was actually testing out up to the phase II level or something, right?</p>



<p><strong>KOHANE:</strong> Oh, this is Marinka’s work.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah, Marinka, Marinka Zitnik. And … yeah. So, I mean, I think it avoids a lot of the, sort of, dilemmas that are involved with safety and so on with AI coming into medicine. And it&#8217;s just the discovery process, which we all want to advance as quickly as possible. And it seems like it actually has a great deal of potential that&#8217;s already starting to be realized.&nbsp;</p>



<p><strong>LEE:</strong> Oh, absolutely.&nbsp;</p>



<p><strong>KOHANE:</strong> I love this topic. First of all, I thought, actually, I think Bill and Seb, actually, had interesting things to say on that very topic, rationales which I had not really considered why, in fact, things might progress faster in the discovery space than in the clinical delivery space, just because we don&#8217;t know in clinical medicine what we&#8217;re trying to maximize precisely. Whereas for a drug effect, we do know what we&#8217;re trying to maximize.&nbsp;</p>



<p><strong>LEE: </strong>Well, in fact, I happened to save that snippet from Bill Gates saying that. So let&#8217;s cue that up.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-6ca7a5edfbf41028162616b68d8380d1"><strong><em>BILL GATES: </em></strong><em>I think it’s very much within the realm of possibility that the AI is not only accelerating healthcare discovery but substituting for a lot of the roles of, you know, “I’m an organic chemist,” or “I run various types of assays.” I can see those, which are, you know, testable-output-type jobs but with still very high value, I can see, you know, some replacement in those areas before the doctor.</em>&nbsp;</p>



<p><strong>LEE: </strong>So, Zak, isn&#8217;t that Bill saying exactly what you’re saying?&nbsp;</p>



<p><strong>KOHANE:</strong> That is my point. I have to say that this is another great bet, that either we&#8217;re all going to be surprised or a large group of people will be surprised or disappointed.&nbsp;</p>



<p>There&#8217;s still a lot of people in the sort of medicinal chemist, trialist space who are still extremely skeptical that this is going to work. And we haven&#8217;t quite shown them yet that it is. Why have we not shown them? Because we haven&#8217;t gone all the way to a phase III study, which showed that the drug behaves as expected to, is effective, and basically doesn&#8217;t hurt people. That turns out to require a lot of knowledge. I actually think we&#8217;re getting there, but I understand the skepticism.&nbsp;</p>



<p><strong>LEE: </strong>Carey, what are your thoughts?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I mean, there will be no way around going through full-on clinical trials for anything to ever reach the market. But at the same time, you know, it&#8217;s clearly very promising. And just to throw out something for the pure fun of it, Peter, I saw &#8230; one of my favorite tweets recently was somebody saying, you know, isn&#8217;t it funny how computer science is actually becoming a lot more like biology in that it&#8217;s just becoming empirical.&nbsp;</p>



<p>It&#8217;s like you just throw stuff at the AI and see what it does. [LAUGHTER] And I was like,<em> oh, yeah, that&#8217;s what Peter was doing when we wrote the book.</em> I mean, he understood as many innards as anybody can. But at the same time, it was a totally empirical exercise in seeing what this thing would do when you threw things at it.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>GOLDBERG: </strong>So it&#8217;s the new biology.&nbsp;</p>



<p><strong>LEE:</strong> Well, yeah. So I think we talked in our book about accelerating, you know, biomedical knowledge and medical science. And that actually seems to be happening. And I really had fun talking to Daphne Koller about some of the accomplishments that she&#8217;s made. And so here&#8217;s a little snippet from Daphne.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-b519fe67233d03b2946f95df076d6451"><strong><em>DAPHNE KOLLER: </em></strong><em>This will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient. And I think there’s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.</em>&nbsp;</p>



<p><strong>LEE:</strong> So, Zak, when I was listening to that, I was reminded of one of the very first examples that you had where, you know, you had a very rare case of a patient, and you&#8217;re having to narrow down some pretty complex and very rare genetic conditions. This thing that Daphne says, that seems to be the logical conclusion that everyone who&#8217;s thinking hard about AI and biology is coming to. Does it seem more real now two years on?&nbsp;</p>



<p><strong>KOHANE:</strong> It absolutely seems more real. Here&#8217;s some sad facts. If you are at a cancer center, you will get targeted therapies if you qualify for it. Outside cancer centers, you won&#8217;t. And it&#8217;s not that the therapies aren&#8217;t available. It&#8217;s just that you won&#8217;t have people thinking about it in that way. And especially if you have some of the rare and more aggressive cancers, if you&#8217;re outside one of those cancer centers, you&#8217;re at a significant disadvantage for survival for that reason. And so anything that provides just the “simple,” in quotes, dogged investigation of the targeted therapies for patients, it&#8217;s a home run.&nbsp;</p>



<p>So my late graduate student, Atul Butte, died recently at UCSF, where he was both a professor and the leader of the Bakar Institute, and he was a Zuckerberg Chan Professor of Pediatrics.&nbsp;</p>



<p>He was diagnosed with a rare tumor two years ago. His wife is a PhD biologist, and when he was first diagnosed, she sent me the diagnosis and the mutations. And I don&#8217;t know if you know this, Peter, but this was still when we were writing the book and people didn&#8217;t know about GPT-4.&nbsp;</p>



<p>I put in those mutations into GPT-4 and the diagnosis. And I said, “I&#8217;d like to help treat my friend. What&#8217;s the right treatment?” And GPT, to paraphrase, GPT-4 said, “Before we start talking about treatment, are you sure this is the right diagnosis? Those mutations are not characteristic for that tumor.&#8221; And he had been misdiagnosed. And then they changed the diagnosis therapy and some personnel. </p>



<p>So I don&#8217;t have to hallucinate this. It&#8217;s already happened, and we&#8217;re going to need this. And so I think targeted therapy for cancers is the most obvious use. And if God forbid one of you has a family member who has cancer, it&#8217;s moral malpractice not to look at the genetics and run it past GPT-4 and say, “What are the available therapies?”&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> I really deeply believe that.&nbsp;</p>



<p><strong>LEE:</strong> Carey, I think one thing you&#8217;ve always said is that you&#8217;re surprised that we don&#8217;t hear more stories along these lines. And I think you threw a quote from Mustafa Suleyman back at me. Do you want to share that?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes. Recently, I believe it was a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bigtechnology.com/p/microsoft-ai-ceo-mustafa-suleyman-c45" target="_blank" rel="noopener noreferrer">Big Technology interview<span class="sr-only"> (opens in new tab)</span></a>, and the reporter asked Mustafa Suleyman, “So you guys are seeing 50 million queries, medical queries, a day [to Copilot and Bing]. You know, how&#8217;s that going?” And I think I am a bit surprised that we&#8217;re not seeing more stories of <em>all</em> types. Both here&#8217;s how it helped me and also here was maybe, you know, a suggestion that was not optimal.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. I do think in our book, we did predict both positive and negative outcomes of this. And it is odd. Atul was very open with his story. And of course, he is such … he was such a prominent leader in the world of medicine.&nbsp;</p>



<p>But I think I share your surprise, Carey. I expected by now that a lot more public stories would be out. Maybe there is someone writing a book collecting these things, I don&#8217;t know.&nbsp;</p>



<p><strong>KOHANE:</strong> Maybe someone called Carey Goldberg should write that book. [LAUGHTER]&nbsp;</p>



<p><strong>GOLDBERG: </strong>Write a book, maybe. I mean, we have <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://patientsuseai.substack.com/" target="_blank" rel="noopener noreferrer">Patients Use AI<span class="sr-only"> (opens in new tab)</span></a>, which is a wonderful blog by Dave deBronkart, the patient advocate.&nbsp;</p>



<p>But I wonder if it&#8217;s also something structural, like who would be or what would be the institution that would be gathering these stories? I don’t know.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> And that&#8217;s the problem. You see, this goes back to the same problem that [Ethan] Mollick was talking about. Individual doctors are using them. The hospital as a whole is not doing that. So it&#8217;s not judging the quality, as part of its quality metrics, of how good the AI is performing and what new has happened. And the other audience, namely the patients, have no mechanism. There is no mechanism to go to Better Business Bureau and say, “They screwed up,” or “This was great.”&nbsp;</p>



<p><strong>LEE: </strong>So now I want to get a little more futuristic. And this gets into whether AI is really going to get almost to the <em>ab initio</em> understanding of human biology. And so Eric Topol, who is one of the guests, spoke to this a bit. So let&#8217;s hear this.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-a8cd8394bbdf383bb04d1e5902cb8b97"><strong><em>LEE: </em></strong><em>So you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?</em><br><br><strong><em>ERIC TOPOL:</em></strong><em> No, I think within 10 years for sure. You know, the group that got assembled, that Steve Quake pulled together, I think has 42 authors in a paper in Cell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree that not only is this a worthy goal, but it’s actually going to be realized, that was impressive.</em> </p>



<p><strong>LEE:</strong> You know, I have to say Eric&#8217;s optimism took me aback. Just speaking as a techie, I think I started off being optimistic: as soon as we can figure out molecular dynamics, biology can be solved. And then you start to learn more about biochemistry, about the human cell, and then you realize, oh, my God, this is just so vast and unknowable. And now you have Eric Topol saying, “Well, in less than 10 years.”&nbsp;</p>



<p><strong>KOHANE: </strong>So what&#8217;s delightful about this period is that those of us who are cautious were so incredibly wrong about AI two years ago. [LAUGHTER] That&#8217;s a true joy &#8230; I mean, absolute joy. It&#8217;s great to have your futurism made much more positive.&nbsp;</p>



<p>But I think that we&#8217;re going from, you know, for example, AlphaFold has had tremendous impact. But remember, that was built on years of acquisition of crystallography data that was annotated. And of course, the annotation process becomes less relevant as you go down the pipe, but it started from that.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE:</strong> And there&#8217;s lots of parts of the cell. So when people talk about virtual cells—I don&#8217;t mean to get too technical—mostly they&#8217;re talking about perturbation of gene expression. They&#8217;re not talking about, “Oh, this is how the liposome and the centrosome interact, and notice how the Golgi bodies bump into each other.”&nbsp;</p>



<p>There&#8217;s a whole bunch of other levels of abstraction we know nothing about. This is a complex factory. And right now, we&#8217;re sort of the level from code into loading code into memory. We&#8217;re not talking about how the rest of the robots work in that cell, and how the rest of those robots work in the cell turns out to be pretty important to functioning.&nbsp;</p>



<p>So I&#8217;d love to be wrong again. And in 10 years, oh yeah, not only, you know, our first in-human study will be you, Dr. Zak. We&#8217;re going put the drug because we fully simulated you. That&#8217;d be great.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>And, by the way, just to give people their due, there probably was a lot of animal research that could be done <em>in silico </em>and that for various political reasons we&#8217;re now seeing happen. That&#8217;s a good thing. But I think that sometimes it takes a lot of hubris to get us where we need to get, but my horizon is not the same as his.&nbsp;</p>



<p><strong>LEE: </strong>So I guess I have to take this time to brag. Just recently out of our AI for Science team did publish in <em>Science</em> a <a href="https://www.microsoft.com/en-us/research/publication/scalable-emulation-of-protein-equilibrium-ensembles-with-generative-deep-learning/" target="_blank" rel="noreferrer noopener">biological emulator that does pretty long timespan, very, very precise, and very efficient molecular dynamics</a>, biomolecular dynamics emulation. We call it <em>emulation</em> because it&#8217;s not simulating every single time step but giving you the final confirmations.&nbsp;</p>



<p><strong>KOHANE: </strong>That&#8217;s an amazing result.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> But … that is an amazing result. And you&#8217;re doing it in some very important interactions. But there&#8217;s so much more to do.&nbsp;</p>



<p><strong>LEE:</strong> I know, and it&#8217;s single molecules; it&#8217;s not even two molecules. There&#8217;s so much more to go for here. But on the other hand, Eric is right, you know, 42 experts writing for <em>Cell</em>, you know, that&#8217;s not a small matter.&nbsp;</p>



<p><strong>KOHANE:</strong> So I think sometimes you really need to drink your own hallucinogens to actually succeed. Because remember, when the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.genome.gov/human-genome-project" target="_blank" rel="noopener noreferrer">Human Genome Project<span class="sr-only"> (opens in new tab)</span></a> was launched, we didn&#8217;t know how to sequence at scale.&nbsp;</p>



<p>We said maybe we would get there. And then in order to get the right funding and excitement and, I think, focus, we predicted that by early 2000s we&#8217;d be transforming medicine. Has not happened yet. Things have happened, but at a much slower pace. And we&#8217;re 25 years out. In fact, we&#8217;re 35 years out from the launch.&nbsp;</p>



<p>But again, things are getting faster and faster. Maybe the singularity is going to make a whole bunch of things easier. And GPT-6 will just say, “Zak, you are such a pessimist. Let me show you how it&#8217;s done.”&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p>It really is a pessimism versus optimism. Like is it, I mean, biology is such a bitch, right. [LAUGHTER] Can we actually get there?&nbsp;</p>



<p>At the same time, everyone was surprised and blown away by the, you know, the quantum leap of GPT-4. Who knows when enough data gets in there if we might not have a similar leap.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. All right.&nbsp;</p>



<p>So let&#8217;s get back to healthcare delivery. Besides Morgan Cheatham, we talked to [a] more junior medical student who&#8217;s at the Kaiser Permanente School of Medicine, Daniel Chen. And, you know, I asked him about this question of patients who come in armed [LAUGHS] with a lot of their own information. Let&#8217;s hear what he said about this.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-b854f48f19d32b116ea80de7d13c8a99"><strong><em>DANIEL CHEN: </em></strong><em>But for those that come in with a list, I sometimes sit down with them, and we’ll have a discussion, honestly. … “I don’t think you have meningitis because, you know, you’re not having a fever. Some of the physical exam maneuvers we did were also negative. So I don’t think you have anything to worry about that,” you know. So I think it’s having that very candid conversation with the patient that helps build that initial trust.</em>&nbsp;</p>



<p><strong>LEE:</strong> So, Zak, as far as I can tell, Daniel and Morgan are figuring this out on their own as medical students. I don&#8217;t think this is part of the curriculum. Does it need to be?&nbsp;</p>



<p><strong>KOHANE:</strong> It&#8217;s missing the bigger point. The incentives and economic forces are such that even if you were Daniel, and things have not changed in terms of incentives, and it&#8217;s 2030, he still has to see this many patients in an hour.&nbsp;</p>



<p>And sitting down, going over that with a patient, let&#8217;s say some might need more &#8230; in fact, I think computer scientists are enriched for these sort of neurotic “explain [to] me why this works,” when often the answer is, “I have no idea; empirically it does.”&nbsp;</p>



<p>And patients in some sense deserve that conversation, and we&#8217;re taught about joint decision making, but in practice, there&#8217;s a lot of skills that are deployed to actually deflect so that you can get through the appointment and see enough patients per hour.&nbsp;</p>



<p>And that&#8217;s why I think that one of the central … another task for AI is how to engage with patients to actually explain to them why their doctor is doing what he&#8217;s doing and perhaps ask the one or two questions that you should be asking the doctor in order to reassure you that they&#8217;re doing the right thing.</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>I<strong> </strong>just …<strong> </strong>right now, we are going to have less doctor time, not more doctor time.&nbsp;</p>



<p>And so I&#8217;ve always been struck by the divide between medicine that we&#8217;re taught as it should be practiced as a gentle person&#8217;s vocation or sport as opposed to assembly line, heads down “you&#8217;ve got to see those patients by the end of the day” because, otherwise, you haven&#8217;t seen all the patients at the end of the day.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Carey, I&#8217;ve been dying to ask you this, and I have not asked you this before. When you go see a doctor, are you coming in armed with ChatGPT information?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I haven&#8217;t needed to yet, but I certainly would. And also my reaction to the medical student description was, I think we need to distinguish between the last 20 years, when patients would come in armed with Google, and what they&#8217;re coming in with now because at least the experiences that I&#8217;ve witnessed, it is miles better to have gone back and forth with GPT-4 than with, you know, dredging what you can from Google. And so I think we should make that distinction.&nbsp;</p>



<p>And also, the other thing that most interested me was this question for medical students of whether they should not use AI for a while so that they can learn …&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… how to think and similarly maybe don&#8217;t use the automated scribes for a while so they can learn how to do a note. And at what point should they then start being able to use AI? And I suspect it&#8217;s fairly early on that, in fact, they&#8217;re going be using it so consistently that there&#8217;s not that much they need to learn before they start using the tools.&nbsp;</p>



<p><strong>LEE:</strong> These two students were incredibly impressive. And so I have wondered, you know, if we got a skewed view of things. I mean, Morgan is, of course, a very, very impressive person. And Daniel was handpicked by the dean of the medical school to be a subject of this interview.&nbsp;</p>



<p><strong>KOHANE: </strong>You know, we filter our students, by and large, I mean, there&#8217;s exceptions, but students in medical school are so starry eyed. And they are really &#8230; they got into medical school—I mean, some of them may have faked it—but a lot of them because they really wanted to do good.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>KOHANE: </strong>And they really wanted to help. And so this is very constant with them. And it&#8217;s only when they&#8217;re in the machine, past medical school, that they realize, oh my God, this is a very, very different story.&nbsp;</p>



<p>And I can tell you, because I teach a course in computational-enabled medicine, so I get a lot of these nerd medical students, and I&#8217;m telling them, “You&#8217;re going to experience this. And you&#8217;re going to say, ‘I&#8217;m not going to able to change medicine until I get enough cred 10, 15 years from now, whereas I could start my own company and immediately change medicine.’”&nbsp;</p>



<p>And increasingly I&#8217;m getting calls in like residency and saying, “Zak, help me. How do I get out of this?”&nbsp;</p>



<p><strong>GOLDBERG: </strong>Wow.&nbsp;</p>



<p><strong>KOHANE:</strong> And so I think there&#8217;s a real disillusionment of, like, between what we&#8217;re asking for people coming to medical school—we&#8217;re looking for a phenotype—and then we&#8217;re disappointing them massively, not everywhere, but massively.&nbsp;</p>



<p>And for me, it&#8217;s very sad because among our best and brightest, and then because of economics and expectations and the nature of the beast, they&#8217;re not getting to enjoy the most precious part of being a doctor, which is that real human connection, and longitudinality, you know, the connection between the same doctor visit after visit, is more and more of a luxury.&nbsp;</p>



<p><strong>LEE:</strong> Well, maybe this gets us to the last episode, you know, where I talk to a former, you know, state director of public health, Umair Shah, and with Gianrico Farrugia, who&#8217;s the CEO of Mayo Clinic. And I think if there&#8217;s one theme that I took away from those conversations is that we&#8217;re not thinking broadly enough nor big enough.&nbsp;</p>



<p>And so here&#8217;s a little quote of exchange that Umair Shah, who was the former head of public health in the State of Washington and prior to that in Harris County, Texas, and we had a conversation about what techies tend to focus on when they&#8217;re thinking about AI and medicine.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-0a05c3ff13da1926875eed47432d27b2"><strong><em>UMAIR SHAH: </em></strong><em>I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.</em><br><br><strong><em>LEE: </em></strong><em>Yes.</em><strong><em> </em></strong><em>And in fact, it&#8217;s not even delivery. I think techies—I did this, too—tend to gravitate specifically to diagnosis.</em></p>



<p><strong>LEE:</strong> I have been definitely guilty. I think Umair, of course, was speaking as a former frustrated public health official in just thinking about all the other things that are important to maintain a healthy population.&nbsp;</p>



<p>Is there some lesson that we should take away? I think our book also focused a lot on things like diagnosis.&nbsp;</p>



<p><strong>KOHANE:</strong> Yeah. Well, first of all, I think we just have to have humility. And I think it&#8217;s a really important ingredient. I found myself staring at the increase in lifespan in human beings over the last two centuries and looking for bumps that were attributable.&nbsp;</p>



<p>I&#8217;m in medical school. I&#8217;ve already made this major commitment. What are the bumps that are attributable to medicine? And there was one bump that was due to vaccines, a small bump. Another small bump that was due to antibiotics. And the rest of it is nutrition, sanitation, yeah, nutrition and sanitation.&nbsp;</p>



<p>And so I think doctors can be incredibly valuable, but not all the time. And we&#8217;re spending now one-sixth of our GDP on it. The majority of it is not effectively prolonging life. And so the humility has to be the right medicine at the right time.&nbsp;</p>



<p>But that runs, (A) against a bunch of business models. It runs against the primacy of doctors in healthcare. It was one thing when there were no textbooks; there was no PubMed. You know, the doctor was the repository of all the probably knowledge that we have. But I think your guests were right. We have to think more broadly in the public health way. How do we make knowledge pervasive like sanitation?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Although I would add that since what we&#8217;re talking about is AI, it&#8217;s harder to see if &#8230; and if what you&#8217;re talking about is public health, I mean, it was certainly very important to have good data during the pandemic, for example.&nbsp;</p>



<p>But most of the ways to improve public health, like getting people to stop smoking and eat better and sleep better and exercise more, are not things that AI can help with that much. Whereas diagnosis or trying to improve treatment are places that it could tackle.&nbsp;</p>



<p>And in fact, Peter, I wanted to put you—oh, wait, Zak&#8217;s going to say something—but, Peter, I wanted to put you on the spot.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, if you had a medical issue now, and you went to a physician, would you be OK with them not using generative AI?&nbsp;</p>



<p><strong>LEE:</strong> I think if it&#8217;s a complex or a mysterious case, I would want them to use generative AI. I would want that second opinion on things. And I would personally be using it. If for no other reason than just to understand what the chart is saying.&nbsp;</p>



<p>I don&#8217;t see, you know, how or why one wouldn&#8217;t do that now.&nbsp;</p>



<p><strong>KOHANE:</strong> It&#8217;s such a cheap second opinion, and people are making mistakes. And even if there are mistakes on the part of AI, if there&#8217;s a collision, discrepancy, that&#8217;s worth having a discussion. And again, this is something that we used to do more of when we had more time with the patients; we&#8217;d have clinic conferences.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> And we don&#8217;t have that now. So I do think that there is a role for AI. But I think again, it&#8217;s much more of a continual presence, being part of a continued conversation rather than an oracle.&nbsp;</p>



<p>And I think that&#8217;s when you&#8217;ll start seeing, when the AI is truly a colleague, and saying, “You know, Zak, that&#8217;s the second time you made that mistake. You know, that&#8217;s not obesity. That&#8217;s the effect of your drugs that you&#8217;re giving her. You better back off of it.” And that&#8217;s what we need to see happen.&nbsp;</p>



<p><strong>LEE:</strong> Well, and for the business of healthcare, that also relates directly to quality scores, which translates into money for healthcare providers.&nbsp;</p>



<p>So the last person that we interviewed was Gianrico Farrugia. And, you know, I was sort of wondering, I was expecting to get a story from a CEO saying, “Oh, my God, this has been so disruptive, incredibly important, meaningful, but wow, what a headache.”&nbsp;</p>



<p>At least Gianrico didn&#8217;t expose any of that. Here&#8217;s one of the snippets to give you a sense.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-e494c3919056a8b99abdfa66f7e6fae6"><strong><em>GIANRICO</em></strong><em> </em><strong><em>FARRUGIA:</em></strong><em> When generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, because something as disruptive as that instantly became enabling at Mayo Clinic.</em>&nbsp;</p>



<p><strong>LEE:</strong> So I tried pretty hard in that interview to get Gianrico to admit that there was a period of headache and disruption here. And he never, ever gave me that. And so I take him at his word.&nbsp;</p>



<p>Zak, maybe I should ask you, what about Harvard and the whole Harvard medical ecosystem?&nbsp;</p>



<p><strong>KOHANE:</strong> I would be surprised if there are system-wide measurable gains in health quality right now from AI. And I do have to say that Mayo is one of the most marvelous organizations in terms of team behavior. So if there&#8217;s someone who&#8217;s gotten the team part of it right, they&#8217;ve come the closest, which relates to our prior conversation. They have the quarterback idea …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>… pretty well down compared to others.&nbsp;</p>



<p>Nonetheless, I take him at his word, that it hasn&#8217;t disrupted them. But I&#8217;m also, I have yet to see the evidence that there&#8217;s been a quantum leap in quality or efficacy. And I do believe that it&#8217;s possible to have a quantum leap in efficacy in the right system.&nbsp;</p>



<p>So if they haven&#8217;t been disrupted, I would venture that they&#8217;ve absorbed it, but they haven&#8217;t used it to its fullest potential. And the way I could be proven wrong is next year, also the metrics showing that over the last year, they&#8217;ve had, you know, decreased readmissions, decreased complications, decreased errors and all that. And if so, God bless them. And we should all be more like Mayo.&nbsp;</p>



<p><strong>LEE:</strong> So I thought a little bit about two other quotes from the interviews that sort of maybe would send us off with some more inspirational kind of view of the future. And so there&#8217;s one from Bill Gates and one from Gianrico Farrugia. So what I&#8217;d like to do is to play both of those and then maybe we can have our last comments.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-34fa8f74d3c5dd1ddf63456b250b860b"><strong><em>BILL GATES</em></strong><em>: You know, I’ve gone so far as to tell politicians with national health systems that if they deploy AI appropriately, that the quality of care, the overload of the doctors, the improvement in the economics will be enough that their voters will be stunned because they just don’t expect this, and, you know, they could be reelected just on this one thing of fixing what is a very overloaded and economically challenged health system in these rich countries.</em>&nbsp;</p>



<p>And now Gianrico.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-daa9fe33118b2710490d542ab9959977"><strong><em>GIANRICO FARRUGIA: </em></strong><em>And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. … And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, no, is that let&#8217;s start with that aim, the last aim …</em><strong><em> </em></strong><em>because the others will come automatically if you&#8217;re working on that harder problem. Because one, to get to that harder problem, you&#8217;ll find all the other solutions.</em>&nbsp;</p>



<p>All right. I think these are both kind of calls to be more assertive about this and more forward leaning. I think two years into the GPT-4 era, those are pretty significant and pretty optimistic calls to action. So maybe just to give you both one last word. What would be one hope that you would have for the world of healthcare and medicine two years from now?&nbsp;</p>



<p><strong>KOHANE:</strong> I would hope for businesses that whoever actually owns them at some holding company level, regardless of who owns them, are truly patient-focused companies, companies where the whole AI is about improving your care, and it&#8217;s only trying to maximize your care and it doesn&#8217;t care about resource limitations.&nbsp;</p>



<p>And as I was listening to Bill, and the problem with what he was saying about saving dollars for governments is for many things, we have some very expensive things that work. And if the AI says, “This is the best thing,” it&#8217;s going to break your bank. And instead, because of research limitations, we play a human-based fancy footwork to get out of it.&nbsp;</p>



<p>That&#8217;s a hard game to play, and I leave it to the politicians and the public health officials who have to do those trades of utilities.&nbsp;</p>



<p>In my role as doctor and patient, I&#8217;d like to see very informed, authoritative agents acting only on our behalf so that when we go and we seek to have our maladies addressed, the only issue is, what&#8217;s the best and right thing for me now? And I think that is both technically realizable. And even in our weird system, there are business plans that will work that can achieve that. That&#8217;s my hope for two years from now.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, fantastic. Carey.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I second that so enthusiastically. And I think, you know, we have this very glass half full/glass half empty phenomenon two years after the book came out.&nbsp;</p>



<p>And it&#8217;s certainly very nice to see, you know, new approaches to administrative complexity and to prior authorization and all kinds of ways to make physicians&#8217; lives easier. But really what we all care about is our own health and that we would like to be able to optimize the use of this truly glorious technological achievement to be able to live longer and better lives. And I think what Zak just described is the most logical way to do that.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think for me, two years from now, I would like to see all of this digital data that&#8217;s been so painful, such a burden on every doctor and nurse to record, actually amount to something meaningful in the care of patients. And I think it&#8217;s possible.&nbsp;</p>



<p><strong>KOHANE:</strong> Amen.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p><strong>LEE:</strong> All right, so it&#8217;s been quite a journey. We were joking before we&#8217;re still on speaking terms after having written a book. [LAUGHS]&nbsp;</p>



<p>And then, um, I think listeners might enjoy knowing that we debated amongst ourselves what to do about a second edition, which seemed too painful to me, and so I suggested the podcast, which seemed too painful to the two of you [LAUGHTER]. And in the end, I don&#8217;t know what would have been easier, writing a book or doing this podcast series, but I do think that we learned a lot.&nbsp;</p>



<p>Now, last bit of business here.<strong> </strong>To avoid having the three of us try to write a book again and do this podcast, I leaned on the production team in Microsoft Research and the Microsoft Research Podcast. And I thought it would be good to give an explicit acknowledgment to all the people who&#8217;ve contributed to this.&nbsp;</p>



<p>So it&#8217;s a long list of names. I&#8217;m going to read through them all. And then I suggest that we all give an applaud [LAUGHTER] to them. And so here we go.&nbsp;</p>



<p>There’s Neeltje Berger, Tetiana Bukhinska, David Celis Garcia, Matt Corwine, Jeremy Crawford, Kristina Dodge, Chris Duryee, Ben Ericson, Kate Forster, Katy Halliday, Alyssa Hughes, Jake Knapp, Weishung Liu, Matt McGinley, Jeremy Mashburn, Amanda Melfi, Wil Morrill, Joe Plummer, Brenda Potts, Lindsay Shanahan, Sarah Sobolewski, David Sullivan,&nbsp;Stephen Sullivan, Amber Tingle, Caitlyn Treanor, Craig Tuschhoff, Sarah Wang, and Katie Zoller.&nbsp;</p>



<p>Really a great team effort, and they made it super easy for us.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Thank you. Thank you. Thank you.&nbsp;</p>



<p><strong>KOHANE:</strong> Thank you. Thank you.</p>



<p><strong>GOLDBERG: </strong>Thank you.&nbsp;</p>



<p>[THEME MUSIC]&nbsp;</p>



<p><strong>LEE: </strong>A big thank you again to all of our guests for the work they do and the time and expertise they shared with us.&nbsp;</p>



<p>And, last but not least, to our listeners, thank you for joining us. We hope you enjoyed it and learned as much as we did. If you want to go back and catch up on any episodes you may have missed or to listen to any again, you can visit <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://aka.ms/airevolutionpodcast">aka.ms/AIrevolutionPodcast<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>Until next time.</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-2"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--3"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/">Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</title>
		<link>https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</link>
		
		<dc:creator><![CDATA[Yuncong Yang, Reuben Tan, Swadheen Shukla, Jianfeng Gao]]></dc:creator>
		<pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1147961</guid>

					<description><![CDATA[<p>MindJourney can enable AI to navigate and interpret 3D environments from limited visual input, potentially improving performance in navigation, planning, and safety-critical tasks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called <a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">MindJourney</a>, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&nbsp;&nbsp;</p>



<p>While VLMs&nbsp;are strong&nbsp;at identifying objects in&nbsp;static&nbsp;images,&nbsp;they struggle to&nbsp;interpret&nbsp;the interactive 3D world behind 2D images.&nbsp;This&nbsp;gap shows up&nbsp;in spatial&nbsp;questions&nbsp;like&nbsp;“If I sit on the couch&nbsp;that is on my right&nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&nbsp;interpret&nbsp;its&nbsp;position and movement through space.&nbsp;</p>



<p>People&nbsp;overcome this challenge by mentally exploring a space,&nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&nbsp;MindJourney&nbsp;applies the same process&nbsp;to&nbsp;AI agents,&nbsp;letting&nbsp;them roam a virtual&nbsp;space before answering spatial questions.&nbsp;</p>



<h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space">How&nbsp;MindJourney&nbsp;navigates 3D space</h2>



<p>To perform this type of spatial navigation, MindJourney uses a <em>world model</em>—in this case, a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left or right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.</p>



<p>At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user&#8217;s question.</p>



<p>These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-300x183.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-1024x625.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-768x468.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-240x146.jpg 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM&#8217;s spatial interpretation through generated observations when encountering new challenges.<em>&nbsp;</em></figcaption></figure>



<p>&nbsp;</p>



<p>To make its search through&nbsp;a simulated&nbsp;space both effective and efficient,&nbsp;MindJourney&nbsp;uses a <em>spatial beam search</em>—an&nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&nbsp;MindJourney&nbsp;to gather strong supporting evidence.&nbsp;This process is illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" alt="MindJourney pipeline diagram" class="wp-image-1147897" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.</figcaption></figure>



<p class="has-text-align-left">By iterating through&nbsp;simulation,&nbsp;evaluation, and integration,&nbsp;MindJourney&nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&nbsp;On&nbsp;the&nbsp;Spatial Aptitude Training (SAT)&nbsp;benchmark,&nbsp;it improved the accuracy of&nbsp;VLMs&nbsp;by&nbsp;8%&nbsp;over&nbsp;their&nbsp;baseline&nbsp;performance.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144027">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-label="AI Testing and Evaluation: Learnings from Science and Industry" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP2-AI-TE_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshots of Daniel Carpenter, Timo Minssen, Chad Atalla, and Kathleen Sullivan for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Testing and Evaluation: Learnings from Science and Industry</h2>
				
								<p id="ai-testing-and-evaluation-learnings-from-science-and-industry" class="large">Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-describedby="ai-testing-and-evaluation-learnings-from-science-and-industry" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="MindJourney: Test-Time Scaling with World Models for Spatial Reasoning" width="500" height="375" src="https://www.youtube-nocookie.com/embed/Z4-5NZmdV44?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<h2 class="wp-block-heading" id="building-smarter-agents">Building&nbsp;smarter agents&nbsp;&nbsp;</h2>



<p>MindJourney&nbsp;showed&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">strong performance</a>&nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&nbsp;improved&nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&nbsp;capabilities&nbsp;of VLMs, create a more complete spatial capability&nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&nbsp;interpret&nbsp;the physical world&nbsp;more accurately.&nbsp;</p>



<p>It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&nbsp;interpreting&nbsp;and acting in real-world environments. This opens the way to&nbsp;possible&nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&nbsp;</p>



<p>By converting systems that simply describe static images into active agents that continually evaluate where to look next,&nbsp;MindJourney&nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&nbsp;potentially&nbsp;reducing wear, energy use, and collision risk.&nbsp;</p>



<p>Looking ahead, we plan to extend the framework to&nbsp;use&nbsp;world models that&nbsp;not only&nbsp;predict&nbsp;new viewpoints&nbsp;but also forecast&nbsp;how the scene might change over time.&nbsp;We envision&nbsp;MindJourney&nbsp;working&nbsp;alongside VLMs that interpret&nbsp;those predictions&nbsp;and use&nbsp;them to&nbsp;plan&nbsp;what to do&nbsp;next. This&nbsp;enhancement could enable&nbsp;agents&nbsp;more accurately&nbsp;interpret&nbsp;spatial relationships and physical dynamics, helping them to operate effectively&nbsp;in changing environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
