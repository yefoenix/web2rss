<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Tue, 08 Apr 2025 00:49:25 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.7.2</generator>
	<item>
		<title>Real-world healthcare AI development and deployment—at scale</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/</link>
		
		<dc:creator><![CDATA[Peter Lee, Matthew Lungren, Seth Hain]]></dc:creator>
		<pubDate>Thu, 03 Apr 2025 13:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135527</guid>

					<description><![CDATA[<p>Microsoft’s Dr. Matthew Lungren and Epic’s Seth Hain discuss the challenges and opportunities of leveraging generative AI for enhanced patient care and improved clinical documentation and recordkeeping at scale—plus what’s next for the technology in the field.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/">Real-world healthcare AI development and deployment—at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg" alt="AI Revolution podcast | Episode 2 - Real-world healthcare AI development and deployment—at scale | outline illustration of Seth Hain, Peter Lee, Dr. Matthew Lungren" class="wp-image-1135817" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe src="https://player.blubrry.com/?podcast_id=144098908&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this episode, <a href="https://www.microsoft.com/en-us/research/people/mlungren/" target="_blank" rel="noreferrer noopener">Dr. Matthew Lungren<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/seth-hain-12760647/" target="_blank" rel="noreferrer noopener">Seth Hain<span class="sr-only"> (opens in new tab)</span></a>, leaders in the implementation of healthcare AI technologies and solutions at scale, join Lee to discuss the latest developments. Lungren, the chief scientific officer at Microsoft Health and Life Sciences, explores the creation and deployment of generative AI for automating clinical documentation and administrative tasks like clinical note-taking. Hain, the senior vice president of R&D at the healthcare software company Epic, focuses on the opportunities and challenges of integrating AI into electronic health records at global scale, highlighting AI-driven workflows, decision support, and Epic’s Cosmos project, which leverages aggregated healthcare data for research and clinical insights.&nbsp;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more:</h2>



<p><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/03/03/meet-microsoft-dragon-copilot-your-new-ai-assistant-for-clinical-workflow/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">Meet Microsoft Dragon Copilot: Your new AI assistant for clinical workflow</a>&nbsp;<br>Microsoft Industry Blog | March 2025&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2024/10/10/unlocking-next-generation-ai-capabilities-with-healthcare-ai-models/" target="_blank" rel="noreferrer noopener">Unlocking next-generation AI capabilities with healthcare AI models</a>&nbsp;<br>Microsoft Industry Blog | October 2024&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/articles/multimodal-generative-ai-the-next-frontier-in-precision-health/" target="_blank" rel="noreferrer noopener">Multimodal Generative AI: the Next Frontier in Precision Health</a>&nbsp;<br>Microsoft Research Forum | March 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/learning/an-introduction-to-how-generative-ai-will-transform-healthcare" target="_blank" rel="noreferrer noopener">An Introduction to How Generative AI Will Transform Healthcare with Dr. Matthew Lungren<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>LinkedIn Learning&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/video/ai-for-precision-health/" target="_blank" rel="noreferrer noopener">AI for Precision Health</a>&nbsp;<br>Video | July 2023&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/chexnet-radiologist-level-pneumonia-detection-on-chest-x-rays-with-deep-learning/" target="_blank" rel="noreferrer noopener">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</a>&nbsp;<br>Publication | December 2017&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cosmos.epic.com/" target="_blank" rel="noreferrer noopener">Epic Cosmos<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Homepage</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a>&nbsp;<br>Book | April 2023</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC] &nbsp;</p>



<p>[BOOK PASSAGE]  &nbsp;</p>



<p><strong>PETER LEE:</strong> “It&#8217;s hard to convey the huge complexity of today&#8217;s healthcare system. Processes and procedures, rules and regulations, and financial benefits and risks all interact, evolve, and grow into a giant edifice of paperwork that is well beyond the capability of any one human being to master. This is where the assistance of an AI like GPT-4 can be not only useful—but crucial.”&nbsp;&nbsp;&nbsp;</p>



<p>[END OF BOOK PASSAGE] &nbsp;</p>



<p>[THEME MUSIC] &nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. &nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?  &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The passage I read at the top there is from Chapter 7 of the book, “The Ultimate Paperwork Shredder.”&nbsp;&nbsp;</p>



<p>Paperwork plays a particularly important role in healthcare. It helps convey treatment information that supports patient care, and it’s also used to help demonstrate that providers are meeting regulatory responsibilities, among other things. But if we’re being honest, it’s taxing—for everyone—and it’s a big contributor to the burnout our clinicians are experiencing today. Carey, Zak, and I identified this specific pain point as one of the best early avenues to pursue as far as putting generative AI to good work in the healthcare space. &nbsp;</p>



<p>In this episode, I’m excited to welcome Dr. Matt Lungren and Seth Hain to talk about matching technological advancements in AI to clinical challenges, such as the paperwork crisis, to deliver solutions in the clinic and in the health system back office. &nbsp;</p>



<p>Matt is the chief scientific officer for Microsoft Health and Life Sciences, where he focuses on translating cutting-edge technology, including generative AI and cloud services, into innovative healthcare applications. He&#8217;s a clinical interventional radiologist and a clinical machine learning researcher doing collaborative research and teaching as an adjunct professor at Stanford University. His scientific work has led to more than 200 publications, including work on new computer vision and natural language processing approaches for healthcare. &nbsp;</p>



<p>Seth is senior vice president of research and development at Epic, a leading healthcare software company specializing in electronic health record systems, also known as <em>EHR</em>, as well as other solutions for connecting clinicians and patients. During his 19 years at Epic, Seth has worked on enhancing the core analytics and other technologies in Epic&#8217;s platforms as well as their applications across medicine,<strong> </strong>bringing together his graduate training in mathematics and his dedication to better health. &nbsp;</p>



<p>I&#8217;ve had the pleasure of working closely with both Matt and Seth. Matt, as a colleague here at Microsoft, really focused on our health and life sciences business. And Seth, as a collaborator at Epic, as we embark on the questions of how to integrate and deploy generative AI into clinical applications at scale.  &nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Here&#8217;s my conversation with Dr. Matt Lungren: &nbsp;</p>



<p><strong>LEE:</strong> Matt, welcome. It&#8217;s just great to have you here.&nbsp;</p>



<p><strong>MATTHEW LUNGREN:</strong> Thanks so much, Peter. Appreciate being here.&nbsp;</p>



<p><strong>LEE:</strong> So, I&#8217;d like to just start just talking about you. You know, I had mentioned your role as the chief scientific officer for Microsoft Health and Life Sciences. Of course, that&#8217;s just a title. So, what the heck is that? What is your job exactly? And, you know, what does a typical day at work look like for you?&nbsp;</p>



<p><strong>LUNGREN: </strong>So, really what you could boil my work down to is essentially cross collaboration, right. We have a very large company, lots of innovation happening all over the place, lots of partners that we work with and then obviously this sort of healthcare mission.</p>



<p>And so, what innovations, what kind of advancements are happening that can actually solve clinical problems, right, and sort of kind of direct that. And we can go into some examples, you know, later. But then the other direction, too, is important, right. So, identifying problems that may benefit from a technologic application or solution and kind of translating that over into the, you know, pockets of innovation saying, “Hey, if you kind of tweaked it this way, this is something that would really help, you know, the clinical world.”&nbsp;&nbsp;</p>



<p>And so, it&#8217;s really a bidirectional role. So, my day to day is … every day is a little different, to be honest with you. Some days it&#8217;s very much in the science and learning about new techniques. On the other side, though, it can be very much in the clinic, right. So, what are the pain points that we&#8217;re seeing? Where are the gaps in the solutions that we&#8217;ve already rolled out? And, you know, again, what can we do to make healthcare better broadly?&nbsp;</p>



<p><strong>LEE:</strong> So, you know, I think of you as a technologist, and, Matt, you and I actually are colleagues working together here at Microsoft. But you also do spend time in the clinic still, as well, is that right?&nbsp;</p>



<p><strong>LUNGREN:</strong> You know, initially it was kind of a … very much a non-negotiable for me … in sort of taking an industry role. I think like a lot of, you know, physicians, you know, we&#8217;re torn with the idea of like, hey, I spent 20 years training. I love what I do, you know, with a lot of caveats there in terms of some of the administrative burden and some of the hassle sometimes. But for the most part, I love what I do, and there&#8217;s no greater feeling than using something that you trained years to do and actually see the impact on a human life. It&#8217;s unbelievable, right.&nbsp;&nbsp;</p>



<p>So, I think part of me was just, like, I didn&#8217;t want to let that part of my identity go. And frankly, as I often say, to this day, I walk by a fax machine in our office <em>today</em>, like in 2025.&nbsp;&nbsp;</p>



<p>So just to be extra clear, it really grounds me in, like, yes, I love the possibilities. I love thinking about what we can do. But also, I have a very stark understanding of the reality on the ground, both in terms of the technology but also the burnout, right. The challenges that we&#8217;re facing in taking care of patients has gotten, you know, much, much more difficult in the last few years, and, you know, I like to think it keeps my perspective, yeah.&nbsp;</p>



<p><strong>LEE: </strong>You know, I think some listeners to this podcast might be surprised that we have doctors on staff in technical roles at Microsoft. How do you explain that to people?&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Yeah, no, yeah, it is interesting. I would say that, you know, from, you know, the legacy Nuance <a href="#ftn_1">[1]</a> world, it wasn&#8217;t so far-fetched that you have physicians that were power users and eventually sort of, you know, became, “Hey, listen, I think this is a strategic direction; you should take it&#8221; or whatever. And certainly maybe in the last, I want to say, five years or so, I&#8217;ve seen more and more physicians who have, you know, taken the time, sometimes on their own, to learn some of the AI capabilities, learn some of the principles and concepts; and frankly, some are, you know, even coding solutions and leading companies.</p>



<p>So, I do think that that has shifted a bit in terms of like, “Hey, doctor, this is your lane, and over here, you know, here&#8217;s a technical person.” And I think that&#8217;s fused quite a bit more.&nbsp;&nbsp;</p>



<p>But yeah, it is an unusual thing, I think, in sort of how we&#8217;ve constructed what at least my group does. But again, I can&#8217;t see any other way around some of the challenges.&nbsp;&nbsp;</p>



<p>I think, you know, an anecdote I’d like to tell you, when I was running the AIMI [Artificial Intelligence in Medicine and Imaging] Center, you know, we were bringing the medical school together with the computer science department, right, at Stanford. And I remember one day a student, you know, very smart, came into my office, you know, a clinical day or something, and he&#8217;s like, is there just, like, a book or something where I can just learn medicine? Because, like, I feel like there&#8217;s a lot of, like, translation you have to do for me.&nbsp;&nbsp;</p>



<p>It really raised an important insight, which is that you can learn the, you know, medicine, so to speak. You know, go to med school; you know, take the test and all that. But it really … you don&#8217;t really understand the practice of medicine until you are doing that.&nbsp;&nbsp;</p>



<p>And in fact, I even push it a step further to say after training those first two or three years of … <em>you</em> are the responsible person; you can turn around, and there&#8217;s no one there. Like, you are making a decision. Getting used to that and then having a healthy respect for that actually I think provides the most educational value of anything in healthcare.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>You know, I think what you&#8217;re saying is so important because as I reflect on my own journey. Of course, I&#8217;m a computer scientist. I don&#8217;t have medical training, although at this point, I feel confident that I could pass a Step 1 medical exam.&nbsp;&nbsp;</p>



<p><strong>LUNGREN: </strong>I have no doubt. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> But I think that the tech industry, because of people like you, have progressed tremendously in having a more sophisticated and nuanced understanding of what actually goes on in clinic and also what goes on in the boardrooms of healthcare delivery organizations. And of course, at the end of the day, I think that&#8217;s really been your role.&nbsp;&nbsp;</p>



<p>So roughly speaking, your job as an executive at a big tech company has been to understand what the technology platforms need to be, particularly with respect to machine learning, AI, and cloud computing, to best support healthcare. And so maybe let&#8217;s start <em>pre</em>-GPT-4, <em>pre</em>-ChatGPT, and tell us a little bit, you know, about maybe some of your proudest moments in getting advanced technologies like AI into the clinic.&nbsp;</p>



<p><strong>LUNGREN:</strong> You know, when I first started, so remember, like you go all the way back to about 2013, right, my first faculty job, and, you know, we&#8217;re building a clinical program and I, you know, I had a lot of interest in public health and building large datasets for pop [population] health, etc. But I was doing a lot of that, you know, sort of labeling to get those insights manually, right. So, like, I was the person that you&#8217;d probably look at now and say, “What are you doing?” Right?&nbsp;&nbsp;</p>



<p>So … but I had a complete random encounter with Andrew Ng, who I didn&#8217;t know at the time, at Stanford. And I, you know, went to one of the seminars that he was holding at the Gates building, and, you know, they were talking about their performance on ImageNet. You know, cat and dog and, you know, tree, bush, whatever. And I remember sitting in kind of the back, and I think I maybe had my scrubs on at the time and just kind of like, what? Like, why … like, this … we could use this in healthcare, you know. [LAUGHS]&nbsp;&nbsp;</p>



<p>But for me, it was a big moment. And I was like, this is huge, right. And as you remember, the deep learning really kind of started to show its stuff with, you know, Fei-Fei Li&#8217;s ImageNet stuff.</p>



<p>So anyway, we started the collaboration that actually became <strong>a NIDUS</strong>. And one of the first things we worked on, we just said, “Listen, one of the most common medical imaging examinations in the world is the chest x-ray.” Right? Two, three billion are done every year in the world, and so is that not a great place to start?</p>



<p>And of course, we had a very democratizing kind of mission. As you know, Andrew has done a lot of work in that space, and I had similar ambitions. And so, we really started to focus on bringing the, you know, the sort of the clinical and the CS together and see what could be done.&nbsp;&nbsp;</p>



<p>So, we did <a href="https://www.microsoft.com/en-us/research/publication/chexnet-radiologist-level-pneumonia-detection-on-chest-x-rays-with-deep-learning/" target="_blank" rel="noreferrer noopener">CheXNet</a>. And this is, remember this is around the time when, like, Geoffrey Hinton was saying things like we should stop training radiologists, and all this stuff was going on. [LAUGHTER] So there&#8217;s a lot of hype, and this is the narrow AI days just to remind the audience.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> How did you feel about that since you <em>are</em> a radiologist?&nbsp;</p>



<p><strong>LUNGREN:</strong> Well, it was so funny. So, Andrew is obviously very prolific on social media, and I was, who am I, right? So, I remember he tagged me. Well, first he said, “Matt, you need to get a Twitter account.” And I said OK. And he tagged me on the very first post of our, what we call, CheXNet that was kind of like the “Hello, World!” for this work.&nbsp;&nbsp;</p>



<p>And I remember it was a clinical day. I had set my phone, as you do, outside the OR. I go in. Do my procedure. You know, hour or so, come back, my phone&#8217;s dead. I&#8217;m like, oh, that&#8217;s weird. Like I had a decent charge. So, you know, I plug it in. I turn it on. I had like hundreds of thousands of notifications because Andrew had tweeted out to his millions or whatever about CheXNet.&nbsp;&nbsp;</p>



<p>And so, then of course, as you point out, I go to RSNA that year, which is our large radiology conference, and that Geoffrey Hinton quote had come out. And everyone&#8217;s looking at me like, “What are you doing, Matt?” You know, like, are you coming after our specialty? I&#8217;m like, “No, no,” that&#8217;s, [LAUGHS] you know, it&#8217;s a way to interpret it, but you have to take a much longer horizon view, right.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Well, you know, we&#8217;re going to, just as an enticement for listeners to this podcast to listen to the very end, I&#8217;m going to pin you down toward the end on your assessment of whether Geoffrey Hinton will eventually be proven right or not. [LAUGHTER] But let&#8217;s take our time to get there.&nbsp;&nbsp;</p>



<p>Now let&#8217;s go ahead and enter the generative AI era. When we were first exposed to what we now know of as GPT-4—this was before it was disclosed to the world—a small number of people at Microsoft and Microsoft Research were given access in order to do some technical assessment.&nbsp;&nbsp;</p>



<p>And, Matt, you and I were involved very early on in trying to assess what might this technology mean for medicine.<strong> </strong>Tell us, you know, what was the first encounter with this new technology like for you?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> It was the weirdest thing, Peter. Like … I joined that summer, so the summer before, you know, the actual GPT came out. I had literally no idea what I was getting into.&nbsp;&nbsp;</p>



<p>So, I started asking it questions, you know, kind of general stuff, right. Just, you know, I was like, oh, all right, it&#8217;s pretty good. And so, then I would sort of go a little deeper. And eventually I got to the point where I&#8217;m asking questions that, you know, maybe there&#8217;s three papers on it in my community, and remember I&#8217;m a sub-sub specialist, right, <em>pediatric interventional radiology</em>. And the things that we do in vascular malformations and, you know, rare cancers are really, really strange and not very commonly known.&nbsp;&nbsp;</p>



<p>And I kind of walked away from that—first I said, can I have this thing, right? [LAUGHS]&nbsp;&nbsp;</p>



<p>But then I, you know, I don&#8217;t want to sound dramatic, but I didn&#8217;t sleep that well, if I&#8217;m being honest, for the first few nights. Partially because I couldn&#8217;t tell anybody, except for the few that I knew were involved, and partially because I just couldn&#8217;t wrap my head around how we went from what I was doing in LSTMs [long short-term memory networks], right, which was state of the artish at the time for NLP [natural language processing].&nbsp;&nbsp;</p>



<p>And all of a sudden, I have this thing that is broadly, you know, domain experts, you know, representations of knowledge that there&#8217;s no way you could think of it would be in distribution for a normal approach to this.&nbsp;&nbsp;</p>



<p>And so, I really struggled with it, honestly. Interpersonally, like, I would be like, uh, well, let&#8217;s not work on that. They&#8217;re like, why not? You were just excited about it last week. I&#8217;m like, I don&#8217;t know. I think that we could think of another approach later.<strong> </strong>[LAUGHS]&nbsp;&nbsp;</p>



<p>And so yeah, when we were finally able to really look at some of the capabilities and really think clearly, it was really clear that we had a massive opportunity on our hands to impact healthcare in a way that was never possible before.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, and at that time you were still a part of Nuance. Nuance, I think, was in the process of being acquired by Microsoft. Is that right?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> That’s right.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> And so, of course, this was also a technology that would have profound and very direct implications for Nuance. How did you think about that?&nbsp;</p>



<p><strong>LUNGREN:</strong> Nuance, for those in the audience who don&#8217;t know, for 25 years was, sort of, <em>the</em> medical speech-to-text thing that all, you know, physicians used. But really the brass ring had always been … and I want to say going back to like 2013, 2014, Nuance had tried to figure out, OK, we see this pain point. Doctors are typing on their computers while they&#8217;re trying to talk to their patients, right.&nbsp;&nbsp;</p>



<p>We should be able to figure out a way to get that ambient conversation turned into text that then, you know, accelerates the doctor … takes all the important information. That&#8217;s a really hard problem, right. You&#8217;re having a conversation with a patient about their knee pain, but you&#8217;re also talking about, you know, their cousin&#8217;s wedding and their next vacation and their dog is sick or whatever and all that gets recorded, right.&nbsp;&nbsp;</p>



<p>And so, then you have to have the intelligence/context to be able to tease out what&#8217;s important for a note. And then it has to be at the performance level that a physician who, again, 20 years of training and education plus a huge, huge amount of, you know, need to get through his cases efficiently, that&#8217;s a really difficult problem.&nbsp;&nbsp;</p>



<p>And so, for a long time, there was a human-in-the-loop aspect to doing this because you needed a human to say, “This transcript&#8217;s great, but here&#8217;s actually what needs to go on the note.” And that can&#8217;t scale, as you know.&nbsp;&nbsp;</p>



<p>When the GPT-4, you know, model kind of, you know, showed what it was capable of, I think it was an immediate light bulb because there was no … you can ask any physician in your life, anyone in the audience, you know, what are your … what is the biggest pain point when you go to see your doctor? Like, “Oh, they don&#8217;t talk to me. They don&#8217;t look me in the eye. They&#8217;re rushing around trying to finish a note.”&nbsp;&nbsp;</p>



<p>If we could get that off their plate, that&#8217;s a huge unlock, Peter. And I think that, again, as you know, it&#8217;s now led to so much more. But that was kind of the initial, I think, reaction.&nbsp;</p>



<p><strong>LEE: </strong>And so, maybe that gets us into our next set of questions, our next topic, which is about the book and all the predictions we made in the book. Because Carey, Zak, and I—actually we did make a prediction that this technology would have a huge impact on this problem of clinical note-taking.&nbsp;&nbsp;</p>



<p>And so, you&#8217;re just right in the middle of that. You&#8217;re directly hands-on creating, I think, what is probably the most popular early product for doing exactly that. So, were we right? Were we wrong? What else do we need to understand about this?&nbsp;</p>



<p><strong>LUNGREN:</strong> No, you were right on. I think in the book, I think you called it like a paper shredder or something. I think you used a term like that. That&#8217;s exactly where the activity is right now and the opportunity.&nbsp;&nbsp;</p>



<p>I&#8217;ve even taken that so far as to say that when folks are asking about what the technology is capable of doing, we say, well, listen, it&#8217;s going to save time before it saves lives. It&#8217;ll do both. But right now, it&#8217;s about saving time.&nbsp;&nbsp;</p>



<p>It&#8217;s about peeling back the layers of the onion that if you, you know, put me in where I started medicine in 2003, and then fast-forward and showed me a day in the life of 2025, I would be shocked at what I was doing that <em>wasn&#8217;t</em> related to patient care, right. So, all of those layers that have been stacked up over the years, we can start finding ways to peel that back. And I think that&#8217;s exactly what we&#8217;re seeing.</p>



<p>And to your point, I think you mentioned this, too, which is, well, sure, we can do this transcript, and we can turn a note, but then we can do other things, right. We can summarize that in the patient&#8217;s language or education level of choice. We can pend orders. We can eventually get to a place of decision support. So, “Hey, did you think about this diagnosis, doctor?” Like those kinds of things.&nbsp;&nbsp;</p>



<p>And all those things, I think you highlighted beautifully, and again, it sounds like with, you know, a lot of, right, just kind of guesswork and prediction, but those things are actually happening every single day <em>right now</em>.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Well, so now, you know, in this episode, we&#8217;re really trying to understand, you know, where the technology industry is in delivering these kinds of things. And so from your perspective, you know, in the business that you&#8217;re helping to run here at Microsoft, you know, what are the things that are actually shipping as product versus things that clinicians are doing, let&#8217;s say, off label, just by using, say, ChatGPT on their personal mobile devices, and then what things aren&#8217;t happening?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah. I&#8217;ll start with the shipping part because I think you, again, you know my background, right. Academic clinician, did a lot of research, hadn&#8217;t had a ton of <em>product</em> experience.&nbsp;&nbsp;</p>



<p>In other words, like, you know, again, I&#8217;m happy to show you what benchmarks we beat or a new technique or, you know, get a grant to do all this, or even frankly, you know, talk about startups. But to actually have an audience that is accustomed to a certain level of performance for the solutions that they use, to be able to deliver something new at that same level of expectation, wow, that&#8217;s a big deal.&nbsp;&nbsp;</p>



<p>And again, this is part of the learning by, you know, kind of being around this environment that we have, which is we have this, you know, incredibly focused, very experienced clinical product team, right.</p>



<p>And then I think on the other side, to your point about the general-purpose aspect of this, it&#8217;s no secret now, right, that, you know, this is a useful technology in a lot of different medical applications. And let&#8217;s just say that there&#8217;s a lot of knowledge that can be used, particularly by the physician community. And I think the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://informatics.bmj.com/content/31/1/e101102" target="_blank" rel="noreferrer noopener">most recent survey I saw</a> was from the British Medical Journal, which said, hey, you know, which doctors are using … are you willing to tell us, you know, what you&#8217;re doing? And it turns out that folks are, what, 30% or so said that they were using it regularly in clinic <a href="#ftn_2">[2]</a>. And again, this is the general, this is the API or whatever off the shelf.</p>



<p>And then frankly, when they ask what they&#8217;re using it for, tends to be things like, “Hey, differential, like, help me fill in my differential or suggest … ” and to me, I think what that created, at least—and you&#8217;re starting to see this trend really accelerate in the US especially—is, well, listen, we can&#8217;t have everybody pulling out their laptops and potentially exposing, you know, patient information by accident or something to a public API.&nbsp;&nbsp;</p>



<p>We have to figure this out, and so brilliantly, I think NYU [New York University] was one of the first. Now I think there&#8217;s 30 plus institutions that said, listen, “OK, we know this is useful to the entire community in the healthcare space.” Right?<strong> </strong>We know the administrators and nurses and everybody thinks this is great.&nbsp;&nbsp;</p>



<p>We can&#8217;t allow this sort of to be a very loosey-goosey approach to this, right, given this sort of environment. So, what we&#8217;ll do is we&#8217;ll set up a HIPAA-compliant instance to allow anyone in the community—you know, in the health system—to use the models, and then whatever, the newest model comes, it gets hosted, as well.&nbsp;&nbsp;</p>



<p>And what&#8217;s cool about that—and that&#8217;s happened now a lot of places—is that at the high level … first of all, people get to use it and experiment and learn. But at the high level, they&#8217;re actually seeing what are the common use cases. Because you could ask 15 people and you might get super long lists, and it may not help you decide what to operationalize in your health system.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>But let me ask you about that. When you observe that, are there times when you think, “Oh, some specific use cases that we&#8217;re observing in that sort of organic way need to be taken into specialized applications and made into products?” Or is it best to keep these things sort of, you know, open-chat-interface types of general-purpose platform?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> Honestly, it&#8217;s both, and that&#8217;s exactly what we&#8217;re seeing. I&#8217;m most familiar with Stanford, kind of, the work that Nigam Shah leads on this. But he, he basically, … you know, there&#8217;s a really great paper that is coming out in JAMA, but basically saying, “Here&#8217;s what our workforce is using it for. Here are the things in the literature that would suggest what would be popular.”&nbsp;&nbsp;</p>



<p>And some of those line up, like helping with a clinical diagnosis or documentation, but some of them don&#8217;t. But for the most part, the stuff that flies to the top, those are opportunities to operationalize and productize, etc. And I think that&#8217;s exactly what we&#8217;re seeing.&nbsp;</p>



<p><strong>LEE: </strong>So, let&#8217;s get into some of the specific predictions. We&#8217;ve, I think, beaten note-taking to death here. But there&#8217;s other kinds of paperwork, like filling out prior authorization request forms or referral letters, an after-visit note or summary to give instructions to patients, and so on. And these were all things that we were making guesses in our book might be happening. What&#8217;s the reality there?&nbsp;</p>



<p><strong>LUNGREN:</strong> I&#8217;ve seen every single one of those. In fact, I&#8217;ve probably seen a dozen startups too, right, doing exactly those things. And, you know, we touched a little bit on translation into the actual clinic. And that&#8217;s actually another thing that I used to kind of underappreciate, which is that, listen, you can have a computer scientist and a physician or nurse or whatever, like, give the domain expertise, and you think you&#8217;re ready to build something.&nbsp;&nbsp;</p>



<p>The health IT [LAUGHS] is another part of that Venn diagram that&#8217;s so incredibly critical, and then exactly how are you going to bring that into the system. That&#8217;s a whole new ballgame.&nbsp;</p>



<p>And so I do want to do a callout because the collaboration that we have with Epic is monumental because here, you have the system of record that most physicians, at least in the US, use. And they&#8217;re going to use an interface and they&#8217;re going to have an understanding of, hey, we know these are pain points, and so I think there&#8217;s some really, really cool, you know, new innovations that are coming out of the relationship that we have with Epic. And certainly the audience may be familiar with those, that I think will start to knock off a lot of the things that you predicted in your book relatively soon.&nbsp;</p>



<p><strong>LEE: </strong>I think most of the listeners to this podcast will know what Epic is. But for those that are unfamiliar with the health industry, and especially the technology foundation, Epic is probably the largest provider of electronic health record systems. And, of course, in collaboration with you and your team, they&#8217;ve been integrating generative AI quite a bit. Are there specific uses that Epic is making and deploying that get you particularly excited?&nbsp;</p>



<p><strong>LUNGREN:</strong> First of all, the ambient note generation, by the way, is integrated into Epic now. So like, you know, it&#8217;s not another screen, another thing for physicians. So that&#8217;s a huge, huge unlock in terms of the translation.</p>



<p>But then Epic themselves, so they have, I guess, on the last roadmap that they talked [about], more than 60, but the one that&#8217;s kind of been used now is this inbox response.&nbsp;</p>



<p>So again, maybe someone might not be familiar with, why is it such a big deal? Well, if you&#8217;re a physician, you already have, you know, 20 patients to see that day and you got all those notes to do, and then Jevons paradox, right. So if you give me better access to my doctor, well, maybe I won&#8217;t make an appointment. I&#8217;m just going to send him a note and this is kind of this inbox, right.&nbsp;&nbsp;</p>



<p>So then at the end of my day, I got to get all my notes done. And then I got to go through all the inbox messages I&#8217;ve received from all of my patients and make sure that they&#8217;re not like having chest pain and they&#8217;re blowing it off or something.&nbsp;&nbsp;</p>



<p>Now that&#8217;s a lot of work and the cold start problem of like, OK, I to respond to them. So Epic has leveraged this system to say, “Let me just draft a note for you,” understanding the context of, you know, what&#8217;s going on with the patient, etc. And you can edit that and sign it, right. So you can accelerate some of those … so that&#8217;s probably one I&#8217;m most excited about. But there&#8217;s so many right now.&nbsp;</p>



<p><strong>LEE: </strong>Well, I think I need to let you actually state the name of the clinical note-taking product that you&#8217;re associated with. Would you like to do that? [LAUGHS]&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Sure. Yeah, it&#8217;s called DAX Copilot <a href="#ftn_3">[3]</a>. And for the record, it is the fastest-growing copilot in the Microsoft ecosystem. We&#8217;re very proud of that. Five hundred institutions already are using it, and millions of notes have already been created with it. And the feedback has been tremendous.</p>



<p><strong>LEE: </strong>So, you sort of referred to this a little bit, you know, this idea of AI being a second set of eyes. So, doctor makes some decisions in diagnosis or kind of working out potential treatments or medication decisions. And in the book, you know, we surmise that, well, AI might not replace the doctor doing those things. It could but might not. But AI could possibly reduce errors if doctors and nurses are making decisions by just looking at those decisions and just checking them out. Is that happening at all, and what do you see the future there?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, I would say, you know, that&#8217;s kind of the jagged edge of innovation, right, where sometimes the capability gets ahead of the ability to, you know, operationalize that. You know, part of that is just related to the systems. The evidence has been interesting on this. So, like, you know this, our colleague Eric Horvitz has been doing a lot of work in sort of looking at physician, physician with GPT-4, let&#8217;s say, and then GPT-4 alone for a whole variety of things. You know, we&#8217;ve been saying to the world for a long time, particularly in the narrow AI days, that AI plus human is better than either alone. We&#8217;re not really seeing that bear out really that well yet in some of the research.&nbsp;&nbsp;</p>



<p>But it is a signal to me and to the use case you&#8217;re suggesting, which is that if we let this system, in the right way, kind of handle a lot of the safety-net aspects of what we do but then also potentially take on some of the things that maybe are not that challenging or at least somewhat simple.&nbsp;&nbsp;</p>



<p>And of course, this is really an interesting use case in my world, in the vision world, which is that we know these models are multimodal, right. They can process images and text. And what does that look like for pathologists or radiologists, where we do have a certain percentage of the things we look at in a given day are normal, right? Or as close to normal as you can imagine. So is there a way to do that? And then also, by the way, have a safety net.&nbsp;&nbsp;</p>



<p>And so I think that this is an extremely active area right now. I don&#8217;t think we&#8217;ve figured out exactly how to have the human and AI model interact in this space yet. But I know that there&#8217;s a lot of attempts at it right now.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think, you know, this idea of a true copilot, you know, a true collaborator, you know, I think is still something that&#8217;s coming. I think we&#8217;ve had a couple of decades of people being trained to think of computers as question-answering machines. Ask a question, get an answer. Provide a document, get a summary. And so on.&nbsp;&nbsp;</p>



<p>But the idea that something might actually be this second set of eyes just assisting you all day continuously, I think, is a new mode of interaction. And we haven&#8217;t quite figured that out.&nbsp;&nbsp;</p>



<p>Now, in preparation for this podcast, Matt, you said that you actually used AI to assist you in getting ready. [LAUGHS] Would you like to share what you learned by doing that?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, it&#8217;s very funny. So, like, you may have heard <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.oneusefulthing.org/p/detecting-the-secret-cyborgs" target="_blank" rel="noreferrer noopener">this term coined by Ethan Mollick called the “secret cyborg,”<span class="sr-only"> (opens in new tab)</span></a> which is sort of referring to the phenomena of folks using GPT, realizing it can actually help them a ton in all kinds of parts of their work, but not necessarily telling anybody that they&#8217;re using it, right.&nbsp;&nbsp;</p>



<p>And so in a similar secret cyborgish way, I was like, “Well, listen, you know, I haven&#8217;t read your book in like a year. I recommend it to everybody. And [I need] just a refresher.” So what I did was I took your book, I put it into GPT-4, OK, and asked it to sort of talk about the predictions that you made.&nbsp;&nbsp;</p>



<p>And then I took that and put it in the stronger reasoning model—in this case, the “deep research” that you may have just seen or heard of and the audience from OpenAI—and asked it to research all the current papers, you know, and blogs and whatever else and tell me like what was right, what was wrong in terms of the predictions. [LAUGHS]&nbsp;&nbsp;</p>



<p>So it, actually, it was an incredible thing. It&#8217;s a, like, what, six or seven pages. It probably would have taken me two weeks, frankly, to do this amount of work.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>I&#8217;ll be looking forward to reading that in the New England Journal of Medicine shortly.&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] That&#8217;s right. Yeah, no, don&#8217;t, before this podcast comes out, I&#8217;ll submit it as an opinion piece. No. [LAUGHS] But, yeah, but I think on balance, incredibly insightful views. And I think part of that was, you know, your team that got together really had a lot of different angles on this. But, you know, and I think the only area that was, like, which I&#8217;ve observed as well, it&#8217;s just, man, this can do a lot for education.&nbsp;&nbsp;</p>



<p>We haven&#8217;t seen … I don&#8217;t think we&#8217;re looking at this as a tutor. To your point, we&#8217;re kind of looking at it as a transactional in and out. But as we&#8217;ve seen in all kinds of data, both in low-, middle-income countries and even in Harvard, using this as a tutor can really accelerate your knowledge and in profound ways.&nbsp;&nbsp;</p>



<p>And so that is probably one area where I think your prediction was maybe slightly even further ahead of the curve because I don&#8217;t think folks have really grokked that opportunity yet.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, and for people who haven&#8217;t read the book, you know, the guess was that you might use this as a training aid if you&#8217;re an aspiring doctor. For example, you can ask GPT-4 to pretend to be a patient that presents a certain way and that you are the doctor that this patient has come to see. And so you have an interaction. And then when you say end of encounter, you ask GPT-4 to assess how well you did. And we thought that this might be a great training aid, and to your point, it seems not to have materialized.&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> There&#8217;s some sparks. You know, with, like, communication, end-of-life conversations that no physician loves to have, right. It&#8217;s very, very hard to train someone in those. I&#8217;ve seen some work done, but you&#8217;re right. It&#8217;s not quite hit mainstream yet.&nbsp;</p>



<p><strong>LEE:</strong> On the subject of things that we missed, one thing that you&#8217;ve been very, very involved in in the last several months has been in shipping products that are multimodal. So that was something I think that we missed completely. What is the current state of affairs for multimodal, you know, healthcare AI, medical AI?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, the way I like to explain it—and first of all, no fault to you, but this is not an area that, like, we were just so excited about the text use cases that I can&#8217;t fault you. But yeah, I mean, so if we look at healthcare, right, how we take care of patients today, as you know, the vast majority of the data in terms of just data itself is actually not in text, right. It&#8217;s going be in pathology and genomics and radiology, etc.&nbsp;&nbsp;</p>



<p>And it seems like an opportunity here to watch this huge curve just goes straight up in the general reasoning and frankly medical competency and capabilities of the models that are coming and continue to come but then to see that it&#8217;s not as proficient for medical-specific imaging and video and, you know, other data types. And that gap is, kind of, what I describe as the multimodal medical AI gap.&nbsp;&nbsp;</p>



<p>We&#8217;re probably in GPT-2 land, right, for this other modality types versus the, you know, we&#8217;re now at o3, who knows where we&#8217;re going to go. At least in our view, we can innovate in that space.&nbsp;&nbsp;</p>



<p>How do we help bring those innovations to the broader community to close that gap and see some of these use cases really start to accelerate in the multimodal world?&nbsp;&nbsp;</p>



<p>And I think we&#8217;ve taken a pretty good crack at that. A lot of that is credit to the innovative work. I mean, MSR [Microsoft Research] was two or three years ahead of everyone else on a lot of this. And so how do we package that up in a way that the community can actually access and use? And so, we took a lot of what your group had done in, let&#8217;s just say, radiology or pathology in particular, and say, “OK, well, let&#8217;s put this in an ecosystem of other models.” Other groups can participate in this, but let&#8217;s put it in a platform where maybe I&#8217;m really competent in radiology or pathology. How do I connect those things together? How do I bring the general reasoner knowledge into a multimodal use case?&nbsp;&nbsp;</p>



<p>And I think that&#8217;s what we&#8217;ve done pretty well so far. We have a lot of work to do still, but this is very, very exciting. We&#8217;re seeing just such a ton of interest in building with the tools that we put out there.&nbsp;</p>



<p><strong>LEE:</strong> Well, I think how rapidly that&#8217;s advancing has been a surprise to me. So I think we&#8217;re running short on time. So two last questions to wrap up this conversation. The first one is, as we think ahead on AI in medicine, what do you think will be the biggest changes or make the biggest differences two years from now, five years from now, 10 years from now?</p>



<p><strong>LUNGREN:</strong> This is really tough. OK. I think the two-year timeframe, I think we will have some autonomous agent-based workflows for a lot of the &#8230; what I would call undifferentiated heavy lifting in healthcare.&nbsp;&nbsp;</p>



<p>And this is happening in, you know, the pharmaceutical industry, the payer … every aspect is sort of looking at their operations at a macro level: where are these big bureaucratic processes that largely involve text and where can we shrink those down and really kind of unlock a lot of our workforce to do things that might be more meaningful to the business? I think that&#8217;s my safe one.&nbsp;&nbsp;</p>



<p>Going five years out, you know, I have a really difficult time grappling with this seemingly shrinking timeline to AGI [artificial general intelligence] that we hear from people who I would respect and certainly know more than me. And in that world, I think there&#8217;s only been <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/abs/10.1056/AIp2400559" target="_blank" rel="noreferrer noopener">one paper that I&#8217;ve seen that has attempted to say, what does that mean in healthcare<span class="sr-only"> (opens in new tab)</span></a> when we have this?&nbsp;&nbsp;</p>



<p>And the fact is, I actually don&#8217;t know. [LAUGHS] I wonder whether there&#8217;ll still be a gap in some modalities. Maybe there&#8217;ll be the ability to do new science, and all kinds of interesting things will come of that.&nbsp;&nbsp;</p>



<p>But then if you go all the way to your 10-year, I do feel like we&#8217;re going to have systems that are acting autonomously in a variety of capacities, if I&#8217;m being honest.&nbsp;&nbsp;</p>



<p>What I would like to see if I have any influence on some of this is, can we start to celebrate the closing of hospitals instead of opening them? Meaning that, can we actually start to address—at a personal, individual level—care? And maybe that&#8217;s outside the home, maybe that&#8217;s, you know, in a way that doesn&#8217;t have to use so many resources and, frankly, really be very reactive instead of proactive.&nbsp;&nbsp;</p>



<p>I really want to see that. That&#8217;s been the vision of precision medicine for, geez, 20-plus years. I feel like we&#8217;re getting close to that being something we can really tackle.&nbsp;</p>



<p><strong>LEE: </strong>So, we talked about Geoff Hinton and his famous prediction that we would soon not have human radiologists. And of course, maybe he got the date wrong. So, let&#8217;s reset the date to 2028. So, Matt, do you think Geoff is right or wrong?&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Yeah, so the way … I&#8217;m not going to dodge the question, but let me just answer this a different way.&nbsp;&nbsp;</p>



<p>We have a clear line of sight to go from images to draft reports. That is unmistakable. And that&#8217;s now in 2025. How it will be implemented and what the implications of that will be, I think, will be heavily dependent on the health system or the incentive structure for where it&#8217;s deployed.&nbsp;&nbsp;</p>



<p>So, if I&#8217;m trying to take a step back, back to my global health days, man, that can&#8217;t come fast enough. Because, you know, you have entire health systems, you know, in fact entire countries that have five, you know, medical imaging experts for the whole country, but they still need this to you know take care of patients.&nbsp;&nbsp;</p>



<p>Zooming in on today&#8217;s crisis in the US, right, we have the burnout crisis just as much as the doctors who are seeing patients and write notes. We can&#8217;t keep up with the volume. In fact, we&#8217;re not training folks fast enough, so there is a push pull; there may be a flip to your point of autonomous reads across some segments of what we do.&nbsp;&nbsp;</p>



<p>By 2028, I think that&#8217;s a reasonable expectation that we&#8217;ll have some form of that. Yes.&nbsp;</p>



<p><strong>LEE: </strong>I tend to agree, and I think things get reshaped, but it seems very likely that even far into the future we&#8217;ll have humans wanting to take care of other humans and be taken care of by humans.&nbsp;&nbsp;</p>



<p>Matt, this has been a fantastic conversation, and, you know, I feel it&#8217;s always a personal privilege to have a chance to work with someone like you so keep it up.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>LUNGREN: </strong>Thank you so much, Peter. Thanks for having me.&nbsp;</p>



<p><strong>LEE:</strong> I&#8217;m always so impressed when I talk to Matt, and I feel lucky that we get a chance to work together here at Microsoft. You know, one of the things that always strikes me whenever I talk to him is just how disruptive generative AI has been to a business like Nuance. Nuance has had clinical note-taking as part of their product portfolio for a long, long time. And so, you know, when generative AI comes along, it&#8217;s not only an opportunity for them, but also a threat because in a sense, it opens up the possibility of almost anyone being able to make clinical note-taking capabilities into products.&nbsp;&nbsp;</p>



<p>It&#8217;s really interesting how Matt&#8217;s product, DAX Copilot, which since the time that we had our conversation has expanded into a full healthcare workflow product called <a href="https://www.microsoft.com/en-us/health-solutions/clinical-workflow/dragon-copilot">Dragon Copilot</a>, has really taken off in the marketplace and how many new competing AI products have also hit the market, and all in just two years, because of generative AI.&nbsp;&nbsp;</p>



<p>The other thing, you know, that I always think about is just how important it is for these kinds of systems to work together and especially how they integrate into the electronic health record systems. This is something that Carey, Zak, and I didn&#8217;t really realize fully when we wrote our book. But you know, when you talk to both Matt and Seth, of course, we see how important it is to have that integration.&nbsp;&nbsp;</p>



<p>Finally, what a great example of yet another person who is both a surgeon and a tech geek. [LAUGHS] People sometimes think of healthcare as moving very slowly when it comes to new technology, but people like Matt are actually making it happen much more quickly than most people might expect.&nbsp;&nbsp;</p>



<p>Well, anyway, as I mentioned, we also had a chance to talk to Seth Hain, and so here&#8217;s my conversation with Seth:</p>



<p><strong>LEE:</strong> Seth, thank you so much for joining.&nbsp;&nbsp;</p>



<p><strong>SETH HAIN:</strong> Well, Peter, it&#8217;s such an exciting time to sit down and talk about this topic. So much has changed in the last two years. Thanks for inviting me.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, in fact, I think in a way both of our lives have been upended in many ways by the emergence of AI. [LAUGHTER]&nbsp;&nbsp;</p>



<p>The traditional listeners of the Microsoft Research Podcast, I think for the most part, aren&#8217;t steeped in the healthcare industry. And so maybe we can just start with two things. One is, what is Epic, really? And then two, what is your job? What does the senior vice president for R&D at Epic do every day?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, well, let&#8217;s start with that first question. So, what is Epic? Most people across the world experience Epic through something we call MyChart. They might use it to message their physician. They might use it to check the lab values after they&#8217;ve gotten a recent test. But it&#8217;s an app on their phone, right, for connecting in with their doctors and nurses and really making them part of the care team.&nbsp;&nbsp;</p>



<p>But the software we create here at Epic goes beyond that. It&#8217;s what runs in the clinic, what runs at the bedside, in the back office to help facilitate those different pieces of care, from collecting vital information at the bedside to helping place orders if you&#8217;re coming in for an outpatient visit, maybe with a kiddo with an earache, and capturing that note and record of what happened during that encounter, all the way through back-office encounters, back-office information for interacting with payers as an example.&nbsp;&nbsp;</p>



<p>And so, we provide a suite of software that health systems and increasingly a broader set of the healthcare ecosystem, like payers and specialty diagnostic groups, use to connect with that patient at the center around their care.&nbsp;</p>



<p>And my job is to help our applications across the company take advantage of those latest pieces of technology to help improve the efficiency of folks like clinicians in the exam room when you go in for a visit. We&#8217;ll get into, I imagine, some use cases like ambient conversations, capturing that conversation in the exam room to help drive some of that documentation.&nbsp;&nbsp;</p>



<p>But then providing that platform for those teams to build those and then strategize around what to create next to help both the physicians be efficient and also the health systems. But then ultimately continuing to use those tools to advance the science of medicine.&nbsp;</p>



<p><strong>LEE:</strong> Right. You know, one thing that I explain to fellow technologists is that I think today health records are almost entirely digital. I think the last figures I saw is well over 99% of all health records are digital.&nbsp;&nbsp;</p>



<p>But in the year 2001, fewer than 15% of health records were digital. They were literally in folders on paper in storerooms, and if you&#8217;re old enough, you might even remember seeing those storerooms.&nbsp;&nbsp;</p>



<p>So, it&#8217;s been quite a journey. Epic and Epic&#8217;s competitors—though I think Epic is really the most important company—have really moved the entire infrastructure of record keeping and other communications in healthcare to a digital foundation.&nbsp;&nbsp;</p>



<p>And I think one thing we&#8217;ll get into, of course, one of the issues that has really become, I think, a problem for doctors and nurses is the kind of clerical or paperwork, record-keeping, burden. And for that reason, Epic and Epic systems end up being a real focus of attention. And so, we&#8217;ll get into that in a bit here.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> And I think that hits, just to highlight it, on both sides. There is both the need to capture documentation; there&#8217;s also the challenge in reviewing it.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>HAIN: </strong>The average medical record these days is somewhere between the length of <em>Fahrenheit 451</em> and <em>To Kill a Mockingbird</em>. [LAUGHTER] So there&#8217;s a fair amount of effort going in on that review side, as well.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, indeed. So much to get into there. But I would like to talk about encounters with AI. So obviously, I think there are two eras here: before the emergence of ChatGPT and what we now call of as generative AI and afterwards. And so, let&#8217;s take the former.&nbsp;&nbsp;</p>



<p>Of course, you&#8217;ve been thinking about machine learning and health data probably for decades. Do you have a memory of how you got into this? Why did you get an interest in data analytics and machine learning in the first place?&nbsp;</p>



<p><strong>HAIN: </strong>Well, my background, as you noted, is in mathematics before I came to Epic. And the sort of patterns and what could emerge were always part of what drove that. Having done development and kind of always been around computers all my life, it was a natural transition as I came here.&nbsp;&nbsp;</p>



<p>And I started by really focusing on, how do we scale systems for the very largest organizations, making sure they are highly available and also highly responsive? Time is critical in these contexts in regards to rapidly getting information to doctors and nurses.&nbsp;&nbsp;</p>



<p>And then really in the, say, in the 2010s, there started to be an emergence of capabilities from a storage and compute perspective where we could begin to build predictive analytics models. And these were models that were very focused, right. It predicted the likelihood somebody would show up for an appointment. It predicted the likelihood that somebody may fall during an inpatient stay, as an example.&nbsp;&nbsp;</p>



<p>And I think a key learning during that time period was thinking through the full workflow. What information was available at that point in time, right? At the moment somebody walks into the ED [emergency department], you don&#8217;t have a full picture to predict the likelihood that they may deteriorate during an inpatient encounter.&nbsp;&nbsp;</p>



<p>And in addition to what information was available was, what can you do about it? And a key part of that was how do we help get the right people in the right point in time at the bedside to make an assessment, right? It was a human-in-the-loop type of workflow where, for example, you would predict deterioration in advance and have a nurse come to the bedside or a physician come to the bedside to assess.&nbsp;&nbsp;</p>



<p>And I think that combination of narrowly focused predictive models with an understanding that to have them make an impact you had to think through the full workflow of where a human would make a decision was a key piece.&nbsp;</p>



<p><strong>LEE: </strong>Obviously there is a positive human impact. And so, for sure, part of the thought process for these kinds of capabilities comes from that.<strong>&nbsp;</strong>&nbsp;</p>



<p>But Epic is also a business, and you have to worry about, you know, what are doctors and clinics and healthcare systems willing to buy. And so how do you balance those two things, and do those two things ever come into conflict as you&#8217;re imagining what kinds of new capabilities and features and products to create?&nbsp;</p>



<p><strong>HAIN:</strong> Two, sort of, two aspects I think really come to mind. First off, generally speaking, we see analytics and AI as a <em>part</em> of the application. So, in that sense, it&#8217;s not something we license separately. We think that those insights and those pieces of data are part of what makes the application meaningful and impactful.&nbsp;&nbsp;</p>



<p>At the scale that many of these health systems operate and the number of patients that they care for, as well as having tens of thousands of users in the system daily, one needs to think about the compute overhead …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>HAIN:</strong> … that these things cause. And so, in that regard, there is always a ROI assessment that is taking place to some degree around, what happens if this runs at full scale? And in a way, that really got accelerated as we went into the generative AI era.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right. OK. So, you mentioned generative AI. What was the first encounter, and what was that experience for you?</p>



<p><strong>HAIN:</strong> So, in the winter of ’22 and into 2023, I started experimenting alongside you with what we at that time called DV3, or Davinci 3, and eventually became GPT-4. And immediately, a few things became obvious. The tool was highly general purpose. One was able to, in putting in a prompt, have it sort of convert into the framing and context of a particular clinical circumstance and reason around that context. But I think the other thing that started to come to bear in that context was there was a fair amount of latent knowledge inside of it that was very, very different than anything we&#8217;d seen before. And, you know, there&#8217;s some examples from the <a href="https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/?msockid=12da2addb70263b40f2e3f57b6c56288" target="_blank" rel="noreferrer noopener">Sparks of AGI paper from Microsoft Research</a>, where a series of objects end up getting stacked together in the optimal way to build height. Just given the list of objects, it seems to have a understanding of physical space that it intuited from the training processes we hadn&#8217;t seen anywhere. So that was an entirely new capability that programmers now had access to.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Well in fact, you know, I think that winter of 2022, and we&#8217;ll get into this, one of your projects that you&#8217;ve been running for quite a few years is something called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cosmos.epic.com/" target="_blank" rel="noreferrer noopener">Cosmos<span class="sr-only"> (opens in new tab)</span></a>, which I find exceptionally interesting. And I was motivated to understand whether this type of technology could have an impact there.&nbsp;&nbsp;</p>



<p>And so, I had to receive permission from both OpenAI and Microsoft to provide you with early access.&nbsp;&nbsp;</p>



<p>When I did first show this technology to you, you must have had an emotional response, either skepticism or … I can&#8217;t imagine you just trusted, you know, trusted me to the extent of believing everything I was telling you.&nbsp;</p>



<p><strong>HAIN:</strong> I think there&#8217;s always a question of, what is it actually, right? It&#8217;s often easy to create demos. It&#8217;s often easy to show things in a narrow circumstance. And it takes getting your hands on it and really spending your 10,000 hours digging in and probing it in different ways to see just how general purpose it was.&nbsp;&nbsp;</p>



<p>And so, the skepticism was really around, how applicable can this be broadly? And I think the second question—and we&#8217;re starting to see this play out now in some of the later models—was, is this just a language thing? Is it narrowly only focused on that? Or can we start to imagine other modalities really starting to factor into this? How will it impact basic sciences? Those sorts of things.</p>



<p>On a personal note, I mean, I had, at that point, now they&#8217;re now 14 and 12, two kids that I wondered, what did this mean for them? What is the right thing for them to be studying? And so I remember sleepless nights on that topic, as well.&nbsp;</p>



<p><strong>LEE: </strong>OK, so now you get early access to this technology; you&#8217;re able to do some experimentation. I think one of the things that impressed me is just less than four months later at the major health tech industry conference, HIMSS, which also happened timing-wise to take place just after the public disclosure of GPT-4, Epic showed off some early prototype applications of generative AI. And so, describe what those were, and how did you choose what to try to do there?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, and we were at that point, we actually had the very first users live on that prototype, on that early version.&nbsp;&nbsp;</p>



<p>And the key thing we&#8217;d focused on—we started this development in very, very late December, January of 2023—was a problem that its origins really were during the pandemic.&nbsp;&nbsp;</p>



<p>So, during the pandemic, we started to see patients increasingly messaging their providers, nurses, and clinicians through MyChart, that patient portal I mentioned with about 190 million folks on it. And as you can imagine, that was a great opportunity in the context of COVID to limit the amount of direct contact between providers and patients while still getting their questions answered.&nbsp;&nbsp;</p>



<p>But what we found as we came out of the pandemic was that folks preferred it regardless. And that messaging volume had stayed very, very high and was a time-consuming effort for folks.&nbsp;&nbsp;</p>



<p>And so, the first use case we came out with was a draft message in the context of the message from the patient and understanding of their medical history using that medical record that we talked about.&nbsp;&nbsp;</p>



<p>And the nurse or physician using the tool had two options. They could either click to start with that draft and edit it and then hit send, or they could go back to the old workflow and start with a blank text box and write it from their own memory as they preferred.</p>



<p>And so that was that very first use case. There were many more that we had started from a development perspective, but, yeah, we had that rolling out right in March of 2023 there with the first folks.&nbsp;</p>



<p><strong>LEE: </strong>So, I know from our occasional discussions that some things worked very well. In fact, this is a real product now for Epic. And it seems to be really a very, very popular feature now. I know from talking to you that a lot of things have been harder. And so, I&#8217;d like to dive into that. As a developer, tech developer, you know, what&#8217;s been easy, what&#8217;s been hard, what&#8217;s in your mind still is left to do in terms of the development of AI?&nbsp;</p>



<p><strong>HAIN: </strong>Yeah. You know, the first thing that comes to mind sort of starting foundationally, and we hinted at this earlier in our conversation, was at that point in time, it was kind of per a message, rather compute-intensive to run these. And so, there were always trade-offs we were making in regards to how many pieces of information we would send into the model and how much would we request back out of it.&nbsp;&nbsp;</p>



<p>The result of that was that while kind of theoretically or even from a research perspective, we could achieve certain outcomes that were quite advanced, one had to think about, where you make those trade-offs from a scalability perspective as you wanted to roll that out to lot of folks. So …&nbsp;</p>



<p><strong>LEE:</strong> Were you charging your customers more money for this feature?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, essentially the way that we handle that is there&#8217;s compute that&#8217;s required. As I mentioned, the feature is just part of our application. So, it&#8217;s just what they get with an upgrade.&nbsp;&nbsp;</p>



<p>But that compute overhead is something that we needed to pass through to them. And so, it was something, particularly given both the staffing challenges, but also the margin pressures that health systems are feeling today, we wanted to be very cautious and careful about.&nbsp;</p>



<p><strong>LEE:</strong> And let&#8217;s put that on the stack because I do want to get into, from the selling perspective, that challenge and how you perceive health systems as a customer making those trade-offs. But let&#8217;s continue on the technical side here.&nbsp;</p>



<p><strong>HAIN:</strong> Yeah. On the technical side, it was a consideration, right. We needed to be thoughtful about how we used them. But going up a layer in the stack, at that time, there&#8217;s a lot of conversation in the industry around something called RAG, or <em>retrieval-augmented generation</em>.&nbsp;&nbsp;</p>



<p>And the idea was, could you pull the relevant bits, the relevant pieces of the chart, into that prompt, that information you shared with the generative AI model, to be able to increase the usefulness of the draft that was being created? And that approach ended up proving and continues to be to some degree, although the techniques have greatly improved, somewhat brittle, right. You have a general-purpose technology that is drafting the response.&nbsp;</p>



<p>But in many ways, you needed to, for a variety of pragmatic reasons, have somewhat brittle capability in regards to what you pulled into that approach. It tended to be pretty static. And I think this becomes one of the things that, looking forward, as these models have gotten a lot more efficient, we are and will continue to improve upon because, as you get a richer and richer amount of information into the model, it does a better job of responding.&nbsp;&nbsp;</p>



<p>I think the third thing, and I think this is going to be something we&#8217;re going to continue to work through as an industry, was helping users understand and adapt to these circumstances. So many folks when they hear AI think, it will just magically do everything perfectly.&nbsp;&nbsp;</p>



<p>And particularly early on with some of those challenges we&#8217;re talking about, it doesn&#8217;t. You know, if it&#8217;s helpful 85% of the time, that&#8217;s great, but it&#8217;s not going to be 100% of the time. And it&#8217;s interesting as we started, we do something we call immersion, where we always make sure that developers are right there elbow to elbow with the users of the software.&nbsp;</p>



<p>And one of the things that I realized through that experience with some of the very early organizations like UCSD [UC San Diego] or University of Wisconsin here in Madison was that even when I&#8217;m responding to an email or a physician is responding to one of these messages from a patient, depending on the patient and depending on the person, they respond differently.&nbsp;&nbsp;</p>



<p>In that context, there&#8217;s opportunity to continue to mimic that behavior as we go forward more deeply. And so, you learn a lot about, kind of, human behavior as you&#8217;re putting these use cases out into the world.&nbsp;</p>



<p><strong>LEE: </strong>So, you know, this increasing burden of electronic communications between doctors, nurses, and patients is centered in one part of Epic. I think that&#8217;s called your in-basket application, if I understand correctly.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> That&#8217;s correct.&nbsp;</p>



<p><strong>LEE:</strong> But that also creates, I think, a reputational risk and challenge for Epic because as doctors feel overburdened by this and they&#8217;re feeling burnt out—and as we know, that&#8217;s a big issue—then they point to, you know, “Oh, I&#8217;m just stuck in this Epic system.”&nbsp;&nbsp;</p>



<p>And I think a lot of the dissatisfaction about the day-to-day working lives of doctors and nurses then focuses on Epic. And so, to what extent do you see technologies like generative AI as, you know, a solution to that or contributing either positively or negatively to this?&nbsp;</p>



<p><strong>HAIN:</strong> You know, earlier I made the comment that in December, as we started to explore this technology, we realized there were a class of problems that now might have solutions that never did before.&nbsp;&nbsp;</p>



<p>And as we&#8217;ve started to dig into those—and we now have about 150 different use cases that are under development, many of which are live across … we&#8217;ve got about 350 health systems using them—one of the things we&#8217;ve started to find is that physicians, nurses, and others start to react to saying it&#8217;s helping them move forward with their job.&nbsp;&nbsp;</p>



<p>And examples of this, obviously the draft of the in-basket message response is one, but using ambient voice recognition as a kind of new input into the software so that when a patient and a physician sit down in the exam room, the physician can start a recording and that conversation then ends up getting translated or summarized, if you will, including using medical jargon, into the note in the framework that the physician would typically write.&nbsp;&nbsp;</p>



<p>Another one of those circumstances where they then review it, don&#8217;t need to type it out from scratch, for example, …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>HAIN:</strong> … and can quickly move forward.&nbsp;&nbsp;</p>



<p>I think looking forward, you know, you brought up Cosmos earlier. It&#8217;s a suite of applications, but at its core is a dataset of about 300 million de-identified patients. And so using generative AI, we built research tools on top of it. And I bring that up because it’s a precursor of how that type of deep analytics can be put into context at the point of care. That&#8217;s what we see this technology more deeply enabling in the future.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, when you are creating … so you said there are about 150 sort of integrations of generative AI going into different parts of Epic&#8217;s software products.&nbsp;&nbsp;</p>



<p>When you are doing those developments and then you&#8217;re making a decision that something is going to get deployed, one thing that people might worry about is, well, these AI systems hallucinate. They have biases. There are unclear accountabilities, you know, maybe patient expectations.&nbsp;&nbsp;</p>



<p>For example, if there&#8217;s a note drafted by AI that&#8217;s sent to a patient, does the patient have a right to know what was written by AI and what was written by the human doctor? So, can we run through how you have thought about those things?&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> I think one thing that is important context to set here for folks, and I think it’s often a point of confusion when I&#8217;m chatting with folks in public, is that their interaction with generative AI is typically through a chatbot, right. It&#8217;s something like ChatGPT or Bing or one of these other products where they&#8217;re essentially having a back-and-forth conversation.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>HAIN: </strong>And that is a dramatically different experience than how we think it makes sense to embed into an enterprise set of applications.&nbsp;&nbsp;</p>



<p>So, an example use case may be in the back office, there are folks that are <em>coding</em> encounters. So, when a patient comes in, right, they have the conversation with the doctor, the doctor documents it, that encounter needs to be billed for, and those folks in the back-office associate to that encounter a series of codes that provide information about how that billing should occur.</p>



<p>So, one of the things we did from a workflow perspective was add a selector pane to the screen that uses generative AI to suggest a likely code. Now, this suggestion runs the risk of hallucination. So, the question is, how do you build into the workflow additional checks that can help the user do that?&nbsp;&nbsp;</p>



<p>And so in this context, we always include a citation back to the part of the medical record that justifies or supports that code. So quickly on hover, the user can see, does this make sense before selecting it? And it&#8217;s those types of workflow pieces that we think are critical to using this technology as an aid to helping people make decisions faster, right. It&#8217;s similar to drafting documentation that we talked about earlier.&nbsp;&nbsp;</p>



<p>And it&#8217;s interesting because there&#8217;s a series of patterns that are … going back to the <em>AI Revolution</em> book you folks wrote two years ago. Some of these are really highlighted there, right. This idea of things like a universal translator is a common pattern that we ended up applying across the applications. And in my mind, translation, this may sound a little bit strange, but summarization is an example of translating a very long series of information in a medical record into the context that an ED physician might care about, where they have three or four minutes to quick review that very long chart.&nbsp;&nbsp;</p>



<p>And so, in that perspective, and back to your earlier comment, we added the summary into the workflow but always made sure that the full medical record was available to that user, as well. So, a lot of what we&#8217;ve done over the last couple of years has been to create a series of repeatable techniques in regards to both how to build the backend use cases, where to pull the information, feed it into the generative AI models.&nbsp;&nbsp;</p>



<p>But then I think more importantly are the user experience design patterns to help mitigate those risks you talked about and to maintain consistency across the integrated suite of applications of how those are deployed.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> You might remember from our book, we had a whole chapter on reducing paperwork, and I think that&#8217;s been a lot of what we&#8217;ve been talking about. I want to get beyond that, but before transitioning, let&#8217;s get some numbers.&nbsp;&nbsp;</p>



<p>So, you talked about messages drafted to patients, to be sent to patients. So, give a sense of the volume of what&#8217;s happening right now.&nbsp;</p>



<p><strong>HAIN:</strong> Oh, we are seeing across the 300 and, I think it&#8217;s, 48 health systems that are now using generative AI—and to be clear, we have about 500 health systems we have the privilege of working with, each with many, many hospitals—there are tens of thousands of physicians and nurses using the software. That includes drafting million-plus, for example, notes a month at this point, as well as helping to generate in a similar ballpark that number of responses to patients.&nbsp;&nbsp;</p>



<p>The thing I&#8217;m increasingly excited about is the broader set of use cases that we&#8217;re seeing folks starting to deploy now. One of my favorites has been … it&#8217;s natural that as part of, for example, a radiology workflow, in studying that image, the radiologist made note that it would be worth double checking, say in six to eight months, that the patient have this area scanned of their chest. Something looks a little bit fishy there, but there&#8217;s not &#8230;&nbsp;</p>



<p><strong>LEE:</strong> There&#8217;s not a definitive finding yet.&nbsp;</p>



<p><strong>HAIN:</strong> … there&#8217;s not a definitive finding at that point. Part of that workflow is that the patient&#8217;s physician place an order for that in the future. And so, we&#8217;re using generative AI to note that back to the physician. And with one click, allow them to place that order, helping that patient get better care.&nbsp;&nbsp;</p>



<p>That&#8217;s one example of dozens of use cases that are now live, both to help improve the care patients are getting but also help the workforce. So going back to the translation-summarization example, a nurse at the end of their shift needs to write up a summary of that shift for the next nurse for each …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>HAIN: </strong>… each patient that they care for. Well, they&#8217;ve been documenting information in the chart over those eight or 12 hours, right.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yep, yep.&nbsp;</p>



<p><strong>HAIN:</strong> So, we can use that information to quickly draft that end-of-shift note for the nurse. They can verify it with those citations we talked about and make any additions or edits that they need and then complete their end of day far more efficiently.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right. OK. So now let&#8217;s get to Cosmos, which has been one of these projects that I think has been your baby for many years and has been something that has had a profound impact on my thinking about possibilities. So first off, what is Cosmos?&nbsp;</p>



<p><strong>HAIN: </strong>Well, just as an aside, I appreciate the thoughtful comments. There is a whole team of folks here that are really driving these projects forward. And a large part of that has been, as you brought up, both Cosmos as a foundational capability but then beginning to integrate it into applications. And that&#8217;s what those folks spend time on.&nbsp;&nbsp;</p>



<p>Cosmos is this effort across hundreds of health systems that we have the privilege of working with to build out a de-identified dataset with today—and it climbs every day—but 300 million unique patient records in it.&nbsp;&nbsp;</p>



<p>And one of the interesting things about that structure is that, for example, if I end up in a hospital in Seattle and have that encounter documented at a health system in Seattle, I still—a <em>de-identified version of me</em>—still only shows up once in Cosmos, stitching together both my information from here in Madison, Wisconsin, where Epic is at, with that extra data from Seattle. The result is these 300 million unique longitudinal records that have a deep history associated with them.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>And just to be clear, a patient record might have hundreds or even thousands of individual, I guess what you would call, clinical records or elements.&nbsp;</p>



<p><strong>HAIN: </strong>That&#8217;s exactly right. It&#8217;s the breadth of information from orders and allergies and blood pressures collected, for example, in an outpatient setting to cancer staging information that might have come through as part of an oncology visit. And it&#8217;s coming from a variety of sources. We exchange information about 10 million times a day between different health systems. And that full picture is available within Cosmos in that way of the patient.&nbsp;</p>



<p><strong>LEE:</strong> So now why? Why Cosmos?&nbsp;</p>



<p><strong>HAIN:</strong> Why Cosmos? Well, the real ultimate aim is to put a deeply informed in-context perspective at the point of care. So, as a patient, if I&#8217;m in the exam room, it&#8217;s helpful for the physician and me to know what have similar patients like me experienced in this context. What was the result of that line of treatment, for example?&nbsp;</p>



<p>Or as a doctor, if I&#8217;m looking and working through a relatively rare or strange case to me, I might be able to connect with—this as an example workflow we built called Look-Alikes—with another physician who has seen similar patients or within the workflow see a list of likely diagnoses based on patients that have been in a similar context. And so, the design of Cosmos is to put those insights into the point of care in the context of the patient.&nbsp;&nbsp;</p>



<p>To facilitate those steps there, the first phase was building out a set of research tooling. So, we see dozens of papers a year being published by the health systems that we work with. Those that participate in Cosmos have access to it to do research on it. And so they use both a series of analytical and data science tools to do that analysis and then publish research. So, building up trust that way.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> The examples you gave are, like with Look-Alikes, it&#8217;s very easy, I think, for people outside of the healthcare world to imagine how that could be useful. So now why is GPT-4 or any generative AI relevant to this?&nbsp;</p>



<p><strong>HAIN:</strong> Well, so a couple of different pieces, right. Earlier we talked about—and I think this is the most important—how generative AI is able to cast things into a specific context. And so, in that way, we can use these tools to help both identify a cohort of patients similar to you when you&#8217;re in the exam room. And then also help present that information back in a way that relates to other research and understandings from medical literature to understand what are those likely outcomes.&nbsp;&nbsp;</p>



<p>I think more broadly, these tools and generative AI techniques in the transformer architecture envision a deeper understanding of sequences of events, sequences of words. And that starts to open up broader questions about what can really be understood about patterns and sequences of events in a patient&#8217;s journey.&nbsp;&nbsp;</p>



<p>Which if you didn&#8217;t know, the name Epic, just like a great long nation&#8217;s journey is told through an epic story, is a patient&#8217;s story. So that&#8217;s where it came from.&nbsp;</p>



<p><strong>LEE:</strong> So, we&#8217;re running up against our time together. And I always like to end with a more provocative question.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> Certainly.&nbsp;</p>



<p><strong>LEE:</strong> And for you, I wanted to raise a question that I think we had asked ourselves in the very earliest days that we were sharing Davinci 3, what we now know of as GPT-4, with each other, which is, is there a world in the future because of AI where we don&#8217;t need electronic health records anymore? Is there a world in the future without EHR?&nbsp;</p>



<p><strong>HAIN:</strong> I think it depends on how you define EHR. I see a world coming where we need to manage a hybrid workforce, where there is a combination of humans and something folks are sometimes calling <em>agents</em> working in concert together to care for more and more of our … of the country and of the world. And there is and will need to be a series of tools to help orchestrate that hybrid workforce. And I think things like EHRs will transform into helping that operate … be operationally successful.&nbsp;&nbsp;</p>



<p>But as a patient, I think there&#8217;s a very different opportunity that starts to be presented. And we&#8217;ve talked about kind of understanding things deeply in context. There&#8217;s also a real acceleration happening in science right now. And the possibility of bringing that second- and third-order effects of generative AI to the point of care, be that through the real-world evidence we were talking about with Cosmos or maybe personalized therapies that really are well matched to that individual. These generative AI techniques open the door for that, as well as the full lifecycle of managing that from a healthcare perspective all the way through monitoring after the fact.&nbsp;&nbsp;</p>



<p>And so, I think we&#8217;ll still be recording people&#8217;s stories. Their stories are relevant to them, and they can help inform the bigger picture. But I think the real question is, how do you put those in a broader context? And these tools open the door for a lot more.&nbsp;</p>



<p><strong>LEE:</strong> Well, that&#8217;s really a great vision for the future.&nbsp;&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Seth, I always really learn so much talking to you, and thank you so much for this great chat.&nbsp;</p>



<p><strong>HAIN:</strong> Thank you for inviting me.&nbsp;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> I see Seth as someone on the very leading frontier of bringing generative AI to the clinic and into the healthcare back office and at the full scale of our massive healthcare system. It&#8217;s always impressive to me how thoughtful Seth has had to be about how to deploy generative AI into a clinical setting.&nbsp;&nbsp;</p>



<p>And, you know, one thing that sticks out—and he made such a point of this—is, you know, generative AI in the clinical setting isn&#8217;t just a chatbot. They&#8217;ve had to really think of other ways that will guarantee that the human stays in the loop. And that&#8217;s of course exactly what Carey, Zak, and I had predicted in our book. In fact, we even had a full chapter of our book entitled “Trust but Verify,” which really spoke to the need in medicine to always have a human being directly involved in overseeing the process of healthcare delivery.&nbsp;</p>



<p>One technical point that Carey, Zak, and I completely missed, on the other hand, in our book, was the idea of something that Seth brought up called RAG, which is <em>retrieval-augmented generation</em>. That&#8217;s the idea of giving AI access to a database of information and allowing it to use that database as it constructs its answers. And we heard from Seth how fundamental RAG is to a lot of the use cases that Epic is deploying.&nbsp;</p>



<p>And finally, I continue to find Seth&#8217;s project called Cosmos to be a source of inspiration, and I&#8217;ve continued to urge every healthcare organization that has been collecting data to consider following a similar path.&nbsp;</p>



<p>In our book, we spent a great deal of time focusing on the possibility that AI might be able to reduce or even eliminate a lot of the clerical drudgery that currently exists in the delivery of healthcare. We even had a chapter entitled “The Paperwork Shredder.” And we heard from both Matt and Seth that that has indeed been the early focus of their work.&nbsp;&nbsp;</p>



<p>But we also saw in our book the possibility that AI could provide diagnoses, propose treatment options, be a second set of eyes to reduce medical errors, and in the research lab be a research assistant. And here in Epic’s Cosmos, we are seeing just the early glimpses that perhaps generative AI can actually provide new research possibilities in addition to assistance in clinical decision making and problem solving. On the other hand, that still seems to be for the most part in our future rather than something that&#8217;s happening at any scale today.&nbsp;</p>



<p>But looking ahead to the future, we can still see the potential of AI helping connect healthcare delivery experiences to the advancement of medical knowledge. As Seth would say, the ability to connect bedside to the back office to the bench. That&#8217;s a pretty wonderful future that will take a lot of work and tech breakthroughs to make it real. But the fact that we now have a credible chance of making that dream happen for real, I think that&#8217;s pretty wonderful.&nbsp;</p>



<p>[MUSIC TRANSITIONS TO THEME]&nbsp;</p>



<p>I&#8217;d like to say thank you again to Matt and Seth for sharing their experiences and insights. And to our listeners, thank you for joining us. We have some really great conversations planned for the coming episodes, including a look at how patients are using generative AI for their own healthcare, as well as an episode on the laws, norms, and ethics developing around AI and health, and more. We hope you&#8217;ll continue to tune in.</p>



<p>Until next time.</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>



<p id="ftn_1">[1] A provider of conversational, ambient, and generative AI, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/2022/03/04/microsoft-completes-acquisition-of-nuance-ushering-in-new-era-of-outcomes-based-ai/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">Nuance was acquired by Microsoft in March 2022<span class="sr-only"> (opens in new tab)</span></a>. Nuance solutions and capabilities are now part of Microsoft Cloud for Healthcare.</p>



<p id="ftn_2">[2] According to the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://informatics.bmj.com/content/31/1/e101102" target="_blank" rel="noreferrer noopener">survey<span class="sr-only"> (opens in new tab)</span></a>, of the 20% of respondents who said they use generative AI in clinical practice, 29% reported using the technology for patient documentation and 28% said they use it for differential diagnosis.</p>



<p id="ftn_3">[3] A month after the conversation was recorded, <a href="https://www.microsoft.com/en-us/health-solutions/clinical-workflow/dragon-copilot" target="_blank" rel="noreferrer noopener">Microsoft Dragon Copilot</a> was unveiled. Dragon Copilot combines and extends the capabilities of DAX Copilot and Dragon Medical One.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--2"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/">Real-world healthcare AI development and deployment—at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>VidTok introduces compact, efficient tokenization to enhance AI video processing</title>
		<link>https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/</link>
		
		<dc:creator><![CDATA[Tianyu He, Junliang Guo, Jiang Bian]]></dc:creator>
		<pubDate>Wed, 02 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135315</guid>

					<description><![CDATA[<p>The VidTok method can enable AI systems to process and generate videos more effectively. Compact tokenization reduces computational costs while maintaining video quality across a potentially diverse set of applications.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/">VidTok introduces compact, efficient tokenization to enhance AI video processing</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1024x576.png" alt="Diagram showing an overview of how video tokenizers work with stages labeled as Input, Encoder, Regularizer (Latent Space), Decoder, and Output. " class="wp-image-1135333" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1280x720.png 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1.png 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>Every day, countless videos are uploaded and processed online, putting enormous strain on computational resources. The problem isn’t just the sheer volume of data—it’s how this data is structured. Videos consist of raw pixel data, where neighboring pixels often store nearly identical information. This redundancy wastes resources, making it harder for systems to process visual content effectively and efficiently.</p>



<p>To tackle this, we’ve developed a new approach to compress visual data into a more compact and manageable form. In our paper “<a href="https://www.microsoft.com/en-us/research/publication/vidtok-a-versatile-and-open-source-video-tokenizer/" target="_blank" rel="noreferrer noopener">VidTok: A Versatile and Open-Source Video Tokenizer</a>,” we introduce a method that converts video data into smaller, structured units, or <em>tokens</em>. This technique provides researchers and developers in visual world modeling—a field dedicated to teaching machines to interpret images and videos—with a flexible and efficient tool for advancing their work.&nbsp;</p>



<h2 class="wp-block-heading" id="how-vidtok-works">How VidTok works<strong></strong></h2>



<p>VidTok is a technique that converts raw video footage into a format that AI can easily work with and understand, a process called <em>video tokenization</em>. This process converts complex visual information into compact, structured tokens, as shown in Figure 1.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="692" height="213" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1.png" alt="Diagram showing an overview of how video tokenizers work with stages labeled as Input, Encoder, Regularizer (Latent Space), Decoder, and Output. " class="wp-image-1135323" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1.png 692w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1-300x92.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1-240x74.png 240w" sizes="auto, (max-width: 692px) 100vw, 692px" /><figcaption class="wp-element-caption">Figure 1. An overview of how video tokenizers work, which form the basis of VidTok.</figcaption></figure>



<p>By simplifying videos into manageable chunks, VidTok can enable AI systems to learn from, analyze, and generate video content more efficiently. VidTok offers several potential advantages over previous solutions:</p>



<p><strong>Supports both discrete and continuous tokens.</strong> Not all AI models use the same “language” for video generation. Some perform best with continuous tokens—ideal for high-quality diffusion models—while others rely on discrete tokens, which are better suited for step-by-step generation, like language models for video. VidTok is a tokenizer that has demonstrated seamless support for both, making it adaptable across a range of AI applications.</p>



<p><strong>Operates in both causal and noncausal modes. </strong>In some scenarios, video understanding depends solely on past frames (causal), while in others, it benefits from access to both past and future frames (noncausal). VidTok can accommodate both modes, making it suitable for real-time use cases like robotics and video streaming, as well as for high-quality offline video generation.</p>



<p><strong>Efficient training with high performance.</strong> AI-powered video generation typically requires substantial computational resources. VidTok can reduce training costs by half through a two-stage training process—delivering high performance and lowering costs.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1002645">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: AI-POWERED EXPERIENCE</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/01/MSR-Chat-Promo.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft research copilot experience</h2>
				
								<p class="large">Discover more about research at Microsoft through our AI-powered experience</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
							Start now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="architecture">Architecture</h2>



<p>The VidTok framework builds on a classic 3D encoder-decoder structure but introduces 2D and 1D processing techniques to handle spatial and temporal information more efficiently. Because 3D architectures are computationally intensive, VidTok combines them with less resource-intensive 2D and 1D methods to reduce computational costs while maintaining video quality.</p>



<p><strong>Spatial processing.</strong> Rather than treating video frames solely as 3D volumes, VidTok applies 2D convolutions—pattern-recognition operations commonly used in image processing—to handle spatial information within each frame more efficiently.</p>



<p><strong>Temporal processing.</strong> To model motion over time, VidTok introduces the <em>AlphaBlender</em> operator, which blends frames smoothly using a learnable parameter. Combined with 1D convolutions—similar operations applied over sequences—this approach captures temporal dynamics without abrupt transitions.</p>



<p>Figure 2 illustrates VidTok’s architecture in detail.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1269" height="523" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2.png" alt="A diagram illustrating VidTok’s architecture, which integrates 2D+1D operations instead of relying solely on 3D techniques. The left side represents the encoder pathway, starting with a 3D InputBlock, followed by multiple 2D+1D DownBlocks and AlphaBlender Temporal DownBlocks. The right side shows the decoder pathway, mirroring the encoder with 2D+1D UpBlocks and AlphaBlender Temporal UpBlocks before reaching the 3D OutputBlock. A Regularizer module is connected at the bottom.  This approach strikes a balance between computational speed and high-quality video output. " class="wp-image-1135324" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2.png 1269w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-300x124.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-1024x422.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-768x317.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-240x99.png 240w" sizes="auto, (max-width: 1269px) 100vw, 1269px" /><figcaption class="wp-element-caption">Figure 2. VidTok’s architecture. It uses a combination of 2D and 1D operations instead of solely relying on 3D techniques, improving efficiency. For smooth frame transitions, VidTok employs the AlphaBlender operator in its temporal processing modules. This approach strikes a balance between computational speed and high-quality video output.</figcaption></figure>



<h2 class="wp-block-heading" id="quantization">Quantization</h2>



<p>To efficiently compress video data, AI systems often use quantization to reduce the amount of information that needs to be stored or transmitted. A traditional method for doing this is vector quantization (VQ), which groups values together and matches them to a fixed set of patterns (known as a codebook). However, this can lead to an inefficient use of patterns and lower video quality.</p>



<p>For VidTok, we use an approach called finite scalar quantization (FSQ). Instead of grouping values, FSQ treats each value separately. This makes the compression process more flexible and accurate, helping preserve video quality while keeping the file size small. Figure 3 shows the difference between the VQ and FSQ approaches.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="831" height="179" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17.png" alt="A diagram comparing Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). VQ maps input z to a learned codebook, selecting the closest entry, while FSQ quantizes z using fixed sets independently for each value. FSQ simplifies optimization and improves training stability. " class="wp-image-1135319" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17.png 831w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-300x65.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-768x165.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-240x52.png 240w" sizes="auto, (max-width: 831px) 100vw, 831px" /><figcaption class="wp-element-caption">Figure 3. VQ (left) relies on learning a codebook, while FSQ (right) simplifies the process by independently grouping values into fixed sets, making optimization easier. VidTok adopts FSQ to enhance training stability and reconstruction quality.</figcaption></figure>



<h2 class="wp-block-heading" id="training">Training</h2>



<p>Training video tokenizers requires significant computing power. VidTok uses a two-stage process:</p>



<ol class="wp-block-list">
<li>It first trains the full model on low-resolution videos.</li>



<li>Then, it fine-tunes only the decoder using high-resolution videos.</li>
</ol>



<p>This approach cuts training costs in half—from 3,072 to 1,536 GPU hours—while maintaining video quality. Older tokenizers, trained on full-resolution videos from the start, were slower and more computationally intensive.&nbsp;</p>



<p>VidTok&#8217;s method allows the model to quickly adapt to new types of videos without affecting its token distribution. Additionally, it trains on lower-frame-rate data to better capture motion, improving how it represents movement in videos.</p>



<h2 class="wp-block-heading" id="evaluating-vidtok">Evaluating VidTok</h2>



<p>VidTok&#8217;s performance evaluation using the MCL-JCV benchmark—a comprehensive video quality assessment dataset—and an internal dataset demonstrates its superiority over existing state-of-the-art models in video tokenization. The assessment, which covered approximately 5,000 videos of various types, employed four standard metrics to measure video quality:</p>



<ol class="wp-block-list">
<li>Peak Signal-to-Noise Ratio (PSNR)</li>



<li>Structural Similarity Index Measure (SSIM)</li>



<li>Learned Perceptual Image Patch Similarity (LPIPS)</li>



<li>Fréchet Video Distance (FVD)</li>
</ol>



<p>The following table and Figure 4 illustrate VidTok’s performance:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1651" height="680" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1.png" alt="Result table showing VidTok's performance compared to other models (MAGVIT-v2, OmniTokenizer, Cosmos-DV, CV-VAE, Open-Sora-v1.2, Open-Sora-Plan-v1.2, CogVideoX, Cosmos-CV) on two datasets (MCL-JCV and Internal-Val) with metrics including PSNR, SSIM, LPIPS, and FVD." class="wp-image-1135326" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1.png 1651w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-300x124.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-1024x422.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-768x316.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-1536x633.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-240x99.png 240w" sizes="auto, (max-width: 1651px) 100vw, 1651px" /><figcaption class="wp-element-caption">Table 1</figcaption></figure>



<p>The results indicate that VidTok outperforms existing models in both discrete and continuous tokenization scenarios. This improved performance is achieved even when using a smaller model or a more compact set of reference patterns, highlighting VidTok&#8217;s efficiency.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1181" height="372" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2.png" alt="Radar charts comparing the performance of discrete and continuous tokenization methods in VidTok and state-of-the-art methods using four metrics: PSNR, SSIM, LPIPS, and FVD. Larger chart areas indicate better overall performance. " class="wp-image-1135327" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2.png 1181w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-300x94.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-1024x323.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-768x242.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-240x76.png 240w" sizes="auto, (max-width: 1181px) 100vw, 1181px" /><figcaption class="wp-element-caption">Figure 4. Quantitative comparison of discrete and continuous tokenization performance in VidTok and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. Larger chart areas indicate better overall performance.</figcaption></figure>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead</h2>



<p>VidTok represents a significant development in video tokenization and processing. Its innovative architecture and training approach enable improved performance across various video quality metrics, making it a valuable tool for video analysis and compression tasks. Its capacity to model complex visual dynamics could improve the efficiency of video systems by enabling AI processing on more compact units rather than raw pixels.</p>



<p>VidTok serves as a promising foundation for further research in video processing and representation. The code for VidTok is available on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/VidTok" target="_blank" rel="noreferrer noopener">GitHub<span class="sr-only"> (opens in new tab)</span></a>, and we invite the research community to build on this work and help advance the broader field of video modeling and generation.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/">VidTok introduces compact, efficient tokenization to enhance AI video processing</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Ideas: Accelerating Foundation Models Research: AI for all</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/</link>
		
		<dc:creator><![CDATA[Gretchen Huizinga, Evelyne Viegas, Muhammed Idris, Cesar Torres Jr.]]></dc:creator>
		<pubDate>Mon, 31 Mar 2025 13:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1134446</guid>

					<description><![CDATA[<p>Innovative AI research often depends on access to resources. Microsoft wants to help. Technical Advisor Evelyne Viegas and distinguished faculty from two Minority Serving Institutions discuss the benefits of Microsoft’s Accelerating Foundation Models Research program in their lives and research.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/">Ideas: Accelerating Foundation Models Research: AI for all</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788.jpg" alt="Microsoft Research Podcast | Ideas: Evelyne Viegas, Muhammed Idris, Cesar Torres" class="wp-image-1134454" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=143858112&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&nbsp;</p>



<p>In this episode, host Gretchen Huizinga talks with three researchers about <a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/">Accelerating Foundation Models Research (AFMR)<span class="sr-only"> (opens in new tab)</span></a>, a global research network and resource platform that allows members of the larger academic community to push the boundaries of AI foundation models and explore exciting and unconventional collaborations across disciplines and institutions. <a href="https://www.microsoft.com/en-us/research/people/evelynev/">Evelyne Viegas<span class="sr-only"> (opens in new tab)</span></a>, a technical advisor at Microsoft Research, shares her vision for the program from the Microsoft perspective, while <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.uta.edu/academics/faculty/profile?user=cearto" target="_blank" rel="noreferrer noopener">Cesar Torres<span class="sr-only"> (opens in new tab)</span></a>, an assistant professor of computer science at the University of Texas at Arlington, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.msm.edu/about_us/FacultyDirectory/Medicine/MuhammedIdris/index.php" target="_blank" rel="noreferrer noopener">Muhammed Idris<span class="sr-only"> (opens in new tab)</span></a>, an assistant professor in the departments of medicine and public health at the Morehouse School of Medicine, tell their stories of how access to state-of-the-art foundation models is helping creative practitioners find inspiration from both their physical and virtual environments and making cancer-related health information more accessible and culturally congruent. The three recount their research journeys, including both frustrations and aspirations, and relate how AFMR resources have provided game-changing opportunities for Minority Serving Institutions and the communities they serve.&nbsp;</p>



<p> &nbsp;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<p><a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/">Accelerating Foundation Models Research</a><br>Collaboration homepage</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://hybridatelier.uta.edu/" target="_blank" rel="noreferrer noopener">The Hybrid Atelier<span class="sr-only"> (opens in new tab)</span></a><br>Homepage, The University of Texas at Arlington</p>



<p><a href="https://www.microsoft.com/en-us/research/blog/announcing-recipients-of-the-afmr-minority-serving-institutions-grant/" target="_blank" rel="noreferrer noopener">Announcing recipients of the AFMR Minority Serving Institutions grant</a><br>Microsoft Research Blog, January 30, 2024</p>



<p>&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/features/ai/ai-for-all-how-access-to-new-models-is-advancing-academic-research-from-astronomy-to-education/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">AI ‘for all’: How access to new models is advancing academic research, from astronomy to education<span class="sr-only"> (opens in new tab)</span></a><br>Microsoft Blog, March 12, 2024</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.press.jhu.edu/newsroom/morehouse-model-how-one-school-medicine-revolutionized-community-engagement-and-health-equity" target="_blank" rel="noreferrer noopener">The Morehouse Model: How One School of Medicine Revolutionized Community Engagement and Health Equity<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Book, July 10, 2020&nbsp;</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[TEASER]&nbsp;</p>



<p>[MUSIC PLAYS UNDER DIALOG]&nbsp;&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> So AFMR is really a program which enabled us to provide access to foundation models, but it&#8217;s also a global network of researchers. And so for us, I think when we started that program, it was making sure that AI was made available to anyone and not just the few, right? And really important to hear from our academic colleagues, what they were discovering and covering and what were those questions that we&#8217;re not even really thinking about, right? So that&#8217;s how we started with AFMR.</p>



<p><strong>CESAR TORRES:</strong> One of the things that the AFMR program has allowed me to see is this kind of ability to better visualize the terrain of creativity. And it&#8217;s a little bit of a double-edged sword because when we talk about disrupting creativity and we think about tools, it&#8217;s typically the case that the tool is making something easier for us. So my big idea is to actually think about tools that are purposely making us slower, that have friction, that have errors, that have failures. To say that maybe the easiest path is not the most advantageous, but the one that you can feel the most fulfillment or agency towards.</p>



<p><strong>MUHAMMED IDRIS:</strong> For me, I think what programs like AFMR have enabled us to do is really start thinking outside the box as to how will these or how can these emerging technologies revolutionize public health? What truly would it take for an LLM to understand context? And really, I think for the first time, we can truly, truly achieve personalized, if you want to use that term, health communication.&nbsp;</p>



<p>[TEASER ENDS]&nbsp;</p>



<p>[MUSIC PLAYS]&nbsp;</p>



<p><strong>GRETCHEN HUIZINGA:</strong> You’re listening to Ideas, a Microsoft Research podcast that dives deep into the world of technology research and the profound questions behind the code. I&#8217;m Gretchen Huizinga. In this series, we&#8217;ll explore the technologies that are shaping our future and big ideas that propel them forward.</p>



				</span>
				<span id="show-more-show-less-toggle-3" class="show-more-show-less-toggleable-content">
					



<p>[MUSIC FADES]&nbsp;</p>



<p>I&#8217;m excited to share the mic today with three guests to talk about a really cool program called Accelerating Foundation Models Research, or AFMR for short. With me is Cesar Torres, an assistant professor of computer science at the University of Texas, Arlington, and the director of a program called The Hybrid Atelier. More on that soon. I&#8217;m also joined by Muhammed Idris, an assistant professor of medicine at the Morehouse School of Medicine. And finally, I welcome Evelyne Viegas, a technical advisor at Microsoft Research. Cesar, Muhammed, Evelyne, welcome to Ideas!&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> Pleasure.&nbsp;</p>



<p><strong>CESAR TORRES:</strong> Thank you.&nbsp;</p>



<p><strong>MUHAMMED IDRIS:</strong> Thank you.&nbsp;</p>



<p><strong>HUIZINGA: </strong>So I like to start these episodes with what I&#8217;ve been calling the “research origin story” and since there are three of you, I&#8217;d like you each to give us a brief overview of your work. And if there was one, what big idea or larger than life person inspired you to do what you&#8217;re doing today? Cesar let&#8217;s start with you and then we&#8217;ll have Muhammed and Evelyne give their stories as well. </p>



<p><strong>CESAR TORRES:</strong> Sure, thanks for having me. So, I work at the frontier of creativity especially thinking about how technology could support or augment the ways that we manipulate our world and our ideas. And I would say that the origin of why I happened into this space can really come back down to a “bring your kid to work” day. [LAUGHTER] My dad, who worked at Maquiladora, which is a factory on the border, took me over – he was an accountant – and so he first showed me the accountants and he&#8217;s like look at the amazing work that these folks are doing. But the reality is that a lot of what they do is hidden behind spreadsheets and so it wasn&#8217;t necessarily the most engaging. Suffice to say I did not go into accounting like my dad! [LAUGHTER] But then he showed us the chemical engineer in the factory, and he would tell me this chemical engineer holds the secret formula to the most important processes in the entire company. But again, it was this black box, right? And I got a little bit closer when I looked at this process engineer who was melting metal and pulling it out of a furnace making solder and I thought wow, that&#8217;s super engaging but at the same time it&#8217;s like it was hidden behind machinery and heat and it was just unattainable. And so finally I saw my future career and it was a factory line worker who was opening boxes. And the way that she opened boxes was incredible. Every movement, every like shift of weight was so perfectly coordinated. And I thought, here is the peak of human ability. [LAUGHTER] This was a person who had just like found a way to leverage her surroundings, to leverage her body, the material she was working with. And I thought, this is what I want to study. I want to study how people acquire skills. And I realized … that moment, I realized just how important the environment and visibility was to being able to acquire skills. And so from that moment, everything that I&#8217;ve done to this point has been trying to develop technologies that could get everybody to develop a skill in the same way that I saw that factory line worker that day. </p>



<p><strong>HUIZINGA:</strong> Wow, well, we&#8217;ll get to the specifics on what you&#8217;re doing now and how that&#8217;s relevant in a bit. But thank you for that. So Muhammed, what&#8217;s the big idea behind your work and how did you get to where you are today?&nbsp;</p>



<p><strong>MUHAMMED IDRIS:</strong> Yeah, no. First off, Cesar, I think it&#8217;s a really cool story. I wish I had an origin story [LAUGHTER] from when I was a kid, and I knew exactly what my life&#8217;s work was going to be. Actually, my story, I figured out my “why” much later. Actually, my background was in finance. And I started my career in the hedge fund space at a company called BlackRock, really large financial institution you might have heard of. Then I went off and I did a PhD at Penn State. And I fully intended on going back. I was going to basically be working in spreadsheets for the rest of my life. But actually during my postdoc at the time I was living in Montreal, I actually had distant relatives of mine who were coming to Montreal to apply for asylum and it was actually in helping them navigate the process, that it became clear to me, you know, the role, it was very obvious to me, the role that technology can play in helping people help themselves. And kind of the big idea that I realized is that, you know, oftentimes, you know, the world kind of provides a set of conditions, right, that strip away our rights and our dignity and our ability to really fend for ourselves. But it was so amazing to see, you know, 10-, 12-year-old kids who, just because they had a phone, were able to help their families navigate what shelter to go to, how to apply for school, and more importantly, how do they actually start the rest of their lives? And so actually at the time, I, you know, got together a few friends, and, you know, we started to think about, well, you know, all of this information is really sitting on a bulletin board somewhere. How can we digitize it? And so we put together a pretty, I would say, bad-ass team, interdisciplinary team, included developers and refugees, and we built a prototype over a weekend. And essentially what happened was we built this really cool platform called Atar. And in many ways, I would say that it was the first real solution that leveraged a lot of the natural language processing capabilities that everyone is using today to actually help people help themselves. And it did that in three really important ways. The first way is that people could essentially ask what they needed help with in natural language. And so we had some algorithms developed that would allow us to identify somebody&#8217;s intent. Taking that information then, we had a set of models that would then ask you a set of questions to understand your circumstances and determine your eligibility for resources. And then from that, we&#8217;d create a customized checklist for them with everything that they needed to know, where to go, what to bring, and who to talk to in order to accomplish that thing. And it was amazing to see how that very simple prototype that we developed over a weekend really became a lifeline for a lot of people. And so that&#8217;s really, I think, what motivated my work in terms of trying to combine data science, emerging technologies like AI and machine learning, with the sort of community-based research that I think is important for us to truly identify applications where, in my world right now, it&#8217;s really studying health disparities. </p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, tell us how you got into doing what you&#8217;re doing as a technical advisor. What&#8217;s the big idea behind what you do and how you got here?&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> So as a technical advisor in Microsoft Research, I really look for ideas out there. So ideas can come from anywhere. And so think it of scanning the horizon to look for some of those ideas out there and then figuring out, are there scientific hypotheses we should be looking at? And so the idea here is, once we have identified some of those ideas, the goal is really to help nurture a healthy pipeline for potential big bets. What I do is really about “subtle science and exact art” and we discover as we do and it involves a lot of discussions and conversations working with our researchers here, our scientists, but of course with the external research community. And how I got here … well first I will say that I am so excited to be alive in a moment where AI has made it to industry because I&#8217;ve looked and worked in AI for as long as I can remember with very different approaches. And actually as important, importantly for me is really natural languages which have enabled this big evolution. People sometimes also talk about revolution in AI, via the language models. Because when I started, so I was very fortunate growing up in an environment where my family, my extended family spoke different languages, but then it was interesting to see the different idioms in those natural languages. Just to give you an example, in English you say, it rains cats and dogs. Well, in France, in French it doesn&#8217;t mean anything, right? In French, actually, it rains ropes, right? Which probably doesn&#8217;t mean anything in English. [LAUGHTER] And so I was really curious about natural languages and communication. When I went to school, being good at math, I ended up doing math, realizing very quickly that I didn&#8217;t want to do a career in math. You know, proofs all that is good in high school, doing a full career, was not my thing, math. You know, proofs, all that. It’s good in high school, but doing a full career, it was not my thing, math. But there was that class I really, really enjoyed, which was mathematical logic. And so little by little, I started discovering people working in that field. And at the same time, I was still restless with natural languages. And so I also took some classes in linguistics on the humanity university in Toulouse in France. And I stumbled on those people who were actually working in … some in linguistics, some in computer science, and then there was this lab doing computational linguistics. And then that was it for me. I was like, that&#8217;s, you know, so that&#8217;s how I ended up doing my PhD in computational linguistics. And the last aspect I&#8217;ll talk about, because in my role today, the aspect of working with a network of people, with a global network, is still so important to me, and I think for science as a whole. At the time, there was this nascent field of computational lexical semantics. And for me, it was so important to bring people together because I realized that we all had different approaches, different theories, not even in France, but across the world, and actually, I worked with somebody else, and we co-edited the first book on computational lexical semantics, where we started exposing what it meant to do lexical semantics and the relationships between words within a larger context, with a larger context of conversations, discourse, and all those different approaches. And that&#8217;s an aspect which for me to this day is so important and that was also really important to keep as we develop what we&#8217;re going to talk about today, Accelerating Foundation Models Research program.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, this is fascinating because I didn&#8217;t even know all of these stories. I just knew that there were stories here and this is the first time I&#8217;m hearing them. So it&#8217;s like this discovery process and the sort of pushing on a door and having it be, well, that&#8217;s not quite the door I want. [LAUGHTER] Let&#8217;s try door number two. Let&#8217;s try door number three. Well, let&#8217;s get onto the topic of Accelerating Foundation Models Research and unpack the big idea behind that. Evelyne, I want to stay with you on this for a minute because I&#8217;m curious as to how this initiative even came to exist and what it hopes to achieve. So, maybe start out with a breakdown of the title. It might be confusing for some people, Accelerating Foundation Models Research. What is it?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, thank you for the question. So I think I&#8217;m going to skip quickly on accelerate research. I think people can understand it&#8217;s just like to bring …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Make it faster …&nbsp;</p>



<p><strong>VIEGAS:</strong> … well, faster and deeper advances. I mean, there are some nuances there, but I think the terms like foundation models, maybe that&#8217;s where I&#8217;ll start here. So when we talk about foundation models, just think about any model which has been trained on broad data, and which actually enables you to really do any task. That&#8217;s, I think, the simplest way to talk about it. And indeed, actually people talk a lot about large language models or language models. And so think of language models as just one part, right, for those foundation models. The term was actually coined at Stanford when people started looking at GPTs, the generative pre-trained transformers, this new architecture. And so that term was coined like to go not just talk about language models, but foundation models, because actually it&#8217;s not just language models, but there are also vision models. And so there are other types of models and modalities really. And so when we started with Accelerating Foundation Models Research and from now on, I will say AFMR if that&#8217;s okay.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Not to be confused with ASMR, which is that sort of tingly feeling you get in your head when you hear a good sound, but AFMR, yes.&nbsp;</p>



<p><strong>VIEGAS:</strong> So with the AFMR, so actually I need to come a little bit before that and just remind us that actually that this is not just new. The point I was making earlier about it’s so important to engage with the external research community in academia. So Microsoft Research has been doing it for as long as I&#8217;ve been at Microsoft and I&#8217;ve been 25 years, I just did 25 in January.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Congrats!&nbsp;</p>



<p><strong>VIEGAS:</strong> And so, I … thank you! &#8230;  and so, it&#8217;s really important for Microsoft Research, for Microsoft. And so we had some programs even before the GPT, ChatGPT moment where we had engaged with the external research community on a program called the Microsoft Turing Academic Program where we provided access to the Turing model, which was a smaller model than the one then developed by OpenAI. But at that time, it was very clear that we needed to be responsible, to look at safety, to look at trustworthiness of those models. And so we cannot just drink our own Kool-Aid and so we really had to work with people externally. And so we were already doing that. But that was an effort which we couldn&#8217;t scale really because to scale an effort and having multiple people that can have access to the resources, you need more of a programmatic way to be able to do that and rely on some platform, like for instance, Azure, which has security and privacy, confidentiality which enables to scale those type of efforts. And so what happens as we&#8217;re developing this program on the Turing model with a small set of academic people, then there was this ChatGPT moment in November 2022, which was the moment like the “aha moment,” I think, as I mentioned, for me, it&#8217;s like, wow, AI now has made it to industry. And so for us, it became very clear that we could not with this moment and the amount of resources needed on the compute side, access to actually OpenAI that new that GPT, at the beginning of GPT-3 and then 4 and then … So how could we build a program? First, should we, and was there interest? And academia responded “Yes! Please! Of course!” right? [LAUGHTER] I mean, what are you waiting for? So AFMR is really a program which enabled us to provide access to foundation models, but it&#8217;s also a global network of researchers. And so for us, I think when we started that program, it was making sure that AI was made available to anyone and not just the few, right? And really important to hear from our academic colleagues, what they were discovering and covering and what were those questions that we were not even really thinking about, right? So that&#8217;s how we started with AFMR. </p>



<p><strong>HUIZINGA:</strong> This is funny, again, on the podcast, you can&#8217;t see people shaking their heads, nodding in agreement, [LAUGHTER] but the two academic researchers are going, yep, that&#8217;s right. Well, Muhammed, let&#8217;s talk to you for a minute. I understand AFMR started a little more than a year ago with a pilot project that revolved around health applications, so this is a prime question for you. And since you&#8217;re in medicine, give us a little bit of a “how it started, how it&#8217;s going” from your perspective, and why it&#8217;s important for you at the Morehouse School of Medicine.&nbsp;</p>



<p><strong>IDRIS:</strong> For sure. You know, it&#8217;s something as we mentioned that really, I remember vividly is when I saw my first GPT-3 demo, and I was absolutely blown away. This was a little bit before the ChatGPT moment that Evelyne was mentioning, but just the possibilities, oh my God, were so exciting! And again, if I tie that back to the work that we were doing, where we were trying to kind of mimic what ChatGPT is today, there were so many models that we had to build, very complex architectures, edge cases that we didn&#8217;t even realize. So you could imagine when I saw that, I said, wow, this is amazing. It&#8217;s going to unlock so many possibilities. But at the same time, this demo was coming out, I actually saw a tweet about the inherent biases that were baked into these models. And I&#8217;ll never forget this. I think it was at the time he was a grad student at Stanford, and they were able to show that if you asked the model to complete a very simple sentence, a sort of joke, “Two Muslims walk into a bar …” what is it going to finish? And it was scary.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Wow.&nbsp;</p>



<p><strong>IDRIS:</strong> Two thirds, it was about 66% of the time, the responses referenced some sort of violence, right? And that really was an “aha moment” for me personally, of course, not being that I&#8217;m Muslim, but beyond that, that there are all of these possibilities. At the same time, there&#8217;s a lot that we don&#8217;t know about how these models might operate in the real world. And of course, the first thing that this made me do as a researcher was wonder how do these emerging technologies, how may they unintentionally lead to greater health disparities? Maybe they do. Maybe they don&#8217;t. The reality is that we don&#8217;t know.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>IDRIS: </strong>Now I tie that back to something that I&#8217;ve been fleshing out for myself, given my time here at Morehouse School of Medicine. And kind of what I believe is that, you know, the likely outcome, and I would say this is the case for really any sort of emerging technology, but let&#8217;s specifically talk about AI, machine learning, large language models, is that if we&#8217;re not intentional in interrogating how they perform, then what&#8217;s likely going to happen is that despite overall improvements in health, we&#8217;re going to see greater health disparities, right? It&#8217;s almost kind of that trickle-down economics type model, right? And it&#8217;s really this addressing of health disparities, which is at the core of the mission of Morehouse School of Medicine. It is literally the reason why I came here a few years ago. Now, the overarching goal of our program, without getting too specific, is really around evaluating the capabilities of foundation models. And those, course, as Evelyne mentioned, are large language models. And we&#8217;re specifically working on facilitating accessible and culturally congruent cancer-related health information. And specifically, we need to understand that communities that are disproportionately impacted have specific challenges around trust. And all of these are kind of obstacles to taking advantage of things like cancer screenings, which we know significantly reduce the likelihood of mortality. And it&#8217;s going very well. We have a pretty amazing interdisciplinary team. And I think we&#8217;ve been able to develop a pretty cool research agenda, a few papers and a few grants. I&#8217;d be happy to share about a little bit later.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, that&#8217;s awesome. And I will ask you about those because your project is really interesting. But I want Cesar to weigh in here on sort of the goals that are the underpinning of AFMR, which is aligning AI with human values, improving AI-human interaction, and accelerating scientific discovery. Cesar, how do these goals, writ large, align with the work you&#8217;re doing at UT Arlington and how has this program helped?&nbsp;</p>



<p><strong>TORRES:</strong> Yeah, I love this moment in time that everybody&#8217;s been talking about, that GPT or large language model exposure. Definitely when I experienced it, the first thing that came to my head was, I need to get this technology into the hands of my students because it is so nascent, there&#8217;s so many open research questions, there&#8217;s so many things that can go wrong, but there&#8217;s also so much potential, right? And so when I saw this research program by Microsoft I was actually surprised. I saw that, hey, they are actually acknowledging the human element. And so the fact that there was this call for research that was looking at that human dimension was really refreshing. So like what Muhammad was saying, one of the most exciting things about these large language models is you don&#8217;t have to be a computer scientist in order to use them. And it reminded me to this moment in time within the arts when digital media started getting produced. And we had this crisis. There was this idea that we would lose all the skills that we have learned from working traditionally with physical materials and having to move into a digital canvas.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>TORRES:</strong> And it&#8217;s kind of this, the birth of a new medium. And we&#8217;re kind of at this unique position to guide how this medium is produced and to make sure that people develop that virtuosity in being able to use that medium but also understand its limitations, right? And so one of the fun projects that we&#8217;ve done here has been around working with our glass shop. Specifically, we have this amazing neon-bending artists here at UTA, Jeremy Scidmore and Justin Ginsberg. We&#8217;ve been doing some collaborations with them, and we&#8217;ve been essentially monitoring how they bend glass. I run an undergraduate research program here and I’ve had undergrads try to tackle this problem of how do you transfer that skill of neon bending? And the fact is that because of AFMR, here is just kind of a way to structure that undergraduate research process so that people feel comfortable to ask those dumb questions exactly where they are. But what I think is even more exciting is that they start to see that questions like skill acquisition is still something that our AI is not able to do. And so it&#8217;s refreshing to see; it&#8217;s like the research problems have not all been solved. It just means that new ones have opened and ones that we previously thought were unattainable now have this groundwork, this foundation in order to be researched, to be investigated. And so it&#8217;s really fertile ground. And I really thank AFMR … the AFMR program for letting us have access to those grounds.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. I&#8217;m really eager to get into both your projects because they&#8217;re both so cool. But Evelyne, I want you to just go on this “access” line of thought for a second because Microsoft has given grants in this program, AFMR, to several Minority Serving Institutions, or MSIs, as they&#8217;re called, including Historically Black Colleges and Universities and Hispanic Serving Institutions, so what do these grants involve? You&#8217;ve alluded to it already, but can you give us some more specifics on how Microsoft is uniquely positioned to give these and what they&#8217;re doing?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yes. So the grant program, per se, is really access to resources, actually compute and API access to frontier models. So think about Azure, OpenAI … but also now actually as the program evolves, it&#8217;s also providing access to even our research models, so Phi, I mean if you … like smaller models …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, P-H-I.&nbsp;</p>



<p><strong>VIEGAS:</strong> Yes, Phi! [LAUGHTER] OK! So, so it&#8217;s really about access to those resources. It&#8217;s also access to people. I was talking about this global research network and the importance of it. And I&#8217;ll come back to that specifically with the Minority Serving Institutions, what we did. But actually when we started, I think we started a bit in a naive way, thinking … we did an open call for proposals, a global one, and we got a great response. But actually at the beginning, we really had no participation from MSIs. [LAUGHTER] And then we thought, why? It&#8217;s open … it’s … and I think what we missed there, at the beginning, is like we really focused on the technology and some people who were already a part of the kind of, this global network, started approaching us, but actually a lot of people didn&#8217;t even know, didn&#8217;t think they could apply, right? And so we ended up doing a more targeted call where we provided not only access to the compute resources, access to the APIs to be able to develop applications or validate or expand the work which is being done with foundation models, but also we acknowledged that it was important, with MSIs, to also enable the students of the researchers like Cesar, Muhammed, and other professors who are part of the program so that they could actually spend the time working on those projects because there are some communities where the teaching load is really high compared to other communities or other colleges. So we already had a good sense that one size doesn&#8217;t fit all. And I think what came also with the MSIs and others, it&#8217;s like also one culture doesn&#8217;t fit all, right? So it&#8217;s about access. It&#8217;s about access to people, access to the resources and really co-designing so that we can really, really make more advances together.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Cesar let&#8217;s go over to you because big general terms don&#8217;t tell a story as well as specific projects with specific people. So your project is called, and I&#8217;m going to read this, <em>AI-Enhanced Bricolage: Augmenting Creative Decision Making in Creative Practices</em>. That falls under the big umbrella of Creativity and Design. So tell our audience, and as you do make sure to explain what bricolage is and why you work in a Hybrid Atelier, terms I&#8217;m sure are near and dear to Evelyne&#8217;s heart … the French language. Talk about that, Cesar. </p>



<p><strong>TORRES:</strong> So at UTA, I run a lab called The Hybrid Atelier. And I chose that name because “lab” is almost too siloed into thinking about scientific methods in order to solve problems. And I wanted something that really spoke to the ethos of the different communities of practice that generate knowledge. And so The Hybrid Atelier is a space, it&#8217;s a makerspace, and it&#8217;s filled with the tools and knowledge that you might find in creative practices like ceramics, glass working, textiles, polymer fabrication, 3D printing. And so every year I throw something new in there. And this last year, what I threw in there was GPT and large language models. And it has been exciting to see how it has transformed. But speaking to this specific project, I think the best way I can describe bricolage is to ask you a question: what would you do if you had a paperclip, duct tape, and a chewing gum wrapper? What could you make with that, right? [LAUGHTER] And so some of us have these MacGyver-type mentalities, and that is what Claude Lévi-Strauss<strong> </strong>kind of terms as the “bricoleur,” a person who is able to improvise solutions with the materials that they have at hand. But all too often, when we think about bricolage, it&#8217;s about the physical world. But the reality is that we very much live in a hybrid reality where we are behind our screens. And that does not mean that we cannot engage in these bricoleur activities. And so this project that I was looking at, it&#8217;s both a vice and an opportunity of the human psyche, and it&#8217;s known as “functional fixation.” And that is to say, for example, if I were to give you a hammer, you would see everything as a nail. And while this helps kind of constrain creative thought and action to say, okay, if I have this tool, I&#8217;m going to use it in this particular way. At the same time, it limits the other potential solutions, the ways that you could use a hammer in unexpected ways, whether it&#8217;s to weigh something down or like jewelers to texturize a metal piece or, I don&#8217;t know, even to use it as a pendulum &#8230; But my point here is that this is where large language models can come in because they can, from a more unbiased perspective, not having the cognitive bias of functional fixation say, hey, here is some tool, here&#8217;s some material, here&#8217;s some machine. Here are all the ways that I know people have used it. Here are other ways that it could be extended. And so we have been exploring, you know, how can we alter the physical and virtual environment in such a way so that this information just percolates into the creative practitioner’s mind in that moment when they&#8217;re trying to have that creative thought? And we&#8217;ve had some fun with it. I did a workshop at an event known as OurCS here at DFW. It&#8217;s a research weekend where we bring a couple of undergrads and expose them to research. And we found that it&#8217;s actually the case that it&#8217;s not AI that does better, and it&#8217;s also not the case that the practitioner does better! [LAUGHTER] It&#8217;s when they hybridize that you really kind of lock into the full kind of creative thought that could emerge. And so we&#8217;ve been steadily moving this project forward, expanding from our data sets, essentially, to look at the corpus of video tutorials that people have published all around the web to find the weird and quirky ways that they have extended and shaped new techniques and materials to advance creative thought. So …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Wow.&nbsp;&nbsp;</p>



<p><strong>TORRES:</strong> … it&#8217;s been an exciting project to say the least.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Okay, again, my face hurts because I&#8217;m grinning so hard for so long. I have to stop. No, I don&#8217;t because it&#8217;s amazing. You made me think of that movie Apollo 13 when they&#8217;re stuck up in space and this engineer comes in with a box of, we&#8217;ll call it bricolage, throws it down on the table and says, we need to make this fit into this using this, go. And they didn&#8217;t have AI models to help them figure it out, but they did a pretty good job. Okay, Cesar, that&#8217;s fabulous. I want Muhammed&#8217;s story now. I have to also calm down. It&#8217;s so much fun. [LAUGHTER]&nbsp;</p>



<p><strong>IDRIS:</strong> No, know I love it. I love it and actually to bring it back to what Evelyne was mentioning earlier about just getting different perspectives in a room, I think this is a perfect example of it. Actually, Cesar, I never thought of myself as being a creative person but as soon as you said a paperclip and was it the gum wrapper …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Duct tape.&nbsp;</p>



<p><strong>IDRIS:</strong> … duct tape or gum wrapper, I thought to myself, my first internship I was able to figure out how to make two paper clips and a rubber band into a … this was of course before AirPods, right? But something that I could wrap my wires around and it was perfect! [LAUGHTER] I almost started thinking to myself, how could I even scale this, or maybe get a patent on it, but it was a paper clip … yeah. Uh, so, no, no, I mean, this is really exciting stuff, yeah.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, Muhammed, let me tee you up because I want to actually … I want to say your project out loud …&nbsp;</p>



<p><strong>IDRIS:</strong> Please.&nbsp;</p>



<p><strong>HUIZINGA:</strong> … because it&#8217;s called <em>Advancing Culturally Congruent Cancer Communication with Foundation Models</em>. You might just beat Cesar&#8217;s long title with yours. I don&#8217;t know. [LAUGHTER] You include alliteration, which as an English major, that makes my heart happy, but it’s positioned under the Cognition and Societal Benefits bucket, whereas Cesar&#8217;s was under Creativity and Design, but I see some crossover. Evelyne&#8217;s probably grinning too, because this is the whole thing about research is how do these things come together and help? Tell us, Muhammed, about this cultury … culturally … Tell us about your project! [LAUGHTER]&nbsp;</p>



<p><strong>IDRIS:</strong> So, you know, I think again, whenever I talk about our work, especially the mission and the “why” of Morehouse School of Medicine, everything really centers around health disparities, right? And if you think about it, health disparities usually comes from one of many, but let&#8217;s focus on kind of three potential areas. You might not know you need help, right? If you know you need help, you might not know where to go. And if you end up there, you might not get the help that you need. And if you think about it, a lot of like the kind of the through line through all of these, it really comes down to health communication at the end of the day. It&#8217;s not just what people are saying, it&#8217;s how people are saying it as well. And so our project focuses right now on language and text, right? But we are, as I&#8217;ll talk about in a second, really exploring the kind of multimodal nature of communication more broadly and so, you know, I think another thing that&#8217;s important in terms of just background context is that for us, these models are more than just tools, right? We really do feel that if we&#8217;re intentional about it that they can be important facilitators for public health more broadly. And that&#8217;s where this idea of our project fitting under the bucket at benefiting society as a whole. Now, you know, the context is that over the past couple of decades, how we&#8217;ve talked about cancer, how we&#8217;ve shared health information has just changed dramatically. And a lot of this has to do with the rise, of course, of digital technologies more broadly, social media, and now there&#8217;s AI. People have more access to health information than ever before. And despite all of these advancements, of course, as I keep saying over and over again, not everyone&#8217;s benefiting equally, especially when it comes to cancer screening. Now, breast and cervical cancer, that&#8217;s what we&#8217;re focusing on specifically, are two of the leading causes of cancer-related deaths in women worldwide. And actually, black and Hispanic women in the US are at particular risk and disproportionately impacted by not just lower screening rates, but later diagnoses, and of course from that, higher mortality rates as well. Now again, an important part of the context here is COVID-19. I think there are, by some estimates, about 10 million cancer screenings that didn&#8217;t happen. And this is also happening within a context of just a massive amount of misinformation. It&#8217;s actually something that the WHO termed as an infodemic. And so our project is trying to kind of look for creative emerging technologies-based solutions for this. And I think we&#8217;re doing it in a few unique ways. Now the first way is that we&#8217;re looking at how foundation models like the GPTs but also open-source models and those that are, let&#8217;s say, specifically fine-tuned on medical texts, how do they perform in terms of their ability to generate health information? How accurate are they? How well is it written? And whether it&#8217;s actually useful for the communities that need it the most. We developed an evaluation framework, and we embedded within that some qualitative dimensions that are important to health communications. And we just wrapped up an analysis where we compared the general-purpose models, like a ChatGPT, with medical and more science-specific domain models and as you&#8217;d expect, the general-purpose models kind of produced information that was easier to understand, but that was of course at the risk of safety and more accurate responses that the medically tuned models were able to produce. Now a second aspect of our work, and I think this is really a unique part of not what I&#8217;ve called, but actually literally there&#8217;s a book called <em>The Morehouse Model</em>, is how is it that we could actually integrate communities into research? And specifically, my work is thinking about how do we integrate communities into the development and evaluation of language models? And that&#8217;s where we get the term “culturally congruent.” That these models are not just accurate, but they&#8217;re also aligned with the values, the beliefs, and even the communication styles of the communities that they&#8217;re meant to serve. One of the things that we&#8217;re thinking, you know, quite a bit about, right, is that these are not just tools to be published on and maybe put in a GitHub, you know, repo somewhere, right? That these are actually meant to drive the sort of interventions that we need within community. So of course, implementation is really key. And so for this, you know, not only do you need to understand the context within which these models will be deployed, the goal here really is to activate you and prepare you with information to be able to advocate for yourself once you actually see your doctor, right? So that again, I think is a good example of that. But you also have to keep in mind Gretchen that, you know, our goal here is, we don&#8217;t want to create greater disparities between those who have and those who don&#8217;t, right? And so for example, thinking about accessibility is a big thing and that&#8217;s been a part of our project as well. And so for example, we&#8217;re leveraging some of Azure API services for speech-to-text and we&#8217;re even going as far as trying to leverage some of the text-to-image models to develop visuals that address health literacy barriers and try to leverage these tools to truly, truly benefit health. </p>



<p><strong>HUIZINGA:</strong> One of the most delightful and sometimes surprising benefits of programs like AFMR is that the technologies developed in conjunction with people in minority communities have a big impact for people in majority communities as well, often called the Curb Cut Effect. Evelyne, I wonder if you&#8217;ve seen any of this happen in the short time that AFMR has been going?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, so, I&#8217;m going to focus a bit more maybe on education and examples there where we&#8217;ve seen, as Cesar was also talking about it, you know for scaling and all that. But we&#8217;ve seen a few examples of professors working with their students where English is not the first language.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah …&nbsp;</p>



<p><strong>VIEGAS:</strong> Another one I would mention is in the context of domains. So for domains, what I mean here is application domains, like not just in CS, but we&#8217;ve been working with professors who are, for instance, astronomers, or lawyers, or musicians working in universities. So they started looking actually at these LLMs as more of the “super advisor” helping them. And so it&#8217;s another way of looking at it. And actually they started focusing on, can we actually build small astronomy models, right? And I&#8217;m thinking, okay, that could … maybe also we learn something which could be potentially applied to some other domain. So these are some of the things we are seeing.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yes.&nbsp;</p>



<p><strong>VIEGAS:</strong> But I will finish with something which may, for me, kind of challenges this Curb Cut Effect to certain extent, if I understand the concept correctly, is that I think, with this technology and the way AI and foundation models work compared to previous technologies, I feel it&#8217;s kind of potentially the opposite. It&#8217;s kind of like the tail catching up with the head. But here I feel that with the foundation models, I think it&#8217;s a different way to find information and gain some knowledge. I think that actually when we look at that, these are really broad tools that now actually can be used to help customize your own curb, as it were! So kind of the other way around.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Oh, interesting …&nbsp;</p>



<p><strong>VIEGAS:</strong> So I think it&#8217;s maybe there are two dimensions. It&#8217;s not just I work on something small, and it applies to everyone. I feel there is also a dimension of, this is broad, this is any tasks, and it enables many more people. I think Cesar and Muhammed made that point earlier, is you don&#8217;t have to be a CS expert or rocket scientist to start using those tools and make progress in your field. So I think that maybe there is this dimension of it.&nbsp;</p>



<p><strong>HUIZINGA:</strong> I love the way you guys are flipping my questions back on me. [LAUGHTER] So, and again, that is fascinating, you know, a custom curb, not a curb cut. Cesar, Muhammad, do you, either of you, have any examples of how perhaps this is being used in your work and you&#8217;re having accidental or serendipitous discoveries that sort of have a bigger impact than what you might&#8217;ve thought?&nbsp;</p>



<p><strong>TORRES:</strong> Well, one thing comes to mind. It&#8217;s a project that two PhD students in my lab, Adam Emerson and Shreyosi Endow have been working on. It&#8217;s around this idea of communities of practice and that is to say, when we talk about how people develop skills as a group, it&#8217;s often through some sort of tiered structure. And I&#8217;m making a tree diagram with my hands here! [LAUGHTER] And so we often talk about what it&#8217;s like for an outsider to enter from outside of the community, and just how much effort it takes to get through that gate, to go through the different rungs, through the different rites of passage, to finally be a part of the inner circle, so to speak. And one of the projects that we&#8217;ve been doing, we started to examine these known communities of practice, where they exist. But in doing this analysis, we realized that there&#8217;s a couple of folks out there that exist on the periphery. And by really focusing on them, we could start to see where the field is starting to move. And these are folks that have said, I&#8217;m neither in this community or another, I&#8217;m going to kind of pave my own way. While we&#8217;re still seeing those effects of that research go through, I think being able to monitor the communities at the fringe is a really telling sign of how we&#8217;re advancing as a society. I think shining some light into these fringe areas, it&#8217;s exactly how research develops, how it&#8217;s really just about expanding at some bleeding edge. And I think sometimes we just have to recontextualize that that bleeding edge is sometimes the group of people that we haven&#8217;t been necessarily paying attention to.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. Love it. Muhammad, do you have a quick example … or, I mean, you don&#8217;t have to, but I just was curious.&nbsp;</p>



<p><strong>IDRIS:</strong> Yeah, maybe I&#8217;ll just give one quick example that I think keeps me excited, actually has to do with the idea of kind of small language models, right? And so, you know, I gave the example of GPT-3 and how it&#8217;s trained on the entirety of the internet and with that is kind of baked in some unfortunate biases, right? And so we asked ourselves the flip side of that question. Well, how is it that we can go about actually baking in some of the good bias, right? The cultural context that&#8217;s important to train these models on. And the reality is that we started off by saying, let&#8217;s just have focus groups. Let&#8217;s talk to people. But of course that takes time, it takes money, it takes effort. And what we quickly realized actually is there are literally generations of people who have done these focus groups specifically on breast and cervical cancer screening. And so what we actually have since done is leverage that real world data in order to actually start developing synthetic data sets that are …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Ahhhh.&nbsp;&nbsp;</p>



<p><strong>IDRIS:</strong> … small enough but are of higher quality enough that allow us to address the specific concerns around bias that might not exist. And so for me, that&#8217;s a really like awesome thing that we came across that I think in trying to solve a problem for our kind of specific use case, I think this could actually be a method for developing more representative, context-aware, culturally sensitive models and I think overall this contributes to the overall safety and reliability of these large language models and hopefully can create a method for people to be able to do it as well.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, I see why it&#8217;s so cool for you to be sitting at Microsoft Research and working with these guys … It&#8217;s about now that I pose the “what could possibly go wrong if you got everything right?” question on this podcast. And I&#8217;m really interested in how researchers are thinking about the potential downsides and consequences of their work. So, Evelyne, do you have any insights on things that you&#8217;ve discovered along the path that might make you take preemptive steps to mitigate?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, I think it&#8217;s coming back to actually what Muhammed was just talking about, I think Cesar, too, around data, the importance of data and the cultural value and the local value. I think an important piece of continuing to be positive for me [LAUGHTER] is to make sure that we fully understand that at the end of the day, data, which is so important to build those foundation models is, especially language models in particular, are just proxies to human beings. And I feel that it’s uh … we need to remember that it&#8217;s a proxy to humans and that we all have some different beliefs, values, goals, preferences. And so how do we take all that into account? And I think that beyond the data safety, provenance, I think there&#8217;s an aspect of “data caring.” I don&#8217;t know how to say it differently, [LAUGHTER] but it&#8217;s kind of in the same way that we care for people, how do we care for the data as a proxy to humans? And I&#8217;m thinking of, you know, when we talk about like in, especially in cases where there is no economic value, right? [LAUGHTER] And so, but there is local value for those communities. And I think actually there is cultural value across countries. So just wanted to say that there is also an aspect, I think we need to do more research on, as data as proxies to humans. And as complex humans we are, right?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. Well, one of the other questions I like to ask on these Ideas episodes is, is about the idea of “blue sky” or “moonshot” research, kind of outrageous ideas. And sometimes they&#8217;re not so much outrageous as they are just living outside the box of traditional research, kind of the “what if” questions that make us excited. So just briefly, is there anything on your horizon, specifically Cesar and Muhammed, that you would say, in light of this program, AFMR, that you&#8217;ve had access to things that you think, boy, this now would enable me to ask those bigger questions or that bigger question. I don&#8217;t know what it is. Can you share anything on that line?&nbsp;</p>



<p><strong>TORRES:</strong> I guess from my end, one of the things that the AFMR program has allowed me to see is this kind of ability to better visualize the terrain of creativity. And it&#8217;s a little bit of a double-edged sword because when we talk about disrupting creativity and we think about tools, it&#8217;s typically the case that the tool is making something easier for us. But at the same time, if something&#8217;s easier, then some other thing is harder. And then we run into this really strange case where if everything is easy, then we are faced with the “blank canvas syndrome,” right? Like what do you even do if everything is just equally weighted with ease? And so my big idea is to actually think about tools that are purposely making us slower …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Mmmmm …&nbsp;</p>



<p><strong>TORRES:</strong> … that have friction, that have errors, that have failures and really design how those moments can change our attitudes towards how we move around in space. To say that maybe the easiest path is not the most advantageous, but the one that you can feel the most fulfillment or agency towards. And so I really do think that this is hidden in the latent space of the data that we collect. And so we just need to be immersed in that data. We need to traverse it and really it becomes an infrastructure problem. And so the more that we expose people to these foundational models, the more that we&#8217;re going to be able to see how we can enable these new ways of walking through and exploring our environment.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. I love this so much because I&#8217;ve actually been thinking some of the best experiences in our lives haven&#8217;t seemed like the best experiences when we went through them, right? The tough times are what make us grow. And this idea that AI makes everything accessible and easy and frictionless is what you&#8217;ve said. I&#8217;ve used that term too. I think of the people floating around in that movie WALL-E and all they have to do is pick whether I&#8217;m wearing red or blue today and which drink I want. I love this, Cesar. That&#8217;s something I hadn&#8217;t even expected you might say and boom, out of the park. Muhammad, do you have any sort of outrageous …? That was flipping it back!&nbsp;</p>



<p><strong>IDRIS:</strong> I was going to say, yeah, no, I listen, I don&#8217;t know how I could top that. But no, I mean, so it&#8217;s funny, Cesar, as you were mentioning that I was thinking about grad school, how at the time, it was the most, you know, friction-filled life experience. But in hindsight, I wouldn&#8217;t trade it in for the world. For me, you know, one of the things I&#8217;m often thinking about in my job is that, you know, what if we lived in a world where everyone had all the information that they needed, access to all the care they need? What would happen then? Would we magically all be the healthiest version of ourselves? I&#8217;m a little bit skeptical. I&#8217;m not going to lie, right? [LAUGHTER] But that&#8217;s something that I&#8217;m often thinking about. Now, bringing that back down to our project, one of the things that I find a little bit amusing is that I tend to ping-pong between, this is amazing, the capabilities are just, the possibilities are endless; and then there will be kind of one or two small things where it&#8217;s pretty obvious that there&#8217;s still a lot of research that needs to be done, right? So my whole, my big “what if” actually, I want to bring that back down to a kind of a technical thing which is, what if AI can truly understand culture, not just language, right? And so right now, right, an AI model can translate a public health message. It&#8217;s pretty straightforward from English to Spanish, right? But it doesn&#8217;t inherently understand why some Spanish speaking countries may be more hesitant about certain medical interventions. It doesn&#8217;t inherently appreciate the historical context that shapes that hesitancy or what kinds of messaging would build trust rather than skepticism, right? So there’s literal like cultural nuances. That to me is what, when I say culturally congruent or cultural context, what it is that I mean. And I think for me, I think what programs like AFMR have enabled us to do is really start thinking outside the box as to how will these, or how can these, emerging technologies revolutionize public health? What truly would it take for an LLM to understand context? And really, I think for the first time, we can truly, truly achieve personalized, if you want to use that term, health communication. And so that&#8217;s what I would say for me is like, what would that world look like?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, the big animating “what if?” I love this. Go ahead, Evelyne, you had something. Please.&nbsp;</p>



<p><strong>VIEGAS:</strong> Can I expand? I cannot talk. I&#8217;m going to do like Muhammed, I cannot talk! Like that friction and the cultural aspect, but can I expand? And as I was listening to Cesar on the education, I think I heard you talk about the educational rite of passage at some point, and Muhammed on those cultural nuances. So first, before talking about “what if?” I want to say that there is some work, again, when we talk about AFMR, is the technology is all the brain power of people thinking, having crazy ideas, very creative in the research being done. And there is some research where people are looking at what it means, actually, when you build those language models and how you can take into account different language and different culture or different languages within the same culture or between different cultures speaking the same language, or … So there is very interesting research. And so it made me think, expanding on what Muhammed and Cesar were talking about, so this educational rite of passage, I don&#8217;t know if you&#8217;re aware, so in Europe in the 17th, 18th century, there was this grand tour of Europe and that was reserved to just some people who had the funds to do that grand tour of Europe, [LAUGHTER] let&#8217;s be clear! But it was this educational rite of passage where actually they had to physically go to different countries to actually get familiar and experience, experiment, philosophy and different types of politics, and … So that was kind of this “passage obligé” we say in French. I don&#8217;t know if there is a translation in English, but kind of this rite of passage basically. And so I am like, wow, what if actually we could have, thanks to the AI looking at different nuances of cultures, of languages … not just language, but in a multimodal point of viewpoint, what if we could have this “citizen of the world” rite of passage, where we … before we are really citizens of the world, we need to understand other cultures, at least be exposed to them. So that would be my “what if?” How do we make AI do that? And so without … and for anyone, right, not just people who can afford it.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, I don&#8217;t even want to close, but we have to. And I&#8217;d like each of you to reflect a bit. I think I want to frame this in a way you can sort of pick what you&#8217;d like to talk about. But I often have a little bit of vision casting in this section. But there are some specific things I&#8217;d like you to talk about. What learnings can you share from your experience with AFMR? Or/and what&#8217;s something that strikes you as important now that may not have seemed that way when you started? And you can also, I&#8217;m anticipating you people are going to flip that and say, what wasn&#8217;t important that is now? And also, how do see yourself moving forward in light of this experience that you&#8217;ve had? So Muhammed, let&#8217;s go first with you, then Cesar, and then Evelyne, you can close the show.&nbsp;</p>



<p><strong>IDRIS:</strong> Awesome. One of the things that, that I&#8217;m often thinking about and one of the concepts I&#8217;m often reminded of, given the significance of the work that institutions like a Morehouse School of Medicine and UT Arlington and kind of Minority Serving Institutions, right, it almost feels like there is an onslaught of pushback to addressing some of these more systemic issues that we all struggle with, is what does it mean to strive for excellence, right? So in our tradition there&#8217;s a concept called Ihsan. Ihsan … you know there&#8217;s a lot of definitions of it but essentially to do more than just the bare minimum to truly strive for excellence and I think it was interesting, having spent time at Microsoft Research in Redmond as part of the AFMR program, meeting other folks who also participated in the program that, that I started to appreciate for myself the importance of this idea of the responsible design, development, and deployment of technologies if we truly are going to achieve the potential benefits. And I think this is one of the things that I could kind of throw out there as something to take away from this podcast, it&#8217;s really, don&#8217;t just think of what we&#8217;re developing as tools, but also think of them as how will they be applied in the real world? And when you&#8217;re thinking about the context within which something is going to be deployed, that brings up a lot of interesting constraints, opportunities, and just context that I think is important, again, to not just work on an interesting technology for the sake of an interesting technology, but to truly achieve that benefit for society.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Hmm. Cesar.&nbsp;</p>



<p><strong>TORRES:</strong> I mean, echoing Muhammad, I think the community is really at the center of how we can move forward. I would say the one element that really struck a chord with me, and something that I very much undervalued, was the power of infrastructure and spending time laying down the proper scaffolds and steppingstones, not just for you to do what you&#8217;re trying to do, but to allow others to also find their own path. I was setting up Azure from one of my classes and it took time, it took effort, but the payoff has been incredible in … in so much the impact that I see now of students from my class sharing with their peers. And I think this culture of entrepreneurship really comes from taking ownership of where you&#8217;ve been and where you can go. But it really just, it all comes down to infrastructure. And so AFMR for me has been that infrastructure to kind of get my foot out the door and also have the ability to bring some folks along the journey with me, so …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, how blessed are you to be working with people like this? Again, my face hurts from grinning so hard. Bring us home. What are your thoughts on this?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, so first of all, I mean, it&#8217;s so wonderful just here live, like listening to the feedback from Muhammed and Cesar of what AFMR brings and has the potential to bring. And first, let me acknowledge that to put a program like AFMR, it takes a village. So I&#8217;m here, the face here, or well, not the face, the voice rather! [LAUGHTER] But it&#8217;s so many people who have, at Microsoft on the engineering side, we&#8217;re just talking about infrastructure, Cesar was talking about, you know, the pain and gain of leveraging an industry-grade infrastructure like Azure and Azure AI services. So, also our policy teams, of course, our researchers. But above all, the external research community … so grateful to see. It’s, as you said, I feel super blessed and fortunate to be working on this program and really listening what we need to do next. How can we together do better? There is one thing for me, I want to end on the community, right? Muhammed talked about this, Cesar too, the human aspect, right? The technology is super important but also understanding the human aspect. And I will say, actually, my “curb cut moment” for me [LAUGHTER] was really working with the MSIs and the cohort, including Muhammed and Cesar, when they came to Redmond, and really understanding some of the needs which were going beyond the infrastructure, beyond you know a small network, how we can put it bigger and deployments ideas too, coming from the community and that&#8217;s something which actually we also try to bring to the whole of AFMR moving forward. And I will finish on one note, which for me is really important moving forward. We heard from Muhammed talking about the really importance of interdisciplinarity, right, and let us not work in silo. And so, and I want to see AFMR go more international, internationality if the word exists … [LAUGHTER]&nbsp;</p>



<p><strong>HUIZINGA:</strong> It does now!&nbsp;</p>



<p><strong>VIEGAS:</strong> It does now! But it&#8217;s just making sure that when we have those collaborations, it&#8217;s really hard actually, time zones, you know, practically it&#8217;s a nightmare! But I think there is definitely an opportunity here for all of us.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, Cesar Torres, Muhammed Idris, Evelyne Viegas. This has been so fantastic. Thank you so much for coming on the show to share your insights on AFMR today.&nbsp;</p>



<p>[MUSIC PLAYS]&nbsp;</p>



<p><strong>TORRES:</strong> It was a pleasure.&nbsp;&nbsp;</p>



<p><strong>IDRIS:</strong> Thank you so much.&nbsp;</p>



<p><strong>VIEGAS:</strong> Pleasure.&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-3"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/">Ideas: Accelerating Foundation Models Research: AI for all</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of March 24, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/</link>
		
		<dc:creator><![CDATA[Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, Lili Qiu, Jianfeng Gao, Yangyu Huang, Tengchao Lv, Lei Cui, Scarlett Li, Furu Wei, Xenofon Foukas, Bozidar Radunovic, Margus Veanes, Sarah Alamdari, Carles Domingo-Enrich, Ava Amini, Kevin Kaichuang Yang, Peter Lee, Ade Famoti, Christopher Bishop]]></dc:creator>
		<pubDate>Wed, 26 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135033</guid>

					<description><![CDATA[<p>In this issue, we examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/">Research Focus: Week of March 24, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p class="has-text-align-left">We examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="789" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus -- Week of March 24" class="wp-image-1135034" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1024x577.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-768x433.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="secom-on-memory-construction-and-retrieval-for-personalized-conversational-agents">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents</h3>



<p>Researchers from Microsoft and Tsinghua University propose a new method to help conversational AI agents deliver more coherent and personalized responses during complex long-term dialogue.</p>



<p>Large language models (LLMs) are widely used to enable more complicated discussions across a broader range of topics than traditional dialogue systems. However, managing excessively long context that contains irrelevant information is a major challenge. Existing solutions typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization.</p>



<p>The proposed new approach, <a href="https://www.microsoft.com/en-us/research/project/secom/" target="_blank" rel="noreferrer noopener">SeCom</a>, constructs the memory bank at segment level by introducing a conversation <strong>Se</strong>gmentation model that partitions long-term conversations into topically coherent segments, while applying <strong>Com</strong>pression based denoising on memory units to enhance memory retrieval. Experimental results show that <strong>SeCom</strong> exhibits a significant performance advantage over baselines on long-term conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-2 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--4"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/secom-on-memory-construction-and-retrieval-for-personalized-conversational-agents/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="peace-empowering-geologic-map-holistic-understanding-with-mllms">PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</h3>



<p>Microsoft Researchers and external colleagues introduce <a href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">GeoMap-Agent</a>, an AI system specifically designed for geologic map understanding and analysis. In the lab, they measure its effectiveness using a new benchmark called GeoMap-Bench, a novel gauge for evaluating multimodal large language models (MLLMs) in geologic map understanding. Geologic maps provide critical insights into the structure and composition of Earth&#8217;s surface and subsurface. They are indispensable in fields including disaster detection, resource exploration, and civil engineering.</p>



<p>Current MLLMs often fall short in understanding geologic maps, largely due to the challenging nature of cartographic generalization, which involves handling high-resolution maps, managing multiple associated components, and requiring domain-specific knowledge.</p>



<p>This paper presents results of experiments in which GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming the 0.369 score of GPT-4o. The researchers intend to enable advanced AI applications in geology, powering more efficient and accurate geological investigations.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-3 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--5"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="the-future-of-the-industrial-ai-edge-is-cellular">The future of the industrial AI edge is cellular</h3>



<p>Reliable, high-bandwidth wireless connectivity and local processing at the edge are crucial enablers for emerging industrial AI applications. This work proposes that cellular networking is the ideal connectivity solution for these applications, due to its virtualization and support for open APIs. The researchers project the emergence of a converged industrial AI edge encompassing both computing and connectivity, in which application developers leverage the API to implement advanced functionalities. They present a case study showing evidence of the effectiveness of this approach, evaluated on an enterprise-grade 5G testbed.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--6"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/the-future-of-the-industrial-ai-edge-is-cellular/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-fd412f1d3373c86bbb4f4db1930ad0fa" id="new-research-3">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds">RE#: High Performance Derivative-Based Regex Matching with Intersection, Complement, and Restricted Lookarounds</h3>



<p>A regular expression (regex or RE) is a sequence of characters used to match, search, and manipulate strings in text based on specific criteria. REs are used in programming languages for data validation, text parsing, and search operations.</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds/">This paper</a> presents a tool and theory built on symbolic derivatives that does not use backtracking, while supporting both classical operators and complement, intersection, and restricted lookarounds. The researchers show that the main matching algorithm has input-linear complexity both in theory as well as experimentally. They apply thorough evaluation on popular benchmarks that show that RE# is over 71% faster than the next fastest regex engine in Rust on the baseline, and outperforms all state-of-the-art engines on extensions of the benchmarks, often by several orders of magnitude.&nbsp;</p>



<p>This work could potentially enable new applications in LLM prompt engineering frameworks, new applications in medical research and bioinformatics, and new opportunities in access and resource policy language design by web service providers.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--7"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-1c1c9f6b894c49f1e811e2f568b9620a" id="research-areas-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="toward-deep-learning-sequence-structure-co-generation-for-protein-design">Toward deep learning sequence–structure co-generation for protein design</h3>



<p>Researchers review recent advances in deep generative models for protein design, with a focus on sequence-structure co-generation methods. They describe the key methodological and evaluation principles underlying these methods, highlight recent advances from the literature, and discuss opportunities for continued development of sequence-structure co-generation approaches.</p>



<p>Deep generative models that learn from the distribution of natural protein sequences and structures may enable the design of new proteins with valuable functions. While most of today’s models focus on generating either sequences or structures, emerging co-generation methods promise more accurate and controllable protein design, ideally achieved by modeling both modalities simultaneously.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--8"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/toward-deep-learning-sequence-structure-co-generation-for-protein-design/">Read the paper</a></div>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1115757">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft research podcast</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/ideas-ai-and-democracy-with-madeleine-daepp-and-robert-osazuwa-ness/" aria-label="Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness" data-bi-cN="Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/12/NEWGinny-Madeleine-and-Robert_Ideas_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated headshots of Ginny Badanes, Madeleine Daepp and Robert Ness" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness</h2>
				
								<p class="large">As the “biggest election year in history” comes to an end, researchers Madeleine Daepp and Robert Osazuwa Ness and Democracy Forward GM Ginny Badanes discuss AI’s impact on democracy, including the tech’s use in Taiwan and India.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/podcast/ideas-ai-and-democracy-with-madeleine-daepp-and-robert-osazuwa-ness/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Ideas: AI and democracy with Madeleine Daepp and Robert Osazuwa Ness" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-fcf31b9f10037de288b1b6ba58a7098f" id="podcast-1">PODCAST</h2>



<h3 class="wp-block-heading h2" id="new-series-the-ai-revolution-in-medicine-revisited">New Series: The AI Revolution in Medicine, Revisited</h3>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, <a href="https://www.microsoft.com/en-us/research/people/petelee/">Peter Lee</a>, president of Microsoft Research, cowrote <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pearson.com/en-us/subject-catalog/p/the-ai-revolution-in-medicine-gpt-4-and-beyond/P200000011399/9780138279516"><em>The AI Revolution in Medicine: GPT-4 and Beyond</em></a>, a book full of optimism for the potential of advanced AI models to transform the world of healthcare. In this special <a href="https://www.microsoft.com/en-us/research/podcast/">Microsoft Research Podcast</a> series, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="The AI Revolution in Medicine, Revisited: An Introduction" width="500" height="281" src="https://www.youtube-nocookie.com/embed/3qOWQpnZDmI?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--9"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">Watch the series</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h3 class="wp-block-heading h2" id="the-future-of-generative-ai-for-scientific-discovery">The future of generative AI for scientific discovery</h3>



<p>Most of us think of generative AI in the context of text or image generation, but it’s also a powerful tool for scientific discovery. In this episode of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=d3V0caeGIFU&feature=youtu.be">Leading the Shift podcast<span class="sr-only"> (opens in new tab)</span></a>, host Susan Etlinger speaks with Ade Famoti, a senior leader on the Microsoft Research Accelerator team. Ade discusses what he calls “AI’s physics moment,” and why he believes generative AI feels fundamentally different from past platform shifts. Ade shares examples of the work Microsoft Research is doing to uncover the opportunities of generative AI for materials discovery—to improve energy efficiency and carbon capture, and for drug discovery, to fight disease. Ade also highlights the role of culture in building trust, informing priorities and driving adoption of emerging technologies.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="The future of generative AI for scientific discovery | Microsoft Research" width="500" height="281" src="https://www.youtube-nocookie.com/embed/d3V0caeGIFU?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-ad7541ccbcbf3a60c1bd4dd58805b339" id="video">VIDEO</h2>



<h3 class="wp-block-heading h2" id="microsoft-research-s-chris-bishop-talks-ai-for-science-what-it-really-means">Microsoft Research’s Chris Bishop talks AI for Science (what it really means)</h3>



<p>In this interview, the director of Microsoft Research AI for Science, Chris Bishop, discusses how AI is unlocking new scientific outcomes, from drug creation to materials generation to improved climate modeling.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Director of Microsoft Research talks AI for science (what it really means)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/rfyS_rLOKUc?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="microsoft-research-in-case-you-missed-it">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">Microsoft Research | In case you missed it</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Tech Life - The doctor will see you now"
						href="https://www.bbc.co.uk/sounds/play/w3ct5wnm"
					>
						<span>Tech Life &#8211; The doctor will see you now</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>BBC Sounds | March 4, 2025</p><p>An update on live trials in Ghana of 3D telemedicine technology, developed by Microsoft Research and external collaborators. Using portable equipment and holoportation technology, patients in remote locations can connect with a doctor many miles away. The BBC speaks to Spencer Fowers, who is the lead engineer on the project, as well as a patient and a doctor benefiting from the program.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Katja Hofmann: Why we're training AI on video games"
						href="https://www.ted.com/talks/katja_hofmann_why_we_re_training_ai_on_video_games"
					>
						<span>Katja Hofmann: Why we're training AI on video games</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>TED Talk | October 2024</p><p>In a recent TED Talk: <a href="https://www.ted.com/talks/katja_hofmann_why_we_re_training_ai_on_video_games" target="_blank" rel="noreferrer noopener">Why we&#8217;re training AI on video games,</a> Microsoft researcher Katja Hofmann discusses the work the Game Intelligence team at Microsoft Research is doing to develop AI that can transform video games. Using AI trained on years of human gameplay data, the team built World and Human Action Model, which can learn to think, play and innovate alongside humans, enabling video game creators to build more robust games. Hoffmann was also interviewed in a related article: <a href="https://spectrum.ieee.org/ai-video-games" target="_blank" rel="noreferrer noopener">Microsoft’s Muse AI Edits Video Games on the Fly</a>.</p>				</div>
			
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/">Research Focus: Week of March 24, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The reality of generative AI in the clinic</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-the-reality-of-generative-ai-in-the-clinic/</link>
		
		<dc:creator><![CDATA[Peter Lee, Dr. Sara Murray, Dr. Christopher Longhurst]]></dc:creator>
		<pubDate>Thu, 20 Mar 2025 21:50:50 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1134402</guid>

					<description><![CDATA[<p>UC San Diego Health’s Dr. Christopher Longhurst and UC San Francisco Health’s Dr. Sara Murray explore how generative AI is changing patient care, clinical workflows, and decision-making and how they envision the technology impacting the future of healthcare.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-the-reality-of-generative-ai-in-the-clinic/">The reality of generative AI in the clinic</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="AI Revolution podcast | Episode 1 - The reality of generative AI in the clinic | outline illustration of Dr. Sara Murray, Peter Lee, Dr. Christopher Longhurst" class="wp-image-1134404" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Episode1-Peter-Sara-Christopher-AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=143567237&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series—The AI Revolution in Medicine, Revisited—Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.</p>



<p>In this episode, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://health.ucsd.edu/about-us/leadership/christopher-longhurst/" target="_blank" rel="noreferrer noopener">Dr. Christopher Longhurst<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://profiles.ucsf.edu/sara.murray" target="_blank" rel="noreferrer noopener">Dr. Sara Murray<span class="sr-only"> (opens in new tab)</span></a>, leading experts in healthcare AI implementation, join Lee to discuss the current state and future of AI in clinical settings. Longhurst, chief clinical and innovation officer at UC San Diego Health and executive director of the Jacobs Center for Health Innovation, details his healthcare system&#8217;s collaboration with Epic and Microsoft to integrate GPT into their electronic health record system, offering clinicians support in responding to patient messages. Dr. Murray, chief health AI officer at UC San Francisco Health, discusses AI’s integration into clinical workflows, the promise and risks of AI-driven decision-making, and how generative AI is reshaping patient care and physician workload.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more:</h2>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/10.1056/AIcs2400420" target="_blank" rel="noreferrer noopener">Large Language Models for More Efficient Reporting of Hospital Quality Measures<span class="sr-only"> (opens in new tab)</span></a><br>Publication | October 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://academic.oup.com/jamiaopen/article/7/2/ooae028/7643693" target="_blank" rel="noreferrer noopener">Generative artificial intelligence responses to patient messages in the electronic health record: early lessons learned<span class="sr-only"> (opens in new tab)</span></a><br>Publication | July 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIp2400109">The Chief Health AI Officer — An Emerging Role for an Emerging Technology<span class="sr-only"> (opens in new tab)</span></a><br>Publication | June 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2817615" target="_blank" rel="noreferrer noopener">AI-Generated Draft Replies Integrated Into Health Records and Physicians&#8217; Electronic Communication<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Publication | April 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank" rel="noreferrer noopener">Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum <span class="sr-only"> (opens in new tab)</span></a><br>Publication | April 2023</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a><br>Book | April 2023</p>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript&nbsp;</h2>



<p>[MUSIC]&nbsp;</p>



<p>[BOOK PASSAGE]&nbsp;&nbsp;</p>



<p><strong>PETER LEE:</strong> “The workload on healthcare workers in the United States has increased dramatically over the past 20 years, and in the worst way possible. &#8230; Far too much of the practical, day-to-day work of healthcare has evolved into a crushing slog of filling out and handling paperwork.<strong> </strong>&#8230; GPT-4 indeed looks very promising as a foundational technology for relieving doctors of many of the most taxing and burdensome aspects of their daily jobs.”&nbsp;</p>



<p>[END OF BOOK PASSAGE]&nbsp;</p>



<p>[THEME MUSIC]&nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee.&nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here. </p>



				</span>
				<span id="show-more-show-less-toggle-10" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]</p>



<p>What I read there at the top is a passage from Chapter 2 of the book, which captures part of what we&#8217;re going to cover in this episode. </p>



<p>In our book, we predicted how AI would be leveraged in the clinic. Some of those predictions, I felt, were slam dunks, for example, AI being used to listen to doctor-patient conversations and write clinical notes. There were already early products coming out in the world <em>not</em> using generative AI that were doing just that. But other predictions we made were bolder, for instance, on the use of generative AI as a second set of eyes, to look over the shoulder of a doctor or a nurse or a patient and spot mistakes.</p>



<p>In this episode, I’m pleased to welcome Dr. Chris Longhurst and Dr. Sara Murray to talk about how clinicians in their respective systems are using AI, their reactions to it, and what’s ahead. Chris is the chief clinical and innovation officer at UC San Diego Health, and he is also the executive director of the Joan & Irwin Jacobs Center for Health Innovation. He&#8217;s in charge of UCSD Health&#8217;s digital strategy, including the integration of new technologies from bedside to bench and reaching across UCSD Health, the School of Medicine, and the Jacobs School of Engineering. Chris is a board-certified pediatrician and clinical informaticist.</p>



<p>Sara is vice president and chief health AI officer at UC San Francisco Health. Sara is an internal medicine specialist and associate professor of clinical medicine. A doctor, a professor of medicine, and a strategic health system leader, she builds infrastructure and governance processes to ensure that UCSF&#8217;s deployment of AI, including both AI procured from companies as well as AI-powered tools developed in-house, are trustworthy and ethical.</p>



<p>I’ve known Chris and Sara for years, and what’s really impressed me about their work—and frankly, the work of all the guests we&#8217;ll have on the show—is that they’ve all done something significant to advance the use of AI in healthcare.</p>



<p>[TRANSITION MUSIC]</p>



<p>Here’s my conversation with Dr. Chris Longhurst:&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Chris, thank you so much for joining us today.&nbsp;</p>



<p><strong>CHRISTOPHER LONGHURST:</strong> Peter, it&#8217;s a pleasure to be here. Really appreciate it.&nbsp;</p>



<p><strong>LEE:</strong> We&#8217;re going to get into, you know, what&#8217;s happening in the clinic with AI. But I think we need to find out a little bit more about you first. I introduced you as a person with a fancy title, chief clinical and innovation officer.<strong> </strong>What is that exactly, and how do you spend a typical day at work?&nbsp;</p>



<p><strong>LONGHURST:</strong> Well, I have a little bit of a unicorn job because my portfolio includes information technology, and I&#8217;m a recovering CIO after spending seven years in that role. It also includes quality patient safety, case management, and the office of our chief medical officer.&nbsp;&nbsp;</p>



<p>And so I&#8217;m really trying to unify our mission to deliver highly reliable care with these new tools in a way that allows us to transform that care. One good analogy, I think, is it’s about the game, right. Our job is not only to play the game and win the game using the existing tools but also to change the game by leveraging these new tools and showing the rest of the country how that can be done.&nbsp;</p>



<p><strong>LEE:</strong> And so as you&#8217;re doing that, I can understand, of course, you&#8217;re working at a very, kind of, senior executive level. But, you know, when I&#8217;ve visited you at UCSD Health, you&#8217;re also working with clinicians, doctors, and nurses all the time. In a way, I viewed you as, sort of, connective tissue between these things. Is that accurate?&nbsp;</p>



<p><strong>LONGHURST:</strong> Well, sure. And we&#8217;ve got, you know, several physicians who are part of the executive team who are also continuing to practice, and I think that&#8217;s one of the ways in which doctors on the executive team can bring value, is being that connective tissue, being the ears on the ground and a little dose of reality.&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] Well, in fact, that reality is really what I want to delve into. But I just want to, before getting into that, talk a little bit about AI and your encounters with AI. And I think we have to do it in two stages because there is AI and machine learning and data analytics <em>prior</em> to the rise of generative AI and then, of course, after. And so tell us a little bit about, you know, what got you into health informatics and AI to begin with.&nbsp;</p>



<p><strong>LONGHURST:</strong> Well, Peter, I know that you play video games, and I did too for many years. So I was an early John Carmack id Software, Castle Wolfenstein, and Doom fan.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Love it.&nbsp;&nbsp;</p>



<p><strong>LONGHURST:</strong> And that kept me occupied because I lived out in the country on 50 acres of almond trees. And so it was computer gaming that first got me into computers.&nbsp;&nbsp;</p>



<p>But during medical school, I decided to pursue graduate work in this field called health informatics. And actually my master&#8217;s thesis was using machine learning to help identify and distinguish innocent from pathologic heart murmurs in children. And I worked with Dr. Nancy Reed at UC Davis, who had programmed using Lisp, a really fancy tool to do exactly that.&nbsp;&nbsp;</p>



<p>And I will tell you that if I never see another parentheses in Lisp code again, it&#8217;ll be too soon. So I spent a solid year on that.&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] No, no, but you should wear that as a badge of honor. And I will guess that no other guest on this podcast series will have programmed in Lisp. So kudos to you.&nbsp;</p>



<p><strong>LONGHURST:</strong> [LAUGHS] Well, it was a lot of work, and I learned a lot, but as you can imagine, it wasn&#8217;t highly successful at the time. And fast forward, we&#8217;ve had lots of traditional machine learning kind of activities using discrete data for predictive analytics to help predict flow in the hospital and even sepsis, which we can talk about. But as you said, the advent of generative AI in the fall of 2022 was a real game-changer.&nbsp;</p>



<p><strong>LEE:</strong> Well, you have this interest in technology, and, in fact, I do know you as a fairly intensely geeky person. Really, I think maybe that&#8217;s one reason why we&#8217;ve been attracted to each other. But you also got drawn into medicine. Where did that come from?&nbsp;</p>



<p><strong>LONGHURST:</strong> So my father was a practicing cardiologist and scientist. He was MD, PhD trained, and he really shared with me both a love of medicine but also science. I worked in his lab for three summers, and it was during college I decided I wanted to apply to medical school because the human side of the science really drew me in.&nbsp;&nbsp;</p>



<p>But my father was the one who really identified it was important to cross-train. And that&#8217;s why I decided to take time off to do that master&#8217;s degree in health informatics and see if I could figure out how to take two disparate fields and really combine them into one.&nbsp;&nbsp;</p>



<p>I actually went down to Stanford to become a pediatrician because they have a standalone children&#8217;s hospital that&#8217;s one of the best in the country. And I still practice pediatrics and see newborns, and it&#8217;s a passion for me and part of my identity.</p>



<p><strong>LEE:</strong> Well, I&#8217;m just endlessly fascinated and impressed with people who can span these two worlds in the way that you&#8217;ve done. So now, you know, 2022, in November, ChatGPT gets released to the world, and then, you know, a few months later, GPT-4, and then, of course, in the last two years, so much has happened. But what was your first encounter with what we now know of as generative AI?&nbsp;</p>



<p><strong>LONGHURST:</strong> So I remember when ChatGPT was released, and, you know, some of my computer science-type of nerd friends, we were on text threads, you know, with a lot of mind-blowing emojis. But when it really hit medicine was when I got a call right after Thanksgiving in 2022 from my colleague.&nbsp;He was playing with ChatGPT, and he said to me, Chris, I&#8217;ve been feeding it patient questions and you wouldn&#8217;t believe the responses. And he emailed some of the examples to me, and my mind <em>was</em> blown.</p>



<p>And so that&#8217;s when I became one of the reviewers on the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank" rel="noreferrer noopener">paper that was published in April of 2023</a> that showed not only could ChatGPT help answer questions from patients in a high-quality way, but it also expressed a tremendous amount of empathy.<a href="#ftn_1">[1]</a> And in fact, in our review, the clickbait headlines that came out of the paper were that the chatbot was both higher quality and more empathetic than doctors.</p>



<p>But that wasn&#8217;t my takeaway at all. In fact, I&#8217;ll<strong> </strong>take my doctors any day and put them against your chatbot if you give them an hour to Google and construct a really long, thoughtful response. To me, part of the takeaway was that this was really an opportunity to improve efficiency and save time. And so I called up our colleagues at Epic. I think it was right around December of 2022. And I said, Sumit, have you seen this? I&#8217;d like to share some results with you. And I showed him the data from our paper before we had actually had it published. And he said, “Well, that&#8217;s great because we&#8217;re working with Peter Lee and the team at Microsoft to integrate GPT into Epic.”&nbsp;&nbsp;</p>



<p>And so, of course, that&#8217;s how we became one of the first two sites in the country to roll out GPT inside our electronic health record to help draft answers to patient questions.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> And, you know, one thing that&#8217;s worth emphasizing in the story that you&#8217;ve just told is that there is no other major health system that has been confronting the reality of generative AI longer than UC San Diego Health—and I think largely because of your drive and early adoption.&nbsp;&nbsp;</p>



<p>And many listeners of this podcast will know what Epic is, but many will not. And so it&#8217;s worth saying that Epic is a very important creator of an electronic health records system. And of course, UC San Diego Health uses Epic to store all of the clinical data for its patients.&nbsp;&nbsp;</p>



<p>And then Sumit is, of course, Sumit Rana, who is president at Epic.&nbsp;&nbsp;</p>



<p><strong>LONGHURST:</strong><strong> </strong>So in partnership with Epic, we decided to tackle a really important challenge in healthcare today, which is, particularly since the pandemic and the increase in virtual and telehealth care, our clinicians get more messages than ever from patients. But answering those asynchronous messages is an unreimbursed, noncompensated activity that can often take time after hours—what we call “pajama time”—for our doctors.&nbsp;&nbsp;</p>



<p>And in truth, you know, health systems that have thought through this, most of the answers are not actually generated by the doctors themselves. Many times, it&#8217;s mid-level providers, protocol schedulers, other things, because the questions can be about anything from rescheduling an appointment to a medication refill. They don&#8217;t all require doctors.&nbsp;&nbsp;</p>



<p>When they do, it&#8217;s a more complicated question, and sometimes can require a more complicated answer. And in many cases, the clinicians will see a long complex question, and rather than typing an answer, they&#8217;ll say, “You know, this is complicated. Why don&#8217;t you schedule a visit with me so we can talk about it more?”&nbsp;</p>



<p><strong>LEE:</strong> Yeah, so now you&#8217;ve made a decision to contact people at Epic to what &#8230; posit the idea that AI might be able to make responding to patient queries easier? Is that the story here?</p>



<p><strong>LONGHURST:</strong> That&#8217;s exactly right. And Sumit knew well that this is a challenge across many organizations. This is not unique to UC San Diego or Stanford. And there&#8217;s been a lot of publications about it. It&#8217;s even been in the lay press. So our hypothesis was that using GPT to help draft responses for doctors would save them time, make it easier, and potentially result in higher-quality, more empathetic answers to patients.&nbsp;</p>



<p><strong>LEE:</strong> And so now the thing that I was so impressed with is you actually did a carefully controlled study to try to understand how well does that work. So tell us a little bit first about the results of that study but then how you set it up.&nbsp;</p>



<p><strong>LONGHURST:</strong> Sure. Well, first, I want to acknowledge something you said at the beginning, which is one of my hats is the executive director of the Joan & Irwin Jacobs Center for Health Innovation. And we&#8217;re incredibly grateful to the Jacobs for their gift, which has allowed us to not only implement AI as part of hospital operations but also to have resources that other health systems may not have to be able to study outcomes. And so that really enabled what we&#8217;re going to talk about here.&nbsp;</p>



<p><strong>LEE: </strong>Right. By the way, one of the things I was personally so fascinated by is, of course, in our book, we speculated that things like after-visit notes to patients, responding to patient queries might be something that happens. And you, at the same time we were writing the book, were actually actively trying to make that real, which is just incredible and for me, and I think my coauthors, pretty affirming.&nbsp;</p>



<p><strong>LONGHURST:</strong> I think you guys were really prescient in your vision. The book is tremendous. I have a signed copy of Peter&#8217;s book, and I recommend it for all your listeners. [LAUGHTER]&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> All right, so now what have you found about …&nbsp;</p>



<p><strong>LONGHURST:</strong> Yeah.&nbsp;</p>



<p><strong>LEE:</strong> … generative AI?&nbsp;&nbsp;</p>



<p><strong>LONGHURST:</strong> Yeah. Well, first to understand what we found, you have to understand how we built [the AI inbox response tool]. And so Stanford and UC San Diego really collaborated with Epic on designing what this would look like. So doctor gets that patient message. We feed some information to GPT that&#8217;s not only the message but also some information about the patient—their problems and medications and past medical and surgical history and that sort of thing.&nbsp;</p>



<p><strong>LEE:</strong> Is there a privacy concern that patients should be worried about when that happens?&nbsp;</p>



<p><strong>LONGHURST:</strong> Yeah, it&#8217;s a really good question. There&#8217;s not because we&#8217;re operating in partnership with Epic and Microsoft in a HIPAA-compliant cloud. And so that data is not only secure and private, but that&#8217;s our top priority, is keeping it that way.&nbsp;</p>



<p><strong>LEE:</strong> Great.&nbsp;</p>



<p><strong>LONGHURST:</strong> So once we feed that into GPT, of course, we very quickly get a draft message that we could send to a patient. But we chose not to just send that message to a patient. So part of our AI governance is keeping a human in the loop. And there&#8217;s two buttons that allow that clinician to review the message. One button says <strong>Edit draft message</strong>, and the other button says <strong>Start new blank message</strong>. So there&#8217;s no button that says just <strong>Send now</strong>. And that really is illustrative of the approach that we took. The second thing, though, that we chose to do I think is really interesting from a conversation standpoint is that our AI governance, as they were looking at this, said, “You know, AI is new and novel. It can be scary to patients. And if we want to maximize trust with our patients, we should maximize transparency.” And so anytime a clinician uses the button that says <strong>Edit draft response</strong>, we automatically append something in the message that says, “This message was automatically generated and reviewed and edited by your doctor.” We felt strongly that was the right approach, and we&#8217;ve had a lot of positive feedback.&nbsp;</p>



<p><strong>LEE:</strong> And so we&#8217;ll want to get into, you know, how good these messages are, whether there are issues with bias or hallucination, but before doing that, you know, on this human in loop, this was another theme in our book. And in fact, we recommended this. But there were other health systems around the country that were also later experimenting with similar ideas. And some have taken different approaches. In fact, as time has gone on, if anything, it seems like it&#8217;s become a little bit less clear, this sort of labeling idea. Has your view on this evolved at all over the last two years?&nbsp;</p>



<p><strong>LONGHURST:</strong> First of all, I&#8217;m glad that we did it. I think it was the right choice for University of California, and in fact, the other four UC sites are all doing this, as well. There is variability across the organizations that are using this functionality, and as you suggest, there&#8217;s tens of thousands of physicians and hundreds of thousands if not millions of patients receiving these messages. And it&#8217;s been highlighted a bit in the press.&nbsp;&nbsp;</p>



<p>I can tell you that talking about our approach to transparency, one of our lawmakers in the state of California heard about this and actually proposed a bill that was signed into legislation by our governor so that effective Jan. 1, any communication with patients that uses AI has to be disclosed with those patients. And so there is some thought that this is perhaps the right approach.&nbsp;&nbsp;</p>



<p>I don&#8217;t think that it&#8217;s a perfect approach, though. We&#8217;re using AI in more and more ways, and it&#8217;s not as if we&#8217;re going to be able to disclose every single time that we&#8217;re doing it to prioritize, you know, scheduling for the sickest patients or to help operationally on billing or something else. And so I think that there are other ways we need to figure it out. But we have called on national societies and others to try to create some guidelines around this because we should be as transparent as we can with our patients.&nbsp;</p>



<p><strong>LEE:</strong> Obviously, one of the issues—and we highlighted this a lot in our book—is the problem of hallucination. And surely this must be an issue when you&#8217;re having AI draft these notes to patients. What have you found?&nbsp;</p>



<p><strong>LONGHURST:</strong> We were worried about that when we rolled it out. And what we found is not only were there very few hallucinations, in some cases, our doctors were <em>learning</em> from the GPT. And I can give you an example. When a patient who had had a visit wrote their doctor afterwards and said, “Doc, I&#8217;ve been thinking a lot about what we discussed in quitting smoking marijuana.” And the GPT draft reply said something to the effect of, “That&#8217;s great news. Here&#8217;s a bunch of evidence on how smoking marijuana can harm your lungs and cause other effects. And by the way, since you live in the state of California, here&#8217;s the marijuana quitters helpline.” And the doctor who was sending this called me up to tell me about it. And I said, well, is there a marijuana quitters helpline in the state of California? And he said, “I didn&#8217;t know, so I Googled it. And yeah, there is.” And so that&#8217;s an example of the GPT actually having more information than, you know, a primary care clinician might have. And so there are cases clearly where the GPT can help us increase the quality. In addition, some of the feedback that we&#8217;ve been getting both anecdotally and now measuring is that these draft responses do carry that tone of empathy that Dr. [John] Ayers <a href="#ftn_2">[2]</a> and I saw in the original manuscript. And we&#8217;ve heard from our clinicians that it&#8217;s reminding them to be empathetic because you don&#8217;t always have that time when you&#8217;re hammering out a quick short message, right?&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> You know,<strong> </strong>I think the thing that we&#8217;ve observed, and we&#8217;ve discussed this also, is exactly that reminding thing. There might be in the encounter between a doctor and patient, maybe a conversation about, you know, going to a football game for the first time. That could be part of the conversation. But in a busy doctor&#8217;s life, when writing a note, you might forget about that. And, of course, an AI has the endless ability to remember that it might be friendly to send well wishes.&nbsp;</p>



<p><strong>LONGHURST:</strong> Exactly right, Peter. In fact, one of the findings in Dr. Ayers’s manuscript that didn&#8217;t get as much attention but I think is really important was the difference in length between the responses. So I was one of the putatively blinded reviewers, but as I was looking at the questions and answers, it was really obvious which ones were the chatbot and which ones were the doctors because the chatbot was always, you know, three or four paragraphs and the doctor was three or four sentences, right. It&#8217;s about time. And so we saw that in the results of our study.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> All right, so now let&#8217;s get into those results.</p>



<p><strong>LONGHURST:</strong> OK. Well, first of all, my hypothesis was that this would help us save time, and I was wrong. It turns out a busy primary care clinician might get about 30 messages a day from patients, and each one of those messages might take about 30 seconds to type a quick response, a two-sentence response, a dot phrase, a macro. <em>Your labs are normal. No need to worry. I&#8217;ll call you if anything comes up.</em> After we implemented the AI tool, it still took about 30 seconds per message to respond. But we saw that the responses were two to three times longer on average, and they carried a more empathetic tone. <a href="#ftn_3">[3]</a> <em>And</em> our physicians told us it decreased cognitive burden, which is not surprising because any of you have written know that it&#8217;s much easier to edit somebody else&#8217;s copy than it is to face a blank screen, right. That&#8217;s why I like to be senior author, not lead author.</p>



<p>And so the tool actually helped quite a bit, but it didn&#8217;t help in the ways that we had expected necessarily. There are some other sites that have now found a little bit of time savings, but it&#8217;s really nominal overall. The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2816494" target="_blank" rel="noreferrer noopener">Stanford study<span class="sr-only"> (opens in new tab)</span></a> that was done at the same time—and we actually had some shared coauthors—measured physician burnout using a validated survey, and they saw a decrease in measured physician burnout. And so there are clear advantages to this, and we&#8217;re still learning more.</p>



<p>In fact, we&#8217;ve now rolled this out not only to all of our physicians, but to all of our nurses who help answer those messages in many different clinics. And one of the things that we&#8217;re finding—and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2824738" target="_blank" rel="noreferrer noopener">Dr. CT Lin at University of Colorado recently published<span class="sr-only"> (opens in new tab)</span></a>—is that this tool might actually help those mid-level providers even more because it&#8217;s really good at protocolized responses. I mentioned at the beginning, some of the questions that come to the physicians may be more the edge cases that require a little bit less protocolized kind of answers. And so as we get into academic subspecialties like gynecology oncology, the GPT might not be dishing up a draft message that&#8217;s quite as useful. But if you&#8217;re a nurse in obstetrics and you&#8217;re getting very routine pregnancy questions, it could save a ton of time. And so we&#8217;ve rolled this out broadly.&nbsp;&nbsp;</p>



<p>I want to acknowledge the partnership with Seth Hain and the team at Epic, who&#8217;ve just been fantastic. And we&#8217;re finding all sorts of new ways to integrate the GPT tools into our electronic health record, as well.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Certainly the doctors and nurses that I&#8217;ve encountered that have access to this feature, they just don&#8217;t want to give it up. But it&#8217;s so interesting that it actually doesn&#8217;t really save time. Is that a problem? Because, of course, you know, there seems to be a workforce shortage in healthcare, a need to lower costs and have greater efficiencies. You know, how do you think about that?</p>



<p><strong>LONGHURST:</strong> Great question. There are so many opportunities, as you&#8217;ve kind of mentioned. I mean, healthcare is full of waste and inefficiency, and I am super bullish on how these generative AI tools are going to help us reduce some of that inefficiency.&nbsp;&nbsp;</p>



<p>So everything from revenue cycle to our call centers to operations efficiency, I think, can be positively impacted, and those things make more resources available for clinicians and others. When we think about, you know, saving clinicians time, I don&#8217;t think it&#8217;s necessarily, sort of, the communicating with patients where you want to save that time actually. I think what we want to do is we want to offload some of those administrative tasks that, you know, take a lot of time for our physicians.&nbsp;&nbsp;</p>



<p>So we&#8217;ve measured “pajama time” in our doctors, and on average, a busy primary care clinician can spend one to two hours after clinic doing things. But only about 15 minutes is answering messages from patients. Actually, the bulk of the time after hours is documenting the notes that are required from those visits, right. And those notes are used for a number of different purposes, not only communicating to the next doctor who sees the patient but also for billing purposes and compliance purposes and medical legal purposes. So another really exciting area is AI scribes.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. And so, you know, we&#8217;ll get into scribes and actually other possibilities. I wonder, though, about this empathy issue. Because as computer scientists, we know that you can fall into traps if you anthropomorphize these AI systems or any machine. So in this study, how was that measured, and how real do think that is?&nbsp;</p>



<p><strong>LONGHURST:</strong> So in the study, you&#8217;ll see anecdotal or qualitative evidence about empathy. We have a follow-up study that will be published soon where we&#8217;ve actually measured empathy using some more quantitative tools, and there is no doubt that the chatbot-generated drafts are coming through with more empathy. And we&#8217;ve heard this from a number of our doctors, so it&#8217;s not surprising. Here&#8217;s one of the more surprising things though. I published <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://academic.oup.com/jamiaopen/article/7/2/ooae028/7643693" target="_blank" rel="noreferrer noopener">a paper last year with Dr. Sally Baxter<span class="sr-only"> (opens in new tab)</span></a>, one of our ophthalmologists, and she actually looked at messages with a negative tone. It turns out, not surprisingly, healthcare can be frustrating. And stressed patients can send some pretty nasty messages to their care teams. [LAUGHTER] And you can imagine being a busy, …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> I&#8217;ve done it. [LAUGHS]</p>



<p><strong>LONGHURST:</strong> … tired, exhausted clinician, and receiving a bit of a nasty gram from one of your patients can be pretty frustrating. And the GPT is actually really helpful in those instances in helping draft a pretty empathetic response when I think the human instinct would be a pretty nasty one. [LAUGHTER] I should probably use it in my email, Peter.&nbsp;</p>



<p><strong>LEE:</strong> And is the patient experience, the actually lived experience of patients when they receive these notes, are you absolutely convinced and certain that they are also benefiting from this empathetic tone?&nbsp;</p>



<p><strong>LONGHURST:</strong> I am. In fact, in our paper, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2817615" target="_blank" rel="noreferrer noopener">we also found that the messages going to patients that had been drafted with the AI tool were two to three times longer<span class="sr-only"> (opens in new tab)</span></a> than the messages going to patients that weren&#8217;t using the drafts. And so it&#8217;s clear there&#8217;s more content going and that content is either contributing to a greater sense of empathy and relationship among the patients as well as the clinicians, and/or in some cases, that content may be educating the patients or even reducing the need for follow-up visits.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, so now I think an important thing to share with the audience here is, you know, healthcare, of course, is a very highly regulated industry for good reasons. There are issues of safety and privacy that have to be guarded very, very carefully and thoroughly. And for that reason, clinical studies oftentimes have very carefully developed controls and randomization setups. And so to what extent was that done in this case? Because here, it&#8217;s not like you&#8217;re testing a new drug. It&#8217;s something that&#8217;s a little fuzzier, isn&#8217;t it?</p>



<p><strong>LONGHURST:</strong> Yeah, that&#8217;s right, Peter. And credit to the lead author, Dr. Ming Tai-Seale, we actually did randomize. And so that&#8217;s unusual in these type of studies. We actually got IRB [institutional review board] exemption to do this as a randomized QI study. And it was a crossover study because all the doctors wanted the functionality. So what we tested was the early adopters versus the late adopters. And we compared at the same time the early adopters to those who weren&#8217;t using the functionality and then later the late adopters to the folks that weren&#8217;t using the functionality.&nbsp;</p>



<p>&nbsp;<strong>LEE:</strong> And in that type of study, you might also, depending on how the randomization is set up, also have to have doctors some days using it and some days not having access. Did that also happen?&nbsp;</p>



<p><strong>LONGHURST:</strong> We did, but it wasn&#8217;t on a day-to-day basis. It was more a month-to-month basis.&nbsp;</p>



<p><strong>LEE:</strong> Uh-huh. And what kind of conversation do you have with a doctor that might be attached to a technology and then be told for the next month you don&#8217;t get to use it?&nbsp;&nbsp;</p>



<p><strong>LONGHURST:</strong> [LAUGHS] The good news is because of a doctor&#8217;s medical training, they all understood the need for it. And the conversation was sort of, hey, we&#8217;re going to need you to stop using that for a month so that we can compare it, but we&#8217;ll give it back to you afterwards.&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] OK, great. All right. So now we made some other predictions. So we talked about, you know, responding to patients. You briefly mentioned clinical note-taking. We also made guesses about other types of paperwork, you know, filling out prior authorization requests or referral letters, maybe for a doctor to refer to a specialist. We even made some guesses about a second set of eyes on medications, on various treatment options, diagnoses. What of these things have happened and what hasn&#8217;t happened, at least in your clinical experience?</p>



<p><strong>LONGHURST:</strong> Your guesses were spot on. And I would say almost all of them have already happened and are happening today at UC San Diego and many other health systems. We have a HIPAA-compliant GPT instance that can be used for things like generating patient letters, generating referral letters, even generating patient education with patient-friendly language. And that&#8217;s a common use case. The second set of eyes on medications is something that we&#8217;re exploring but have not yet rolled out. One of the areas I&#8217;m really excited about is reporting. So Johns Hopkins did <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jama/fullarticle/2805705" target="_blank" rel="noreferrer noopener">a study a couple of years ago that showed an average academic medical center our size spends about $5 million annually<span class="sr-only"> (opens in new tab)</span></a> just reporting on quality measures that are regulatory requirements. And that&#8217;s about accurate for us.&nbsp;</p>



<p>We published a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIcs2400420" target="_blank" rel="noreferrer noopener">paper just last fall showing that large language models could help to pre-populate quality data<span class="sr-only"> (opens in new tab)</span></a> for things like sepsis reporting in a really effective way. It was like 91% accurate. And so that&#8217;s a huge time savings and efficiency opportunity. Again, allows us to redeploy those qualities staff. We&#8217;re now looking at things like how do we use large language models to review charts for peer review to help ensure ongoing, you know, accuracy and mitigate risk. I&#8217;m really passionate about the whole space of using AI to improve quality and patient safety in particular.&nbsp;&nbsp;</p>



<p>Your readers may be familiar with the famous report in 1999, “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://pubmed.ncbi.nlm.nih.gov/25077248/" target="_blank" rel="noreferrer noopener">To Err is Human<span class="sr-only"> (opens in new tab)</span></a>,” that suggests a hundred thousand Americans die on an annual basis from medical errors. And unfortunately the data shows we really haven&#8217;t made great progress in 25 years, but these new tools give us the opportunity to impact that in a really meaningful way. This is a turning point in healthcare.</p>



<p><strong>LEE:</strong> Yeah, medication errors—actually, all manner of medical errors—I think has been just such a frustrating problem. And, you know, I think this gives us some new hope. Well, let&#8217;s look ahead a little bit. And just to be a little bit provocative, you know, one question that I get asked a lot by both patients and clinicians is, you know, “Will AI replace doctors sometime in the future?” What are your thoughts?&nbsp;</p>



<p><strong>LONGHURST:</strong> So the pat response is AI won&#8217;t replace doctors, but AI will replace doctors who don&#8217;t use AI. And the implication there, of course, is that a doctor using AI will end up being a more effective practitioner than a doctor who doesn&#8217;t. And I think that&#8217;s absolutely true. From a medical legal standpoint, what is standard of care today and what is standard of care five or 10 years from now will be different. And I think there will be a point where doctors who aren&#8217;t using AI regularly, it would almost be unconscionable.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think there are already some areas where we&#8217;ve seen this happen. My favorite example is with the technology of ultrasound, where if you&#8217;re a gynecologist or some part of internal medicine, there are some diagnostic procedures where it would really be malpractice not to use ultrasound. Whereas in the late 1950s, the safety and also the doctor training to read ultrasound images were all called into question. And so let&#8217;s look ahead two years from now, five years from now, 10 years from now. And on those three time frames, you know, what do you think—based on the practice of medicine today, what doctors and nurses are doing in clinic every day today—what do you think the biggest differences will be two years from now, five years from now, and 10 years from now?&nbsp;</p>



<p><strong>LONGHURST:</strong> Great question, Peter. So first of all, 10 years from now, I think that patients will be still coming to clinic. Doctors will still be seeing them. Hopefully we&#8217;ll have more house calls and care occurring outside the clinic with remote monitoring and things like that. But the most important part of healthcare is the humanism. And so what I&#8217;m really excited about is AI helping to restore humanism in medical care. Because we&#8217;ve lost some of it over the last 20, 30 years as healthcare has become more corporate.&nbsp;&nbsp;</p>



<p>So in the next two to five years, some things I expect to see is AI baked into more workflows. So AI scribes are going to become incredibly commonplace. I also think that there are huge opportunities to use those scribes to help reduce errors in diagnosis. So five or seven years from now, I think that when you&#8217;re speaking to your physician about your symptoms and other things, the scribe is going to be developing a differential diagnosis and helping recommend not only the right follow-up tests or imaging but even the physical exam findings that the doctor might want to look for in particular to help make a diagnosis.&nbsp;&nbsp;</p>



<p>Dirty secret in healthcare, Peter, is that 50% of doctors are below average. It&#8217;s just math. And I think that the AI can help raise all of our doctors. So it&#8217;s like Lake Wobegon. They&#8217;re all above average. It has important implications for the workforce as you were saying. Do we need all visits to be with primary care doctors? Will mid-level providers augmented by AI be able to do as great a job as many of our physicians do? I think these are unanswered questions today that need to be explored. And then there was a really stimulating <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nytimes.com/2025/02/02/opinion/ai-doctors-medicine.html" target="_blank" rel="noreferrer noopener">editorial in <em>The New York Times</em> recently by Dr. Eric Topol<span class="sr-only"> (opens in new tab)</span></a>, and he was waxing philosophic about a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://economics.mit.edu/sites/default/files/2023-07/agarwal-et-al-diagnostic-ai.pdf" target="_blank" rel="noreferrer noopener">recent study that showed AI could interpret X-rays with 90% accuracy and radiologists actually achieve about 72% accuracy<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>LONGHURST:</strong> The study looked at, how did the radiologists do with AI working together? And they got about 74% accuracy. So the doctors didn&#8217;t believe the AI. They thought that they were in the right, and the inference that Eric took that I agree with is that rather than always looking for ways to combine the two, we should be thinking about those tasks that are amenable to automation that could be offloaded with AI. So that our physicians are focused on the things that they&#8217;re great at, which is not only the humanism in healthcare but a lot of those edge cases we talked about. So let&#8217;s take mammogram screening as an example, chest X-ray screening. There&#8217;s going to be a point in the next five years where all first reads are being done by AI, and then it&#8217;s a subset of those that are positive that need to be reviewed by physicians. And that helps free up radiologists to do a lot of other things that we need them to do.&nbsp;</p>



<p><strong>LEE:</strong> Wow, that is really just such a great vision for the future. And I call some of this <em>the flip</em>, where even patient expectations on the use of technology flips from fear and uncertainty to, you know, you would try to do this without the technology? And I think you just really put a lot of color and detail on that. Well, Chris, thank you so much for this. On that groundbreaking paper from April of 2023, we&#8217;ll put a link to it. It&#8217;s a really great thing to read. And of course, you&#8217;ve published extensively since then. But I can&#8217;t thank you enough for just all the great work that you&#8217;re doing. It&#8217;s really changing medicine.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;&nbsp;</p>



<p><strong>LONGHURST:</strong> Peter, can&#8217;t thank you enough for the opportunity to be here today and the partnership with Microsoft to make this all possible.&nbsp;</p>



<p><strong>LEE:</strong> I always love talking to Chris because he really is a prime example of an important breed of doctor, a doctor who has clinical experience but is also world-class tech geek. [LAUGHS] You know, it&#8217;s surprising to me, and pleasantly so, that the traditional gold standard of randomized trials that Chris has employed can be used to assess the viability of generative AI, not just for things like medical diagnosis, but even for seemingly mundane things like writing email notes to patients.&nbsp;&nbsp;</p>



<p>The other surprise is that the use of AI, at least in the in-basket task, which involves doctors having to respond to emails from patients, doesn&#8217;t seem to save much time for doctors, even though the AI is drafting those notes. Doctors seem to love the reduced cognitive burden, and patients seem to appreciate the greater detail and friendliness that AI provides, but it&#8217;s not yet a big timesaver. And of course, the biggest surprise out of the conversation with Chris was his celebrated paper back two years ago now on the idea that AI notes are perceived by patients as being more empathetic than notes written by human doctors. Wow.</p>



<p>Let&#8217;s go ahead to my conversation with Dr. Sara Murray:&nbsp;</p>



<p><strong>LEE:</strong> Sara, I&#8217;m thrilled you&#8217;re here. Welcome.&nbsp;</p>



<p><strong>SARA MURRAY:</strong> Thank you so much for having me.&nbsp;</p>



<p><strong>LEE:</strong> You know, you have actually a lot of roles, and I know that&#8217;s not so uncommon for people at the leading academic medical institutions. But, you know, I think for our audience, understanding what a chief health AI officer does, an associate professor of clinical medicine—what does it all mean? And so to start, when you talk to someone, say, like your parents, how do you describe your job? You know, how do you spend a typical day at work?&nbsp;</p>



<p><strong>MURRAY:</strong> So first and foremost, I do always introduce myself as a physician because that&#8217;s how I identify, that’s how I trained. But in my current role, as the chief health AI officer, I&#8217;m really responsible for the vision and strategy for how we use <em>trustworthy</em> AI at scale to solve the biggest problems in our health system. And so I think there&#8217;s a couple key important points about that. One is that we have to be very careful that everything we&#8217;re doing in healthcare is trustworthy, meaning it&#8217;s safe, it&#8217;s ethical, it&#8217;s doing what we hope it&#8217;s doing, and it&#8217;s not causing any unexpected harm.&nbsp;&nbsp;</p>



<p>And then, you know, second, we really want to be doing things that affect, you know, the population at large of the patients we&#8217;re taking care of. And so I think if you look historically at what&#8217;s happened with AI in healthcare, you&#8217;ve seen little studies here and there, but nothing broadly affecting or transforming how we deliver care. And I think now that we&#8217;re in this generative AI era, we have the tools to start thinking about how we&#8217;re doing that. And so that&#8217;s part of my role.&nbsp;</p>



<p><strong>LEE:</strong> And I&#8217;m assuming a chief health AI officer is not a role that has been around for a long time. Is this fairly new at UCSF, or has this particular job title been around?</p>



<p><strong>MURRAY:</strong> No, it&#8217;s a relatively new role, actually. I came into this role about 18 months ago. I am the first chief health AI officer at UCSF, and I actually wrote <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIp2400109" target="_blank" rel="noreferrer noopener">the paper defining the role<span class="sr-only"> (opens in new tab)</span></a> with<strong> </strong>Dr. Ashley Beecy, Dr. Chris Longhurst, Dr. Karandeep Singh, and Dr. Bob Wachter where we discuss what is this role in healthcare, why do we actually need it now, and what is this person accountable for. And I think it&#8217;s very important that as we roll these technologies out in health systems, we have someone who&#8217;s really accountable for thinking about, you know, whether we&#8217;re selecting the right tools and whether they&#8217;re being used in the right ways to impact our patients.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> It&#8217;s so interesting because I would say in the old days, you know, like five years ago, [LAUGHS] information technology in a hospital or health-system setting might be under the control and responsibility of a chief information officer, a CIO, or an IT, you know, chief. Or if it&#8217;s maybe some sort of medical device technology integration, maybe it&#8217;s some engineering-type of leader, a chief technology officer. But you&#8217;re different, and in fact the role that I think I would credit you with, sort of, making the blueprint for seems different because it&#8217;s actually doctors, practicing clinicians, who tend to inhabit these roles. Is there a reason why it&#8217;s different that way? Like, a typical CIO is not a clinician.</p>



<p><strong>MURRAY:</strong> Yeah, so I report to our CIO. And I think that there&#8217;s a recognition that you need a clinician who really understands in practice how the tools can be deployed effectively. So it&#8217;s not enough to just understand the technology, but you really have to understand the use cases. And I think when you&#8217;re seeing physician chief health AI officers pop up around the country, it&#8217;s because they’re people who both understand the technology—not to the level you do obviously—but to some sufficient level and then understand how to use these tools in clinical care and where they can drive value and what the risks are in clinical care and that type of thing. And so I think it&#8217;d be hard for it not to be some type of clinician in this role.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> So I&#8217;m going to want to get into, you know, what&#8217;s really happening in clinic, but before that, I&#8217;ve been asking our guests about their “stages of AI grief,” [LAUGHS] as I like to put it. And for most people, I&#8217;ve been talking about the experiences and encounters with machine learning and AI before ChatGPT and then afterwards. And so can you tell us a little bit about, you know, how did you get into AI in the first place and what were your first encounters like?&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah. So I actually started out as a health services researcher, and this was before we had electronic health records [EHR], when we were still writing our notes on carbon copy in the elevators, and a lot of the data we used was actually from claims data. And that was the kind of rich data source at the time, but as you know, that was very limited.&nbsp;&nbsp;</p>



<p>And so when we went live with our electronic health record, I realized there was this tremendous opportunity to really use rich clinical data for research. And so I initially started collaborating with folks down at Stanford to do machine learning to identify, you know, rare diseases like lupus in the electronic health record but quickly realized there was this real gap in the health system for using data in an actionable way.&nbsp;&nbsp;</p>



<p>And so I built what was initially our advanced analytics team, grew into our data science team, and is now our health AI team as our ability to use the data in more sophisticated ways evolved. But if we think about, I guess, the pre-generative era and my first encounter with AI or at least AI deployment in healthcare, you know, we initially, gosh, it was probably eight or nine years ago where we got access through our EHR vendor to some initial predictive tools, and these were relatively simple tools, but they were predicting things we care about in healthcare, like who&#8217;s not going make it to a clinic visit or how long patients are going stay in the hospital.&nbsp;&nbsp;</p>



<p>And so there&#8217;s a lot of interest in, you know, predicting who might not make it to a clinic visit because we have big access issues with it being difficult for patients to get appointments, and the idea was that if you knew who wouldn&#8217;t show, you could actually put someone else in that slot, and it&#8217;s called overbooking. And so when we looked at the initial model, it was striking to me how risky it was for vulnerable patient populations because immediately it was obvious that this model was likely to overbook people by race, by body weight, by things that were clearly protected patient characteristics.&nbsp;&nbsp;</p>



<p>And so we did a lot of work initially with that model and a lot of education around how these tools could be biased. But the risk existed, and as we continued to look at more of these models, we found there were a lot of issues with trustworthiness. You know, there was a length-of-stay prediction model that my team was able to outperform with a pair of dice. And when I talked to other systems about not implementing this model, you know, folks said, but it must be useful a little bit. I was like, actually, you know, if the dice is better, it&#8217;s not useful at all. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> Right!&nbsp;&nbsp;</p>



<p><strong>MURRAY:</strong> And so there was very little out there to frame this, but we quickly realized we have to start putting something together because there&#8217;s a lot of hype and there&#8217;s a lot of hope, but there&#8217;s also a lot of risk here. And so that was my pre-generative moment.&nbsp;</p>



<p><strong>LEE:</strong> You know, just before I get to your post-generative moment, you know, this story that you told, I sometimes refer to it as the healthcare IT world&#8217;s version of <em>irrational exuberance</em>. Because I think one thing that I&#8217;ve learned, and I have to say I&#8217;ve been guilty personally as a techie, you look at some of the problems that the world of healthcare faces, and to a techie first encountering this, a lot of it looks like common sense. Of course, we can build a model and predict these things.&nbsp;&nbsp;</p>



<p>And you sort of don&#8217;t understand some of the realities, as you&#8217;ve described, that make this complicated. And at the same time, from healthcare professionals, I sometimes think they look at all of this dazzling machine learning magic and also are kind of overly optimistic that it can solve so many problems.&nbsp;&nbsp;</p>



<p>And it does create this danger, this irrational exuberance, that both sides kind of get into a reinforcing cycle where they&#8217;re too quick to adopt technologies without thinking through the implications more carefully. I don&#8217;t know if that resonates with you at all.&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah, totally. I think there&#8217;s a real educational opportunity here because it&#8217;s the “you don&#8217;t know what you don&#8217;t know” phenomenon. And so I do think there is a lot of work in healthcare to be done around, you know, people understanding the strengths and limitations of these tools because they&#8217;re not magic, but they are perceived to be magic.&nbsp;&nbsp;</p>



<p>And likewise, you know, I think the tech world often doesn&#8217;t understand, you know, how healthcare is practiced and doesn&#8217;t think through these risks in the same way we do, right. So I know that some of the vulnerable patients who might&#8217;ve been overbooked by that algorithm are the people who I most need to see in clinic and are the people who would be, you know, most slighted if that they show up and the other patient shows up and now you have an overworked clinician. But I just think those are stages, you know, further down the pathway of utilization of these algorithms that people don&#8217;t think of when they&#8217;re initially developing them.&nbsp;&nbsp;</p>



<p>And so one of the things we actually, you know, require in our AI oversight process is when folks come to the table with a tool, they have to have a plan for how it&#8217;s going to be used and operationalized. And a lot of things die right there, honestly, because folks have built a cool tool, but they don&#8217;t know who&#8217;s going to use it in clinic, who the clinical champions are, how it&#8217;ll be acted on, and you can&#8217;t really evaluate whether these tools are trustworthy unless you&#8217;ve thought through all of that.&nbsp;&nbsp;</p>



<p>Because you can imagine using the same algorithm in dramatically different ways, right. If you&#8217;re using the no-show model to do targeted outreach and send people a free Lyft if they have transportation issues, that&#8217;s going to have very different outcomes than overbooking folks.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> It&#8217;s so interesting and I&#8217;m going to want to get back to this topic because I think it also speaks to the challenges of how do you integrate technologies into the daily workflow of a clinic. And I know this is something you think about a lot, but let&#8217;s get back now to my original question about your AI moments. So now November 2022, ChatGPT happens, and what is your encounter with this new technology?&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah. So I used to be on MedTwitter, or I still am actually; it&#8217;s just not as active anymore. But I would say, you know, MedTwitter went crazy after ChatGPT was initially released and it was largely filled with catchy poems and people, you know, having fun &#8230;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] Guilty.&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah, exactly. I still use poems. And people having fun trying to make it hallucinate. And so, you know, I went—I was guilty of that, as well—and so one of the things I initially did was I asked it to do something crazy. So I asked it, draft me a letter for a prior authorization request for a drug called Apixaban, which is a blood thinner, to treat insomnia. And if you practice clinical medicine, you know that we would never use a blood thinner to treat insomnia. But it wrote me such a compelling letter that I actually went back to PubMed and I made sure that I wasn&#8217;t missing anything, like some unexpected side effect. I wasn&#8217;t missing anything and in fact it was hallucination. And so at that moment I said, this is very promising technology, but this is still a party trick.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>MURRAY:</strong> A few months later, I went and did the exact same prompt, and I got a lecture, instead of a draft, about how it would be unethical [LAUGHTER] and unsafe for me to draft such a request. And so, you know, I realized these tools were rapidly evolving, and the game was just going to be changing very quickly. I think the other thing that, you know, we&#8217;ve never seen before is the deployment of a technology at scale like we have with AI scribes.&nbsp;&nbsp;</p>



<p>So this is a technology that was in its infancy, you know, two years ago, and is now largely a commodity deployed at scale across many health systems. A very short period of time. There&#8217;s been no government incentives for people to do this. And so it clearly works well enough to be used in clinics. And I think these tools, you know, like AI scribes, have the opportunity to really undo a lot of the harm that the electronic health record implementations were perceived to have caused.<strong>&nbsp;</strong>&nbsp;</p>



<p><strong>LEE:</strong> What is a scribe, first off?&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah, so AI scribes or, as we&#8217;re now calling them, <em>AI assistants</em> or <em>ambient assistants</em>, are tools that essentially listen to your clinical interaction. We record them with the permission of a patient, with consent, and then they draft a clinical note, and they can also draft other things like the patient instructions. And the idea is those drafts are very helpful to clinicians, and they have to review them and edit them, but it saves a lot of the furious typing that was previously happening during patient encounters.&nbsp;</p>



<p><strong>LEE:</strong> We have been talking also to Chris Longhurst, your colleague at UC San Diego, and, you know, he mentions also the importance of having appropriate billing codes in those notes, which is yet another burden. Of course, when Carey, Zak, and I wrote our book, we predicted that AI scribes would get better and would find wider use because of the improvement in technology. Let me start by asking, do you yourself use an AI scribe?&nbsp;</p>



<p><strong>MURRAY:</strong> So I do not use it yet because I&#8217;m an inpatient doctor, and we have deployed them to all ambulatory clinic doctors because that&#8217;s where the technology is tried and true. So we&#8217;re looking now to deploy it in the inpatient setting, but we&#8217;re doing very initial testing.&nbsp;</p>



<p><strong>LEE:</strong> And what are the reasons for not integrating it into the inpatient setting?&nbsp;</p>



<p><strong>MURRAY:</strong> Well, there&#8217;s two things actually. Most inpatient documentation work, I would say, is follow-up documentation. And so you&#8217;re often taking your prior notes and making small changes to it as you change the care from day to day. And so the tools are just, all of the companies are working on this, but right now they don&#8217;t really incorporate your prior documentation or note when they draft your note for today.&nbsp;&nbsp;</p>



<p>The second reason is that a lot of the decision-making that we do in the inpatient setting is asynchronous with the patient. So we&#8217;ll often have a conversation in the morning with the patient in their room, and then I&#8217;ll see some labs come back and I&#8217;ll make decisions and act on those labs and give the patient a call later and let them know what&#8217;s going on. And so it&#8217;s not a very succinct encounter, and so the technology is going to have to be a little bit different to work in that case, I think.&nbsp;</p>



<p><strong>LEE:</strong> Right, and so these are distinct workflows from the ambulatory setting, where it is the classic, you&#8217;re sitting with a patient in an exam room having an encounter.&nbsp;</p>



<p><strong>MURRAY:</strong> Mm-hmm. Exactly. And all your decisions are made there. And I would say it&#8217;s also different from nursing. We&#8217;re also looking at deploying these tools to nurses. But a lot of their documentation is in something called flowsheets. They write in columns, you know, specific numbers, and so for them to use these tools, they&#8217;d have to start saying to the patient, sounds like your pain is a five. Your blood pressure is 120 over 60. And so those are different workflows they&#8217;d have to adopt to use the tools.&nbsp;</p>



<p><strong>LEE: </strong>So you&#8217;ve been in the position of having to oversee the integration of AI scribes into UCSF health. From your perspective how were clinical staff actually viewing all of this?&nbsp;&nbsp;</p>



<p><strong>MURRAY:</strong> So I would say clinical staff are largely very excited, receptive, and would like us to move faster. And in fact, I gave a town hall to UCSF, and all of the comments were, when is this coming for APPs [advanced practice providers]? When is this coming for allied health professionals? And so people want this across healthcare. It&#8217;s not just doctors. But at the same time, you know, I think there&#8217;s a technology adoption curve, and about half of our ambulatory clinicians have signed up and about a third of them are now using the tool. And so we are now doing outreach to figure out who is not using it, why aren&#8217;t they using it, and what can we do to increase adoption. Or are there true barriers that we need to help folks overcome?&nbsp;</p>



<p><strong>LEE:</strong> And when you do these things, of course, there are risks. And as you were mentioning several times before, you were really concerned about hallucinations, about trustworthiness. So what were the steps that you took at UCSF to make these integrations happen?&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah, so we have a AI oversight process for all tools that come into our healthcare with AI, regardless of where they&#8217;re coming from. So industry tools, internally developed tools, and research tools come through the same process. And we have a committee that is quite multidisciplinary. We have health system leaders, data scientists, bioethicists, researchers, health-equity experts. And through our process, we break down the AI lifecycle to a couple key places where these tools come for committee review. And so for every AI deployment, we expect people to establish performance metrics, fairness metrics, and we help them with figuring out what those things should be.</p>



<p>We were also fortunate to receive a donation to build a AI monitoring platform, which we&#8217;re working on now at UCSF. We call it our Impact Monitoring Platform for AI and Clinical Care, <em>IMPACC</em>, and AI scribes is actually our first use case. And so on that platform, we have a metric adjudication process where we&#8217;ve established, you know, what do we really care about for our health system executive leaders, what do we really care about for, you know, ensuring safety and trustworthiness, and then, you know, what are our patients going to want to know? Because we want to also be transparent with our patients about the use of these tools. And so we have processes for doing all this work.&nbsp;&nbsp;</p>



<p>I think the challenge is actually how we scale these processes as more and more tools come through because as you could imagine, a lot of conversation with a lot of stakeholders to figure out what and how we measure things right now.&nbsp;</p>



<p><strong>LEE:</strong> And so there&#8217;s so much to get into there, but I actually want to zoom in on the actual experience that doctors, nurses, and patients are having. And, you know, do you find that AI is meeting expectations? Is it making a difference, positive or negative, in people&#8217;s lives? And what kinds of potential surprises are people encountering?&nbsp;</p>



<p><strong>MURRAY:</strong> Mm-hmm. So we&#8217;re collecting data in a couple of ways. First, we&#8217;re surveying clinicians before and after their experience, and we are hearing from folks that they feel like their clinic work is more manageable, that they&#8217;re more able to finish their documentation in a timely fashion.&nbsp;&nbsp;</p>



<p>And then we&#8217;re looking at actual metrics that we can extract from the EHR around how long people are spending doing things. And that data is largely aligning with what people are reporting, although the caveat is they&#8217;re not saving enough time for us to have them see more patients. And so we&#8217;ve been very explicit at UCSF around making it clear that this is a tool to improve experience and not to improve efficiency.&nbsp;&nbsp;</p>



<p>So we&#8217;re not expecting for people to see more patients as a result of using this tool. We want their clinic experience to be more meaningful. But then the other thing that&#8217;s interesting that folks share is this tremendous relief of cognitive burden that folks feel when using this tool. So they may have been really efficient before. You know, they could get all their work done. They could type while they were talking to their patients. But they didn&#8217;t actually, you know, get to look at their patients eye to eye and have the meaningful conversation that people went into medicine for. And so we&#8217;re hearing that, as well.&nbsp;&nbsp;</p>



<p>And I think one of the things that&#8217;s going to be important to us is actually measuring that moving forward. And that is matched by some of the feedback we&#8217;re getting from patients. So we have quotes from patients where they&#8217;ve said, you know, my doctor is using this new tool and it&#8217;s amazing. We&#8217;re just having eye-to-eye conversations. Keep using it. So I think that&#8217;s really important.&nbsp;</p>



<p><strong>LEE:</strong> I&#8217;ve been pushing my own primary care doctor to get into this because I really depend on her. I love her dearly, but we never … I&#8217;m always looking at her back as she&#8217;s typing at a computer during our encounters.&nbsp;[LAUGHS]&nbsp;</p>



<p>So, Sara, while we&#8217;re talking about efficiency, and at least the early evidence doesn&#8217;t show clear efficiency gains, it does actually beg the question about how or why health systems, many of which are financially, you know, not swimming in money, how or why they could adopt these things.&nbsp;&nbsp;</p>



<p>And then we could also even imagine that there are even more important applications in the future that, you know, might require quite a bit of expense on developers as well as procurers of these things. You know, what&#8217;s your point of view on the I guess we would call this the ROI question about AI?&nbsp;</p>



<p><strong>MURRAY:</strong> Mm-hmm. I think this is a really challenging area because return on investment is very important to health systems that are trying to figure out how to spend a limited budget to improve care delivery. And so I think we&#8217;ve started to see a lot of small use cases that prove this technology could likely be beneficial.&nbsp;&nbsp;</p>



<p>So there are use cases that you may have heard of from Dr. Longhurst around drafting responses to patient messages, for example, where we&#8217;ve seen that this technology is helpful but doesn&#8217;t get us all the way there. And that&#8217;s because these technologies are actually quite expensive. And when you want to process large amounts of data, that&#8217;s called tokens, and tokens cost money.&nbsp;&nbsp;</p>



<p>And so I think one of the challenges when we envision the future of healthcare, we&#8217;re not really envisioning the expense of querying the entire medical record through a large language model. And we&#8217;re going to have to build systems from a technology standpoint that can do that work in a more affordable way for us to be able to deliver really high-value use cases to clinicians that involve processing that.&nbsp;&nbsp;</p>



<p>And so those are use cases like summarizing large parts of the patient&#8217;s medical record, providing really meaningful clinical decision support that takes into account the patient&#8217;s entire medical history. We haven&#8217;t seen those types of use cases really come into being yet, largely because, you know, they&#8217;re technically a bit more complex to do well and they&#8217;re expensive, but they&#8217;re completely feasible.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. You know, what you&#8217;re saying really resonates so strongly from the tech industry&#8217;s perspective. You know, one way that that problem manifests itself is shareholders in big tech companies like ours more or less expect … they&#8217;re paying a high premium—a high multiple on the share price—because they&#8217;re expecting our revenues to grow at very spectacular rates, double digit rates. But that isn&#8217;t obviously compatible with how healthcare works and the healthcare business works. It doesn&#8217;t grow, you know, at 30% year over year or anything like that.</p>



<p>And so how to make these things financially make sense for all comers. And it&#8217;s sort of part and parcel also with the problem that sometimes efficiency gains in healthcare just translate into heavier caseloads for doctors, which isn&#8217;t obviously the best outcome either. And so in a way, I think it&#8217;s another aspect of the work on impact and trustworthiness when we think about technology, at all, in healthcare.<strong>&nbsp;</strong>&nbsp;</p>



<p><strong>MURRAY: </strong>Mm-hmm. I think that&#8217;s right. And I think, you know, if you look at the difference between the AI scribe market and the rest of the summarization work that&#8217;s largely happening within the electronic health record, in the AI scribe market, you have a lot of independent companies, and they all are competing to be the best. And so because of that, we&#8217;re seeing the technology get more efficient, cheaper. There&#8217;s just a lot of investment in that space.&nbsp;&nbsp;</p>



<p>Whereas like with the electronic health record providers, they&#8217;re also invested in really providing us with these tools, but it&#8217;s not their main priority. They&#8217;re delivering an entire electronic health record, and they also have to do it in a way that is affordable for, you know, all kinds of health systems, big UCSF health systems, smaller settings, and so there&#8217;s a real tension, I think, between delivering good-enough tools and truly transformative tools.<strong>&nbsp;</strong>&nbsp;</p>



<p><strong>LEE:</strong> So I want to go back for a minute to this idea of cognitive burden that you described. When we talk about cognitive burden, it’s often in the context of paperwork, right. There are maybe referral letters, after-visit notes, all of these things. How do you see these AI tools progressing with respect to that stream of different administrative tasks?&nbsp;</p>



<p><strong>MURRAY:</strong> These tools are going to continue to be optimized to do more and more tasks for us. So with AI scribes, for example, you know, we&#8217;re starting to look at whether it can draft the billing and coding information for the clinician, which is a tedious task with many clicks.&nbsp;&nbsp;</p>



<p>These tools are poised to start pending orders based on the conversation. Again, a tedious task. All of this with clinician oversight. But I think as we move from them being AI scribes to AI assistants, it&#8217;s going to be like a helper on the side for clinicians doing more and more work so they can really focus on the conversations, the shared decision-making, and the reason they went into medicine really.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, let me, since you mentioned AI assistants and that&#8217;s such an interesting word and it does connect with something that was apparent to us even, you know, as we were writing the book, which is this phenomenon that these AI systems might make mistakes.&nbsp;&nbsp;</p>



<p>They might be guilty of making biased decisions or showing bias, and yet they at the same time seem incredibly effective at spotting other people&#8217;s mistakes or other people&#8217;s biased decisions. And so is there a point where these AI scribes do become AI assistants, that they&#8217;re sort of looking over a doctor&#8217;s shoulder and saying, “Hey, did you think about something else?” or “Hey, you know, maybe you&#8217;re wrong about a certain diagnosis?”&nbsp;</p>



<p><strong>MURRAY:</strong> Mm-hmm. I mean, absolutely. You&#8217;re just really talking about combining technologies that already exist into a more streamlined clinical care experience, right. So you can—and I already do this when I&#8217;m on rounds—I&#8217;ll kind of give the case to ChatGPT if it&#8217;s a complex case, and I&#8217;ll say, “Here&#8217;s how I&#8217;m thinking about it; are there other things?” And it&#8217;ll give me additional ideas that are sometimes useful and sometimes not but often useful, and I&#8217;ll integrate them into my conversation about the patient.&nbsp;&nbsp;</p>



<p>I think all of these companies are thinking about that. You know, how do we integrate more clinical decision-making into the process? I think it&#8217;s just, you know, healthcare is always a little bit behind the technology industry in general, to say the least. And so it&#8217;s kind of one step at a time, and all of these use cases need a lot of validation. There&#8217;s regulatory issues, and so I think it&#8217;s going to take time for us to get there.&nbsp;</p>



<p><strong>LEE:</strong> Should I be impressed or concerned that the chief health AI officer at UC San Francisco Health is using ChatGPT off label?&nbsp;</p>



<p><strong>MURRAY:</strong> [LAUGHS] Well, I actually, every time I go on service, I encourage my residents to use it because I think we need to learn how to use these technologies. And, you know, when our medical education leaders start thinking about how do we teach students to use these, we don&#8217;t know how to teach students to use them if we&#8217;re not using them ourselves, right. And so I&#8217;ve learned a lot about what I perceive the strengths and limitations of the tools are.&nbsp;&nbsp;</p>



<p>And I think … but you know, one of the things that we&#8217;ve learned is—and you&#8217;ve written about this in your book—but the prompting really matters. And so I had a resident ask it for a differential for abnormal liver tests. But in asking for that differential, there is a key important blood finding, something called eosinophilia. It&#8217;s a type of blood cell that was mildly, mildly elevated, and they didn&#8217;t know it. So they didn&#8217;t give it in the prompt, and as a result, they didn&#8217;t get the right differential, but it wasn&#8217;t actually ChatGPT&#8217;s fault. It just didn&#8217;t get the right information because the trainee didn&#8217;t recognize the right information. And so I think there&#8217;s a lot to learn as we practice using these tools clinically. So I&#8217;m not ashamed of it. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] Yeah. Well, in fact, I think my coauthor Carey Goldberg would find what you said really validating because in our book, she actually wrote this fictional account of what it might be like in the future. And this medical resident was also using an AI chatbot off label for pretty much the same kinds of purposes. And it&#8217;s these kinds of things that, you know, it seems like might be coming next.&nbsp;</p>



<p><strong>MURRAY:</strong> I mean, medicine, the practice of medicine, is a very imperfect science, and so, you know, when we have a difficult case, I might sit in the workroom with my colleagues and run it by people. And everyone has different thoughts and opinions on, you know, things I should check for. And so I think this is just one other resource where you can kind of run cases, obviously, just reviewing all of the outputs yourself.&nbsp;</p>



<p><strong>LEE:</strong> All right, so we&#8217;re running short on time and so I want to be a little provocative at the end here. And since we&#8217;ve gotten into AI assistants, two questions: First off, do we get to a point in the near future when it would be unthinkable and maybe even bordering on malpractice for a doctor <em>not</em> to use AI assistants in his or her daily work?&nbsp;</p>



<p><strong>MURRAY:</strong> So it&#8217;s possible that we see that in the future. We don&#8217;t see it right now. And that&#8217;s part of the reason we don&#8217;t force this on people. So we see AI scribes or AI assistants as a tool we offer to people to improve their daily work because we don&#8217;t have sufficient data that the outcomes are markedly better from using these tools.&nbsp;&nbsp;</p>



<p>I think there is a future where specific, you know, tools do actually improve outcomes. And then their use should be incentivized either through, you know, CMS [Centers for Medicare & Medicaid Services] or other systems to ensure that, you know, we’re delivering standard of care. But we&#8217;re not yet at the place where any of these tools are standard of care, which means they <em>should</em> be used to practice good medicine.&nbsp;</p>



<p><strong>LEE:</strong> And I think I would say that it&#8217;s the work of people like you that would make it possible for these things to become standard of care. And so now, final provocation. It must have crossed your mind through all of this, the possibility that AI might replace doctors in some ways. What are your thoughts?&nbsp;</p>



<p><strong>MURRAY:</strong> I think we&#8217;re a long way from that happening, honestly. And I think even when I talk to my colleagues in radiology about this, where I perceive, as an internist, they might be the most replaceable, there&#8217;s a million reasons why that&#8217;s not the case. And so I think these tools are going to augment our work. They&#8217;re going to help us streamline access for patients. They&#8217;re going to maybe change what clinicians have to do, but I don&#8217;t think they&#8217;re going fully replace doctors. There&#8217;s just too much complexity and nuance in providing clinical care for these tools to do that work fully.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think you&#8217;re right. And actually, you know, I think there&#8217;s plenty of evidence because in the history of modern medicine, we actually haven&#8217;t seen technology replace human doctors. Maybe you could say that we don&#8217;t use barbers for bloodletting anymore because of technology. But I think, as you say, we&#8217;re at least a long ways away.&nbsp;&nbsp;</p>



<p><strong>MURRAY:</strong> Yeah.&nbsp;</p>



<p><strong>LEE:</strong> Sara, this has been just a great conversation. And thank you for the great work that you&#8217;re doing, you know, and for being so open with us on your personal use of AI, but also how you see the adoption of AI in our health system.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>MURRAY:</strong> Thank you, it was really great talking with you.&nbsp;</p>



<p><strong>LEE: </strong>I get so much out of talking to Sara. Every time, she manages to get me refocused on two things: the quality of the user experience and the importance of trust in any new technology that is brought into the clinic. I felt like there were several good takeaways from the conversation. One is that she really validated some predictions that Carey, Zak, and I made in our book, first and foremost, that automated note taking would be a highly desirable and practical reality. The other validation is Sara revealing that even she uses ChatGPT as a daily assistant in her clinical work, something that we guessed would happen in the book, but we weren&#8217;t really sure since health systems oftentimes are very locked down when it comes to the use of technological tools.&nbsp;&nbsp;</p>



<p>And of course, maybe the biggest thing about Sara&#8217;s work is her role in defining a new type of job in healthcare, the health AI officer. This is something that Carey, Zak, and I didn&#8217;t see coming at all, but in retrospect, makes all the sense in the world. Taken together, these two conversations really showed that we were on the right track in the book. AI has made its way into day-to-day life and work in the clinic, and both doctors and patients seem to be appreciating it.&nbsp;&nbsp;</p>



<p>[MUSIC TRANSITIONS TO THEME]&nbsp;</p>



<p>I&#8217;d like to extend another big thank you to Chris and Sara for joining me on the show and sharing their insights. And to our listeners, thank you for coming along for the ride. We have some really great conversations planned for the coming episodes. We&#8217;ll delve into how patients are using generative AI for their own healthcare, the hype and reality of AI drug discovery, and more. We hope you&#8217;ll continue to tune in. Until next time.&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-10"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>



<p id="ftn_1">[1] The paper, “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank" rel="noreferrer noopener">Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum<span class="sr-only"> (opens in new tab)</span></a>,” presented the findings of a cross-sectional study that compared doctor responses to patient questions posted to the public forum r/AskDocs on Reddit to responses to the same questions generated by ChatGPT.</p>



<p id="ftn_2">[2] Dr. John Ayers is corresponding author of the April 2023 paper “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2804309" target="_blank" rel="noreferrer noopener">Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum.&#8221;<span class="sr-only"> (opens in new tab)</span></a></p>



<p id="ftn_2">[3] These findings are detailed in the paper “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2817615" target="_blank" rel="noreferrer noopener">AI-Generated Draft Replies Integrated Into Health Records and Physicians’ Electronic Communication,&#8221;<span class="sr-only"> (opens in new tab)</span></a> published in <em>JAMA Network Open</em>.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--11"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-the-reality-of-generative-ai-in-the-clinic/">The reality of generative AI in the clinic</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Metasurface: Unlocking the future of wireless sensing and communication</title>
		<link>https://www.microsoft.com/en-us/research/blog/metasurface-unlocking-the-future-of-wireless-sensing-and-communication/</link>
		
		<dc:creator><![CDATA[Lili Qiu, Hao Pan]]></dc:creator>
		<pubDate>Wed, 19 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1134100</guid>

					<description><![CDATA[<p>Metasurfaces explore engineered 2D materials that manipulate electromagnetic and mechanical waves, offering advances in wireless tech. They can power indoor GPS, extend 5G/6G coverage, and enable wireless sensing and imaging.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/metasurface-unlocking-the-future-of-wireless-sensing-and-communication/">Metasurface: Unlocking the future of wireless sensing and communication</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1.jpg" alt="The image features three white icons on a gradient background transitioning from blue on the left to green on the right. The first icon, located on the left, represents a Wi-Fi signal with curved lines radiating from a central point. The middle icon depicts a satellite with solar panels and an antenna emitting waves. The third icon, on the right, shows a bar chart with ascending bars indicating signal strength." class="wp-image-1134231" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Metasurface-BlogHeroFeature-1400x788-1-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>As the demand for faster, more reliable wireless communication continues to grow, but traditional systems face limitations in efficiency and adaptability. To keep up with evolving needs, researchers are investigating new ways to manipulate electromagnetic waves to improve wireless performance. </p>



<p>To address these challenges, researchers are exploring new approaches, including metasurfaces—engineered materials that can control wave propagation in unprecedented ways. By dynamically shaping and directing electromagnetic waves, metasurfaces offer a promising path to overcoming the constraints of conventional wireless systems. </p>



<p>Building on these capabilities, we are developing metasurfaces for a wide range of wireless applications, such as enhancing Low Earth Orbit satellite communication, optimizing acoustic sensing, and enabling acoustic and millimeter-wave technologies for 5G and 6G communication systems with commercial devices. More recently, our work has focused on enabling indoor access to the Global Navigation Satellite System (GNSS), improving millimeter-wave coverage in targeted environments, optimizing heat distribution in microwave ovens, and providing <a href="https://www.microsoft.com/en-us/research/publication/mudis-an-audio-independent-wide-angle-and-leak-free-multi-directional-speaker/" target="_blank" rel="noreferrer noopener">directional sound projection</a> without headphones.</p>



<p>These advances, published at leading networking conferences—including MobiCom 2023 and <a href="https://www.microsoft.com/en-us/research/articles/mobicom-2024-highlights-from-microsoft-research-asia-exploring-innovations-in-wireless-mobile-technology-and-applications/" target="_blank" rel="noreferrer noopener">2024</a>, MobiSys 2024 and 2025, and NSDI 2023—highlight metasurfaces’ potential in wireless communication and sensing. This post explores some of these applications in more detail. </p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085520">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft research podcast</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-silica-in-space-with-richard-black-and-dexter-greene/" aria-label="Collaborators: Silica in space with Richard Black and Dexter Greene" data-bi-cN="Collaborators: Silica in space with Richard Black and Dexter Greene" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/09/Richard-and-Dexter_Collaborators_Hero_Feature_No_Text_1400x788-1.jpg" alt="Headshots of Richard Black and Dexter Greene for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Collaborators: Silica in space with Richard Black and Dexter Greene</h2>
				
								<p class="large">College freshman Dexter Greene and Microsoft research manager Richard Black discuss how technology that stores data in glass is supporting students as they expand earlier efforts to communicate what it means to be human to extraterrestrials.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/podcast/collaborators-silica-in-space-with-richard-black-and-dexter-greene/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Silica in space with Richard Black and Dexter Greene" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="metasurfaces-optimize-gnss-for-accurate-indoor-positioning">Metasurfaces optimize GNSS for accurate indoor positioning</h2>



<p>While GNSS is widely used for outdoor positioning and navigation, its indoor performance is often hindered by signal blockage, reflection, and attenuation caused by physical obstacles. Additional technologies like Wi-Fi and Bluetooth Low Energy (BLE) are often employed to address these issues. However, these solutions require extra infrastructure, are costly, and are complicated to deploy. Accurate positioning also typically depends on specialized hardware and software on mobile devices.&nbsp;</p>



<p>Despite these challenges, GNSS signals hold promise for accurate indoor positioning. By leveraging the vast number of available satellites, GNSS-based solutions eliminate the need for base station deployment and maintenance required by Wi-Fi and BLE systems. This approach also allows seamless integration between indoor and outdoor environments, supporting continuous positioning in scenarios like guiding smart vehicles through indoor and outdoor industrial environments.&nbsp;</p>



<p>To explore this potential, we conducted indoor measurements and found that GNSS satellite signals can penetrate windows at different angles and reflect or diffract from surfaces like floors and ceilings, resulting in uneven signals. Metasurfaces can control structured arrays of electromagnetic signals, allowing them to capture and redirect more GNSS signals. This allows signals to enter buildings in a path parallel to the ground, achieving broader coverage. Using this capability, we developed a <a href="https://www.microsoft.com/en-us/research/publication/gpms-enabling-indoor-gnss-positioning-using-passive-metasurfaces/#:~:text=In%20terms%20of%20indoor%20positioning%20accuracy%2C%20our%20proposed,and%20benefits%20of%20metasurface-assisted%20GNSS%20for%20indoor%20positioning.?msockid=12da2addb70263b40f2e3f57b6c56288">GNSS positioning metasurface system (GPMS) based on passive metasurface technology</a>.</p>



<p>One limitation of passive metasurfaces is their lack of programmability. To overcome this and enable them to effectively guide signals from different angles and scatter them in parallel, we designed a two-layer metasurface system. As shown in Figure 1, this design ensures that electromagnetic waves from different angles follow similar emission trajectories.&nbsp;&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="758" height="508" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-1.png" alt="A diagram showing the optimization of metasurfaces for enhancing GNSS signals indoors. It includes two GNSS satellites, far-field channels, a near-field channel matrix, a passive metasurface grid, and colorful 3D waveforms. The target radiation matrix is shown with indoor users. The text reads: “Optimization problem: The radiation output of our designed metasurfaces should all be close to the target radiation for GNSS signal input at all incidence angles.” " class="wp-image-1134106" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-1.png 758w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-1-300x201.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-1-240x161.png 240w" sizes="auto, (max-width: 758px) 100vw, 758px" /><figcaption class="wp-element-caption">Figure 1: The GPMS two-layer metasurface structure</figcaption></figure>



<p>To improve positioning accuracy, we developed new algorithms that allow signals to pass through metasurfaces, using them as anchor points. Traditional GPS positioning requires signals from at least four satellites to decode location information. In the GPMS system, illustrated in Figure 2, each deployed metasurface functions as a virtual satellite. By deploying at least three metasurfaces indoors, we achieved high-precision positioning through a triangulation algorithm.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="826" height="476" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-2.png" alt="The image depicts a shopping mall indoor environment with three metasurfaces labeled Metasurface 1, Metasurface 2, and Metasurface 3. Each metasurface is associated with a steering and scattering area, labeled Steering and scattering area 1, Steering and scattering area 2, and Steering and scattering area 3 respectively. GNSS satellites are shown outside the building. The image illustrates how GNSS signals interact with metasurfaces within an indoor environment." class="wp-image-1134107" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-2.png 826w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-2-300x173.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-2-768x443.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-2-240x138.png 240w" sizes="auto, (max-width: 826px) 100vw, 826px" /><figcaption class="wp-element-caption">Figure 2. Diagram of the GPMS system. Passive metasurfaces guide GNSS signals indoors, while enhanced positioning algorithms provide precise indoor positioning on mobile devices.&nbsp;</figcaption></figure>



<p>To evaluate the system, we deployed the GPMS with six metasurfaces on a 10×50-meter office floor and a 15×20-meter conference hall. The results show significant improvements in signal quality and availability. C/N₀, a measure of signal-to-noise ratio, increased from 9.1 dB-Hz to 32.2 dB-Hz. The number of visible satellites increased from 3.6 to 21.5. Finally, the absolute positioning error decreased from 30.6 meters to 3.2 meters in the office and from 11.2 meters to 2.7 meters in the conference hall. These findings are promising and highlight the feasibility and advantages of GNSS-based metasurfaces for indoor positioning.&nbsp;</p>



<h2 class="wp-block-heading" id="metasurfaces-extend-millimeter-wave-coverage">Metasurfaces extend millimeter-wave coverage</h2>



<p>Millimeter waves enable the high-speed, low-latency performance needed for 5G and 6G communication systems. While commercial products like 60 GHz Wi-Fi routers and mobile devices are becoming popular, their limited coverage and susceptibility to signal obstruction restrict their widespread application.&nbsp;</p>



<p>Traditional solutions include deploying multiple millimeter-wave access points, such as routers or base stations, or placing reflective metal panels in room corners to reflect electromagnetic waves. However, these approaches are both costly and offer limited performance. Metasurfaces offer a promising alternative for improving millimeter-wave applications. Previous research has shown that programmable metasurfaces can enhance signal coverage in blind spots and significantly improve signal quality and efficiency.&nbsp;&nbsp;</p>



<p>To maximize the benefits of metasurfaces, we developed the <a href="https://www.microsoft.com/en-us/research/publication/automs-automated-service-for-mmwave-coverage-optimization-using-low-cost-metasurfaces/" target="_blank" rel="noreferrer noopener">AutoMS automation service framework</a>, shown in Figure 3. This proposed framework can optimize millimeter-wave coverage using low-cost passive metasurface design and strategic placement.&nbsp;</p>



<p>The three main components of AutoMS can address the limitations of traditional solutions:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Automated joint optimization:</strong> AutoMS determines the optimal network deployment configuration by analyzing phase settings, metasurface placement, and access point positioning. It also refines beam-forming configurations to enhance signal coverage. By iteratively identifying and optimizing the number, size, and placement of metasurfaces, AutoMS adjusts the metasurface phase settings and the access point’s configurations to achieve optimal signal coverage.&nbsp;</li>
</ol>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="725" height="234" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-3.png" alt="A flowchart diagram illustrating the AutoMS framework, which generates optimized passive metasurface and access point deployment plans for a specific 3D model based on environmental scanning results. The process starts with an environment scan, producing a 3D model and reflection coefficients. This information feeds into wireless channel modeling, which along with deployment configurations, is optimized by a hyper-configuration tuner. The output includes phase maps used by the surface and AP optimizer. The optimized deployment configurations are then used for metasurface fabrication and network deployment. " class="wp-image-1134108" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-3.png 725w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-3-300x97.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-3-240x77.png 240w" sizes="auto, (max-width: 725px) 100vw, 725px" /><figcaption class="wp-element-caption">Figure 3. The AutoMS framework generates optimized deployment plans for passive metasurface and access points based on environment scanning results<em>.</em>&nbsp;</figcaption></figure>



<ol start="2" class="wp-block-list">
<li><strong>Fast 3D ray tracing simulator:</strong> Using hardware and software acceleration, our simulator efficiently calculates channel matrices resulting from metasurfaces with tens of thousands of elements. This simulator, capable of tracing 1.3 billion rays in just three minutes on an A100 GPU, significantly accelerates calculations for complex environments.</li>
</ol>



<ol start="3" class="wp-block-list">
<li><strong>Low-cost passive metasurface design:</strong> We designed a high-reflectivity passive metasurface with near-2π phase control and broadband compatibility for the millimeter-wave frequency band. This metasurface is compatible with low-precision, cost-effective thermoforming processes. This process enables users to create metasurfaces at minimal cost, significantly reducing deployment expenses.<br><br>Shown in Figure 4, users can capture the environment using existing 3D scanning apps on mobile devices, generate a 3D layout model, and upload it to the cloud. AutoMS then generates metasurface settings and placement guidelines.&nbsp;&nbsp;<br><br>Users can print metasurface patterns using hot stamping and customize them without affecting functionality, as millimeter waves penetrate paint and paper.&nbsp;</li>
</ol>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="536" height="281" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-4.png" alt="A step-by-step process for creating low-cost passive metasurfaces. Step 1: Print patterns on paper with a laser printer. Step 2: Hot stamp aluminum foil on paper with a laminator. Step 3: Tear the aluminum foil off to get the metallic patterns. Step 4: Paste patterns on the plastic sheet and aluminum board." class="wp-image-1134109" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-4.png 536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-4-300x157.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-4-240x126.png 240w" sizes="auto, (max-width: 536px) 100vw, 536px" /><figcaption class="wp-element-caption">Figure 4: The low-cost passive metasurface creation process&nbsp;</figcaption></figure>



<p>Evaluation using publicly available 3D layout datasets and real-world tests shows that AutoMS significantly improves millimeter-wave coverage across various scenarios. Compared to a single router setup, AutoMS increased signal strength by 12.1 dB. Onsite tests further confirmed gains of 11 dB in target areas and over 20 dB in blind spots, with signal throughput increasing from 77 Mbps to 373 Mbps. AutoMS adapts to diverse environments, ensuring reliable and flexible deployment in real-world applications.&nbsp;</p>



<h2 class="wp-block-heading" id="metasurfaces-support-uniform-heating-in-microwave-ovens">Metasurfaces support uniform heating in microwave ovens&nbsp;</h2>



<p>Microwave ovens often heat unevenly, creating cold spots in food. These can allow harmful bacteria and other pathogens to survive, increasing the risk of foodborne illnesses. Uneven heating can cause eggs to burst or create &#8220;hot spots&#8221; that can scald.</p>



<p>Uneven heating is due to the appliance’s heating mechanism. Microwave ovens generate high-power radio frequency (RF) electromagnetic waves through dielectric heating. These waves create nodes with zero amplitude, which prevents heating. They also create antinodes, where heating occurs more rapidly.&nbsp;&nbsp;</p>



<p>To address this issue, we developed <a href="https://www.microsoft.com/en-us/research/publication/microsurf-guiding-energy-distribution-inside-microwave-oven-with-metasurfaces/" target="_blank" rel="noreferrer noopener">MicroSurf</a>, a low-cost solution that improves heating by using passive metasurfaces to control electromagnetic energy inside the microwave oven. It uses the resonance effect between the metasurface and electromagnetic waves to modify the standing-wave distribution and achieve more uniform heating. This is shown in Figure 5.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="859" height="205" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-5.png" alt="A diagram illustrating the working principle of MicroSurf in four parts. A shows an uneven electric field distribution inside a microwave oven leading to uneven heating, with images of a microwave and thermal images of food. B depicts accurate modeling of the microwave oven, including geometry refinement, dielectric factor tuning, and frequency tuning. C involves designing and optimizing a metasurface that can function in a high-power environment to change the standing wave distribution, with an image of a high-power phase-tuning metasurface. D demonstrates achieving uniform heating of different foods and selectively heating specific parts of food, with thermal images showing uniform heating results." class="wp-image-1134110" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-5.png 859w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-5-300x72.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-5-768x183.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/metasurface-5-240x57.png 240w" sizes="auto, (max-width: 859px) 100vw, 859px" /><figcaption class="wp-element-caption">Figure 5: MicroSurf’s working principle: Uneven electric field distribution inside the microwave oven leads to uneven heating. B. Modeling the microwave oven. C. Designing and optimizing a metasurface that can function in a high-power environment to change the standing wave distribution. D. Achieving uniform heating of different foods and selectively heating specific parts.&nbsp;</figcaption></figure>



<p>Tests across four different microwave oven brands demonstrate that MicroSurf effectively optimizes heating for various liquids and solids, uniformly heating water, milk, bread, and meat. It concentrates heat on specific areas and adapts to differently shaped foods. MicroSurf offers a promising solution for even heating in microwave ovens, demonstrating the potential of metasurface technology in everyday applications. This innovation paves the way for smarter, more efficient home appliances.&nbsp;&nbsp;</p>



<h2 class="wp-block-heading" id="advancing-wireless-innovation">Advancing wireless innovation</h2>



<p>Wireless sensing and communication technologies are evolving rapidly, driving innovation across a wide range of applications. We are continuing to push the boundaries of these technologies—particularly in metasurface development—while working to create practical solutions for a variety of use cases.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/metasurface-unlocking-the-future-of-wireless-sensing-and-communication/">Metasurface: Unlocking the future of wireless sensing and communication</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Claimify: Extracting high-quality claims from language model outputs</title>
		<link>https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/</link>
		
		<dc:creator><![CDATA[Dasha Metropolitansky]]></dc:creator>
		<pubDate>Wed, 19 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1134179</guid>

					<description><![CDATA[<p>While large language models (LLMs) are capable of synthesizing vast amounts of information, they sometimes produce inaccurate or unsubstantiated content. To mitigate this risk, tools like Azure AI’s Groundedness Detection (opens in new tab) can be used to verify LLM outputs.  A common strategy for fact-checking LLM-generated texts – especially complex, highly detailed outputs – [&#8230;]</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/">Claimify: Extracting high-quality claims from language model outputs</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1.jpg" alt="Gradient background transitioning from blue to pink with two white icons. The left icon depicts a network or molecule structure with interconnected nodes, and the right icon shows a laptop and the outline of a person." class="wp-image-1134342" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Claimify-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>While large language models (LLMs) are capable of synthesizing vast amounts of information, they sometimes produce inaccurate or unsubstantiated content. To mitigate this risk, tools like Azure AI’s <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://learn.microsoft.com/en-us/azure/ai-services/content-safety/concepts/groundedness" target="_blank" rel="noreferrer noopener">Groundedness Detection<span class="sr-only"> (opens in new tab)</span></a> can be used to verify LLM outputs. </p>



<p>A common strategy for fact-checking LLM-generated texts – especially complex, highly detailed outputs – is <strong>claim extraction</strong>: instead of evaluating the entire text at once, it’s broken down into simple factual statements that can be verified independently. However, the effectiveness of this approach depends on the quality of the extracted claims: if they are inaccurate or incomplete, the fact-checking results may be compromised. </p>



<p>Our paper “<a href="https://www.microsoft.com/en-us/research/publication/towards-effective-extraction-and-evaluation-of-factual-claims/">Towards Effective Extraction and Evaluation of Factual Claims</a>” proposes a framework for evaluating claim extraction and introduces <strong>Claimify</strong>, a novel LLM-based method that outperforms prior solutions. In this blog post, we discuss common claim extraction issues and explore Claimify’s approach.</p>



<h2 class="wp-block-heading" id="key-issues-in-claim-extraction">Key issues in claim extraction</h2>



<p>As a case study, we prompted OpenAI’s GPT-4o model to “<em>provide an overview of challenges in emerging markets</em>” based on a collection of news articles. Here is its output:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td><strong># Emerging market challenges </strong><br>Emerging markets face a myriad of challenges that span economic, social, and environmental dimensions. These challenges are often interconnected, exacerbating the difficulties faced by these regions. <br><br><strong>## Economic instability and inflation </strong><br>Several emerging markets are grappling with severe economic instability. For instance, Argentina&#8217;s rampant inflation, with monthly rates reaching as high as 25.5%, has made many goods unobtainable and plunged the value of the currency, causing severe economic hardship. Some experts estimate that the annual inflation rate could potentially double to 300%, while others predict even higher rates. <br><br><strong>## Food insecurity and agricultural challenges </strong><br>Food insecurity is another critical issue in emerging markets. Nigeria, for example, is striving to become self-sufficient in wheat production but is hindered by climate change and violence, exacerbated by high grain prices due to the suspension of the Black Sea Grain Initiative. Climate change has played a pivotal role in creating food insecurity and economic instability in farming-dependent economies, such as Zambia and Mozambique. <br><br><strong>## Natural disasters and public health emergencies </strong><br>Public health crises, as demonstrated by the experience of Derna, Libya, remain a significant concern in emerging markets. Catastrophic flooding caused by the collapse of two dams has led to significant destruction, complicating access to healthcare. The U.N. found that the resulting contaminated water caused many residents to fall ill, highlighting the need for improved water management. Countries like Afghanistan and Sudan have experienced similar challenges to those of Libya. <br><br><strong>## Conclusion </strong><br>Addressing emerging market challenges will require comprehensive strategies that consider economic stability, food security, and public health. International cooperation and targeted interventions are crucial in fostering sustainable development in emerging markets.</td></tr></tbody></table></figure>



<p>We then applied a simple prompt<a id="_ftnref1" href="#_ftn1">[1]</a> that extracted the following claims from the answer:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td>1. Emerging markets face economic challenges. <br>2. Emerging markets face social challenges. <br>3. Emerging markets face environmental challenges. <br>4. Argentina experiences severe economic instability. <br>5. Argentina has a monthly inflation rate as high as 25.5%. <br>6. Argentina&#8217;s inflation has made many goods unobtainable. <br>7. Argentina&#8217;s currency value has plunged. <br>8. Some experts estimate Argentina&#8217;s annual inflation rate could reach 300%. <br>9. Food insecurity is a critical issue in emerging markets. <br>10. Nigeria is striving to become self-sufficient in wheat production. <br>11. Nigeria&#8217;s wheat production is hindered by climate change. <br>12. Nigeria&#8217;s wheat production is hindered by violence. <br>13. High grain prices affect Nigeria due to the suspension of the Black Sea Grain Initiative. <br>14. Climate change creates food insecurity in Zambia. <br>15. Climate change creates food insecurity in Mozambique. <br>16. Climate change causes economic instability in farming-dependent economies. <br>17. Public health crises are a concern in emerging markets. <br>18. Derna, Libya, experienced catastrophic flooding. <br>19. The collapse of two dams caused flooding in Derna, Libya. <br>20. Flooding in Derna, Libya, complicated access to healthcare. <br>21. The U.N. found contaminated water in Derna, Libya. <br>22. Contaminated water in Derna, Libya, caused many residents to fall ill. <br>23. Afghanistan has experienced challenges similar to those in Libya. <br>24. Sudan has experienced challenges similar to those in Libya. <br>25. Addressing emerging market challenges requires comprehensive strategies. <br>26. Comprehensive strategies should consider economic stability. <br>27. Comprehensive strategies should consider food security. <br>28. Comprehensive strategies should consider public health. <br>29. International cooperation is crucial for sustainable development in emerging markets. <br>30. Targeted interventions are crucial for sustainable development in emerging markets.</td></tr></tbody></table></figure>



<p>These claims appear to reflect the answer, but a closer look reveals four critical issues:</p>



<figure class="wp-block-table"><table><thead><tr><th>#</th><th class="has-text-align-left" data-align="left">Issue</th><th>Example</th></tr></thead><tbody><tr><td>1</td><td class="has-text-align-left" data-align="left">Some claims are not factual statements that can be verified as true or false.</td><td><strong>Excerpt:</strong> “<em>Addressing emerging market challenges will require comprehensive strategies that consider economic stability, food security, and public health.</em>” <br><br><strong>Claims:</strong> <ul><li>Addressing emerging market challenges requires comprehensive strategies.</li> <li>Comprehensive strategies should consider economic stability.</li> <li>Comprehensive strategies should consider food security.</li> <li>Comprehensive strategies should consider public health.</li></ul><strong>Explanation: </strong>These claims are not verifiable because they are opinions.</td></tr><tr><td>2</td><td class="has-text-align-left" data-align="left">Some claims are missing or incomplete.</td><td><strong>Excerpt:</strong> “<em>Argentina&#8217;s rampant inflation, with monthly rates reaching as high as 25.5%, has made many goods unobtainable and plunged the value of the currency, <u>causing severe economic hardship</u>. Some experts estimate that the annual inflation rate could potentially double to 300%, while <u>others predict even higher rates</u>.</em>” <br><br><strong>Claims:</strong> <ul><li>Argentina has a monthly inflation rate as high as 25.5%.</li> <li>Argentina&#8217;s inflation has made many goods unobtainable.</li> <li>Argentina&#8217;s currency value has plunged.</li> <li>Some experts estimate Argentina’s annual inflation rate could reach 300%.</li></ul> <strong>Explanation: </strong>The phrases “<em>causing severe economic hardship</em>” and “<em>others predict even higher rates</em>” are not reflected in any of the claims. The third claim also omits the fact that inflation caused the currency depreciation.</td></tr><tr><td>3</td><td class="has-text-align-left" data-align="left">Some claims are inaccurate.</td><td><strong>Excerpt: </strong>“<em>The U.N. found that the resulting contaminated water caused many residents to fall ill, highlighting the need for improved water management</em>.”<br><br><strong>Claims:</strong> <ul><li>The U.N. found contaminated water in Derna, Libya.</li> <li>Contaminated water in Derna, Libya, caused many residents to fall ill.</li></ul> <strong>Explanation: </strong>The first claim is inaccurate because the U.N. found the link between contaminated water and illness, not the contaminated water itself. The second claim also misrepresents the sentence since it shifts the meaning from a viewpoint of a specific entity (the U.N.) to a general assertion about the effects of contaminated water in Derna, Libya.</td></tr><tr><td>4</td><td class="has-text-align-left" data-align="left">Some claims cannot be understood without additional context.</td><td><strong>Excerpt: </strong>“<em>Countries like Afghanistan and Sudan have experienced similar challenges to those of Libya.</em>”<br><br><strong>Claims:</strong> <ul><li>Afghanistan has experienced challenges similar to those in Libya.</li> <li>Sudan has experienced challenges similar to those in Libya.</li></ul> <strong>Explanation: </strong>These claims cannot be understood on their own because “<em>those</em>” is not defined.</td></tr></tbody></table></figure>



<h2 class="wp-block-heading" id="introducing-claimify">Introducing Claimify</h2>



<p>The case study highlights that claim extraction is surprisingly error-prone. <a href="https://www.microsoft.com/en-us/research/publication/towards-effective-extraction-and-evaluation-of-factual-claims/">Our paper</a> demonstrates that the issues identified above are common across LLM-based claim extraction methods. To minimize these errors, we created a system called Claimify<a id="_ftnref2" href="#_ftn2">[2]</a>.</p>



<h3 class="wp-block-heading" id="core-principles">Core principles</h3>



<p>Claimify is an LLM-based claim extraction system built on the following principles:</p>



<figure class="wp-block-table"><table><thead><tr><th>#</th><th>Principle</th><th>Example</th></tr></thead><tbody><tr><td>1</td><td>The claims should capture all verifiable content in the source text and exclude unverifiable content.</td><td>In the sentence “<em>The partnership between John and Jane illustrates the importance of collaboration,</em>” the only verifiable content is the existence of a partnership between John and Jane. The rest is subjective interpretation.</td></tr><tr><td>2</td><td>Each claim should be entailed (i.e., fully supported) by the source text.</td><td>Consider the sentence “<em>Governments are curtailing emissions from cars and trucks, which are the largest source of greenhouse gases from transportation</em>.” The following claims are incorrect: <br><br><ul><li>Cars are the largest source of greenhouse gases from transportation.</li> <li>Trucks are the largest source of greenhouse gases from transportation.</li></ul>The sentence attributes the highest emissions to cars and trucks collectively, not individually.</td></tr><tr><td>3</td><td>Each claim should be understandable on its own, without additional context.</td><td>The claim “<em>They will update the policy next year</em>” is not understandable on its own because it’s unclear what “<em>They</em>,” “<em>the policy</em>,” and “<em>next year</em>” refer to.</td></tr><tr><td>4</td><td>Each claim should minimize the risk of excluding critical context.</td><td>Suppose the<em> claim “The World Trade Organization has supported trade barriers” </em>was extracted from the sentence<em> “An exception to the World Trade Organization’s open-market philosophy is its history of supporting trade barriers when member countries have failed to comply with their obligations.”</em> A fact-checking system would likely classify the claim as false, since there is extensive evidence that the WTO aims to reduce trade barriers. However, if the claim had specified that the WTO has supported trade barriers “<em>when member countries have failed to comply with their obligations,</em>” it would likely have been classified as true. This example demonstrates that missing context can distort the fact-checking verdict.</td></tr><tr><td>5</td><td>The system should flag cases where ambiguity cannot be resolved.</td><td>The sentence “<em>AI has advanced renewable energy and sustainable agriculture at Company A and Company B</em>” has two mutually exclusive interpretations: <br><br><ul><li>AI has advanced renewable energy and sustainable agriculture at both Company A and Company B.</li> <li>AI has advanced renewable energy at Company A and sustainable agriculture at Company B.</li></ul>If the context does not clearly indicate that one of these interpretations is correct, the system should flag the ambiguity instead of picking one interpretation arbitrarily.</td></tr></tbody></table></figure>



<h3 class="wp-block-heading" id="implementation">Implementation</h3>



<p>Claimify accepts a question-answer pair as input and performs claim extraction in four stages, illustrated in Figure 1:</p>



<figure class="wp-block-table"><table><thead><tr><th>#</th><th>Stage</th><th>Description</th></tr></thead><tbody><tr><td>1</td><td>Sentence splitting and context creation</td><td>The answer is split into sentences, with “context” – a configurable combination of surrounding sentences and metadata (e.g., the header hierarchy in a Markdown-style answer)&nbsp;– created for each sentence.</td></tr><tr><td>2</td><td>Selection</td><td>An LLM identifies sentences that do not contain verifiable content. These sentences are labeled “No verifiable claims” and excluded from subsequent stages. When sentences contain verifiable and unverifiable components, the LLM rewrites the sentence, retaining only the verifiable components.</td></tr><tr><td>3</td><td>Disambiguation</td><td>For sentences that passed the Selection stage, an LLM detects ambiguity and determines if it can be resolved using the context. If all ambiguity is resolvable, the LLM returns a disambiguated version of the sentence. Otherwise, the sentence is labeled “Cannot be disambiguated” and excluded from the Decomposition stage.</td></tr><tr><td>4</td><td>Decomposition</td><td>For sentences that are unambiguous or were disambiguated, an LLM creates standalone claims that preserve critical context. If no claims are extracted, the sentence is labeled “No verifiable claims.”</td></tr></tbody></table></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1915" height="453" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog.png" alt="A flowchart outlining Claimify’s stages for extracting claims from a question-answer pair. The process begins by splitting the answer into sentences and creating context. Next, the Selection stage asks if a sentence contains any verifiable content. If no, the sentence is labeled "No verifiable claims" and excluded from subsequent stages; if yes, it proceeds to the Disambiguation stage. The Disambiguation stage asks if the sentence contains any ambiguity that cannot be resolved. If yes, the sentence is labeled "Cannot be disambiguated" and excluded from the final stage; if no, it proceeds to the Decomposition stage. The Decomposition stage attempts to decompose the sentence into claims. If it is decomposed into at least one claim, the sentence is labeled "Extracted claims"; otherwise, the sentence is labeled "No verifiable claims." The Selection, Disambiguation, and Decomposition stages apply to each sentence individually. " class="wp-image-1134259" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog.png 1915w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog-300x71.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog-1024x242.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog-768x182.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog-1536x363.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/claimify_figure_blog-240x57.png 240w" sizes="auto, (max-width: 1915px) 100vw, 1915px" /><figcaption class="wp-element-caption">Figure 1: Overview of Claimify’s stages</figcaption></figure>



<h2 class="wp-block-heading" id="results">Results</h2>



<p>In <a href="https://www.microsoft.com/en-us/research/publication/towards-effective-extraction-and-evaluation-of-factual-claims/" target="_blank" rel="noreferrer noopener">our paper</a>, we demonstrate that Claimify outperforms existing LLM-based methods<a href="#_ftn3">[3]</a>. Specifically, we show that: (1) 99% of claims extracted by Claimify are entailed by their source sentence, (2) Claimify strikes the best balance between including verifiable content and excluding unverifiable content, and (3) Claimify is least likely to omit context critical to the fact-checking verdict.</p>



<p>For the above case study on challenges in emerging markets, here are Claimify’s outputs, with source sentences preceded by a letter and claims numbered<a href="#_ftn4" id="_ftnref4">[4]</a>:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td><strong>A. Several emerging markets are grappling with severe economic instability. <br></strong>1. Several emerging markets are grappling with severe economic instability. <br><br><strong>B. For instance, Argentina&#8217;s rampant inflation, with monthly rates reaching as high as 25.5%, has made many goods unobtainable and plunged the value of the currency, causing severe economic hardship. <br></strong>1. Argentina has rampant inflation. <br>2. The monthly inflation rates in Argentina have reached as high as 25.5%. <br>3. Inflation has made many goods unobtainable in Argentina. <br>4. Inflation has plunged the value of the currency in Argentina. <br>5. Inflation has caused severe economic hardship in Argentina. <br><br><strong>C. Some experts estimate that the annual inflation rate could potentially double to 300%, while others predict even higher rates. <br></strong>1. Some experts estimate that Argentina&#8217;s annual inflation rate could double to 300% in the future. <br>2. Some experts predict that Argentina&#8217;s annual inflation rate could be higher than 300% in the future. <br><br><strong>D. Nigeria, for example, is striving to become self-sufficient in wheat production but is hindered by climate change and violence, exacerbated by high grain prices due to the suspension of the Black Sea Grain Initiative. <br></strong>1. Nigeria is striving to become self-sufficient in wheat production. <br>2. Nigeria is hindered by climate change in becoming self-sufficient in wheat production. <br>3. Nigeria is hindered by violence in becoming self-sufficient in wheat production. <br>4. High grain prices exacerbate the hindrance to Nigeria&#8217;s efforts to become self-sufficient in wheat production. <br>5. The suspension of the Black Sea Grain Initiative is a reason for high grain prices. <br><br><strong>E. Climate change has played a pivotal role in creating food insecurity and economic instability in farming-dependent economies, such as Zambia and Mozambique. <br></strong>1. Climate change has played a role in creating food insecurity in farming-dependent economies. <br>2. Zambia is a farming-dependent economy where climate change has played a role in creating food insecurity. <br>3. Mozambique is a farming-dependent economy where climate change has played a role in creating food insecurity. <br>4. Climate change has played a role in creating economic instability in farming-dependent economies. <br>5. Zambia is a farming-dependent economy where climate change has played a role in creating economic instability.<br>6. Mozambique is a farming-dependent economy where climate change has played a role in creating economic instability.<br><br><strong>F. Public health crises, as demonstrated by the experience of Derna, Libya, remain a significant concern in emerging markets.<br></strong>1. Public health crises are a concern in emerging markets.<br>2. Derna, Libya, is an example of a public health crisis in emerging markets.<br><br><strong>G. Catastrophic flooding caused by the collapse of two dams has led to significant destruction, complicating access to healthcare.<br></strong>1. There was catastrophic flooding in Derna, Libya.<br>2. The flooding in Derna, Libya, was caused by the collapse of two dams.<br>3. The flooding in Derna, Libya, has led to significant destruction.<br>4. The flooding in Derna, Libya, has complicated access to healthcare.<br><br><strong>H. Countries like Afghanistan and Sudan have experienced similar challenges to those of Libya.<br></strong>1. Afghanistan has experienced challenges related to public health crises.<br>2. Afghanistan has experienced challenges related to catastrophic flooding.<br>3. Afghanistan has experienced challenges related to contaminated water.<br>4. Sudan has experienced challenges related to public health crises.<br>5. Sudan has experienced challenges related to catastrophic flooding.<br>6. Sudan has experienced challenges related to contaminated water.</td></tr></tbody></table></figure>



<p>Note that the baseline prompt extracted several claims from the sentence “<em>The U.N. found that the resulting contaminated water caused many residents to fall ill, highlighting the need for improved water management</em>,” but it ignored the phrase <em>“highlighting the need for improved water management</em>.”&nbsp;It also failed to capture that the contaminated water resulted from flooding, as implied by “<em>resulting</em>” in the original sentence.</p>



<p>Claimify took a different approach. First, it found two instances of ambiguity – “<em>resulting contaminated water</em>” and “<em>many resident</em>s”&nbsp;– that it determined could be resolved using the context. Here’s an excerpt from its reasoning: “&#8230;<em>the context specifies that the contaminated water is a result of the catastrophic flooding in Derna, Libya, and the residents are those of Derna, Libya.</em>”</p>



<p>However, it also found an instance of ambiguity – “<em>highlighting the need for improved water management”</em> – where it concluded that the context does not definitively support a single interpretation: “<em>The sentence could be interpreted as: (1) The U.N. found that the contaminated water caused illness and also highlighted the need for improved water management, (2) The U.N. only found that the contaminated water caused illness, while the need for improved water management is an implication or conclusion drawn by the writer. Readers … would likely fail to reach consensus about the correct interpretation of this ambiguity.</em>” As a result, Claimify labeled the sentence “Cannot be disambiguated” at the Disambiguation stage and did not proceed to the Decomposition stage.&nbsp;</p>



<p>To the best of our knowledge, Claimify is the first claim extraction system that identifies when the source text has multiple possible interpretations and extracts claims only when there is high confidence in the correct interpretation.</p>



<h2 class="wp-block-heading" id="next-steps">Next steps</h2>



<p>We’re currently working on new methods for evaluating LLM-generated texts. We anticipate that the high-quality claims extracted by Claimify will help not only in verifying the veracity of LLM outputs, but also in assessing their overall quality – especially when gold-standard references are difficult to create (e.g., long-form texts where people may disagree on what defines “good” content). For example, <a href="https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/">we recently used Claimify</a> to evaluate the comprehensiveness and diversity of answers generated by <a href="https://www.microsoft.com/en-us/research/project/graphrag/">GraphRAG</a>, showing that GraphRAG outperforms traditional Retrieval Augmented Generation (RAG) in these areas.</p>



<p>For an in-depth discussion of Claimify and our evaluation framework, please see our paper “<a href="https://www.microsoft.com/en-us/research/publication/towards-effective-extraction-and-evaluation-of-factual-claims/">Towards Effective Extraction and Evaluation of Factual Claims</a>.”</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a id="_ftn1" href="#_ftnref1">[1]<span class="sr-only"> (opens in new tab)</span></a> We used the “proposition chunking” prompt from<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/NirDiamant/RAG_Techniques/blob/main/all_rag_techniques/proposition_chunking.ipynb" target="_blank" rel="noreferrer noopener"> NirDiamant&#8217;s RAG Techniques repository<span class="sr-only"> (opens in new tab)</span></a>. We generated multiple responses using GPT-4o, then picked the response that was most representative of the samples.</p>



<p><a id="_ftn2" href="#_ftnref2">[2]</a> Claimify is currently used for research purposes only and is not available commercially.</p>



<p><a id="_ftn3" href="#_ftnref3">[3]<span class="sr-only"> (opens in new tab)</span></a> We benchmarked Claimify against <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aclanthology.org/2024.findings-emnlp.552/" target="_blank" rel="noreferrer noopener">VeriScore<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2412.13175" target="_blank" rel="noreferrer noopener">DnD<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2403.18802" target="_blank" rel="noreferrer noopener">SAFE<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aclanthology.org/2024.acl-long.104/" target="_blank" rel="noreferrer noopener">AFaCTA<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aclanthology.org/2024.findings-emnlp.830/" target="_blank" rel="noreferrer noopener">Factcheck-GPT<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p><a href="#_ftnref4" id="_ftn4">[4]</a> The outputs were generated using GPT-4o. Sentences not shown were either labeled “No verifiable claims” or “Cannot be disambiguated.”</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/claimify-extracting-high-quality-claims-from-language-model-outputs/">Claimify: Extracting high-quality claims from language model outputs</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Introducing KBLaM: Bringing plug-and-play external knowledge to LLMs</title>
		<link>https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/</link>
		
		<dc:creator><![CDATA[Taketomo Isazawa, Xi Wang, Liana Mikaelyan, Mathew Salvaris, James Hensman]]></dc:creator>
		<pubDate>Tue, 18 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1133691</guid>

					<description><![CDATA[<p>Introducing KBLaM, an approach that encodes and stores structured knowledge within an LLM itself. By integrating knowledge without retraining, it offers a scalable alternative to traditional methods.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/">Introducing KBLaM: Bringing plug-and-play external knowledge to LLMs</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1.png" alt="KBLaM blog | A flowchart illustrating the process of handling a prompt using a language model. The process begins with documents being used to construct and summarize a knowledge base (KB) offline. The summarized KB is then encoded and fed into the main process. A prompt goes through a tokenizer, followed by rectangular attention, and then into the large language model (LLM). The LLM retrieves information from the encoded KB to generate an answer." class="wp-image-1134055" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-768x432.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-640x360.png 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-960x540.png 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-BlogHeroFeature-1400x788-1-1-1280x720.png 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Large language models (LLMs) have demonstrated remarkable capabilities in reasoning, language understanding, and even creative tasks. Yet, a key challenge persists: how to efficiently integrate external knowledge.</p>



<p>Traditional methods such as fine-tuning and Retrieval-Augmented Generation (RAG) come with trade-offs—fine-tuning demands costly retraining, while RAG introduces separate retrieval modules that increase complexity and prevent seamless, end-to-end training. In-context learning, on the other hand, becomes increasingly inefficient as knowledge bases grow, facing quadratic computational scaling that hinders its ability to handle large repositories. A comparison of these approaches can be seen in Figure 1.</p>



<h2 class="wp-block-heading" id="a-new-way-to-integrate-knowledge">A new way to integrate knowledge</h2>



<p>To address these challenges, we introduce the <a href="https://www.microsoft.com/en-us/research/publication/kblam-knowledge-base-augmented-language-model-2/" target="_blank" rel="noreferrer noopener"><strong>Knowledge Base-Augmented Language Model (KBLaM)</strong></a> —a novel approach that integrates structured knowledge bases into pre-trained LLMs. Instead of relying on external retrieval modules or costly fine-tuning, KBLaM encodes knowledge into <strong>continuous key-value vector pairs</strong>, efficiently embedding them within the model’s attention layers using a specialized <strong>rectangular attention mechanism</strong>, which implicitly performs retrieval in an integrated manner.</p>



<p>We use structured knowledge bases to represent the data, allowing us to <strong>consolidate knowledge </strong>and <strong>leverage structure</strong>. This design allows it to scale <strong>linearly</strong> with the size of the knowledge base while maintaining <strong>dynamic</strong> <strong>updates</strong> without retraining, making it far more efficient than existing methods.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1061244">
		

	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/about-microsoft-research/" aria-label="About Microsoft Research" data-bi-cN="About Microsoft Research" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/07/About-page-promo_1066x600.jpg" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">About Microsoft Research</h2>
				
								<p class="large">Advancing science and technology to benefit humanity</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/about-microsoft-research/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="View our story" data-bi-cN="About Microsoft Research" target="_blank">
							View our story						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="scalable-efficient-and-future-ready">Scalable, efficient, and future-ready</h2>



<p>At its core, KBLaM is designed to integrate structured knowledge into LLMs, making them more efficient and scalable. It achieves this by converting external knowledge bases—collections of facts structured as triples consisting of an entity, a property, and a value—into a format that LLMs can process naturally.&nbsp; Such knowledge bases allow for consolidated, reliable sources of knowledge.</p>



<p>To create these knowledge bases, we first extract structured data in JSON format using small language models. We then apply <a href="https://www.microsoft.com/en-us/research/project/alexandria/" target="_blank" rel="noreferrer noopener">Project Alexandria</a>’s probabilistic clustering. Once we have this structured knowledge base, KBLaM follows a three-step pipeline:</p>



<ol class="wp-block-list">
<li><strong>Knowledge Encoding:</strong> Each knowledge triple is mapped into a key-value vector pair using a <strong>pre-trained sentence encoder</strong> with <strong>lightweight linear adapters</strong>. The key vector, derived from the entity name and property, encodes “index information,” while the value vector captures the corresponding property value. This allows us to create <strong>continuous, learnable key-value representations</strong>.</li>



<li><strong>Integration with LLMs:</strong> These key-value pairs, or <strong><em>knowledge tokens</em></strong>, are augmented into the model’s attention layers using a specialized <strong>rectangular attention structure</strong>. Unlike traditional transformer models that process all tokens equally and come with quadratic cost—such as GPT-4, Phi, and Llama—rectangular attention enables the model to attend over knowledge with linear cost, as illustrated in Figure 2. Compared to standard attention mechanisms in generative language models, where each token attends to all preceding tokens, our approach introduces a more efficient structure. In this setup, language tokens (such as those from a user’s question) attend to all knowledge tokens. However, knowledge tokens do not attend to one another, nor do they attend back to the language tokens. This selective attention pattern significantly reduces computational cost while preserving the model’s ability to incorporate external knowledge effectively.<br><br>This linear cost, which is crucial for the efficiency of KBLaM, effectively amounts to treating each fact independently—an assumption that holds for most facts.&nbsp;For example, the model’s name, KBLaM, and the fact that the research was conducted at Microsoft Research are very weakly correlated. This rectangular attention is implemented as an extension of standard attention. During training, we keep the base model’s weights frozen, ensuring that when no knowledge tokens are provided, the model functions exactly as it did originally.</li>



<li><strong>Efficient Knowledge Retrieval:</strong> Through this rectangular attention, the model learns to dynamically retrieve relevant knowledge tokens during inference, eliminating the need for separate retrieval steps.</li>
</ol>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1531" height="832" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit.png" alt="Figure 1: A diagram comparing KBLaM and existing approaches. With RAG, we take the user’s prompt and use that to retrieve relevant documents from an external corpus using some retriever module, and append a tokenized version of those relevant documents in the context. This is relatively cheap, but requires many components. On the other hand, In Context Learning just puts the entire corpus into the context. This is simple, involving only one component, but is expensive. Our method, KBLaM, makes a structured knowledge base from the documents in an offline process, and includes the entire knowledge base to the context, while using a novel variant of attention, rectangular attention, so that the cost is linear in the size of the knowledge base. This results in a system where the retrieval only requires a single, trainable component, that is also cheap." class="wp-image-1133892" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit.png 1531w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit-300x163.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit-1024x556.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit-768x417.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-FigureEdit-240x130.png 240w" sizes="auto, (max-width: 1531px) 100vw, 1531px" /><figcaption class="wp-element-caption"><em>Figure 1:<em> KBLaM allows for attention over the entire knowledge base instead of having an external retriever.</em></em></figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="628" height="209" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM_rec-attn-viz-v2.png" alt="Figure 2: A diagram illustrating rectangular attention. Unlike regular attention, the attention matrix is not square, as we remove the parts where the knowledge base would attend over itself. This allows for KBLaM to scale linearly with the number of items in its context." class="wp-image-1133586" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM_rec-attn-viz-v2.png 628w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM_rec-attn-viz-v2-300x100.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM_rec-attn-viz-v2-240x80.png 240w" sizes="auto, (max-width: 628px) 100vw, 628px" /><figcaption class="wp-element-caption"><em>Figure 2: <em>By having the user’s question attend to the knowledge base, while treating facts in the knowledge base independently, KBLaM scales efficiently and linearly with the size of the knowledge base.</em></em></figcaption></figure>



<p>Unlike RAG, which appends retrieved document chunks to prompts, KBLaM allows for <strong>direct integration</strong> of knowledge into the model. Compared to in-context learning,&nbsp; KBLaM’s rectangular attention maintains a <strong>linear memory footprint</strong>, making it vastly more scalable for large knowledge bases.&nbsp;</p>



<p>Its efficiency is a game-changer. While traditional in-context learning methods struggle with quadratic memory growth due to self-attention overhead, KBLaM’s linear overhead means we can store much more knowledge in the context. In practice, this means KBLaM can store and process <strong>over 10,000 knowledge triples</strong>, the equivalent of approximately <strong>200,000 text tokens on a single GPU</strong>—a feat that would be computationally prohibitive with conventional in-context learning. The results across a wide range of triples and&nbsp;can be seen in Figure 3. Remarkably, it achieves this while extending a base model that has a context length of only <strong>8K tokens</strong>. Additionally, KBLaM enables <strong>dynamic updates: modifying </strong>a single knowledge triple does not require retraining or re-computation of the entire knowledge base.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="547" height="236" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-vs-RAG.png" alt="Figure 3: Two graphs, showing time to first token, and memory usage for both KBLaM and RAG. KBLaM’s time to first token remains relatively constant across a large range of knowledge base sizes, with the time-to-first-token with 4096 triples in the context being lower than that of conventional RAG with 5 triples in the context. The memory usage is also much lower, with KBLaM with 512 triples having a similar memory usage to RAG at 5 triples." class="wp-image-1133584" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-vs-RAG.png 547w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-vs-RAG-300x129.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/KBLaM-vs-RAG-240x104.png 240w" sizes="auto, (max-width: 547px) 100vw, 547px" /><figcaption class="wp-element-caption"><em>Figure 3: <em>KBLaM is much faster and uses much less memory than adding the equivalent number of triples in the context using conventional RAG-like approaches. In particular, we have lower time to first token with 4,096 tripes in the context with KBLaM than we would with 5 triples in the context.</em></em></figcaption></figure>



<h2 class="wp-block-heading" id="enhancing-interpretability-and-reliability">Enhancing interpretability and reliability</h2>



<p>Another major benefit of KBLaM is its <strong>interpretability</strong>. Unlike in-context learning, where knowledge injection is opaque, KBLAM’s <strong>attention weights</strong> provide clear insights into how the model utilizes knowledge tokens. Experiments show that KBLaM assigns high attention scores to relevant knowledge triples, effectively mimicking a soft retrieval process.</p>



<p>Furthermore, KBLaM enhances <strong>model reliability</strong> by learning through its training examples when not to answer a question if the necessary information is missing from the knowledge base. In particular, with knowledge bases larger than approximately 200 triples, we found that the model refuses to answer questions it has no knowledge about more precisely than a model given the information as text in context. This feature helps reduce <strong>hallucinations</strong>, a common problem in LLMs that rely on internal knowledge alone, making responses more accurate and trustworthy.</p>



<h2 class="wp-block-heading" id="the-future-of-knowledge-augmented-ai">The future of knowledge-augmented AI</h2>



<p>KBLaM represents a major step forward in integrating structured knowledge into LLMs. By offering a scalable, efficient, and interpretable alternative to existing techniques, it paves the way for AI systems that can stay up to date and provide reliable, knowledge-driven responses. In fields where accuracy and trust are critical—such as medicine, finance, and scientific research—this approach has the potential to transform how language models interact with real-world information.</p>



<p>As AI systems increasingly rely on dynamic knowledge rather than static model parameters, we hope KBLaM will serve as a bridge between raw computational power and real-world understanding.</p>



<p>However, there is still work to be done before it can be deployed at scale. Our current model has been trained primarily on factual question-answer pairs, and further research is needed to expand its capabilities across more complex reasoning tasks and diverse knowledge domains.</p>



<p>To accelerate progress, we are releasing <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/KBLaM/" target="_blank" rel="noreferrer noopener"><strong>KBLaM’s code and datasets</strong><span class="sr-only"> (opens in new tab)</span></a> to the research community, and we are planning integrations with the Hugging Face transformers library. By making these resources available, we hope to inspire further research and adoption of <strong>scalable, efficient knowledge augmentation</strong> for LLMs. The future of AI isn’t just about generating text—it’s about generating knowledge that is accurate, adaptable, and deeply integrated with the evolving world. KBLaM is a step in that direction.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/introducing-kblam-bringing-plug-and-play-external-knowledge-to-llms/">Introducing KBLaM: Bringing plug-and-play external knowledge to LLMs</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Semantic Telemetry: Understanding how users interact with AI systems</title>
		<link>https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/</link>
		
		<dc:creator><![CDATA[Amber Hoak, Scott Counts, Kate Lytvynets, David Tittsworth, Siddharth Suri, Ben Cutler, Weiwei Yang]]></dc:creator>
		<pubDate>Mon, 10 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1133598</guid>

					<description><![CDATA[<p>AI interactions can be iterative and complex. Learn how the Semantic Telemetry project at Microsoft Research is developing a new data science approach to understand human-AI interactions and their value.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/">Semantic Telemetry: Understanding how users interact with AI systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Semantic Telemetry blog | diagram showing relationships between chat, LLM prompt, and labeled data" class="wp-image-1133588" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<p>AI tools are proving useful across a range of applications, from helping to drive the new era of business transformation to helping artists craft songs. But which applications are providing the most value to users? We’ll dig into that question in a series of blog posts that introduce the <a href="https://www.microsoft.com/en-us/research/project/semantic-telemetry/">Semantic Telemetry</a> project at Microsoft Research. In this initial post, we will introduce a new data science approach that we will use to analyze topics and task complexity of Copilot in Bing usage.</p>



<p>Human-AI interactions can be iterative and complex, requiring a new data science approach to understand user behavior to build and support increasingly high value use cases. Imagine the following chat:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="408" height="232" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure0_example_chat.png" alt="Example chat between user and AI" class="wp-image-1133589" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure0_example_chat.png 408w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure0_example_chat-300x171.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure0_example_chat-240x136.png 240w" sizes="auto, (max-width: 408px) 100vw, 408px" /></figure>



<p>Here we see that chats can be complex and span multiple topics, such as event planning, team building, and logistics. Generative AI has ushered in a two-fold paradigm shift. First, LLMs give us a new thing to measure, that is, how people interact with AI systems. Second, they give us a new way to measure those interactions, that is, they give us the capability to understand and make inferences on these interactions, at scale. The Semantic Telemetry project has created new measures to classify human-AI interactions and understand user behavior, contributing to efforts in developing new approaches for <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/features/ai/measurement-is-the-key-to-helping-keep-ai-on-track/" target="_blank" rel="noreferrer noopener">measuring generative AI<span class="sr-only"> (opens in new tab)</span></a> across various use cases.</p>



<p>Semantic Telemetry is a rethink of traditional telemetry&#8211;in which data is collected for understanding systems&#8211;designed for analyzing chat-based AI. We employ an innovative data science methodology that uses a large language model (LLM) to generate meaningful categorical labels, enabling us to gain insights into chat log data.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="650" height="280" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure1_llm_clasifier.png" alt="Flow chart illustrating the LLM classification process starting with chat input, then prompting LLM with chat using generated label taxonomy, and output is the labeled chat." class="wp-image-1133590" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure1_llm_clasifier.png 650w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure1_llm_clasifier-300x129.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure1_llm_clasifier-240x103.png 240w" sizes="auto, (max-width: 650px) 100vw, 650px" /><figcaption class="wp-element-caption">Figure 1: Prompting an LLM to classify a conversation based on LLM generated label taxonomy</figcaption></figure>



<p>This process begins with developing a set of classifications and definitions. We create these classifications by instructing an LLM to generate a short summary of the conversation, and then iteratively prompting the LLM to generate, update, and review classification labels on a batched set of summaries. This process is outlined in the paper: <a href="https://www.microsoft.com/en-us/research/publication/tnt-llm-text-mining-at-scale-with-large-language-models/">TnT-LLM: Text Mining at Scale with Large Language Models</a>. We then prompt an LLM with these generated classifiers to label new unstructured (and unlabeled) chat log data.</p>



<figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Semantic Telemetry - taxonomy generation" width="500" height="281" src="https://www.youtube-nocookie.com/embed/9O2UaMCtj5c?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Description of LLM generated label taxonomy process</figcaption></figure>



<p>With this approach, we have analyzed how people interact with Copilot in Bing. In this blog, we examine insights into how people are using Copilot in Bing, including how that differs from traditional search engines. Note that all analyses were conducted on anonymous Copilot interactions containing no personal information.</p>



<h2 class="wp-block-heading" id="topics">Topics</h2>



<p>To get a clear picture of how people are using Copilot in Bing, we need to first classify sessions into topical categories. To do this, we developed a topic <strong>classifier</strong>. We used the LLM classification approach described above to label the primary topic (domain) for the entire content of the chat. Although a single chat can cover multiple topics, for this analysis, we generated a single label for the primary topic of the conversation. We sampled five million anonymized Copilot in Bing chats during August and September 2024, and found that globally, 21% of all chats were about <em>technology</em>, with a high concentration of these chats in <em>programming and scripting and computers and electronics</em>.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="564" height="467" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure2_topic_bubble_chart.png" alt="Bubble chart showing topics based on percentage of sample. Primary topics shown are Technology (21%), Entertainment (12.8%), Health (11%), Language, Writing, & Editing (11.6%), Lifestyle (9.2%), Money (8.5%), History, Events, & Law (8.5%), Career (7.8%), Science (6.3%)" class="wp-image-1133591" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure2_topic_bubble_chart.png 564w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure2_topic_bubble_chart-300x248.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure2_topic_bubble_chart-217x180.png 217w" sizes="auto, (max-width: 564px) 100vw, 564px" /><figcaption class="wp-element-caption">Figure 2: Top Copilot in Bing topics based on anonymized data (August-September 2024)</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="346" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure3_technology_topic_bubble_chart.png" alt="Bubble chart of Technology topic showing subtopics: Programming & scripting, Computers & electronics, Engineering & design, Data analysis, and ML & AI." class="wp-image-1133592" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure3_technology_topic_bubble_chart.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure3_technology_topic_bubble_chart-300x173.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure3_technology_topic_bubble_chart-240x138.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 3: Frequent topic summaries in Technology</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="430" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure4_entertainment_topic_bubble_chart.png" alt="Bubble chart of Entertainment showing subtopics: Entertainment, Sports & fitness, Travel & tourism, Small talk & chatbot, and Gaming" class="wp-image-1133593" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure4_entertainment_topic_bubble_chart.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure4_entertainment_topic_bubble_chart-300x215.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure4_entertainment_topic_bubble_chart-240x172.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 4: Frequent topic summaries in Entertainment</figcaption></figure>



<p>Diving into the technology category, we find a lot of professional tasks in <em>programming and scripting</em>, where users request problem-specific assistance such as fixing a SQL query syntax error. In <em>computers and electronics</em>, we observe users getting help with tasks like adjusting screen brightness and troubleshooting internet connectivity issues. We can compare this with our second most common topic, <em>entertainment</em>, in which we see users seeking information related to personal activities like hiking and game nights.</p>



<p>We also note that top topics differ by platform. The figure below depicts topic popularity based on mobile and desktop usage. Mobile device users tend to use the chat for more personal-related tasks such as helping to plant a garden or understanding medical symptoms whereas desktop users conduct more professional tasks like revising an email.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="501" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure5_sankey_platforms.png" alt="Sankey visual showing top topics for Desktop and Mobile users" class="wp-image-1133594" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure5_sankey_platforms.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure5_sankey_platforms-300x251.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure5_sankey_platforms-216x180.png 216w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 5: Top topics for desktop users and mobile users</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1002645">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: AI-POWERED EXPERIENCE</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/01/MSR-Chat-Promo.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft research copilot experience</h2>
				
								<p class="large">Discover more about research at Microsoft through our AI-powered experience</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://aka.ms/research-copilot/?OCID=msr_researchforum_Copilot_MCR_Blog_Promo" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft research copilot experience" data-bi-cN="Microsoft research copilot experience" target="_blank">
							Start now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="search-versus-copilot">Search versus Copilot</h2>



<p>Beyond analyzing topics, we compared Copilot in Bing usage to that of traditional search. Chat extends beyond traditional online search by enabling users to summarize, generate, compare, and analyze information. Human-AI interactions are conversational and more complex than traditional search (Figure 6).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="3535" height="2069" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2.png" alt="Venn diagram showing differences between Bing Search and Copilot in Bing, with intersection in information lookup." class="wp-image-1133922" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2.png 3535w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-300x176.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-1024x599.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-768x450.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-1536x899.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-2048x1199.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-480x280.png 480w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure6_venn_diagram_bing_vs_copilot_v2-240x140.png 240w" sizes="auto, (max-width: 3535px) 100vw, 3535px" /><figcaption class="wp-element-caption">Figure 6: Bing Search Query compared to Copilot in Bing Conversation</figcaption></figure>



<p>A major differentiation between search and chat is the ability to ask more complex questions, but how can we measure this? We think of complexity as a scale ranging from simply asking chat to look up information to evaluating several ideas. We aim to understand the difficulty of a task if performed by a human without the assistance of AI. To achieve this, we developed the <strong>task complexity classifier</strong>, which assesses task difficulty using <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.quincycollege.edu/wp-content/uploads/Anderson-and-Krathwohl_Revised-Blooms-Taxonomy.pdf" target="_blank" rel="noreferrer noopener">Anderson and Krathwohl’s Taxonomy of Learning Objectives<span class="sr-only"> (opens in new tab)</span></a>. For our analysis, we have grouped the learning objectives into two categories: <em>low complexity</em> and <em>high complexity</em>. Any task more complicated than information lookup is classified as <em>high complexity</em>. Note that this would be very challenging to classify using traditional data science techniques.</p>



<figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Semantic Telemetry - task complexity classifier" width="500" height="281" src="https://www.youtube-nocookie.com/embed/T-Rt46aozu4?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Description of task complexity and 6 categories of the Anderson and Krathwohl&#8217;s Taxonomy of Learning Objectives</figcaption></figure>



<p>Comparing <em>low</em> versus <em>high complexity</em> tasks, most chat interactions were categorized as <em>high complexity</em> (78.9%), meaning that they were more complex than looking up information. <em>Programming and scripting, marketing and sales, and creative and professional writing</em> are topics in which users engage in higher complexity tasks (Figure 7) such as learning a skill, troubleshooting a problem, or writing an article.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="356" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure7_high_complexity_tasks.png" alt="Highest and lowest complexity topics based on percent of high complexity chats" class="wp-image-1133596" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure7_high_complexity_tasks.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure7_high_complexity_tasks-300x178.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/Semantic-Telemetry_figure7_high_complexity_tasks-240x142.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 7: Most and least complex topics based on percentage of high complexity tasks.</figcaption></figure>



<p><em>Travel and tourism and history and culture </em>scored lowest in complexity, with users looking up information like flight times and latest news updates.</p>



<figure class="wp-block-embed aligncenter is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Semantic Telemetry - Task Complexity Dashboard" width="500" height="281" src="https://www.youtube-nocookie.com/embed/7ucGpDLbv-U?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div><figcaption class="wp-element-caption">Demo of task complexity and topics on anonymous Copilot interactions</figcaption></figure>



<p>When should you use chat instead of search? A 2024 Microsoft Research study: <a href="https://www.microsoft.com/en-us/research/publication/the-use-of-generative-search-engines-for-knowledge-work-and-complex-tasks/">The Use of Generative Search Engines for Knowledge Work and Complex Tasks</a>, suggests that people are seeing value in technical, complex tasks such as web development and data analysis. Bing Search contained more queries with lower complexity focused on non-professional areas, like <em>gaming and entertainment</em>, <em>travel and tourism</em>, and <em>fashion and beauty</em>, while chat had a greater distribution of complex technical tasks. (Figure 8).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2622" height="1628" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2.png" alt="Comparison of Bing Search and Copilot in Bing topics based on complexity and knowledge work. Copilot in Bing trends greater complexity and greater knowledge work than Bing Search." class="wp-image-1133909" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2.png 2622w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-300x186.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-1024x636.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-768x477.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-1536x954.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-2048x1272.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure8_bing_vs_copilot_v2-240x149.png 240w" sizes="auto, (max-width: 2622px) 100vw, 2622px" /><figcaption class="wp-element-caption">Figure 8: Comparison of Bing Search and Copilot in Bing for anonymized sample data (May-June 2023)</figcaption></figure>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>LLMs have enabled a new era of high-quality human-AI interaction, and with it, the capability to analyze those same interactions with high fidelity, at scale, and in near real-time. We are now able to obtain actionable insight from complex data that is not possible with traditional data science pattern-matching methods. LLM-generated classifications are pushing research into new directions that will ultimately improve user experience and satisfaction when using chat and other user-AI interaction tools.</p>



<p>This analysis indicates that Copilot in Bing is enabling users to do more complex work, specifically in areas such as technology. In our next post, we will explore how Copilot in Bing is supporting professional knowledge work and how we can use these measures as indicators for retention and engagement.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p>FOOTNOTE: This research was conducted at the time the feature Copilot in Bing was available as part of the Bing service; since October 2024 Copilot in Bing has been deprecated in favor of the standalone Microsoft Copilot service.</p>



<p><em>References:</em></p>



<ol class="wp-block-list">
<li>Krathwohl, D. R. (2002). A Revision of Bloom’s Taxonomy: An Overview. <em>Theory Into Practice</em>, 41(4), 212–218. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://doi.org/10.1207/s15430421tip4104_2" target="_blank" rel="noreferrer noopener">https://doi.org/10.1207/s15430421tip4104_2<span class="sr-only"> (opens in new tab)</span></a></li>
</ol>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/">Semantic Telemetry: Understanding how users interact with AI systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The AI Revolution in Medicine, Revisited: An Introduction</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-an-introduction/</link>
		
		<dc:creator><![CDATA[Peter Lee]]></dc:creator>
		<pubDate>Thu, 06 Mar 2025 14:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1132722</guid>

					<description><![CDATA[<p>Host Peter Lee, Microsoft Research president, discusses the motivation behind the new series and the GPT-4 encounter that helped him view the tech not only as a potential tool for improving healthcare but a chance to reexamine what it means to care for people. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-an-introduction/">The AI Revolution in Medicine, Revisited: An Introduction</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="The AI Revolution in Medicine, Revisited: An Introduction" width="500" height="281" src="https://www.youtube-nocookie.com/embed/3qOWQpnZDmI?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=142968395&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.</p>



<p>In this introduction to the series, Lee talks about his early encounters with GPT-4, when the AI model was still in secret development with OpenAI, and the range of emotions he cycled through as he came to understand the new technology better. The emergence of generative AI has created a “new world,” Lee says, one he is eager to investigate with the aim of discovering the technology’s impact so far and what it means for the future of healthcare and medicine.</p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/">The AI Revolution in Medicine: GPT-4 and Beyond</a><br>Book | April 2023</li>
</ul>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>PETER LEE:</strong> This is <em>The AI Revolution in Medicine, Revisited</em>. I’m Peter Lee, president of Microsoft Research, and I&#8217;m pretty excited to introduce this series of conversations as part of the Microsoft Research Podcast.</p>



<p>About two years ago, with Carey Goldberg and Zak Kohane, we wrote a book, <em>The</em> <em>AI Revolution in Medicine</em>. This was a book that was intended to educate the world of healthcare and the world of medical research about this new thing that was emerging. This idea of generative AI. And we wrote the book in secret. In fact, the whole existence of what we now know of as OpenAI&#8217;s GPT-4 AI model hadn&#8217;t been publicly disclosed or revealed to the world. And so when we were working on this book, we had to make some guesses. What is this going to mean for healthcare? If you&#8217;re a doctor or a nurse, in what ways will AI impact your work? If you&#8217;re a patient, in what ways could AI change your experience as you try to navigate a complex healthcare system?</p>



<p>And so now it&#8217;s been about two years. Two years hence, what did we get right? What did we get wrong? What things have come along much faster than we ever would have dreamed of? What did we miss? And what things have turned out to be much harder than we ever could have realized? And so this series of conversations is going to talk to people <em>in</em> the real world. We&#8217;ll delve into exactly what&#8217;s happening in the clinic, the patient experience, how people are thinking about safety and regulatory matters, and what this all means for discovery and advancements of medical science. And even then, we&#8217;ll have guests that will allow us to look into the future—the AI advances that are happening now and what is going to happen next.</p>



				</span>
				<span id="show-more-show-less-toggle-12" class="show-more-show-less-toggleable-content">
					



<p>[MUSIC TRANSITIONS TO SERIES THEME]</p>



<p>[MUSIC FADES]</p>



<p>So now, let me just take a step back here to talk about this book project. And I&#8217;d like to just read the first couple of sentences in Chapter 1, and Chapter 1 is entitled “First Contact.” And it starts with a quote. Quote, “I think that Zak and his mother deserve better than that,” unquote. “I was being scolded. And while I&#8217;ve been scolded plenty in my life, for the first time it wasn&#8217;t a person scolding me; it was an artificial intelligence system.” So that&#8217;s how we started this book, and I wanted to read that because, at least for me, it takes me back to the kind of awe and wonderment in those early days when in secret development, we had access from OpenAI to what we now know of as GPT-4.</p>



<p>And what was that quote about? Well, after getting access to GPT-4, I became very interested in what this might mean for healthcare. But I, not being a doctor, knew I needed help. So I had reached out to a good colleague of mine who is a doctor, a pediatric endocrinologist, and head of the bioinformatics department at Harvard Medical School, Dr. Isaac “Zak” Kohane. And I sought his help. And in our back-and-forth discussions, one of the things that Zak shared with me was an article that he wrote for a magazine where he talked about his use of machine learning in the care of his 90-year-old mother, his 90-year-old mother, who—like many 90-year-old people—was having some health issues.</p>



<p>And this article was very interesting. It really went into some detail about not only the machine learning technology that Zak had created in order to help manage his mother&#8217;s health but also the kind of emotional burden of doing this and in what ways technology was helping Zak cope with that. And so as I read that article, it touched me because at that time, I was struggling in a very similar way with my own father, who was at that time 89 years old and was also suffering from some very significant health issues. And, like Zak, I was feeling some pangs of guilt because my father was living in Southern California; I was way up in the Pacific Northwest, you know, just feeling guilty not being there, present for him, through his struggles. And reading that article a thought that occurred to me was, I wonder if in the future, AI could pretend to be me so that my father could always have a version of me to talk to. And I also had the thought in the other direction. Could AI someday capture enough of my father so that when and if he passes, I always have some memory of my father that I could interact with? A strange and bizarre thought, I admit, but a natural one, I think, for any human being that&#8217;s encountering this amazing AI technology for the first time. And so I ran an experiment. I used GPT-4 to read Zak&#8217;s article and then posed the question to GPT-4, “Based on this article, could you pretend to be Zak? I&#8217;ll pretend to be Zak&#8217;s mother, and let&#8217;s test whether it&#8217;s possible to have a mother-son conversation.”</p>



<p>To my surprise, GPT-4’s response at that time was to scold me, basically saying that this is wrong; that this has a lot of dangers and risks. You know, what if Zak&#8217;s mother really needs the real Zak. And in those early days of this encounter with AI, that was incredibly startling. It just really forces you to reexamine yourself, and it kicked off our writing in the book as really not only being about a technology that could help lead to better diagnoses, help reduce medical errors, reduce the amount of paperwork and clerical burden that doctors go through, could help demystify and help patients navigate a healthcare system, but it could actually be a technology that forces people to reexamine their relationships and reexamine what it really means for people to take care of other people.</p>



<p>And since then, of course, I&#8217;ve come to learn that many people have had similar experiences in their first encounters with AI. And in fact, I&#8217;ve come to think of this as, somewhat tongue in cheek, the <em>nine stages of AI grief</em>. And they actually relate to what we&#8217;ll try to address in this new series of conversations.</p>



<p>For me, the first time that Greg Brockman and Sam Altman presented what we now know of as OpenAI&#8217;s GPT-4 to me, they made some claims about what it could do. And my first reaction was one of skepticism, and it seemed that the claims that were being made just couldn&#8217;t be true. Then that, kind of, passed into, I would say, a period of annoyance because I started to see my colleagues here in Microsoft Research start to show some amazement about the technology. I actually was annoyed because I felt they were being <em>duped</em> by this technology. So that&#8217;s the second phase. And then, the third phase was concern and maybe even a little bit of frustration because it became clear that, as a company here at Microsoft, we were on the verge of making a big bet on this new technology. And that was concerning to me because of my fundamental skepticism. But then I got my hands on the technology myself. And that enters into a fourth stage, of amazement. You start to encounter things that just are fundamentally amazing. This leads to a period of intensity because I immediately surmised that, wow, this could really change everything and in very few areas other than healthcare would be more important areas of change. And that is stage five, a period of serious intensity where you&#8217;re just losing sleep and working so hard to try to imagine what this all could mean. Running as many experiments as you can; trying to lean on as much real expertise as possible. You then lead from there into a period of what I call <em>chagrin</em> because as amazing as the technology is, actually understanding how to harness it in real life is not easy. You finally get into this stage of what I would call <em>enlightenment</em>. [MUSIC] And I won&#8217;t claim to be enlightened. But it is, sort of, a combination of acceptance that we are in a new world today, that things are happening for real, and that there&#8217;s, sort of, no turning back. And at that point, I think we can really get down to work. And so as we think about really the ultimate purpose of this series of conversations that we&#8217;re about to have, it&#8217;s really to help people get to that stage of enlightenment, to really, kind of, roll up our sleeves, to sit down and think through all of the best knowledge and experience that we&#8217;ve gathered over the last two years, and chart the future of this AI revolution in medicine.</p>



<p>[MUSIC TRANSITIONS TO SERIES THEME]</p>



<p>Let&#8217;s get going. <s></s></p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-12"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-an-introduction/">The AI Revolution in Medicine, Revisited: An Introduction</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
