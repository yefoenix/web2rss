<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Wed, 05 Nov 2025 16:31:51 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.8.3</generator>
	<item>
		<title>When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost</title>
		<link>https://www.microsoft.com/en-us/research/blog/when-industry-knowledge-meets-pike-rag-the-innovation-behind-signifys-customer-service-boost/</link>
		
		<dc:creator><![CDATA[Industry  Innovation Center]]></dc:creator>
		<pubDate>Thu, 06 Nov 2025 13:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1154225</guid>

					<description><![CDATA[<p>A collaboration between Signify and Microsoft Research shows how PIKE-RAG improves enterprise knowledge systems, delivering a 12% increase in accuracy and faster, more reliable answers.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-industry-knowledge-meets-pike-rag-the-innovation-behind-signifys-customer-service-boost/">When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a blue-to-purple gradient background: the first icon shows a node cluster, the second shows a person in front of a screen with another person, the third is a magnifying glass" class="wp-image-1154222" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>As a world leader in connected LED lighting products, systems, and services, Signify (formerly Philips Lighting) serves not only everyday consumers but also a large number of professional users who have stringent requirements for technical specifications and engineering compatibility. Faced with thousands of product models, complex component parameters, and technical documentation spanning multiple versions, delivering accurate, professional answers efficiently has become a core challenge for Signify’s knowledge management system.</p>



<p>To address this challenge, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.signify.com" target="_blank" rel="noopener noreferrer">Signify<span class="sr-only"> (opens in new tab)</span></a> collaborated with <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia</a> on a proof-of-concept (PoC) using <a href="https://www.microsoft.com/en-us/research/publication/pike-rag-specialized-knowledge-and-rationale-augmented-generation/">PIKE-RAG technology</a>, integrating it into their upgraded knowledge management system built on Microsoft Azure. The result: a 12% improvement in answer accuracy.</p>



<h2 class="wp-block-heading" id="challenges-of-applying-rag-in-lighting">Challenges of applying RAG in lighting</h2>



<p>In an era where AI is rapidly transforming how enterprises manage information, Signify recognized the strategic importance of precise and efficient knowledge systems. It adopted large AI models and retrieval-augmented generation (RAG) techniques to better support its wide range of customer inquiries.</p>



<p>Yet applying RAG to lighting scenarios involving professional users presented unique challenges. Product data spanned multimodal documents, unstructured tables, and complex product parameters, demanding continuous customization that slowed development and limited scalability. Despite improvements through keyword tuning, system optimization, and refined prompts, Signify sought more advanced approaches to further raise accuracy and reliability.</p>



<p>Seeking to unlock greater value from its knowledge management system, Signify began exploring more suitable technical solutions that are better aligned with their professional use cases. Upon learning that PIKE-RAG had been successfully applied in domains like healthcare and law, significantly improving information accuracy, Signify worked with Microsoft Research Asia on a PoC of PIKE-RAG on Microsoft Azure.</p>



<h2 class="wp-block-heading" id="how-pike-rag-addressed-signify-s-pain-points">How PIKE-RAG addressed Signify’s pain points</h2>



<p>Compared to traditional RAG, PIKE-RAG efficiently retrieves textual information and also understands multimodal content like charts and tables. Its built-in domain adaptation module quickly learns reasoning patterns aligned with specific domains to generate responses that are consistent with engineering contexts. These differentiated advantages stem from PIKE-RAG’s unique approach to understanding and processing professional knowledge. In Signify’s use case, this manifests in three key areas:</p>



<h3 class="wp-block-heading" id="multimodal-document-parsing-and-learning-of-industry-specific-reasoning-patterns">Multimodal document parsing and learning of industry-specific reasoning patterns</h3>



<p>Signify’s product documentation includes diverse formats, such as nonstandard tables (e.g., comparison charts of voltage ranges under different currents) and circuit diagrams (e.g., driver power limits). Traditional systems often fail to process this information effectively—either ignoring it or extracting disorganized text fragments.</p>



<p>PIKE-RAG integrates Microsoft Research Asia’s Document Intelligence technology with Microsoft Azure OpenAI models to accurately identify table structures and parse key parameters in circuit diagrams. For example, when a customer service agent queries, “What is the output voltage of a specific driver model at 0.15A current,” the system automatically locates the curve chart in the document and infers a range of 40–54V based on the current interval—an area where traditional systems frequently err, due to their inability to “read” diagrams.</p>



<h3 class="wp-block-heading" id="end-to-end-knowledge-loop-eliminating-reliance-on-erroneous-data-sources">End-to-end knowledge loop, eliminating reliance on erroneous data sources</h3>



<p>Enterprise knowledge systems often integrate data from multiple sources, which can lead to discrepancies, especially when database updates are not fully synchronized. PIKE-RAG captures diverse information sources and establishes citation relationships, supporting complex reasoning tasks that rely on multi-source data.</p>



<p>In other words, PIKE-RAG can directly use original documents as data sources, efficiently parsing and understanding product manuals and PDF charts. By extracting key information from these text- and graphic-rich documents, PIKE-RAG enables more efficient and trustworthy knowledge retrieval.</p>



<h3 class="wp-block-heading" id="dynamic-task-decomposition-and-multi-hop-reasoning-for-precise-answers-to-complex-questions">Dynamic task decomposition and multi-hop reasoning for precise answers to complex questions</h3>



<p>Traditional RAG systems typically follow a “one question, one answer” model and struggle with multi-step reasoning. In Signify’s lighting domain, customer inquiries often involve multi-level associations. PIKE-RAG dynamically decomposes user questions into executable subtasks and solves them through multi-hop reasoning. For example, when asked, “List all bases compatible with the G8 series lamps,” if no document directly provides the answer, PIKE-RAG’s reasoning proceeds as follows:</p>



<p>Step 1: The system identifies implicit knowledge. One document notes that the G7 and G8 series have identical dimensions and that all bases compatible with the G7 series are also compatible with the G8 series.&nbsp;</p>



<p>Step 2: Based on this, the system retrieves the base list for the G7 series.&nbsp;</p>



<p>Step 3: Since the list uses abbreviations, the system searches for a table that maps abbreviations to full names and generates a complete list of G8-compatible bases.&nbsp;</p>



<p>Through this automated multi-hop reasoning, the system delivers accurate and complete answers.</p>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="865" height="451" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-1-new.png" alt="Figure 1: A flowchart illustrating the PIKE-RAG framework for orchestrating and integrating heterogeneous information across multi-source and multimodal environments. At the center is a language model (LM) connected to PIKE-RAG, which performs iterative retrieval by tool calling. The process starts with a task (e.g., “What wireless drivers are available?”), followed by iterative task decomposition and retrieval from a tools repository. The tools repository includes similarity and keyword retrieval, Text2SQL, decomposers, VLMs, verifiers, and atomizers. Below, domain knowledge is shown in various forms: textual (terminology, specifications), multi-modal (figures, tables), structural (databases, knowledge graphs), and others (search engine, internal FAQ). The LM generates responses and updates memory while fetching context as needed." class="wp-image-1154223" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-1-new.png 865w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-1-new-300x156.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-1-new-768x400.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-1-new-240x125.png 240w" sizes="(max-width: 865px) 100vw, 865px" /><figcaption class="wp-element-caption">Figure 1: PIKE-RAG orchestrates and integrates heterogeneous information in multi-source and multimodal environments. </figcaption></figure>



<p>Testing showed that the PIKE-RAG-powered knowledge management platform provided a significant advantage. It achieved a 12% improvement in performance compared with the original system.</p>



<p>These results were achieved without any question-specific customization, only algorithmic optimization, demonstrating precise knowledge matching and generation. As the system continues to learn and integrate Signify’s proprietary knowledge, accuracy is expected to improve further.</p>



<p>“In the PoC for our product specification insight tool, PIKE-RAG helped us significantly improve the original system’s performance. This will enhance overall customer satisfaction. We’re currently evaluating PIKE-RAG’s application path from multiple angles, including technical implementation, cost control, and future adaptability, and we look forward to deepening our collaboration with Microsoft Research Asia to drive further innovation,” said Haitao Liu, head of Signify Research China.</p>



<p>“It’s also worth noting that the researchers at Microsoft Research Asia demonstrated strong industry knowledge and rigorous scientific methodology. They proactively studied and analyzed the issues, tracing and clarifying the root causes of our issues to make PIKE-RAG better suited to Signify’s real-world needs.”</p>



<h2 class="wp-block-heading" id="beyond-lighting-generalization-across-industries">Beyond lighting: Generalization across industries</h2>



<p>In Signify’s successful test, PIKE-RAG demonstrated strong generalization capabilities in complex industrial scenarios, enabling rapid cross-domain adaptation. Its three core strengths are:</p>



<ul class="wp-block-list">
<li><strong>Support for self-evolution and continuous learning</strong>: PIKE-RAG continuously analyzes error cases in interaction logs and uses evolutionary algorithms to automatically optimize knowledge extraction strategies, such as trying different table parsing methods or adjusting multimodal content weights. Validated strategies are then solidified for future Q&A, allowing the system to adapt to new knowledge types without manual intervention.&nbsp;</li>



<li><strong>Modular architecture driven by capability needs</strong>: PIKE-RAG flexibly combines modules for document parsing, knowledge extraction, storage, retrieval, organization, knowledge-centered reasoning, and task decomposition. It dynamically adjusts focus areas based on scenario needs (e.g., fact retrieval, multi-hop reasoning, innovative generation) and flexibly builds RAG methods that adapt to real-world applications, efficiently handling various complex tasks.&nbsp;</li>



<li><strong>Strong adaptation to domain-specific reasoning patterns</strong>: With dynamic updates through the Domain Tips feature, enterprises can add domain-specific logic (e.g., “the maximum output voltage of an LED driver should be the maximum of the operating range, not the spec sheet’s max output”) in real time, enabling the system to process information according to professional engineering standards and follow industry conventions.&nbsp;</li>
</ul>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="865" height="563" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-2.png" alt="Figure 2: Diagram showing the PIKE-RAG framework overview. At the center is a language model (LM) connected to PIKE-RAG, which performs iterative retrieval by tool calling. The process begins with a task input, decomposes it into sub-tasks, retrieves information from a tools repository, and integrates domain knowledge from multiple modalities such as textual documents, diagrams, tables, relational databases, and knowledge graphs. The LM generates responses and updates memory while orchestrating heterogeneous information sources." class="wp-image-1154224" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-2.png 865w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-2-300x195.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-2-768x500.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/PIKERAG_figure-2-240x156.png 240w" sizes="(max-width: 865px) 100vw, 865px" /><figcaption class="wp-element-caption">Figure 2: Overview of the PIKE-RAG framework</figcaption></figure>



<p>PIKE-RAG’s generalization capabilities have been validated not only in Signify’s knowledge management platform but also in pilot applications across industries like manufacturing, mining, and pharmaceuticals—significantly improving Q&A system accuracy.</p>



<p>“A leader in lighting, Signify presents a complex industrial knowledge system with a highly challenging real-world scenario for PIKE-RAG. Through this collaboration, we validated that PIKE-RAG’s general approach can greatly improve the accuracy of professional knowledge Q&A and accelerate scenario customization. Our researchers also gained valuable experience in handling domain-specific data,” explained <a href="https://www.microsoft.com/en-us/research/people/jiabia/">Jiang Bian</a>, partner research manager at Microsoft Research Asia.</p>



<p>“Our goal isn’t to build a universal chatbot but to create a professional assistant that aligns with domain-specific logic and performs rigorous knowledge reasoning. That’s the true driving force behind intelligent transformation in industrial knowledge management.”</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-industry-knowledge-meets-pike-rag-the-innovation-behind-signifys-customer-service-boost/">When industry knowledge meets PIKE-RAG: The innovation behind Signify’s customer service boost</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Magentic Marketplace: an open-source simulation environment for studying agentic markets</title>
		<link>https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/</link>
		
		<dc:creator><![CDATA[Gagan Bansal, Wenyue Hua, Zachary Huang, Adam Fourney, Amanda Swearngin, Chinmay Singh, Brendan Lucier, Jake Hofman, Markus Mobius, Will Epperson, Tyler Payne, Akshay Nambi, Archana Yadav, Maya Murad, Matthew Vogel, Alex Slivkins, Dan Goldstein, David Rothschild, Hussein Mozannar, Nicole Immorlica, Eric Horvitz, Saleema Amershi]]></dc:creator>
		<pubDate>Wed, 05 Nov 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false"></guid>

					<description><![CDATA[<p>AI agents are poised to transform digital marketplaces. To explore what can happen when AI agents interact and transact at scale, we built Magentic Marketplace, an open-source simulation environment for studying agentic market designs.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/">Magentic Marketplace: an open-source simulation environment for studying agentic markets</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a blue-to-purple gradient background: the first icon shows a node cluster, the second shows two persons, the third is a building, and the fourth is a location pin" class="wp-image-1154335" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticMarketplace-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Autonomous AI agents are here, and they&#8217;re poised to reshape the economy. By automating discovery, negotiation, and transactions, agents can overcome inefficiencies like information asymmetries and platform lock-in, enabling faster, more transparent, and more competitive markets.</p>



<p>We are already seeing early signs of this transformation in digital marketplaces. Customer-facing assistants like OpenAI’s Operator and Anthropic’s Computer Use can navigate websites and complete purchases. On the business side, Shopify Sidekick, Salesforce Einstein, and Meta’s Business AI help merchants with operations and customer engagement. These examples hint at a future where agents become active market participants, but the structure of these markets remains uncertain.</p>



<p>Several scenarios are possible. We might see one-sided markets where only customers or businesses deploy agents; closed platforms (known as <em>walled gardens</em>) where companies tightly control agent interactions; or even open two-sided marketplaces where customer and business agents transact freely across ecosystems. Each path carries different trade-offs for security, openness, convenience, and competition, which will shape how value flows in the digital economy. For a deeper exploration of these dynamics, see our paper, <a href="https://www.microsoft.com/en-us/research/publication/the-agentic-economy/">The Agentic Economy</a>.</p>



<p>To help navigate this uncertainty, we built <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/multi-agent-marketplace" target="_blank" rel="noopener noreferrer">Magentic Marketplace<span class="sr-only"> (opens in new tab)</span></a>— an open-source simulation environment for exploring the numerous possibilities of agentic markets and their societal implications at scale. It provides a foundation for studying these markets and guiding them toward outcomes that benefit everyone.</p>



<p>This matters because most AI agent research focuses on isolated scenarios—a single agent completing a task or two agents negotiating a simple transaction. But real markets involve a large number of agents simultaneously searching, communicating, and transacting, creating complex dynamics that can’t be understood by studying agents in isolation. Capturing this complexity is essential because real-world deployments raise critical questions about consumer welfare, market efficiency, fairness, manipulation resistance, and bias—questions that can’t be safely answered in production environments.</p>



<p>To explore these dynamics in depth, the Magentic Marketplace platform enables controlled experimentation across diverse agentic marketplace scenarios. Its current focus is on two-sided markets, but the environment is modular and extensible, supporting future exploration of mixed human–agent systems, one-sided markets, and complex communication protocols.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="936" height="393" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1.png" alt="Figure 1. Diagram illustrating the Magentic Marketplace Environment. On the left, two sections represent Customers and Businesses. Customers ask, “Could you find me a restaurant serving agua fresca and empanadas with free parking?” and are linked to Customer Agents (blue and purple icons). Businesses display a menu with items like steak tacos and empanadas, connected to Business Agents (purple icons). On the right, a three-step process is shown inside a pink box: Search – Customer agent searches for a restaurant among multiple business agents. Multi-Agent Communication – Customer agent asks about free parking and menu options, interacting with several business agents. Final Transaction – Customer agent places the order with a selected business agent." class="wp-image-1154337" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1-300x126.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1-768x322.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure1-240x101.png 240w" sizes="auto, (max-width: 936px) 100vw, 936px" /><figcaption class="wp-element-caption">Figure 1. With Magentic Marketplace, researchers can model how agents representing customers and businesses interact—shedding light on the dynamics that could shape future digital markets.</figcaption></figure>



<h2 class="wp-block-heading" id="what-is-magentic-marketplace">What is Magentic Marketplace?</h2>



<p>Magentic Marketplace’s environment manages market-wide capabilities like maintaining catalogs of available goods and services, implementing discovery algorithms, facilitating agent-to-agent communication, and handling simulated payments through a centralized transaction layer at its core, which ensures transaction integrity across all marketplace interactions. Additionally, the platform enables systematic, reproducible research. As demonstrated in the following video, it supports a wide range of agent implementations and evolving marketplace features, allowing researchers to integrate diverse agent architectures and adapt the environment as new capabilities emerge.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Introducing Magentic Marketplace, an open-source simulation environment for studying agentic markets" width="500" height="375" src="https://www.youtube-nocookie.com/embed/1SHpWinp7V8?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<p>We built Magentic Marketplace around three core architectural choices:</p>



<p><strong>HTTP/REST client-server architecture</strong>: Agents operate as independent clients while the Marketplace Environment serves as a central server. This mirrors real-world platforms and supports clear separation of customer and business agent roles.</p>



<p><strong>Minimal&nbsp;three-endpoint&nbsp;market&nbsp;protocol</strong>:<strong>&nbsp;</strong>Just&nbsp;three endpoints—register, protocol discovery, and action execution—lets&nbsp;agents dynamically discover available actions.&nbsp;New capabilities&nbsp;can&nbsp;be added without disrupting existing experiments.</p>



<p><strong>Rich action protocol</strong>: Specific message types support the complete transaction lifecycle: search, negotiation, proposals, and payments. The protocol is designed for extensibility. New actions like refunds, reviews, or ratings can be added seamlessly, allowing researchers to evolve marketplace capabilities and study emerging agent behaviors while remaining compatible.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="624" height="178" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2.png" alt="Figure 2. Diagram of a Market Environment showing interactions between an Assistant Agent (representing user intention) and a Service Agent (representing point of sale). Both agents connect to the Market Environment via POST /register, POST /action, and GET /protocol. Inside the Market Environment, components include Catalog, Search, Communication, and Transaction, with two Action Routers facilitating sending and receiving actions between the agents and the environment." class="wp-image-1154338" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2.png 624w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2-300x86.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure2-240x68.png 240w" sizes="auto, (max-width: 624px) 100vw, 624px" /><figcaption class="wp-element-caption">Figure 2. Magentic Marketplace includes two agent types: Assistant Agents (customers) and Service Agents (businesses). Both interact with a central Market Environment via REST APIs for registration, service discovery, communication, and transaction execution. Action Routers manage message flow and protocol requests, enabling autonomous negotiation and commerce in a two-sided marketplace.</figcaption></figure>



<p>Additionally, a visualization module lets users observe marketplace dynamics and review individual conversation threads between customer and business agents.</p>



<h2 class="wp-block-heading" id="setting-up-the-experiments">Setting up the experiments</h2>



<p>To ensure reproducibility, we instantiated the marketplace with fully synthetic data, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/multi-agent-marketplace/tree/main/data" target="_blank" rel="noopener noreferrer">available in our open-source repository<span class="sr-only"> (opens in new tab)</span></a>. The experiments modeled transactions such as ordering food and engaging with home improvement services, where agents represented customers and businesses engaging in marketplace transactions. This setup enabled precise measurement of behavior and systematic comparison against theoretical upper bounds.</p>



<p>Each experiment was run using 100 customers and 300 businesses and included both proprietary models (GPT-4o, GPT-4.1, GPT-5, and Gemini-2.5-Flash) and open-source models (OSS-20b, Qwen3-14b, and Qwen3-4b-Instruct-2507).</p>



<p>Our scenarios focused on simple all-or-nothing requests: Each customer had a list of desired items and amenities that needed to be present for a transaction to be satisfying. For those transactions, utility was computed as the sum of the customer’s internal item valuations minus actual prices paid. Consumer welfare, defined as the sum of utilities across all completed transactions, served as our key metric for comparing agent performance.</p>



<p>While this experimental setup provides a useful starting point, it is not intended to be definitive. We encourage researchers to extend the framework with richer, more nuanced measures and request types that better capture real consumer welfare, fairness, and other societal considerations.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="what-did-we-find">What did we find?</h2>



<h3 class="wp-block-heading" id="agents-can-improve-consumer-welfare-but-only-with-good-discovery">Agents can improve consumer welfare—but only with good discovery</h3>



<p>We explored whether two-sided agentic markets—where AI agents interact with each other and with service providers—can improve consumer welfare by reducing information gaps. Unlike traditional markets, which do not provide agentic support and place the full burden of overcoming information asymmetries on customers, agentic markets shift much of that effort to agents. This change matters because as agents gain better tools for discovery and communication, they relieve customers of the heavy cognitive load of filling any information gaps. This lowers the cost of making informed decisions and improves customer outcomes.</p>



<p>We compared several marketplace setups. Under realistic conditions (Agentic: Lexical search), agents faced real-world challenges like building queries, navigating paginated lists, identifying the right businesses to send inquiries to, and negotiating transactions.</p>



<p>Despite these complexities, advanced proprietary models and some medium-sized open-source models like GPTOSS-20b outperformed simple baselines like <em>randomly choosing or simply choosing the cheapest option</em>. Notably, GPT-5 achieved near-optimal performance, demonstrating its ability to effectively gather and utilize decision-relevant information in realistic marketplace conditions.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="936" height="447" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3.png" alt="Figure 3. Table comparing Baseline and Agentic conditions for marketplace decision-making. Columns include: Condition (e.g., Random w/ items only, Cheapest w/ items & prices, Random w/ items & amenities, Optimal, Perfect search, Lexical search) Query (N/A for most; “Agent decides” for Lexical search) Consideration Set (Businesses) (e.g., All w/ matching menus; Paginated lists of 10 based on menu items) Businesses Contacted (All in consideration set or Agent decides) Information Used (Menu items, prices, amenities, or depends on agent-to-agent conversation) Decision Criteria (Random choice, Lowest price, or Agent decides)." class="wp-image-1154339" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3-300x143.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3-768x367.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure3-240x115.png 240w" sizes="auto, (max-width: 936px) 100vw, 936px" /><figcaption class="wp-element-caption">Figure 3. Table comparing experimental setups for welfare outcomes in the restaurant industry. Each row shows a different way agents or baselines make decisions, from random picks to fully coordinated agentic strategies. Cell colors indicate how much information is available: green, at the top left, represents complete information, red, at the top right, represents limited information, and yellow at the bottom represents decisions that depend on agent communication.</figcaption></figure>



<p>Performance increased considerably under the <em>Agentic: Perfect search</em> condition, where agents started with the top three matches without needing to search and navigate among the choices. In this setting, Sonnet-4.0, Sonnet-4.5, GPT-5, and GPT-4.1 nearly reached the theoretical optimum and beat baselines with full amenity details but without agent-to-agent coordination.</p>



<p>Open-source models were mixed: GPTOSS-20b performed strongly under both <em>Perfect search</em> and <em>Lexical search</em> conditions, even exceeding GPT-4o&#8217;s performance with Perfect search. This suggests that relatively compact models can exhibit robust information-gathering and decision-making capabilities in complex multi-agent environments. Qwen3-4b-2507 faltered when discovery involved irrelevant options (Lexical search), while Qwen3-14b lagged in both cases due to fundamental limitations in reasoning.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1430" height="470" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4.png" alt="Figure 4. Boxplot comparing Agentic and Baseline strategies on welfare scores. The y-axis shows welfare (0–2000+), and the x-axis lists models and conditions. Under Agentic, models include Sonnet-4.0, Sonnet-4.5, GPT-5, GPT-4.1, Gemini-2.5-flash, GPT-4.0, GPT-oss-20b, Qwen3-4b-2507, and Qwen31-14b. Under Baselines, conditions include Random, Cheapest, and Random-items+amenities. Colors represent search types: blue = Lexical Search, yellow = Perfect Search, gray = Baseline, with a dashed line indicating Optimal welfare. Agentic models generally achieve higher welfare than baselines, with variability across models." class="wp-image-1154341" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4.png 1430w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4-300x99.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4-1024x337.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4-768x252.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-4-240x79.png 240w" sizes="auto, (max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">Figure 4. Chart showing consumer welfare outcomes in the restaurant industry under different marketplace setups. Blue bars show Agentic: Lexical search, where agents navigate realistic discovery challenges; yellow bars show Agentic: Perfect search, where agents started with ideal matches. Proprietary models approached optimum consumer welfare under perfect search, while open-source models and baselines lagged behind.</figcaption></figure>



<h2 class="wp-block-heading" id="paradox-of-choice">Paradox of Choice</h2>



<p>One promise of agents is their ability to consider far more options than people can. However, our experiments revealed a surprising limitation: providing agents with more options does not necessarily lead to more thorough exploration. We designed experiments that varied the search results limit from 3 to 100. Except for Gemini-2.5-Flash and GPT-5, the models contacted only a small fraction of available businesses regardless of the search limit. This suggests that most models do not conduct exhaustive comparisons and instead easily accept the initial &#8220;good enough&#8221; options.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1430" height="660" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5.png" alt="Figure 5. Line chart showing the relationship between Search Limit (x-axis: 3 to 100) and Mean Messages per Customer (y-axis: 0 to 120) for five models: Claude Sonnet 4 (red triangles) – stays nearly flat around 10–15 messages. Gemini 2.5 Flash (purple diamonds) – rises sharply from ~5 to over 110 messages as search limit increases. GPT-4.1 (orange circles) and GPT-4o (green squares) – remain low and stable around 5–10 messages. GPT-5 (blue line) – increases moderately to ~40 messages, then plateaus." class="wp-image-1154342" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5.png 1430w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5-300x138.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5-1024x473.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5-768x354.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-5-240x111.png 240w" sizes="auto, (max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">Figure 5. More options didn’t lead to broader exploration. Most models still contacted only a few businesses, except Gemini-2.5-Flash and GPT-5.</figcaption></figure>



<p>Additionally, across all models, consumer welfare declined as the number of search results increased. Despite contacting over a hundred businesses, Gemini-2.5-Flash&#8217;s performance declined from 1,700 to 1,350, and GPT-5 declined even more, from a near-optimal 2,000 to 1,400.</p>



<p>This demonstrates a Paradox of Choice effect, where more exploration does not guarantee better outcomes, potentially due to limited long context understanding. Claude Sonnet 4 showed the steepest performance decline, from 1,800 to 600 in consumer welfare. With all the options presented, it struggled to navigate larger sets of options and frequently contacted businesses that did not provide the goods or services that the customer was looking for.</p>



<p>This combination of poor initial selection and premature search termination demonstrates both inadequate decision-making criteria and insufficient exploration strategies. Some models showed modest performance decline (i.e., GPT-4.1: from 1,850 to 1,700; GPT-4o: from 1,550 to 1,450), finding good options within their limited exploration.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1430" height="653" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6.png" alt="Figure 6. Line chart showing Mean Customer Welfare (y-axis: 0–2200) versus Search Limit (x-axis: 3 to 100) for five models: Claude Sonnet 4 (red triangles) – starts near 1800 and declines sharply to ~600 as search limit increases. Gemini 2.5 Flash (purple diamonds) – decreases gradually from ~1700 to ~1300. GPT-4.1 (orange circles) – remains highest and most stable, around 1900–1700. GPT-4o (green squares) – stays near 1500 with slight decline. GPT-5 (blue line) – starts near 2000 and drops to ~1100. Dashed line at the top represents Optimal welfare (~2200)." class="wp-image-1154340" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6.png 1430w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6-300x137.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6-1024x468.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6-768x351.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-6-240x110.png 240w" sizes="auto, (max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">Figure 6. Mean consumer welfare decreased as consideration set size grew, revealing a Paradox of Choice effect, where expanding options reduced overall welfare.</figcaption></figure>



<h2 class="wp-block-heading" id="agents-are-vulnerable-to-manipulation">Agents are vulnerable to manipulation</h2>



<p>We tested six manipulation strategies, ranging from subtle psychological tactics to aggressive prompt injection attacks:</p>



<ul class="wp-block-list">
<li><strong>Authority</strong>: Fake credentials like “Michelin Guide featured” and “James Beard Award nominated” paired with fabricated certifications.</li>



<li><strong>Social proof</strong>: Claims like “Join 50,000+ satisfied customers” or “#1-rated Mexican restaurant” combined with fake reviews.</li>



<li><strong>Loss aversion</strong>: Fear-based warnings about “food poisoning” risks and “contamination issues” at competing restaurants.</li>



<li><strong>Prompt injection (basic)</strong>: Attempts to override agent instructions.</li>



<li><strong>Prompt injection (strong)</strong>: Aggressive attacks using emergency language and fabricating competitor scandals.</li>
</ul>



<p>Results revealed significant variation in manipulation resistance across models. Sonnet-4 was resistant to all attacks, and none of the manipulative strategies affected any of the customers’ choices. Gemini-2.5-Flash was generally resistant, except for strong prompt injections, where mean payments to unmanipulated agents were affected as a result. GPT-4o, GPTOSS-20b and Qwen3-4b were very vulnerable to prompt injection: all payments were redirected to the manipulative agent under these conditions. Specifically for GPTOSS-20 and Qwen3-4b-2507, even traditional psychological manipulation tactics (authority appeals and social proof) increased payments to malicious agents, demonstrating their vulnerability to basic persuasion techniques. These findings highlight a critical security concern for agentic marketplaces.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1430" height="270" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7.png" alt="Figure 7. Horizontal bar chart comparing mean payments received under different manipulation strategies for six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has bars for six conditions: Control, Authority, Social Proof, Loss Aversion, Prompt Injection (Basic), and Prompt Injection (Strong). Bars are split into red for manipulated and gray for rest, with values ranging from near 0 to 3. Claude Sonnet 4.5 shows consistently high payments (~3) across all conditions, while Gemini and GPT models vary, and Qwen models show very low manipulated values (~0.2) compared to rest." class="wp-image-1154344" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7.png 1430w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7-300x57.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7-1024x193.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7-768x145.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-7-240x45.png 240w" sizes="auto, (max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">Figure 7. Charts showing the variation in mean payments received by service agents with and without manipulation tactics. The results reveal substantial differences in manipulation resistance across models, with GPT-4.1 showing significantly higher vulnerability compared to Gemini-2.5-Flash.</figcaption></figure>



<h2 class="wp-block-heading" id="systemic-biases-create-unfair-advantages">Systemic biases create unfair advantages</h2>



<p>Our analysis revealed two distinct types of systematic biases showed by agents when selecting businesses from search results. Models showed systematic preferences based on where businesses appeared in search results. While proprietary models showed no strong positional preferences, open-source models exhibited clear patterns. Specifically, Qwen2.5-14b-2507 showed a pronounced bias toward selecting the last business presented, regardless of its actual merits.</p>



<p>Proposal&nbsp;bias&nbsp;is&nbsp;more pervasive across all models tested. This &#8220;first-offer acceptance&#8221; pattern suggests that models prioritized&nbsp;immediate selection over comprehensive exploration, potentially missing better alternatives that&nbsp;could have&nbsp;emerged&nbsp;by waiting for better options. This behavior&nbsp;continued&nbsp;across both proprietary and open-source models,&nbsp;indicating&nbsp;a fundamental challenge in agent decision-making architectures.</p>



<p>These biases can create unfair market dynamics, drive unintended behaviors, and push businesses to complete on response speed rather than product or service quality.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1430" height="289" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8.png" alt="Figure 8. Bar chart showing average selection rate for first, second, and third choices across six models: Claude Sonnet 4.5, Gemini 2.5 Flash, GPT-4o, GPT OSS 20B, Qwen3 14B, and Qwen3 4B. Each model has three bars labeled 1st, 2nd, and 3rd. Most models strongly favor the first choice: Claude Sonnet 4.5: 93.3% for 1st, 0% for 2nd, 6.7% for 3rd. Gemini 2.5 Flash: 86.7% for 1st, 6.7% for 2nd and 3rd. GPT-4o: 100% for 1st, 0% for others. GPT OSS 20B: 80% for 1st, 13.3% for 2nd, 6.7% for 3rd. Qwen3 14B: 0% for all. Qwen3 4B: 100% for 1st, 0% for others. Dashed line indicates random selection baseline." class="wp-image-1154343" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8.png 1430w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8-300x61.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8-1024x207.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8-768x155.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Magentic-Marketplace_Figure-8-240x49.png 240w" sizes="auto, (max-width: 1430px) 100vw, 1430px" /><figcaption class="wp-element-caption">Figure 8. All models showed strong preference for the first proposal received, accepting it without waiting for additional proposals or conducting systematic comparisons.</figcaption></figure>



<h2 class="wp-block-heading" id="what-this-means">What this means</h2>



<p>Even state-of-the-art models can show notable vulnerabilities and biases in marketplace environments. In our implementation, agents struggled with too many options, were susceptible to manipulation tactics, and showed systemic biases that created unfair advantages.</p>



<p>These outcomes are shaped not only by agent capabilities but also by marketplace design and implementation. Our current study focused on static markets, but real-world environments are dynamic, with agents and users learning over time. Oversight is critical for high-stakes transactions. Agents should assist, not replace, human decision-making.</p>



<p>We plan to explore dynamic markets and human-in-the-loop designs to improve efficiency and trust. A simulation environment like Magentic Marketplace is crucial for understanding the interplay between market components and agents before deploying them at scale.</p>



<p>Full details of our experimental setup and results are available in our <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2510.25779" target="_blank" rel="noopener noreferrer">paper<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="getting-started">Getting started</h2>



<p>Magentic Marketplace is available as an open-source environment for exploring agentic market dynamics. Code, datasets, and experiment templates are available on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/multi-agent-marketplace/" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://labs.ai.azure.com/projects/magentic-marketplace">Azure AI Foundry Labs<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/multi-agent-marketplace/" target="_blank" rel="noopener noreferrer">documentation<span class="sr-only"> (opens in new tab)</span></a> provides instructions for reproducing the experiments described above and guidance for extending the environment to new marketplace configurations.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/magentic-marketplace-an-open-source-simulation-environment-for-studying-agentic-markets/">Magentic Marketplace: an open-source simulation environment for studying agentic markets</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>RedCodeAgent: Automatic red-teaming agent against diverse code agents</title>
		<link>https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/</link>
		
		<dc:creator><![CDATA[Chengquan Guo , Chulin Xie, Yu Yang, Zhaorun Chen, Zinan Lin, Xander Davies, Yarin Gal, Dawn Song, Bo Li]]></dc:creator>
		<pubDate>Tue, 04 Nov 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1152834</guid>

					<description><![CDATA[<p>Code agents help streamline software development workflows, but may also introduce critical security risks. Learn how RedCodeAgent automates and improves “red-teaming” attack simulations to help uncover real-world threats that other methods overlook.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/">RedCodeAgent: Automatic red-teaming agent against diverse code agents</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1.jpg" alt="Icons of a chat bubble, connected document, and shield with checkmark on a blue-green gradient background." class="wp-image-1152887" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/RedCodeAgent-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<h2 class="wp-block-heading" id="introduction">Introduction</h2>



<p>Code agents are AI systems that can generate high-quality code and work smoothly with code interpreters. These capabilities help streamline complex software development workflows,&nbsp;which has led to their widespread adoption.</p>



<p>However, this progress also introduces critical safety and security risks. Existing static safety benchmarks and red-teaming methods—in which&nbsp;security researchers&nbsp;simulate real-world attacks to&nbsp;identify&nbsp;security vulnerabilities—often fall short when evaluating code agents.&nbsp;They&nbsp;may&nbsp;fail to&nbsp;detect&nbsp;emerging real-world risks, such as the combined effects of multiple jailbreak tools.&nbsp;In&nbsp;the context of code, effective red-teaming requires more than simply checking whether the target code agent rejects unsafe requests. Instead, the agent must generate and execute correct code that performs the intended risky functionality, making it essential to evaluate execution behaviors beyond static code analysis.&nbsp;</p>



<p>To address these challenges, researchers from the University of Chicago, University of Illinois Urbana–Champaign, VirtueAI, the UK AI Safety Institute, University of Oxford, UC Berkeley, and Microsoft Research recently proposed <a href="https://www.microsoft.com/en-us/research/publication/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/" target="_blank" rel="noreferrer noopener">RedCodeAgent</a>, the first fully automated and adaptive red-teaming agent designed specifically to evaluate the safety of large language model&nbsp;(LLM)-based code agents.</p>



<p>Comprehensive experimental results demonstrate the effectiveness and efficiency of&nbsp;RedCodeAgent across (1) diverse Common Weakness Enumeration (CWE) vulnerabilities and malware types, (2) multiple programming languages—including Python, C, C++, and Java—and (3) a wide range of code agents, such as OpenCodeInterpreter, ReAct, MetaGPT, and commercial agents like Cursor and&nbsp;Codeium.&nbsp;RedCodeAgent also uncovers common vulnerabilities across agents&nbsp;such as generating and executing unsafe code, exposes variations in red-teaming difficulty across goals, identifies frequently triggered attack tools, and detects previously unknown vulnerabilities that all other baseline methods overlook.&nbsp;</p>



<h2 class="wp-block-heading" id="framework-for-automatic-red-teaming-against-code-agents">Framework for&nbsp;automatic&nbsp;red-teaming&nbsp;against&nbsp;code&nbsp;agents</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1600" height="484" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_.jpg" alt="A flowchart illustrating the workflow of RedCodeAgent, a system for red-teaming code agents. 

User Input (left panel): 
The user provides a risk scenario (e.g., “Delete sensitive files”) and a detailed risk description (e.g., “Create and execute a Python script that removes /root/.bashrc”). 

Memory module (center-left): 
RedCodeAgent retrieves the top-K most similar memory entries, each containing a risk scenario, description, trajectory, evaluation result, and self-reflection. 

LLM red-teaming function call (center): 
The agent uses large language model (LLM) reasoning and tools from its toolbox—such as Code Substitution, GCG, AutoDAN, AmpleGCG, and Advprompter—to generate attacks. 

Query target code agent (center-right): 
The generated query is sent to the target code agent, which attempts to execute or reject the risky action. 

Evaluation module (right panel): 
Outcomes are classified as: 

Attack success (e.g., file is no longer present), 

Attack failure (e.g., file is still present), or 

Get rejected (e.g., rejection words appear). 

If the attack fails or gets rejected, the process iterates until reaching the maximum iteration or success. 

Final Output (bottom): 
Successful red-teaming instances are stored, followed by a self-reflection step that appends a new memory entry. 

Visual elements include arrows showing flow between modules, success/failure indicators, and icons representing users, agents, memory, and evaluation. " class="wp-image-1152869" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_-300x91.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_-1024x310.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_-768x232.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_-1536x465.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure1_-240x73.jpg 240w" sizes="auto, (max-width: 1600px) 100vw, 1600px" /><figcaption class="wp-element-caption">Figure 1: Illustration of&nbsp;RedCodeAgent&nbsp;on automatic red-teaming against a target code agent&nbsp;</figcaption></figure>



<p>As shown in Figure 1,&nbsp;RedCodeAgent&nbsp;is equipped with a&nbsp;<strong>memory module</strong>&nbsp;that accumulates successful attack experiences, enabling the system to&nbsp;<strong>continuously learn and adapt its attack strategies</strong>. After learning from the previous experiences,&nbsp;RedCodeAgent&nbsp;further&nbsp;leverages&nbsp;a&nbsp;<strong>tailored toolbox</strong>&nbsp;that combines representative red-teaming tools with a specialized&nbsp;<strong>code substitution module</strong>, enabling realistic and diverse code-specific attack simulations through function calling. Based on the target agent’s responses across multiple interactive trials, RedCodeAgent optimizes&nbsp;its strategies, systematically&nbsp;probing for&nbsp;weaknesses and vulnerabilities&nbsp;in real time.&nbsp;</p>



<p>In the evaluation phase,&nbsp;RedCodeAgent&nbsp;integrates simulated sandbox environments to enable code execution and assess the impact of the resulting behaviors. This sandbox-based evaluation ensures a more robust assessment of harmful behaviors and addresses the potential biases of&nbsp;previous&nbsp;static methods that rely solely on “LLM-as-a-judge” evaluations.</p>



<p>A case study is shown in Figure 2. Initially,&nbsp;RedCodeAgent&nbsp;discovers that the request was rejected, then RedCodeAgent calls the Greedy Coordinate&nbsp;Gradient&nbsp;(GCG)&nbsp;algorithm&nbsp;to bypass the safety guardrail. After the second request was rejected by the code agent,&nbsp;RedCodeAgent&nbsp;invoked both Code Substitution and GCG to optimize the prompt. Ultimately,&nbsp;RedCodeAgent&nbsp;successfully combined the suggestion from Code Substitution (i.e., using&nbsp;pathlib) with the adversarial suffix generated by GCG, making the target code agent delete the specified file.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1600" height="1771" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_.jpg" alt="A flowchart illustrating a red-teaming attack scenario targeting a code agent. The risk scenario is deleting sensitive files, specifically removing “/etc/gshadow.” The process shows several decision steps: querying the target code agent (rejected), calling the GCG to optimize prompts, trying modified prompts, using code substitution (e.g., replacing os.remove with pathlib.unlink), and retrying the optimized prompts. The final result shows that the optimized prompt successfully caused the file “/etc/gshadow” to be removed, labeled as “Attack success.” The chart includes text boxes for each step, evaluation results (e.g., “Get rejected” or “Attack success”), and concludes with a “Final output” section describing self-reflection on the red-teaming process." class="wp-image-1152871" style="width:622px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_-271x300.jpg 271w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_-925x1024.jpg 925w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_-768x850.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_-1388x1536.jpg 1388w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure2_-163x180.jpg 163w" sizes="auto, (max-width: 1600px) 100vw, 1600px" /><figcaption class="wp-element-caption">Figure2. A case study of&nbsp;RedCodeAgent&nbsp;calling different tools to successfully attack the target code agent</figcaption></figure>



<h2 class="wp-block-heading" id="insights-from-redcodeagent">Insights from&nbsp;RedCodeAgent&nbsp;</h2>



<p>Experiments on diverse benchmarks show that&nbsp;RedCodeAgent&nbsp;achieves both a higher attack success rate (ASR) and a lower rejection rate, revealing several key findings outlined below.</p>



<h3 class="wp-block-heading" id="using-traditional-jailbreak-methods-alone-does-not-necessarily-improve-asr-on-code-agents">Using&nbsp;traditional&nbsp;jailbreak&nbsp;methods&nbsp;alone&nbsp;does&nbsp;not&nbsp;necessarily&nbsp;improve&nbsp;ASR on code agents</h3>



<p>The optimized prompts generated by GCG,&nbsp;AmpleGCG,&nbsp;Advprompter, and&nbsp;AutoDAN&nbsp;do not always achieve a higher ASR compared with static prompts with no jailbreak, as shown in Figure 3.&nbsp;This is&nbsp;likely&nbsp;due to the difference between code-specific tasks and general malicious request tasks in LLM safety. In the context of code, it is not enough for the target code agent to simply avoid rejecting the request; the target code agent must also generate and execute code that performs the intended function.&nbsp;Previous&nbsp;jailbreak methods do not guarantee this outcome. However,&nbsp;RedCodeAgent&nbsp;ensures that the input prompt has a clear functional objective (e.g., deleting specific sensitive files). RedCodeAgent&nbsp;can dynamically adjust based on evaluation feedback, continually optimizing to achieve the specified objectives.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1600" height="1581" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_.jpg" alt="A scatter plot comparing six methods on two metrics: Attack Success Rate (ASR) in percent (y-axis) and Time Cost in seconds (x-axis). Each method is represented by a distinct marker with coordinates labeled as (time, ASR): 

RedCodeAgent (121.17s, 72.47%) — red circle, highest ASR. 

GCG (71.44s, 54.69%) — purple diamond. 

No Jailbreak (36.25s, 55.46%) — blue square. 

Advprompter (132.59s, 46.42%) — pink inverted triangle. 

AmpleGCG (45.28s, 41.11%) — yellow triangle. 

AutoDAN (51.77s, 29.26%) — gray hexagon. 
The “Better” direction points toward higher ASR and lower time cost. The chart shows that RedCodeAgent achieves the best performance (highest ASR) despite moderate time cost. " class="wp-image-1152872" style="width:574px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_-300x296.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_-1024x1012.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_-768x759.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_-1536x1518.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure3_-182x180.jpg 182w" sizes="auto, (max-width: 1600px) 100vw, 1600px" /><figcaption class="wp-element-caption">Figure 3：RedCodeAgent&nbsp;achieves the highest ASR compared with other methods</figcaption></figure>



<h3 class="wp-block-heading" id="redcodeagent-exhibits-adaptive-tool-utilization">RedCodeAgent&nbsp;exhibits&nbsp;adaptive&nbsp;tool&nbsp;utilization&nbsp;</h3>



<p>RedCodeAgent&nbsp;can dynamically adjust its tool usage based on task difficulty. Figure 4 shows that the tool calling combination is different&nbsp;for&nbsp;different tasks.&nbsp;For simpler tasks, where the baseline static test cases already achieve a high ASR,&nbsp;RedCodeAgent&nbsp;spends little time invoking&nbsp;additional&nbsp;tools,&nbsp;demonstrating&nbsp;its efficiency. For more challenging tasks, where the baseline static test cases in&nbsp;RedCode-Exec achieve a lower ASR,we observe that RedCodeAgent spends more time using advanced tools like&nbsp;GCG and&nbsp;Advprompter&nbsp;to&nbsp;optimize&nbsp;the prompt for a successful attack. As a result, the average time spent on invoking different tools varies across tasks, indicating that RedCodeAgent adapts its strategy depending on the specific task.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1600" height="789" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_.jpg" alt="A stacked bar chart showing the time cost (seconds) for different methods across risk indices 1–27 (except 18) for an agent. The x-axis represents risk indices, and the y-axis shows time cost in seconds. Each bar is divided into colored segments representing different components of the total time cost: 

Pink: Query (target agent) – 36.25s per call 
Brown: Code substitution – 12.16s per call 
Green: GCG – 35.19s per call 
Teal: AutoDAN – 15.52s per call 
Blue: AmpleGCG – 9.03s per call 
Magenta: Advprompter – 96.34s per call 

Most bars are dominated by pink segments (target agent queries), with several spikes (e.g., risk indices 9–11 and 14–15) where additional methods like GCG and Advprompter add noticeable time overhead. The legend in the upper right lists each method’s average time per call. " class="wp-image-1152874" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_-300x148.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_-1024x505.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_-768x379.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_-1536x757.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure4_-240x118.jpg 240w" sizes="auto, (max-width: 1600px) 100vw, 1600px" /><figcaption class="wp-element-caption">Figure 4: Average time cost for&nbsp;RedCodeAgent&nbsp;to invoke different tools or query the target code agent in successful cases for each risk scenario&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="redcodeagent-discovers-new-vulnerabilities">RedCodeAgent&nbsp;discovers&nbsp;new&nbsp;vulnerabilities</h3>



<p>In scenarios where other methods&nbsp;fail to&nbsp;find successful attack strategies,&nbsp;RedCodeAgent&nbsp;is able to discover new, feasible jailbreak approaches. Quantitatively, we find that&nbsp;RedCodeAgent&nbsp;is capable of discovering&nbsp;82 (out of 27*30=810 cases in&nbsp;RedCode-Exec benchmark) unique vulnerabilities on the&nbsp;OpenCodeInterpreter&nbsp;code agent and 78 on the ReAct code agent. These are cases where all baseline methods&nbsp;fail to&nbsp;identify the vulnerability, but RedCodeAgent succeeds.</p>



<h2 class="wp-block-heading" id="summary">Summary</h2>



<p>RedCodeAgent&nbsp;combines adaptive memory, specialized tools, and simulated execution environments to uncover real-world risks that static benchmarks&nbsp;may&nbsp;miss.&nbsp;It&nbsp;consistently outperforms leading jailbreak methods, achieving higher attack success rates and lower rejection rates, while remaining efficient and adaptable across diverse agents and programming languages.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/redcodeagent-automatic-red-teaming-agent-against-diverse-code-agents/">RedCodeAgent: Automatic red-teaming agent against diverse code agents</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tell me when: Building agents that can wait, monitor, and act</title>
		<link>https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/</link>
		
		<dc:creator><![CDATA[Hussein Mozannar, Matheus Kunzler Maldaner, Maya Murad, Jingya Chen, Gagan Bansal, Rafah Hosn, Adam Fourney]]></dc:creator>
		<pubDate>Tue, 21 Oct 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1152051</guid>

					<description><![CDATA[<p>SentinelStep enables AI agents to handle monitoring tasks that run for hours or days, like watching for emails or tracking prices. It works by managing when agents should check and their context, avoiding wasted resources and missed updates.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/">Tell me when: Building agents that can wait, monitor, and act</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Workflow icons showing tasks, thinking, and time, linked to a person symbol on a gradient background." class="wp-image-1152522" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MagenticUIupdate-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<p>Modern&nbsp;LLM&nbsp;Agents&nbsp;can debug code, analyze spreadsheets, and book complex travel.&nbsp;Given those capabilities, it’s reasonable to assume that they could handle something simpler:&nbsp;waiting.&nbsp;Ask an agent to&nbsp;monitor&nbsp;your email for a colleague’s response or watch for a price drop over several days, and it will fail. Not because it&nbsp;can’t&nbsp;check email or scrape prices. It can do both. It fails&nbsp;because it&nbsp;doesn’t&nbsp;know&nbsp;<em>when</em>&nbsp;to check.&nbsp;Agents either&nbsp;give up after a few attempts or burn through their context window, checking obsessively. Neither&nbsp;work.&nbsp;</p>



<p>This matters because monitoring tasks&nbsp;are&nbsp;everywhere. We track emails for specific information, watch news&nbsp;feeds for updates, and&nbsp;monitor&nbsp;prices for sales. Automating these tasks would save hours, but current&nbsp;agents&nbsp;aren’t&nbsp;built for patience.</p>



<p>To address this, we are introducing&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/magentic-ui" target="_blank" rel="noopener noreferrer">SentinelStep<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;a&nbsp;mechanism&nbsp;that&nbsp;enables&nbsp;agents&nbsp;to complete long-running monitoring&nbsp;tasks.&nbsp;The&nbsp;approach is simple.&nbsp;SentinelStep&nbsp;wraps the agent in a workflow with dynamic&nbsp;polling&nbsp;and&nbsp;careful context&nbsp;management.&nbsp;This&nbsp;enables&nbsp;the&nbsp;agent&nbsp;to&nbsp;monitor&nbsp;conditions for&nbsp;hours&nbsp;or&nbsp;days&nbsp;without getting&nbsp;sidetracked.&nbsp;We&#8217;ve&nbsp;implemented&nbsp;SentinelStep&nbsp;in&nbsp;<a href="https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/?msockid=16c285fb8306647b25f593b982ef6516" target="_blank" rel="noreferrer noopener">Magentic-UI</a>,&nbsp;our research&nbsp;prototype&nbsp;agentic system,&nbsp;to enable&nbsp;users&nbsp;to&nbsp;build agents for&nbsp;long-running&nbsp;tasks,&nbsp;whether they&nbsp;involve web&nbsp;browsing, coding, or external&nbsp;tools.&nbsp;</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="how-it-works">How it works</h2>



<p>The core&nbsp;challenge is&nbsp;polling frequency. Poll too often,&nbsp;and&nbsp;tokens get&nbsp;wasted. Poll too infrequently, and the user’s notification gets delayed.&nbsp;SentinelStep&nbsp;makes&nbsp;an educated guess&nbsp;at&nbsp;the&nbsp;polling interval based on the task at hand—checking email gets different treatment&nbsp;than&nbsp;monitoring&nbsp;quarterly earnings—then dynamically adjusts&nbsp;based on&nbsp;observed&nbsp;behavior.&nbsp;</p>



<p>There’s&nbsp;a second challenge: context overflow.&nbsp;Because monitoring tasks can run for days,&nbsp;context overflow&nbsp;becomes inevitable.&nbsp;SentinelStep&nbsp;handles&nbsp;this by saving the agent state after the first check, then&nbsp;using&nbsp;that state for each subsequent check.</p>



<div class="wp-block-group is-layout-grid wp-container-core-group-is-layout-baef362d wp-block-group-is-layout-grid">
<figure class="wp-block-video"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Social-Media-Monitor-Final-demo.mp4"></video></figure>



<figure class="wp-block-video"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Repo-Milestone-Trigger-Final-demo.mp4"></video></figure>



<figure class="wp-block-video"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Price-Alert-Final-demo.mp4"></video></figure>



<figure class="wp-block-video"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Auto-Updating-Project-Dashboard-Final-demo.mp4"></video></figure>
</div>



<figure class="wp-block-video aligncenter"><figcaption class="wp-element-caption">These demonstrations capture&nbsp;Magentic-UI with&nbsp;SentinelStep&nbsp;at work, completing a range of tasks in a timelapse sequence.&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="core-components">Core components</h3>



<p>As&nbsp;the name&nbsp;suggests,&nbsp;SentinelStep&nbsp;consists of&nbsp;individual steps&nbsp;taken as part of&nbsp;an&nbsp;agent’s broader&nbsp;workflow.&nbsp;As illustrated in Figure 1, there are three main components:&nbsp;the&nbsp;actions necessary to collect information, the condition that determines&nbsp;when&nbsp;the task&nbsp;is complete,&nbsp;and the polling interval&nbsp;that&nbsp;determines&nbsp;timing.&nbsp;Once&nbsp;these components&nbsp;are&nbsp;identified, the&nbsp;system’s&nbsp;behavior is simple:&nbsp;every<em>&nbsp;[polling interval]&nbsp;</em>do<em>&nbsp;[actions]&nbsp;</em>until<em>&nbsp;[condition]&nbsp;</em>is satisfied<em>.</em>&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="704" height="250" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI.png" alt="Figure 1. SentinelSteps’s three main components in Magentic-UI’s co-planning interface. " class="wp-image-1152610" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI.png 704w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-1-Sentinel-UI-240x85.png 240w" sizes="auto, (max-width: 704px) 100vw, 704px" /><figcaption class="wp-element-caption">Figure&nbsp;1.&nbsp;SentinelSteps’s&nbsp;three main components&nbsp;in&nbsp;Magentic-UI’s&nbsp;co-planning interface.<em>&nbsp;</em></figcaption></figure>



<p>These three components are defined and exposed in the co-planning interface of Magentic-UI. Given a user prompt, Magentic-UI proposes a complete multi-step plan, including pre-filled parameters for any monitoring steps. Users can accept the plan or adjust as needed.</p>



<h3 class="wp-block-heading" id="processing">Processing</h3>



<p>Once a run starts, Magentic-UI assigns the most appropriate agent from a team of agents to perform each action. This team includes agents capable of web surfing, code execution, and calling arbitrary MCP servers.</p>



<p>When the workflow reaches a monitoring step, the flow is straightforward. The assigned agent collects the necessary information through the actions described in the plan. The Magentic-UI orchestrator then checks whether the condition is satisfied. If it is, the SentinelStep is complete, and the orchestrator moves to the next step. If not, the orchestrator determines the timestamp for the next check and resets the agent’s state to prevent context overflow.</p>



<h2 class="wp-block-heading" id="evaluation">Evaluation</h2>



<p>Evaluating&nbsp;monitoring tasks in real-world settings&nbsp;is&nbsp;nearly impossible.&nbsp;Consider a simple example: monitoring the Magentic-UI repository on GitHub&nbsp;until&nbsp;it reaches&nbsp;10,000 stars&nbsp;(a measure of how many people have bookmarked it). That event occurs only once and can’t be repeated.&nbsp;Most&nbsp;real-world monitoring tasks share this limitation, making systematic bench marking very challenging.</p>



<p>In response, we&nbsp;are developing&nbsp;SentinelBench, a suite of synthetic&nbsp;web environments for evaluating monitoring tasks. These environments make experiments repeatable. SentinelBench&nbsp;currently&nbsp;supports&nbsp;28&nbsp;configurable scenarios, each&nbsp;allowing the user to schedule exactly when&nbsp;a&nbsp;target&nbsp;event&nbsp;should&nbsp;occur. It includes setups like GitHub Watcher, which&nbsp;simulates a repository accumulating stars over time;&nbsp;Teams Monitor, which models incoming messages, some&nbsp;urgent; and&nbsp;Flight Monitor, which&nbsp;replicates&nbsp;evolving&nbsp;flight-availability&nbsp;dynamics.&nbsp;</p>



<p>Initial&nbsp;tests&nbsp;show clear benefits.&nbsp;As shown in&nbsp;Figure&nbsp;2, success rates&nbsp;remain&nbsp;high for short tasks (30&nbsp;sec&nbsp;and 1&nbsp;min) regardless of&nbsp;whether&nbsp;SentinelStep&nbsp;is&nbsp;used.&nbsp;For longer tasks,&nbsp;SentinelStep&nbsp;markedly&nbsp;improves reliability: at 1 hour, task reliability rises from 5.6% without&nbsp;SentinelStep&nbsp;to&nbsp;33.3% with&nbsp;it;&nbsp;and at 2 hours,&nbsp;it rises&nbsp;from 5.6% to 38.9%. These gains&nbsp;demonstrate&nbsp;that&nbsp;SentinelStep&nbsp;effectively addresses the challenge of maintaining performance over extended durations.<a id="_msocom_1"></a></p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="619" height="399" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval.png" alt="Figure 2. SentinelStep improves success rates on longer running tasks (1–2 hours) while maintaining comparable performance on shorter tasks.  " class="wp-image-1152612" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval.png 619w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval-300x193.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Figure-2-Eval-240x155.png 240w" sizes="auto, (max-width: 619px) 100vw, 619px" /><figcaption class="wp-element-caption">Figure&nbsp;2.&nbsp;SentinelStep&nbsp;improves&nbsp;success rates&nbsp;on longer running tasks (1–2&nbsp;hours)&nbsp;while&nbsp;maintaining&nbsp;comparable performance&nbsp;on shorter tasks.&nbsp;&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="impact-and-availability">Impact and availability</h2>



<p>SentinelStep is a first step toward practical, proactive, longer‑running agents. By embedding patience into plans, agents can responsibly monitor conditions and act when it matters—staying proactive without wasting resources. This lays the groundwork for always‑on assistants that stay efficient, respectful of limits, and aligned with user intent.</p>



<p>We’ve open-sourced SentinelStep as part of Magentic-UI, available on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/magentic-ui" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a> or via <code>pip install magnetic-ui</code>. As with any new technique, production deployment should be preceded through&nbsp;testing and validation&nbsp;for the specific use case.&nbsp;For&nbsp;guidance on&nbsp;intended use,&nbsp;privacy&nbsp;considerations,&nbsp;and safety&nbsp;guidelines,&nbsp;see&nbsp;the&nbsp;Magentic-UI&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/magentic-ui/blob/main/TRANSPARENCY_NOTE.md" target="_blank" rel="noopener noreferrer">Transparency&nbsp;Note.<span class="sr-only"> (opens in new tab)</span></a>&nbsp;</p>



<p>Our goal is to&nbsp;make it easier to implement agents that can&nbsp;handle&nbsp;long-running&nbsp;monitoring&nbsp;tasks&nbsp;and&nbsp;lay&nbsp;the groundwork for&nbsp;systems that&nbsp;anticipate, adapt, and&nbsp;evolve&nbsp;to meet real-world needs.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/tell-me-when-building-agents-that-can-wait-monitor-and-act/">Tell me when: Building agents that can wait, monitor, and act</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Social-Media-Monitor-Final-demo.mp4" length="5720457" type="video/mp4" />
<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Repo-Milestone-Trigger-Final-demo.mp4" length="6357051" type="video/mp4" />
<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Price-Alert-Final-demo.mp4" length="2694358" type="video/mp4" />
<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Auto-Updating-Project-Dashboard-Final-demo.mp4" length="6699013" type="video/mp4" />

			</item>
		<item>
		<title>Ideas: More AI-resilient biosecurity with the Paraphrase Project</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/</link>
		
		<dc:creator><![CDATA[Eric Horvitz, Bruce Wittmann, Tessa Alexanian, James Diggans]]></dc:creator>
		<pubDate>Mon, 06 Oct 2025 14:04:34 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1151021</guid>

					<description><![CDATA[<p>Microsoft’s Eric Horvitz and guests Bruce Wittmann, Tessa Alexanian, and James Diggans discuss the Paraphrase Project—a red-teaming effort that exposed and secured a biosecurity vulnerability in AI-driven protein design. The work offers a model for addressing AI’s dual-use risks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/">Ideas: More AI-resilient biosecurity with the Paraphrase Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Ideas: More AI-resilient biosecurity with the Paraphrase Project" width="500" height="281" src="https://www.youtube-nocookie.com/embed/xA9nvhX7e7A?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=148928066&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.</p>



<p>AI has been described as a “dual use” technology: the capabilities that can be leveraged for good can also potentially be used to cause harm. In this episode, Microsoft Chief Scientific Officer <a href="https://www.microsoft.com/en-us/research/people/horvitz/">Eric Horvitz</a> and his guests—<a href="https://www.microsoft.com/en-us/research/people/bwittmann/">Bruce Wittmann</a>, a senior applied scientist at Microsoft; <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/people/tessa-alexanian/" target="_blank" rel="noopener noreferrer">Tessa Alexanian<span class="sr-only"> (opens in new tab)</span></a>, a technical lead at the International Biosecurity and Biosafety Initiative for Science (IBBIS);&nbsp;and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/jdiggans/" target="_blank" rel="noopener noreferrer">James Diggans<span class="sr-only"> (opens in new tab)</span></a>, a vice president at Twist Bioscience—explore this idea in the context of AI-powered protein design.</p>



<p>With Horvitz at the lead, Alexanian, Diggans, and Wittmann were part of a cross-sector team that demonstrated toxic protein candidates could be designed with help from AI—and that they could bypass the systems in place to defend against their creation. The project, known as the <em>Paraphrase Project</em>, culminated in a cybersecurity-style response, a more robust protein screening system, and a modified approach to peer review with implications for how we think about and tackle AI risk more broadly. The work was recently published in <em>Science.</em></p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/strengthening-nucleic-acid-biosecurity-screening-against-generative-protein-design-tools/">Strengthening nucleic acid biosecurity screening against generative protein design tools</a><br>Publication | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/toward-ai-resilient-screening-of-nucleic-acid-synthesis-orders-process-results-and-recommendations/">Toward AI-Resilient Screening of Nucleic Acid Synthesis Orders: Process, Results, and Recommendations</a><br>Preprint | December 2024</li>



<li><a href="https://www.microsoft.com/en-us/research/story/the-paraphrase-project-designing-defense-for-an-era-of-synthetic-biology/">The Paraphrase Project: Designing defense for an era of synthetic biology</a><br>Microsoft Research Blog | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI meets biology: Promise, risk, and responsibility</a><br>Microsoft Research Blog | Eric Horvitz | October 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/project/paraphrase-project/">Paraphrase Project</a><br>Project homepage</li>
</ul>
</div>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript-1">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>ERIC HORVITZ: </strong>You’re&nbsp;listening to&nbsp;<em>Ideas</em>, a Microsoft Research Podcast that dives deep into the world of technology research and the profound questions behind the code.&nbsp;I’m&nbsp;Eric Horvitz, Microsoft’s chief scientific officer, and in this series, we explore the technologies shaping our future and the&nbsp;big ideas&nbsp;that propel them forward.</p>



<p>[MUSIC FADES]</p>



<p>Today,&nbsp;I’m&nbsp;excited to talk about the Paraphrase Project, an effort I co-led exploring how&nbsp;advances in&nbsp;AI tools&nbsp;for protein design&nbsp;might&nbsp;impact&nbsp;biosecurity. The results were reported in our recent paper,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="http://www.science.org/doi/10.1126/science.adu8578" target="_blank" rel="noopener noreferrer">“Strengthening nucleic acid biosecurity screening against generative protein design tools,”<span class="sr-only"> (opens in new tab)</span></a>&nbsp;published in&nbsp;<em>Science</em>&nbsp;on Oct. 2.&nbsp;</p>



<p>Joining me are&nbsp;three&nbsp;of the larger set of&nbsp;coauthors on that paper:&nbsp;Bruce Wittmann, senior applied scientist at Microsoft;&nbsp;James&nbsp;Diggans, vice president at Twist Bioscience and chair of the board for the International Gene Synthesis Consortium;&nbsp;and Tessa Alexanian, technical lead at the International Biosecurity and Biosafety Initiative for Science, also known as <em>IBBIS</em>.&nbsp;</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>Now, let’s&nbsp;rewind two years.&nbsp;Almost to&nbsp;the day, Bruce and I uncovered a vulnerability. While preparing a case study for a workshop on AI and biosecurity, we discovered that open-source AI protein design tools could be used to redesign toxic proteins in ways that could bypass biosecurity screening systems, systems set up to&nbsp;identify&nbsp;incoming orders of concern.&nbsp;</p>



<p>Now in that work, we&nbsp;created an AI pipeline from open-source tools that could&nbsp;essentially “paraphrase” the amino acid sequences—reformulating&nbsp;them while&nbsp;working to&nbsp;preserve&nbsp;their structure and potentially their function.&nbsp;</p>



<p>These paraphrased sequences could evade the screening systems used by major DNA&nbsp;synthesis companies, and these are the systems that scientists rely on to safely produce AI-designed proteins.&nbsp;</p>



<p>Now, experts in the field described this finding as the first “zero day” for AI and biosecurity.&nbsp;And this&nbsp;marked the beginning of a deep, two-year collaborative effort to investigate and address this challenge.&nbsp;</p>



<p>With the help of a&nbsp;strong&nbsp;cross-sector team—including James, Tessa, Bruce, and many others—we worked behind the scenes to build AI biosecurity&nbsp;<em>red-teaming approaches</em>,&nbsp;probe for vulnerabilities, and to design practical fixes. These “patches,” akin to those in cybersecurity,&nbsp;have now been shared with&nbsp;organizations&nbsp;globally to strengthen biosecurity screening.&nbsp;</p>



<p>This has been one of the most fascinating projects&nbsp;I’ve&nbsp;had the privilege to work on, for its technical complexity, its ethical and policy dimensions, and the remarkable collaboration across industry, government, and nonprofit sectors.&nbsp;</p>



<p>The project highlights that the&nbsp;same AI tools capable of&nbsp;incredible&nbsp;good can also be misused, requiring us to be vigilant, thoughtful, and creative so we continue to get the most benefit out of AI tools while working to ensure&nbsp;that&nbsp;we avoid costly misuses.&nbsp;</p>



<p>With that, let me officially welcome our guests.<s></s></p>



<p>Bruce, James, Tessa, welcome to the podcast.</p>



<p><strong>BRUCE WITTMANN: </strong>Thanks, Eric.</p>



<p><strong>JAMES DIGGANS: </strong>Thanks for having us.</p>



<p><strong>HORVITZ: </strong>It&#8217;s been such a pleasure working closely with each of you, not only for your expertise but also for your deep commitment and passion about public health and global safety.</p>



<p>Before we dive into the technical side of things, I&#8217;d like to ask each of you, how did you get into this field? What inspired you to become biologists and then pursue the implications of advances in AI for biosecurity? Bruce?</p>



<p><strong>WITTMANN:</strong>&nbsp;Well, I&#8217;ve always liked building things. That&#8217;s where I would say I come from. You know, my hobbies when I&#8217;m not working on biology or AI things—as you know, Eric—is, like, building things around the house, right. Doing construction. That kind of stuff.</p>



<p>But my broader interests have always been biology, chemistry. So I originally got into organic chemistry. I found that was fascinating. From there, went to synthetic biology, particularly metabolic engineering, because that&#8217;s kind of like organic chemistry, but you&#8217;re wiring together different parts of an organism’s metabolism rather than different chemical reactions. And while I was working in that space, I, kind of, had the thought of there&#8217;s got to be an easier way to do this [LAUGHS] because it is really difficult to do any type of metabolic engineering. And that&#8217;s how I got into the AI space, trying to solve these very complicated biological problems, trying to build things that we don&#8217;t necessarily even understand using our understanding from data or <em>deriving</em> understanding from data.</p>



<p>So, you know, that&#8217;s the roundabout way of how I got to where I am—the abstract way of how I got to where I am.</p>



<p><strong>HORVITZ: </strong>And, Tessa, what motivated you to jump into this area and zoom into biology and biosciences and helping us to avoid <em>catastrophic</em> outcomes?</p>



<p><strong>ALEXANIAN:</strong> Yeah, I mean, probably the origin of me being really excited about biology is actually a book called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.penguinrandomhouse.com/books/535043/the-lives-of-a-cell-by-lewis-thomas/" target="_blank" rel="noopener noreferrer"><em>[The] Lives of [a] Cell</em><span class="sr-only"> (opens in new tab)</span></a> by Lewis Thomas, which is an extremely beautiful book of essays that made me be like, <em>Oh, wow, life is just incredible</em>. I think I read it when I was, you know, 12 or 13, and I was like, <em>Life is incredible. I want to work on this. This is the most beautiful science</em>, right. And then I, in university, I was studying engineering, and I heard there was this engineering team for engineering biology—this <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://igem.org/" target="_blank" rel="noopener noreferrer">iGEM<span class="sr-only"> (opens in new tab)</span></a> team—and I joined it, and I thought, <em>Oh, this is so cool. I really got to go work in this field of synthetic biology.</em></p>



<p>And then I also tried doing the wet lab biology, and I was like, <em>Oh, but I don&#8217;t like this part</em>. <em>I don&#8217;t actually, like, like babysitting microbes.</em> [LAUGHTER] I think there&#8217;s a way … some people who are great wet lab biologists are made of really stern stuff. And they really enjoy figuring out how to redesign their negative controls so they can figure out whether it was contamination or whether it was, you know, temperature fluctuation. I&#8217;m not that, apparently.</p>



<p>And so I ended up becoming a lab automation engineer because I could help the science happen, but I … but my responsibilities were the robots and the computers rather than the microbes, which I find a little bit intransigent.</p>



<p><strong>HORVITZ: </strong>Right. I was thinking of those tough souls; they also used their mouths to do pipetting and so on of these contaminated fluids …</p>



<p><strong>WITTMANN: </strong>Not anymore. <strong>ALEXANIAN: </strong>It&#8217;s true. [LAUGHTER]</p>



<p><strong>DIGGANS: </strong>Not anymore. [LAUGHS]</p>



<p><strong>ALEXANIAN:</strong> They used to be tougher. They used to be tougher.</p>



<p><strong>HORVITZ: </strong>James.</p>



<p><strong>DIGGANS:</strong> So I did my undergrad in computer science and microbiology, mostly because at the time, I couldn&#8217;t pick which of the two I liked more. I liked them both. And by the time I graduated, I was lucky enough that I realized that the intersection of the two could be a thing. And so I did a PhD in computational biology, and then I worked for five years at the MITRE Corporation. It’s a nonprofit. I got the chance to work with the US biodefense community and just found an incredible group of people working to protect forces and the population at large from biological threats and just learned a ton about both biology and also dual-use risk. And then so when Twist called me and asked if I wanted to join Twist and set up their biosecurity program, I leapt at the chance and have done that for the past 10 years.</p>



<p><strong>HORVITZ: </strong>Well, thanks everyone.</p>



<p>I believe that AI-powered protein design in particular is one of the most exciting frontiers of modern science. It holds promise for breakthroughs in medicine, public health, even material science. We&#8217;re already seeing it lead to new vaccines, novel therapeutics, and—on the scientific front—powerful insights into the machinery of life.</p>



<p>So there&#8217;s much more ahead, especially in how AI can help us promote wellness, longevity, and the prevention of disease. But before we get too far ahead, while some of our listeners work in bioscience, many may not have a good understanding of some of the foundations.</p>



<p>So, Bruce, can you just give us a high-level overview of proteins? What are they? Why are they important? How do they figure into human-designed applications?</p>



<p><strong>WITTMANN: </strong>Sure. Yeah. Fortunately, I used to TA a class on AI for protein design, so it’s right in my wheelhouse. [LAUGHS]</p>



<p><strong>HORVITZ: </strong>Perfect, perfect background. [LAUGHS]</p>



<p><strong>WITTMANN:</strong>&nbsp;It&#8217;s perfect. Yeah. I got to go back to all of that. Yeah, so from the very basic level, proteins are the workhorses of life.</p>



<p>Every chemical reaction that happens in our body—well, nearly every chemical reaction that happens in our body—most of the structure of our cells, you name it. Any life process, proteins are central to it.</p>



<p>Now proteins are encoded by what are known as … well, I shouldn&#8217;t say encoded. They are <em>constructed</em> from what are called amino acids—there are 20 of them—and depending on the combination and order in which you string these amino acids together, you get a different protein sequence. So that&#8217;s what we mean when we say protein sequence.</p>



<p>The sequence of a protein then determines what shape that protein folds into in a cell, and that shape determines what the protein does. So we will often say sequence determines structure, which determines function.</p>



<p>Now the challenge that we face in engineering proteins is just how many possibilities there are. For all practical purposes, it&#8217;s infinite. So we have 20 building blocks. There are on average around 300 amino acids in a protein. So that&#8217;s 20 to the power of 300 possible combinations. And a common reference point is that it&#8217;s estimated there are around 10 to the 80 particles in the observable universe. So beyond astronomical numbers of possible combinations that we could have, and the job of a protein engineer is to find that one or a few of the proteins within that space that do what we want it to do.</p>



<p>So when a human has an idea of, OK, here&#8217;s what I want a protein to do, we have various techniques of finding that desired protein, one of which is using artificial intelligence and trying to either sift through that milieu of potential proteins or, as we&#8217;ll talk about more in this podcast, physically generating them. So creating them in a way, sampling them out of some distribution of reasonable proteins.</p>



<p><strong>HORVITZ: </strong>Great. So I wanted to throw it to James now to talk about how protein design goes from computer to reality—from in silico to test tubes. What role does <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.twistbioscience.com/" target="_blank" rel="noopener noreferrer">Twist Bioscience<span class="sr-only"> (opens in new tab)</span></a> play in transforming digital protein designs into synthesized proteins? And maybe we can talk also about what safeguards are in place at your company and why do we need them.</p>



<p><strong>DIGGANS: </strong>So all of these proteins that Bruce has described are encoded in DNA. So the language that our cells use to kind of store the information about how to make these proteins is all encoded in DNA. And so if you as an engineer have designed a protein and you want to test it to see if it does what you think it does, the first step is to have the DNA that encodes that protein manufactured, and companies like Twist carry out that role.</p>



<p>So we are cognizant also, however, that these are what are called <em>dual-use technologies</em>. So you can use DNA and proteins for an incredible variety of amazing applications. So drug development, agricultural improvements, bioindustrial manufacturing, all manner of incredible applications. But you could also potentially use those to cause harm so toxins or other, you know, sort of biological misuse.</p>



<p>And so the industry has since at least 2010 recognized that they have a responsibility to make sure that when we&#8217;re asked to make some sequence of DNA that we understand what that thing is encoding and who we&#8217;re giving it for. So we&#8217;re screening both the customer that&#8217;s coming to us and we&#8217;re screening the sequence that they&#8217;re requesting.</p>



<p>And so Twist has long invested in a very, sort of, complicated system for essentially reverse engineering the constructs that we&#8217;re asked to make so that we understand what they are. And then a system where we engage with our customers and make sure that they&#8217;re going to use those for legitimate purpose and responsibly.</p>



<p><strong>HORVITZ: </strong>And how do the emergence of these new generative AI tools influence how you think about risk?</p>



<p><strong>DIGGANS: </strong>A lot of the power of these AI tools is they allow us to make proteins or design proteins that have never existed before in nature to carry out functions that don&#8217;t exist in the natural world. That&#8217;s an extremely powerful capability.</p>



<p>But the existing defensive tools that we use at DNA synthesis companies generally rely on what&#8217;s called homology, similarity to known naturally occurring sequences, to determine whether something might pose risk. And so AI tools kind of break the link between those two things.</p>



<p><strong>HORVITZ: </strong>Now you also serve as chair of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://genesynthesisconsortium.org/" target="_blank" rel="noopener noreferrer">International Gene Synthesis Consortium<span class="sr-only"> (opens in new tab)</span></a>. Can you tell us a little bit more about the IGSC, its mission, how it supports global biosecurity?</p>



<p><strong>DIGGANS: </strong>Certainly. So the IGSC was founded in 2010<a href="#_ftn1" id="_ftnref1">[1]</a> and right now has grown to more than 40 companies and organizations across 10 countries. And the IGSC is essentially a place where companies who might be diehard competitors in the market around nucleic acid synthesis come together and design and develop best practices around biosecurity screening to, kind of, support the shared interest we all have in making sure that these technologies are not subject to misuse.</p>



<p><strong>HORVITZ: </strong>Thanks, James. Now, Tessa, your organization, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/" target="_blank" rel="noopener noreferrer">IBBIS<span class="sr-only"> (opens in new tab)</span></a> is focused—it&#8217;s a beautiful mission—on advancing science while minimizing <em>catastrophic</em> risk, likelihood of <em>catastrophic</em> risk. When we say catastrophic risk, what do we really mean, Tessa, in the context of biology and AI? And how is that … do you view that risk landscape as evolving as AI capabilities are growing?</p>



<p><strong>ALEXANIAN:</strong> I think the … to be honest, as a person who&#8217;s been in biosecurity for a while, I&#8217;ve been surprised by how much of the conversation about the risks from advances in artificial intelligence has centered on the risk of engineered biological weapons and engineered pandemics.</p>



<p>Even recently, there was a new discussion on introducing redlines for AI that came up at the UN General Assembly. And the very first item they list in their list of risks, if I&#8217;m not mistaken, was engineered pandemics, which is exactly the sort of thing that people fear could be done, could be done with these biological AI tools.</p>



<p>Now, I think that when we talk about catastrophic risk, we talk about, you know, something that has an impact on a large percentage of humanity. And I think the reason that we think that biotechnologies pose a catastrophic risk is that we believe there, as we&#8217;ve seen with many historical pandemics, there&#8217;s a possibility for something to emerge or be created that is beyond our society&#8217;s ability to control.</p>



<p>You know, there were a few countries in COVID that managed to more or less successfully do a zero-COVID policy, but that was not, that was not most countries. That was not any of the countries that I lived in. And, you know, we saw millions of people die. And I think we believe that with something like the 1918 influenza, which had a much higher case fatality rate, you could have far more people die.</p>



<p>Now, why we think about this in the context of AI and where this connects to DNA synthesis is that, you know, there is a … these risks of both, sort of, public health risks, pandemic risks, and misuse risks—people deliberately trying to do harm with biology, as we&#8217;ve seen from the long history of biological weapons programs—you know, we think that those might be accelerated in a few different ways by AI technology, both the potential … and I say potential here because as everyone who has worked in a wet lab—which I think is everyone on this call—knows, engineering biology is really difficult. So there&#8217;s maybe a potential for it to become easier to develop biological technology for the purposes of doing harm, and there&#8217;s maybe also the potential to create novel threats.</p>



<p>And so I think people talk about both of those, and people have been looking hard for possible safeguards. And I think one safeguard that exists in this biosecurity world that, for example, doesn&#8217;t exist as cleanly in the cybersecurity world is that none of these biological threats can do harm until they are realized in physical reality, until you actually produce the protein or produce the virus or the microorganism that could do harm. And so I think at this point of production, both in DNA synthesis and elsewhere, we have a chance to introduce safeguards that can have a really large impact on the amount of risk that we&#8217;re facing—as long as we develop those safeguards in a way that keeps pace with AI.</p>



<p><strong>HORVITZ: </strong>Well, thanks, Tessa. So, Bruce, our project began when I posed a challenge to you of the form: could current open-source AI tools be tasked with rewriting toxic protein sequences in a way that preserves their native structure, and might they evade today&#8217;s screening systems?</p>



<p>And I was preparing for a global workshop on AI and biosecurity that I&#8217;d been organizing with Frances Arnold, David Baker, and Lynda Stuart, and I wanted a concrete case study to challenge attendees. And what we found was interesting and deeply concerning.</p>



<p>So I wanted to dive in with you, Bruce, on the technical side. Can you describe some about the generative pipeline and how it works and what you did to build what we might call an AI and biosecurity red-teaming pipeline for testing and securing biosecurity screening tools?</p>



<p><strong>WITTMANN: </strong>Sure. Yeah. I think the best place to start with this is really by analogy.</p>



<p>An analogy I often use in this case is the type of image generation AI tools we&#8217;re all familiar with now where I can tell the AI model, &#8220;Hey, give me a cartoonish picture of a dog playing fetch.&#8221; And it&#8217;ll do that, and it&#8217;ll give us back something that is likely never been seen before, right. That exact image is new, but the theme is still there. The theme is this dog.</p>



<p>And that&#8217;s kind of the same technology that we&#8217;re using in this red-teaming pipeline. Only rather than using plain language, English, we&#8217;re passing in what we would call conditioning information that is relevant to a protein.</p>



<p>So our AI models aren&#8217;t at the point yet where I can say, &#8220;Give me a protein that does <em>x</em>.&#8221; That would be the dream. We&#8217;re a long way from that. But what instead we do is we pass in things that match that theme that we&#8217;re interested in. So rather than saying, &#8220;Hey, give me back the theme on a dog,&#8221; we pass in information that we know will cause or at least push this generative model to create a protein that has the characteristics that we want.</p>



<p>So in the case of that example you just mentioned, Eric, it would be the protein structure. Like I mentioned earlier, we usually say structure determines function. There&#8217;s obviously a lot of nuance to that, but we can, at a first approximation, say structure determines function. So if I ask an AI model, ”Hey, here&#8217;s this structure; give me a protein sequence that folds to this structure,” just like with that analogy with the dog, it&#8217;s going to give me something that matches that structure but that is likely still never been seen before. It&#8217;s going to be a new sequence.</p>



<p>So you can imagine taking this one step further. In the red-teaming pipeline, what we would do is take a protein that should normally be captured by DNA synthesis screening—that <em>would</em> be captured by DNA synthesis screening—find its structure, pass it through one of these models, and get variants on the theme of that structure so these new sequences, these synthetic homologs that you mentioned, <em>paraphrased</em>, <em>reformulated</em>, whatever phrase we want to use to describe them.</p>



<p>And they have a chance or a greater chance than <em>not</em> of maintaining the structure and so maintaining the function while being sufficiently different that they&#8217;re not detected by these tools anymore.</p>



<p>So that&#8217;s the nuts and bolts of how the red-teaming pipeline comes together. We use more tools than just structure. I think structure is the easiest one to understand. But we have a suite of tools in there, each pass different conditioning information that causes the model to generate sequences that are paraphrased versions of potential proteins of concern.<s></s></p>



<p><strong>HORVITZ: </strong>But to get down to brass tacks, what Bruce did for the framing study was … we took the toxic, well-known toxic protein ricin, as we described in a framing paper that&#8217;s actually part of the appendix now to the <em>Science</em> publication, and we generated through this pipeline, composed of open-source tools, thousands of AI-rewritten versions of ricin.</p>



<p>And this brings us to the next step of our project, way back when, at the early … in the early days of this effort, where Twist Bioscience was one of the companies we approached with what must have seemed like an unusual question to your CEO, in fact, James: would you be open to testing whether current screening systems could detect thousands of AI-rewritten versions of ricin, a well-known toxic protein?</p>



<p>And your CEO quickly connected me with you, James. So, James, what were your first thoughts on hearing about this project, and how did you respond to our initial framing study?</p>



<p><strong>DIGGANS: </strong>I think my first response was gratitude and excitement. So it was fantastic that Microsoft had really leaned forward on this set of ideas and had produced this dataset. But to have it, you know, show up on our doorstep in a very concrete way with a partner that was ready to, sort of, help us try and address that, I think was a really … a valuable opportunity. And so we really leapt at that.</p>



<p><strong>HORVITZ:</strong> And the results were that both for you and another company, major producer IDT [Integrated DNA Technologies], those thousands of variants flew through … flew under the radar of the biosecurity screening software as we covered in that framing paper.</p>



<p>Now, after our initial findings on this, we quietly shared the paper with a few trusted contacts, including some in government. Through my work with the White House Office of Science and Technology Policy, or OSTP, we connected up with biosecurity leads there, and it was an OSTP biosecurity lead who described our results as the first <em>zero day</em> in AI and biosecurity. And now in cybersecurity, a zero day is a vulnerability unknown to defenders generally, meaning there&#8217;s no time to respond before it could be exploited should it be known.</p>



<p>In that vein, we took a cybersecurity approach. We stood up a CERT—C-E-R-T—a <em>cybersecurity [computer] emergency response team</em> approach used in responding to cybersecurity vulnerabilities, and we implemented this process to address what we saw as a vulnerability with AI-enabled challenges to biosecurity.</p>



<p>At one point down the line, it was so rewarding to hear you say, James, “I&#8217;m really glad Microsoft got here first.” I&#8217;m curious how you think about this kind of AI-enabled vulnerability compared to other ones, biosecurity threats, you&#8217;ve encountered, and I&#8217;d love to hear your perspective on how we handled the situation from the early discovery to the coordination and outreach.</p>



<p><strong>DIGGANS: </strong>Yeah, I think in terms of comparison known threats, the challenge here is really there is no good basis on which we can just, sort of, say, <em>Oh, I&#8217;ll build a new tool to detect this concrete universe of things</em>, right. This was more a pattern of I&#8217;m going to use tools—and I love the name “Paraphrase”; it&#8217;s a fantastic name—I can paraphrase anything that I would normally think of as biological … as <em>posing</em> biological risk, and now that thing is harder to detect for existing tools. And so that really was a very eye-opening experience, and I think the practice of forming this CERT response, putting together a group of people who were well versed not just in the threat landscape but also in the defensive technologies, and then figuring out how to mitigate that risk and broaden that study, I think, was a really incredibly valuable response to the entire synthesis industry.</p>



<p><strong>HORVITZ: </strong>Yeah, and, Bruce, can you describe a little bit about the process by which we expanded the effort beyond our initial framing study to more toxins and then to a larger challenge set and then the results that we pursued and achieved?</p>



<p><strong>WITTMANN:</strong>&nbsp;Yeah, of course. So, you know, using machine learning lingo, you don&#8217;t want to <em>overfit</em> to a single example. So early on with this, as part of the framing study, we were able to show or I should say James and coworkers across the screening field were able to show that this could be patched, right. We needed to just make some changes to the tools,&nbsp;and we could at the very least detect ricin or reformulated versions of ricin.</p>



<p>So the next step of course was then, OK, how generalizable are these patches? Can they detect other reformulated sequences, as well? So we had to expand the set of proteins that we had reformulated. We couldn&#8217;t just do 10s of thousands of ricins. We had to do 10s of thousands of name your other potentially hazardous …</p>



<p><strong>HORVITZ: </strong>I think we had 72, was it?</p>



<p><strong>WITTMANN:</strong>&nbsp;It was 72 in the end that we ended up at. I believe, James, it was you and maybe Jake, another one of the authors on the list … on the paper, who primarily put that list together &#8230;</p>



<p><strong>HORVITZ: </strong>This is Jacob Beal … Jacob Beal at Raytheon BBN.</p>



<p><strong>WITTMANN: </strong>I think James actually might be the better one to answer how this list was expanded.</p>



<p><strong>DIGGANS:</strong>&nbsp;Initially the focus [was] on ricin as a toxin so that list expanded to 62 sort of commonly controlled toxins that are subject to an export control restriction or other concern. And then on top of that, we added 10 viral proteins. So we didn&#8217;t really just want to look at toxins. We also wanted to look at viral proteins, largely because those proteins tend to have multiple functions. They have highly constrained structures. And so if we could work in a toxin context, could Paraphrase also do the same for viral proteins, as well.</p>



<p><strong>HORVITZ: </strong>And, Bruce, can you describe some about how we characterize the updates and the, we&#8217;ll say, the boost in capabilities of the patched screening tools?</p>



<p><strong>WITTMANN:</strong> So we had, like you said, Eric, 72 base proteins or template proteins. And for each of those, we had generated a few 100 to a couple thousand reformulated variants of them. The only way to really get any sense of validity of those sequences was to predict their structures. So we predicted protein structures for I think it was 70ish thousand protein structures in the end that we had to predict and score them using in silico metrics. So things like, how similar is this to that template, wild-type protein structure that we used as our conditioning information?</p>



<p>We put them on a big grid. So we have two axes. We have on the x-axis—and this is a figure in our paper—the quality of the prediction. It&#8217;s essentially a confidence metric: how realistic is this protein sequence? And on the other axis is, how similar is the predicted structure of this variant to the original? And ultimately, what we were wanting to see was the proteins that scored well in both of those metrics, so that showed up in the top right of that diagram, were caught primarily, because these are again the ones that are <em>most likely</em>, having to say <em>most likely</em>, to retain function of the original.</p>



<p>So when you compare the original tools—Tool Series A, right, the unpatched tools—what you&#8217;ll find is varying degrees of success in the top right. It varied by tool. But in some cases, barely anything being flagged as potentially hazardous. And so improvement is then in the next series—Series B, the patched version of tools—we have more flagged in that upper-right corner.</p>



<p><strong>HORVITZ: </strong>And we felt confident that we had a more AI-resilient screening solution across the companies, and, James, at this point, the whole team decided it was time to disclose the vulnerability as well as the patch details and pointers to where to go for the updated screening software and to communicate this to synthesis companies worldwide via the IGSC. This was probably July, I think, of 2024. What was that process like, and how did members respond?</p>



<p><strong>DIGGANS: </strong>I think members were really grateful and excited. To present to that group, to say, hey, this activity (a) has gone on, (b) was successful, and (c) was kept close hold until we knew how to mitigate this, I think everyone was really gratified by that and comforted by the fact that now they had kind of off-the-shelf solutions that they could use to improve their resilience against any incoming heavily engineered protein designs.</p>



<p><strong>HORVITZ: </strong>Thanks, James.</p>



<p>Now, I know that we all understand this particular effort to be important but a <em>piece</em> of the biosecurity and AI problem. I&#8217;m just curious to … I’ll ask all three of you to just share some brief reflections.</p>



<p>I know, Bruce, you&#8217;ve been on … you’ve stayed on this, and we’ve—all of us on the original team—have other projects going on that are pushing on the frontiers ahead of where we were with this paper when we published it.</p>



<p>Let me start with Tessa in terms of, like, what new risks do you see emerging as AI accelerates and maybe couple that with thoughts about how do we proactively get ahead of them.</p>



<p><strong>ALEXANIAN: </strong>Yeah, I think with the Paraphrase’s work, as Bruce explained so well, you know, I sometimes use the metaphor of the previous response that the IGSC had to do, the synthesis screening community, where it used to be you could look for similarities to DNA sequences, and then everyone started doing synthetic biology where they were doing codon optimization so that proteins could express more efficiently in different host organisms, and now all of a sudden, well, you&#8217;ve scrambled your DNA sequence and it doesn&#8217;t look very similar even though your protein sequence actually still looks, you know, very similar or often the same once it&#8217;s been translated from DNA to protein, and so that was a, you know, many, many in the industry were already screening both DNA and protein, but they had to start screening … <em>everybody</em> had to start screening protein sequences even just to do the similarity testing as these codon optimization tools became universal.</p>



<p>I feel like we&#8217;re, kind of, in a similar transition phase with protein-design, protein-rephrasing, tools where, you know, these tools are still in many cases drawing from the natural distribution of proteins. You know, I think some of the work we saw in, you know, designing novel CRISPR enzymes, you go, OK, yeah, it is novel; it&#8217;s very unlike any <em>one</em> CRISPR enzyme. But if you do a massive multiple sequence alignment of every CRISPR enzyme that we know about, you&#8217;re like, OK, this fits in the distribution of those enzymes. And so, you know, I think we&#8217;re not … we&#8217;re having to do a more flexible form of screening, where we look for things that are kind of within distribution of natural proteins.</p>



<p>But I feel like broadly, all of the screening tools were able to respond by doing something like that. And I think &#8230; I still feel like the clock is ticking down on that and that as the AI tools get better at predicting function and designing, sort of, novel sequences to pursue a particular function, you know—you have tools now that can go from Gene Ontology terms to a potential structure or potential sequence that may again be much farther out of the distribution of natural protein—I think all of us on the screening side are going to have to be responding to that, as well.</p>



<p>So I think I see this as a necessary ongoing engagement between people at the frontier of designing novel biology and people at the frontier of producing all of the materials that allow that novel biology to be tested in the lab. You know, I think this feels like the first, you know, detailed, comprehensive zero day disclosure and response. But I think that&#8217;s … I think we&#8217;re going to see more of those. And I think what I&#8217;m excited about doing at IBBIS is trying to encourage and set up more infrastructure so that you can, as an AI developer, disclose these new discoveries to the people who need to respond before the publication comes out.</p>



<p><strong>HORVITZ: </strong>Thank you, Tessa.</p>



<p>The, the … Bruce, I mean, you and I are working on all sorts of dimensions. You&#8217;re leading up some efforts at Microsoft, for example, on the foundation model front and so on, among other directions. We&#8217;ve talked about new kinds of embedding models that might go beyond sequence and structure. Can you talk a little bit about just a few of the directions that just paint the larger constellation of the kinds of things that we talk about when we put our worry hats on?</p>



<p><strong>WITTMANN:</strong>&nbsp;I feel like that could have its own dedicated podcast, as well. There&#8217;s a lot … [LAUGHTER] there&#8217;s a lot to talk about.</p>



<p><strong>HORVITZ: </strong>Yeah. We want to make sure that we don&#8217;t tell the world that the whole problem is solved here.</p>



<p><strong>WITTMANN:</strong>&nbsp;Right, right, right. I think Tessa said it really, really well in that most of what we&#8217;re doing right now, it&#8217;s a variant on a known theme. I have to know the structure that does something bad to be able to pass it in as context. I have to know some existing sequence that does something bad to pass it in.</p>



<p>And obviously the goal is to move away from that in benign applications, where when I&#8217;m designing something, I often want to design it because nothing exists [LAUGHS] that already does it. So we are going to be heading to this space where we don&#8217;t know what this protein does. It&#8217;s kind of a circular problem, right, where we&#8217;re going to need to be able to predict what some obscure protein sequence does in order to be able to still do our screening.</p>



<p>Now, the way that I think about this, I often think about it beyond just DNA synthesis screening. It&#8217;s one line of defense, and there needs to be many lines of defense that come into play here that go beyond just relying on this one roadblock. It&#8217;s a very powerful roadblock. It&#8217;s a very powerful barrier. But we need to be proactively thinking about how we broaden the scope of defenses. And there are lots of conversations that are ongoing. I won&#8217;t go into the details of them. Again, that would be its own podcast.</p>



<p>But primarily my big push—and I think this is emerging consensus in the field, though I don&#8217;t want to speak for everybody—is it needs to … any interventions we have need to come more at the systems level and less at the model level, primarily because this is such dual-use technology. If it can be used for good biological design, it can be used for bad biological design. Biology has no sense of morality. There is no bad protein. It&#8217;s just <em>a</em> protein.</p>



<p>So we need to think about this differently than how we would maybe think about looking at the outputs of that image generator model that I spoke about earlier, where I can physically look at an image and say, don&#8217;t want my model producing that, do want my model producing that. I don&#8217;t have that luxury in this space. So it&#8217;s a totally different problem. It&#8217;s an evolving problem. Conversations are happening about it, but the work is very much not done.</p>



<p><strong>HORVITZ: </strong>And, James, I want to give you the same open question, but I&#8217;d like to apply what Bruce just said on system level and so on and in the spirit of the kind of things that you&#8217;re very much involved with internationally to also add to it, just get some comments on programs and policies that move beyond technical solutions for governance mechanisms—logging, auditing nucleic acid orders, transparency, various kinds—that might complement technical approaches like Paraphrase and their status today.</p>



<p><strong>DIGGANS: </strong>Yeah, I&#8217;m very gratified that Bruce said that we, the synthesis industry, should not be the sole bulwark against misuse. That is very comforting and correct.</p>



<p>Yeah, so the US government published a guidance document in 2023 that essentially said you, the entire biotech supply chain, have a responsibility to make sure that you&#8217;re evaluating your customers. You should know your customer; you know that they&#8217;re legitimate. I think that&#8217;s an important practice.</p>



<p>Export controls are designed to minimize the movement of equipment and materials that can be used in support of these kinds of misuse activities. And then governments have really been quite active in trying to incentivize, you know, sort of what we would think of as positive behavior, so screening, for example, in DNA synthesis companies. The US government created a framework in 2024, and it&#8217;s under a rewrite now to basically say US research dollars will only go to companies who make nucleic acid who do these good things. And so that is using, kind of, the government-funding carrot to, kind of, continue to build these layers of defense against potential misuse.</p>



<p><strong>HORVITZ: </strong>Thanks. Now, discussing risk, especially when it involves AI and biosecurity, isn&#8217;t always easy. As we&#8217;ve all been suggesting, some worry about alarming the public or arming bad actors. Others advocate for openness as a principle of doing science with integrity.</p>



<p>A phase of our work as we prepared our paper was giving serious thought to both the benefits and the risks of transparency about what it was that we were doing. Some experts encouraged full disclosure as important for enhancing the science of biosecurity. Other experts, <em>all experts</em>, cautioned against what are called <em>information hazards</em>, the risk of sharing the details to enable malevolent actions with our findings or our approach.</p>



<p>So we faced a real question: how can we support open science while minimizing the risk of misuse? And we took all the input we got, even if it was contradictory, very seriously. We carefully deliberated about a good balance, and even <em>then</em>, once we chose our balance and submitted our manuscript to <em>Science</em>, the peer reviewers came back and said they wanted some of the more sensitive details that we withheld with explanations as to why.</p>



<p>So this provoked some thinking out of the box about a novel approach, and we came up with a perpetual gatekeeping strategy where requests for access to sensitive methods and data and even the software across different risk categories would be carefully reviewed by a committee and a process for access that would continue in perpetuity.</p>



<p>Now, we brought the proposal to Tessa and her team at IBBIS—this is a great nonprofit group; look at their mission—and we worked with Tessa and her colleagues to refine a workable solution that was accepted by <em>Science</em> magazine as a new approach to handling information hazards as first demonstrated by our paper.</p>



<p>So, Tessa, thank you again for helping us to navigate such a complex challenge. Can you share your perspective on information hazards? And then walk us through how our proposed system ensures responsible data and software sharing.</p>



<p><strong>ALEXANIAN: </strong>Yeah. And thanks, Eric.</p>



<p>It&#8217;s all of the long discussions we had among the group of people on this podcast and the other authors on the paper and many people we engaged, you know, technical experts, people in various governments, you know, we heard a lot of contradictory advice.</p>



<p>And I think it showed us that there isn&#8217;t a consensus right now on how to handle information hazards in biotechnology. You know, I think … I don&#8217;t want to overstate how much of a consensus there is in cybersecurity either. If you go to DEF CON, you&#8217;ll hear people about how they&#8217;ve been mistreated in their attempts to do responsible disclosure for pacemakers and whatnot. But I think we&#8217;re … we have even less of a consensus when it comes to handling biological information.</p>



<p>You know, you have some people who say, oh, because the size of the consequences could be so catastrophic if someone, you know, releases an engineered flu or something, you know, we should just never share information about this. And then you have other people who say there&#8217;s no possibility of building defenses unless we share information about this. And we heard very strong voices with both of those perspectives in the process of conducting this study.</p>



<p>And I think what we landed on that I&#8217;m really excited about and really excited to get feedback on now that the paper is out, you know, if you go and compare our preprint, which came out in December of 2024, and this paper in October 2025, you&#8217;ll see a lot of information got added back in.</p>



<p>And I&#8217;m excited to see people&#8217;s reaction to that because even back in January 2025, talking with people who were signatories to the responsible biodesign commitments, they were really excited that this was such an empirically concrete paper because they&#8217;d maybe read a number of papers talking about biosecurity risks from AI that didn&#8217;t include a whole lot of data, you know, often, I think, because of concerns about information hazards. And they found the arguments in this paper are much more convincing because we are able to share data.</p>



<p>So the process we underwent that I felt good about was trying to really clearly articulate, when we talk about an information hazard, what are we worried about being done with this data? And if we put this data in public, completely open source, does it shift the risk at all? You know, I think doing that kind of marginal contribution comparison is really important because it also let us make more things available publicly.</p>



<p>But there were a few tiers of data that after a lot of discussion amongst the authors of the paper, we thought, OK, potentially someone who wanted to do harm, if they got access to this data, it might make it easier for them. Again, not necessarily saying it, you know, it opens the floodgates, but it might make it easier for them. And when we thought about that, we thought, OK, you know, giving all of those paraphrased protein sequences, maybe, maybe that, you know, compared to having to set up the whole pipeline with the open-source tools yourself, just giving you those protein sequences, maybe that makes your life a bit easier if you&#8217;re trying to do harm.</p>



<p>And then we thought, OK, giving you those protein sequences plus whether or not they were successfully flagged, maybe that makes your life, you know, quite a bit easier. And then finally, we thought, OK, the code that we want to share with some people who might try to reproduce these results or might try to build new screening systems that are more robust, we want to share the code with them. But again, if you have that whole code pipeline just prepared for you, it might really help make your life easier if you&#8217;re trying to do harm.</p>



<p>And so we, sort of, sorted the data into these three tiers and then went through a process actually very inspired by the existing customer screening processes in nucleic acid synthesis about how to determine, you know, we tried to take an approach not of what gets you in but what gets you out. You know, for the most part, we think it should be possible to access this data.</p>



<p>You know, if you have an affiliation with a recognizable institution or some good explanation of why you don&#8217;t have one right now, you know, if you have a reason for accessing this data, it shouldn&#8217;t be too hard to meet those requirements, but we wanted to have some in place. And we wanted it to be possible to rule out some people from getting access to this data. And so we&#8217;ve tried to be extremely transparent about what those are. If you go through our data access process and for some reason you get rejected, you&#8217;ll get a list of, &#8220;Here&#8217;s the reasons we rejected you. If you don&#8217;t think that&#8217;s right, get back to us.&#8221;</p>



<p>So I&#8217;m really excited to pilot this in part because I think, you know, we&#8217;re already in conversations with some other people handling potential bio-AI information hazard about doing a similar process for their data of, you know, tiering it, determining which gates to put in which tiers, but I really hope a number of people do get access through the process or if they try and they fail, they tell us why. Because I think as we move toward this world of potentially, you know, biology that is much easier to engineer, partly due to dual-use tools, you know, my dream is it&#8217;s, like, still hard to engineer harm with biology, even if it&#8217;s really easy to engineer biology. And I think these, kind of, new processes for managing access to things, this sort of like, you know, open but not completely public, I think those can be a big part of that layered defense.</p>



<p><strong>HORVITZ: </strong>Thanks, Tessa. So we&#8217;re getting close to closing, and I just thought I would ask each of you to just share some reflections on what we&#8217;ve learned, the process we&#8217;ve demonstrated, the tools, the policy work that we did, this idea of facing the dual-use dilemma with … even at the information hazard level, with sharing information versus withholding it. What do you think about how our whole end to end of the study, now reaching the two-year point, can help other fields facing dual-use dilemmas?</p>



<p>Tessa, Bruce, James … James, have you ever thought about that? And we&#8217;ll go to Bruce and then Tessa.</p>



<p><strong>DIGGANS:</strong>&nbsp;Yeah, I think it was an excellent model. I would like to see a study like this repeated on a schedule, you know, every six months because from where I sit, you know, the tools that we used for this project are now two years old. And so capabilities have moved on. Is the picture the same in terms of defensive capability? And so using that model over time, I think, would be incredibly valuable. And then using the findings to chart, you know, how much should we be investing in alternative strategies for this kind of risk mitigation for AI tool … the <em>products</em> of AI tools?</p>



<p><strong>HORVITZ: </strong>Bruce.</p>



<p><strong>WITTMANN:</strong>&nbsp;Yeah, I think I would extend on what James said. The anecdote I like to point out about this project is, kind of, our schedule. We found the vulnerability and it was patched within a week, two weeks, on all major synthesis screening platforms. We wrote the paper within a month. We expanded on the paper within two months, and then we spent a year and a half to nearly two years [LAUGHS] trying to figure out what goes into the paper; how do we release this information; you know, how do we do this responsibly?</p>



<p>And my hope is similar to what James said. We&#8217;ve made it easier for others to do this type of work. Not this exact work; it doesn&#8217;t have to necessarily do with proteins. But to do this type of work where you are dealing with potential hazards but there is also value in sharing and that hopefully that year and a half we spent figuring out how to appropriately share and what to share will not be a year and a half for other teams because these systems are in place or at least there is an example to follow up from. So that&#8217;s my takeaway.</p>



<p><strong>HORVITZ:</strong> Tessa, bring us home—<em>bring us home!</em> [LAUGHS]</p>



<p><strong>ALEXANIAN: </strong><em>Bring us home!</em> Let&#8217;s do it faster next time. [LAUGHTER] Come talk to any of us if you&#8217;re dealing with this kind of stuff. You know, I think IBBIS, especially, we want to be a partner for building those layers of defense and, you know, having ripped out our hair as a collective over the past year and a half about the right process to follow here, I think we all really hope it&#8217;ll be faster next time.</p>



<p>And I think, you know, the other thing I would encourage is if you&#8217;re an AI developer, I would encourage you to think about how your tool can strengthen screening and strengthen recognition of threats.</p>



<p>I know James and I have talked before about how, you know, our Google search alerts each week send us dozens of cool AI bio papers, and it&#8217;s more like once a year or maybe once every six months, if we&#8217;re lucky, that we get something that&#8217;s like applying AI bio to biosecurity. So, you know, if you&#8217;re interested in these threats, I think we&#8217;d love to see more work that&#8217;s directly applied to facing these threats using the most modern technology.</p>



<p><strong>HORVITZ: </strong>Well said.</p>



<p>Well, Bruce, James, Tessa, thank you so much for joining me today and for representing the many collaborators, both coauthors and beyond, who made this project possible.</p>



<p>It&#8217;s been a true pleasure to work with you. I&#8217;m so excited about what we&#8217;ve accomplished, the processes and the models that we&#8217;re now sharing with the world. And I&#8217;m deeply grateful for the collective intelligence and dedication that really powered the effort from the very beginning. So thanks again.</p>



<p>[MUSIC]</p>



<p><strong>WITTMANN: </strong>Thanks, Eric.</p>



<p><strong>DIGGANS: </strong>Thank you.</p>



<p><strong>ALEXANIAN: </strong>Thank you.</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><a href="#_ftnref1" id="_ftn1">[1]</a> The original organization was founded in 2009 and became the International Gene Synthesis Consortium in 2010.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-more-ai-resilient-biosecurity-with-the-paraphrase-project/">Ideas: More AI-resilient biosecurity with the Paraphrase Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>When AI Meets Biology: Promise, Risk, and Responsibility</title>
		<link>https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/</link>
		
		<dc:creator><![CDATA[Eric Horvitz]]></dc:creator>
		<pubDate>Mon, 06 Oct 2025 14:03:54 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1150818</guid>

					<description><![CDATA[<p>Microsoft researchers reveal a confidential research effort that explored how open-source AI tools could be used to bypass biosecurity checks—and helped create fixes now influencing global standards.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI Meets Biology: Promise, Risk, and Responsibility</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1920" height="1080" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png" alt="Paraphrase Project Protiens" class="wp-image-1151098" style="object-fit:cover" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04.png 1920w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-1024x576.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-768x432.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MS-Paraphrase_Frame4-HeroStill_1920-1080_V04-1536x864.png 1536w" sizes="auto, (max-width: 1920px) 100vw, 1920px" /></figure>



<p>Advances in AI are opening extraordinary frontiers in biology. AI-assisted protein engineering holds the promise of new medicines, materials, and breakthroughs in scientific understandings. Yet these same technologies also introduce biosecurity risks and may lower barriers to designing harmful toxins or pathogens. This “dual-use” potential, where the same knowledge can be harnessed for good or misuse to cause harm, poses a critical dilemma for modern science.</p>



<h2 class="wp-block-heading" id="great-promise-and-potential-threat">Great Promise—and Potential Threat</h2>



<p>I’m excited about the potential for AI-assisted protein design to drive breakthroughs in biology and medicine. At the same time, I’ve also studied how these tools could be misused. In computer-based studies, we found that AI protein design (AIPD) tools could generate modified versions of proteins of concern, such as ricin. Alarmingly, these reformulated proteins were able to evade the biosecurity screening systems used by DNA synthesis companies, which scientists rely on to synthesize AI-generated sequences for experimental use. </p>



<p>In our paper published in <em>Science</em> on October 2, “<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.science.org/doi/10.1126/science.adu8578" target="_blank" rel="noopener noreferrer">Strengthening nucleic acid biosecurity screening against generative protein design tools<span class="sr-only"> (opens in new tab)</span></a>,” we describe a two-year confidential project we began in late 2023 while preparing a case study for a workshop on AI and biosecurity.</p>



<p>We worked confidentially with partners across organizations and sectors for 10 months to develop AI biosecurity “red-teaming” methods that allowed us to better understand vulnerabilities and craft practical solutions—&#8221;patches” that have now been adopted globally, making screening systems significantly more AI-resilient.</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1788" height="693" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png" alt="An illustration of the AI Protein Design red-teaming workflow. [starting at the left] an icon of a database with the heading above that reads: Database of Wild-Type Proteins of Concern. [arrow moves right] Above the arrow the text reads: Generate Synthetic Homologs (x) Conditioned on Wild Types (y). P(x|y) appears below the arrow. [continuing to the right] a computer monitor icon with protein sequences on the screen appears in brackets with N appearing outside the bottom of the right bracket. The text above the computer screen reads: “N” Synthetic Homologs per Wild-Type. [arrows move to the right and fork to an upper arrow and a lower arrow] The text above the upper arrow reads Reverse Translate and the arrow points to a computer monitor icon with a DNA icon on the screen. [upper arrow continues to the right] The arrow points to a computer monitor icon with the text Hazard Screening appearing above and a biohazard icon and a question mark appearing on the screen. [lower arrow moves to the right] A computer monitor icon includes a paraphrased toxin sequence verses a protein sequence on the computer screen. Above the monitor the text reads: Score in silico. [lower arrow continues to the right] An illustration provides an example of the evaluation results (see also table S1 in the paper) tracking the number of flagged sequences (y-axis) and hazardous sequences (x-axis). [the lower arrow moves up to the Hazard Screening step (from the upper arrow process) and another arrow moves from the Hazard Screening to the evaluation results illustration. There is a dotted line with the words Repeat Process moving from the Evaluation illustration to the left and back to the database." class="wp-image-1151201" style="width:812px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard.png 1788w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-300x116.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-1024x397.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-768x298.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-1536x595.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/Fig1_Biohazard-240x93.png 240w" sizes="auto, (max-width: 1788px) 100vw, 1788px" /><figcaption class="wp-element-caption">Summary of AIPD red-teaming workflow.</figcaption></figure>



<p>For structuring, methods, and process in our study, we took inspiration from the cybersecurity community, where “zero-day” vulnerabilities are kept confidential until a protective patch is developed and deployed. Following the acknowledgment by a small group of workshop attendees of a zero-day for AI in biology, we worked closely with stakeholders—including synthesis companies, biosecurity organizations, and policymakers—to rapidly create and distribute patches that improved detection of AI-redesigned protein sequences. We delayed public disclosure until protective measures were in place and widely adopted.</p>



<h2 class="wp-block-heading" id="dilemma-of-disclosure">Dilemma of Disclosure</h2>



<p>The dual use dilemma also complicates how we share information about vulnerabilities and safeguards. Across AI and other fields, researchers face a core question: </p>



<blockquote class="wp-block-quote is-style-spectrum is-layout-flow wp-block-quote-is-layout-flow">
<p>How can scientists share potentially risk-revealing methods and results in ways that enable progress without offering a roadmap for misuse?</p>
</blockquote>



<p>We recognized that our work itself—detailing methods and failure modes—could be exploited by malicious actors if published openly. To guide decisions about what to share, we held a multi-stakeholder deliberation involving government agencies, international biosecurity organizations, and policy experts. Opinions varied: some urged full transparency to maximize reproducibility—and to help others to build on our work; others stressed restraint to minimize risk. It was clear that a <em>new model of scientific communication</em> was needed, one that could balance openness and security.</p>



<h2 class="wp-block-heading" id="the-novel-framework">The Novel Framework</h2>



<p>The risk of sharing dangerous information through biological research has become a growing concern. We have participated in community-wide discussion on the challenges, including a recent National Academies of Science, Engineering, and Medicine workshop and study.&nbsp;</p>



<p>In preparing our manuscript for publication, we worked on designing a process to limit the spread of dangerous information while still enabling scientific progress.&nbsp;</p>



<p>To address the dual challenges, we devised a tiered access system for data and methods, implemented in partnership with the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ibbis.bio/" target="_blank" rel="noopener noreferrer">International Biosecurity and Biosafety Initiative for Science (IBBIS)<span class="sr-only"> (opens in new tab)</span></a>, a nonprofit dedicated to advancing science while reducing catastrophic risks. The system works as follows:</p>



<ul class="wp-block-list">
<li><strong>Controlled access</strong>: Researchers can request access through IBBIS, providing their identity, affiliation, and intended use. Requests are reviewed by an expert biosecurity committee, ensuring that only legitimate scientists conducting relevant research gain access.</li>



<li><strong>Stratified tiers of information</strong>: Data and code are classified into several tiers according to their potential hazard, from low-risk summaries through sensitive technical data to critical software pipelines.</li>



<li><strong>Safeguards and agreements</strong>: Approved users sign tailored usage agreements, including non-disclosure terms, before receiving data.</li>



<li><strong>Resilience and longevity</strong>: Provisions are built in for declassification when risks subside, and for succession of stewardship to trusted organizations should IBBIS be unable to continue its operation.</li>
</ul>



<p>This framework allows replication and extension of our work while guarding against misuse. Rather than relying on secrecy, it provides a durable system of responsible access.</p>



<p>To ensure continued funding for the storage and responsible distribution of sensitive data and software, and for the operation of the sharing program, we provided an endowment to IBBIS to support the program <em>in perpetuity</em>. This approach was modeled after the One Hundred Year Study on AI at Stanford, which is endowed to continue for the life of the university.</p>



<h2 class="wp-block-heading" id="an-important-step-in-scientific-publishing">An Important Step in Scientific Publishing</h2>



<p>We are pleased that the leadership at <em>Science</em> accepted our approach to handling information hazards. To our knowledge, this is the first time a leading scientific journal has formally endorsed a tiered-access approach to manage an information hazard. This recognition validates the idea that rigorous science and responsible risk management can coexist—and that journals, too, can play a role in shaping how sensitive knowledge is shared. We acknowledge the visionary leadership at <em>Science,</em> including editors, Michael Funk and Valda Vinson, and Editor-in-Chief, Holden Thorp.</p>



<h2 class="wp-block-heading" id="beyond-biology-a-model-for-sensitive-research">Beyond Biology: A Model for Sensitive Research</h2>



<p>While developed for AI-powered protein design, our approach offers a generalizable model for dual-use research of concern (DURC) across disciplines. Whether in biology, chemistry, or emerging technologies, scientists will increasingly confront situations where openness and security pull in opposite directions. Our experience shows that these values can be balanced: with creativity, coordination, and new institutional mechanisms, science can uphold both reproducibility and responsibility.</p>



<p>We hope this framework becomes a template for future projects, offering a way forward for researchers who wish to share their insights without amplifying risks. By embedding resilience into <em>how</em> knowledge is communicated—not just <em>what</em> is communicated—we can ensure that scientific progress continues to serve humanity safely.</p>



<p>The responsible management of information hazards is no longer a peripheral concern: it is central to how science will advance in the age of powerful technologies like AI. This approach to managing information hazards demonstrates a path forward, where novel frameworks for access and stewardship allow sensitive but vital research to be shared, scrutinized, and extended responsibly. Approaches like this will be critical to ensuring that scientific openness and societal safety advance hand-in-hand.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h3 class="wp-block-heading" id="additional-reading">Additional reading</h3>



<p><a href="https://www.microsoft.com/en-us/research/publication/strengthening-nucleic-acid-biosecurity-screening-against-generative-protein-design-tools/" target="_blank" rel="noreferrer noopener"><em>Strengthening nucleic acid biosecurity screening against generative protein design tools</em>.</a></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://nap.nationalacademies.org/28868">The Age of AI in the Life Sciences: Benefits and Biosecurity Considerations, National Academies of Science, Engineering, and Medicine, 2025.<span class="sr-only"> (opens in new tab)</span></a></em></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nap.nationalacademies.org/catalog/29174/disseminating-in-silico-and-computational-biological-research-navigating-benefits-and" target="_blank" rel="noopener noreferrer">Disseminating In Silico and Computational Biological Research: Navigating Benefits and Risks: Proceedings of a Workshop, National Academies of Science, Engineering, and Medicine, 2025.<span class="sr-only"> (opens in new tab)</span></a></em></p>



<p><em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.erichorvitz.com/scientific_integrity.htm" target="_blank" rel="noopener noreferrer">Protecting scientific integrity in an age of generative AI, Proceedings of the National Academy of Science, 2024.<span class="sr-only"> (opens in new tab)</span></a></em></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/when-ai-meets-biology-promise-risk-and-responsibility/">When AI Meets Biology: Promise, Risk, and Responsibility</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Using AI to assist in rare disease diagnosis</title>
		<link>https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/</link>
		
		<dc:creator><![CDATA[Mandi Hall, Ashley Conard]]></dc:creator>
		<pubDate>Mon, 22 Sep 2025 14:17:03 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1142402</guid>

					<description><![CDATA[<p>New research from Microsoft, Drexel, and the Broad explores how generative AI could support genetic professionals in rare disease diagnosis.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg" alt="Icons representing individual and group connections to a central computer monitor with a globe, symbolizing online connectivity, set against a gradient background transitioning from blue to pink." class="wp-image-1143080" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;</p>



<p>To better understand and address these challenges, Microsoft Research—in collaboration with Drexel University and the Broad Institute​​—conducted a comprehensive study titled <em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals<span class="sr-only"> (opens in new tab)</span></a>.</em> The study was recently published in a special edition of <em>ACM Transactions on Interactive Intelligent Systems</em> journal focused on generative AI.  </p>



<p>The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;​&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;​We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseases—ultimately aiming to improve patients’ quality of life and life expectancy.</p>



<h2 class="wp-block-heading" id="whole-genome-sequencing-in-rare-disease-diagnosis">Whole genome sequencing in rare disease diagnosis</h2>



<p>Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patient’s DNA sequence to reference genomes.&nbsp;​​Genetic professionals use bioinformatics tools such as&nbsp;<em>seqr,&nbsp;</em>an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;</p>



<p>​​The multi-step sequencing process​​​&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;​​to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysis​​. This creates a backlog of patient cases​​. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;</p>



<h2 class="wp-block-heading" id="the-promise-of-ai-systems-to-assist-with-complex-human-tasks">The promise of AI systems to assist with complex human tasks</h2>



<p>Approximately 87% of AI systems never reach deployment&nbsp;​simply because they solve​​​&nbsp;the wrong problems.&nbsp;​​Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;​To design an effective​​​&nbsp;AI-based system​, one needs to identify​&nbsp;​​tasks AI can support,&nbsp;​​determine​​​​​​&nbsp;the appropriate level of AI involvement, and&nbsp;​​design​​​​​​&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;​​AI&nbsp;can best be incorporated into workflows and tools.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="study-objectives-and-co-designing-a-genetic-ai-assistant">Study objectives and co-designing a genetic AI assistant</h2>



<p>Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.</p>



<h2 class="wp-block-heading" id="identifying-challenges-in-whole-genome-sequencing-analysis">Identifying challenges in whole genome sequencing analysis</h2>



<p>Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:</p>



<ol class="wp-block-list">
<li><em>Information Overload</em>: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.</li>



<li><em>Collaborative Sharing</em>: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.</li>



<li><em>Prioritizing Reanalysis</em>: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.</li>
</ol>



<p>Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.</p>



<h2 class="wp-block-heading" id="co-designing-with-experts-and-ai-human-sensemaking-tasks">Co-designing with experts and AI-human sensemaking tasks</h2>



<p>Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.</p>



<p>Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;​​approach not only&nbsp;​underscores​​​&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.</p>



<h2 class="wp-block-heading" id="design-implications-for-expert-ai-sensemaking">Design implications for expert-AI sensemaking</h2>



<p>In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilities​​. The task of staying&nbsp;​​current​​​​​​, synthesizing information from diverse sources, and making informed decisions&nbsp;​​is challenging​​​​​​.&nbsp;&nbsp;</p>



<p>Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;​​particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.</p>



<h2 class="wp-block-heading" id="distributed-expert-ai-sensemaking-design">Distributed expert-AI sensemaking design</h2>



<p>Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;​​​​​​​​​​Trust is gained by​​​​​​&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated information​​.&nbsp;​​Some​​​​​​&nbsp;users​, however,​&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;​​and annotated​​​​​​. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the process​​.&nbsp;</p>



<h2 class="wp-block-heading" id="initial-expert-ai-sensemaking-and-re-sensemaking-design">Initial expert-AI sensemaking and re-sensemaking design</h2>



<p>In our fast-paced, information-driven world,&nbsp;​​it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.​​&nbsp;​​Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing users—both&nbsp;original decision-makers or new individuals—to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;​​transparency​​​​​​&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;</p>



<h2 class="wp-block-heading" id="combining-evidence-from-multiple-modalities-to-enhance-ai-expert-sensemaking">Combining evidence from multiple modalities to enhance AI-expert sensemaking</h2>



<p>​​​The​​​​​​&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of data—text, images, spatial coordinates, and more—into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;​​could create​​​​​​&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;​​context​​​​​​&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;</p>



<h2 class="wp-block-heading" id="sensemaking-process-with-ai-assistant">Sensemaking Process with AI Assistant</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="952" height="481" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png" alt="Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines." class="wp-image-1142535" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png 952w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-300x152.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-768x388.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-240x121.png 240w" sizes="auto, (max-width: 952px) 100vw, 952px" /><figcaption class="wp-element-caption">Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.</figcaption></figure>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>We explored the potential of generative AI&nbsp;to support​​ genetic professionals​&nbsp;​in diagnosing rare diseases​​. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;​pinpointing​​​&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is needed​​. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;</p>



<h2 class="wp-block-heading" id="join-the-conversation">Join the conversation</h2>



<p>Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">here<span class="sr-only"> (opens in new tab)</span></a>. Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.biorxiv.org/content/10.1101/2025.03.10.642480v1" target="_blank" rel="noopener noreferrer">Evidence Aggregator: AI reasoning applied to rare disease diagnosis.<span class="sr-only"> (opens in new tab)</span></a>&nbsp;&nbsp;</p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tool-space interference in the MCP era: Designing for agent compatibility at scale</title>
		<link>https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/</link>
		
		<dc:creator><![CDATA[Adam Fourney, Tyler Payne, Maya Murad, Saleema Amershi]]></dc:creator>
		<pubDate>Thu, 11 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149210</guid>

					<description><![CDATA[<p>As agentic AI ushers in a new era marked by tool expansion, systems are converging, and complexity is rising. Microsoft Research explores the Model Context Protocol (MCP) as a new standard for agent collaboration across fragmented tool ecosystems.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Three white icons on a gradient background transitioning from blue to purple to pink. From left to right: a globe with a magnifying glass representing internet search, a central circle connected to smaller circles symbolizing network connectivity, and a checklist with two checkmarks and one empty box indicating task management." class="wp-image-1149369" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<p>This year&nbsp;we’ve&nbsp;seen&nbsp;remarkable&nbsp;advances in agentic AI, including&nbsp;systems that conduct deep research,&nbsp;operate&nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&nbsp;multi-step goals. In each case,&nbsp;the industry relied&nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&nbsp;for peak&nbsp;performance. For example,&nbsp;OpenAI&#8217;s&nbsp;recent models&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/openai/gpt-oss?tab=readme-ov-file#tools" target="_blank" rel="noopener noreferrer">presume&nbsp;the&nbsp;availability&nbsp;of web search and document retrieval&nbsp;tools<span class="sr-only"> (opens in new tab)</span></a>. Likewise,&nbsp;the prompts and actions&nbsp;of&nbsp;<a href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/" target="_blank" rel="noreferrer noopener">Magentic-One</a>&nbsp;are&nbsp;set up to make hand-offs easy—for example, allowing the WebSurfer agent to pass downloaded files to the Coder agent. &nbsp;But as agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&nbsp;Agents&nbsp;from&nbsp;different&nbsp;developers&nbsp;or companies will&nbsp;increasingly&nbsp;encounter&nbsp;each other and&nbsp;must&nbsp;work together to complete tasks, in what we refer to as a&nbsp;<em>society of agents</em>.&nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&nbsp;in this&nbsp;setting, or will they hinder one another and slow progress?</p>



<p>Early clues have&nbsp;emerged&nbsp;from an&nbsp;unexpected&nbsp;source:&nbsp;namely,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer">Model Context Protocol<span class="sr-only"> (opens in new tab)</span></a>&nbsp;(MCP). Since January 2025, MCP has grown from a&nbsp;promising spec to a&nbsp;thriving&nbsp;market&nbsp;of&nbsp;tool&nbsp;servers.&nbsp;As an example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://docs.zapier.com/mcp/home" target="_blank" rel="noopener noreferrer">Zapier boasts a catalog of 30,000 tools<span class="sr-only"> (opens in new tab)</span></a>&nbsp;across 7,000 services.&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://mcp.composio.dev/" target="_blank" rel="noopener noreferrer">Composio&nbsp;provide over 100 managed MCP servers<span class="sr-only"> (opens in new tab)</span></a>, surfacing hundreds of tools. Hugging&nbsp;Face is now serving&nbsp;many&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/spaces?filter=mcp-server" target="_blank" rel="noopener noreferrer">Spaces&nbsp;apps over MCP<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://shopify.dev/docs/apps/build/storefront-mcp/servers/storefront" target="_blank" rel="noopener noreferrer">Shopify has enabled MCP for millions of storefronts<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;A&nbsp;society of&nbsp;<em>tools</em>&nbsp;is already here, and it promises to&nbsp;extend&nbsp;agent capabilities through&nbsp;cross-provider&nbsp;horizontal integration.&nbsp;</p>



<p>So,&nbsp;what does MCP have to say about&nbsp;horizontal integration? As catalogs grow,&nbsp;we expect some new failure modes to surface.&nbsp;This&nbsp;blog&nbsp;post introduces&nbsp;these&nbsp;as <em>tool-space interference</em>, and sketches both early observations and some pragmatic interventions to keep the society&nbsp;we’re&nbsp;building&nbsp;from stepping on its own feet.&nbsp;</p>



<p>Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.</p>



<h2 class="wp-block-heading" id="a-framing-example">A framing example</h2>



<p>Consider MCP as a means for extending <a href="https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/">Magentic-One</a>, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won’t change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1021" height="410" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png" alt="Diagram depicting Magentic-One's multi-agentic architecture. An Orchestrator agent has access to 4 specialized sub-agents: a Coder agent that can write code and reason to sol solve tasks, a Computer Terminal Agent that can execute code written by the Coder agent, a WebSurfer agent that browse the internet (navigate pages, fill forms, etc), and a FileSurfer agent that can navigate files (e.g. PDFs, PPTx, etc). The diagram is annotated to show that for any incoming git-related task, the Orchestrator agent has to decide at evert orchestration step whether to access Git CLI via ComputerTerminal, visit Github site via WebSurfer, or directly access Github’s MCP server." class="wp-image-1149211" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png 1021w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-300x120.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-768x308.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-240x96.png 240w" sizes="auto, (max-width: 1021px) 100vw, 1021px" /><figcaption class="wp-element-caption">Figure 1: We can extend&nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&nbsp;&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="tool-space-interference-through-the-lens-of-mcp">Tool-space interference, through the lens of MCP</h2>



<p>To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://smithery.ai/">smithery.ai<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://hub.docker.com/mcp">Docker MCP Hub<span class="sr-only"> (opens in new tab)</span></a>. Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.</p>



<p>To&nbsp;automate the&nbsp;inspection&nbsp;of&nbsp;running MCP servers,&nbsp;we developed an&nbsp;MCP&nbsp;Interviewer&nbsp;tool.&nbsp;The MCP&nbsp;Interviewer&nbsp;begins by cataloging the server’s tools, prompts, resources, resource templates, and capabilities.&nbsp;From&nbsp;this catalog we can compute&nbsp;descriptive statistics&nbsp;such as the number of tools, or the depth of the parameter&nbsp;schemas.&nbsp;&nbsp;Then, given the list of available tools, the interviewer uses&nbsp;an LLM (in our case,&nbsp;OpenAI&#8217;s GPT-4.1)&nbsp;to construct a functional testing&nbsp;plan&nbsp;that&nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&nbsp;the&nbsp;interviewer&nbsp;can&nbsp;also&nbsp;grade&nbsp;more qualitative&nbsp;criteria&nbsp;by&nbsp;using&nbsp;an LLM&nbsp;to&nbsp;apply purpose-built rubrics&nbsp;to&nbsp;tool&nbsp;schemas&nbsp;and&nbsp;tool call outputs.&nbsp;&nbsp;We are excited to&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">release the MCP Interviewer&nbsp;as an open-source CLI&nbsp;tool<span class="sr-only"> (opens in new tab)</span></a>, so server developers can automatically evaluate their MCP servers with agent usability in mind,&nbsp;and users can&nbsp;validate&nbsp;new servers.&nbsp;</p>



<p>While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.</p>



<h3 class="wp-block-heading" id="one-size-fits-all-but-some-more-than-others">One-size fits all (but some more than others)</h3>



<p>So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">OpenAI’s guide on function calling<span class="sr-only"> (opens in new tab)</span></a> advises developers to:</p>



<p>“<em>Include examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).”</em></p>



<p>So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let’s dive into more numbers.</p>



<h3 class="wp-block-heading" id="tool-count">Tool count</h3>



<p>While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">recommends<span class="sr-only"> (opens in new tab)</span></a> that developers:</p>



<p>“<em>Keep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.</em>”</p>



<p>While we expect this to improve with each new model generation, at present, large tool spaces can <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">lower performance by up to 85% for some models<span class="sr-only"> (opens in new tab)</span></a>. Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/microsoft/playwright-mcp">Playwright-MCP<span class="sr-only"> (opens in new tab)</span></a> (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.</p>



<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png" alt="chart" class="wp-image-1149361" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-360x360.png 360w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.</figcaption></figure>



<h3 class="wp-block-heading" id="response-length">Response length</h3>



<p>Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://research.trychroma.com/context-rot">practical limits can be much lower<span class="sr-only"> (opens in new tab)</span></a>. MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/tiktoken"><span class="sr-only"> (opens in new tab)</span></a>, some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">up to 91% in one study<span class="sr-only"> (opens in new tab)</span></a>), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td></td><td></td><td colspan="4"><strong># of tools that would overflow context in</strong></td></tr><tr><td><strong>Model</strong></td><td><strong>Context Window</strong></td><td><strong>1 call</strong></td><td><strong>2 calls</strong></td><td><strong>3-5 calls</strong></td><td><strong>6-10 calls</strong></td></tr><tr><td>GPT 4.1</td><td>1,000,000</td><td>0</td><td>1</td><td>7</td><td>11</td></tr><tr><td>GPT 5</td><td>400,000</td><td>1</td><td>7</td><td>15</td><td>25</td></tr><tr><td>GPT-4o, Llama 3.1,</td><td>128,000</td><td>16</td><td>15</td><td>33</td><td>40</td></tr><tr><td>Qwen 3</td><td>32,000</td><td>56</td><td>37</td><td>86</td><td>90</td></tr><tr><td>Phi-4</td><td>16,000</td><td>93</td><td>60</td><td>116</td><td>109</td></tr></tbody></table></figure>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="936" height="935" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png" alt="Chart showing the average tool call output lengths (in tokens) for 1,312 tools, as observed by the MCP Interviewer’s functional test plan. The x-axis represents individual tools (sorted by index), and the y-axis displays the average output length on a logarithmic scale. Horizontal dashed lines indicate context window limits for GPT-4o (128k tokens) and GPT-5 (400k tokens). A pink annotation box summarizes statistics: total tools (1,312), mean (4,431 tokens), median (98 tokens), minimum (0 tokens), and maximum (557,766 tokens)." class="wp-image-1149213" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-768x767.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-360x360.png 360w" sizes="auto, (max-width: 936px) 100vw, 936px" /><figcaption class="wp-element-caption">Figure 3: Tool call response length averages, in tokens, as&nbsp;observed&nbsp;by the MCP Interviewer’s functional test plan. Only successful tool calls are considered. Horizontal lines&nbsp;indicate&nbsp;context window limits for GPT-4o and GPT-5.</figcaption></figure>



<h3 class="wp-block-heading" id="tool-parameter-complexity">Tool parameter complexity</h3>



<p>Mirroring the challenges from increasing&nbsp;the&nbsp;number of tools,&nbsp;increasing the complexity of a tool’s parameter space can also lead to degradation.&nbsp;For example, while MCP tools can take complex object types and structures as parameters,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://composio.dev/blog/gpt-4-function-calling-example" target="_blank" rel="noopener noreferrer">composio<span class="sr-only"> (opens in new tab)</span></a>&nbsp;found that&nbsp;flattening the parameter space could improve tool-calling performance&nbsp;by 47%&nbsp;compared to baseline performance.&nbsp;&nbsp;In our analysis, we&nbsp;find&nbsp;numerous examples of deeply nested structure—in&nbsp;one&nbsp;case, going&nbsp;20&nbsp;levels deep.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png" alt="Chart showing the maximum depth of each tool’s input properties schema. The x-axis represents individual tools (sorted by index), and the y-axis shows the maximum property schema depth. Most tools have a depth  of 2 (named and annotated properties). A pink annotation box summarizes statistics: total tools (12,643), mean (2.24), median (2.00), standard deviation (1.38), minimum (0.00), and maximum (20.00). " class="wp-image-1149365" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-360x360.png 360w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 4: The maximum depth of each tool’s input properties schema. A depth of 0&nbsp;indicates&nbsp;a tool with no properties. A depth of 1&nbsp;indicates&nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&nbsp;indicates&nbsp;a tool with named and annotated properties.&nbsp;&nbsp;A depth of 3+&nbsp;indicates&nbsp;a tool with structured properties that have&nbsp;additional&nbsp;nested annotations.&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="namespacing-issues-and-naming-ambiguity">Namespacing issues and naming ambiguity</h3>



<p>Another often-cited issue with the current MCP specification is the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/128">lack of a formal namespace mechanism<span class="sr-only"> (opens in new tab)</span></a>. If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/openai-agents-python/issues/464">raise an error<span class="sr-only"> (opens in new tab)</span></a> under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was “search”, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td><strong>Tool Name</strong></td><td><strong>Number of Instances</strong></td></tr><tr><td><strong>search</strong></td><td>32</td></tr><tr><td><strong>get_user</strong></td><td>11</td></tr><tr><td><strong>execute_query</strong></td><td>11</td></tr><tr><td><strong>list_tables</strong></td><td>10</td></tr><tr><td><strong>update_task</strong></td><td>9</td></tr><tr><td><strong>generate_image</strong></td><td>9</td></tr><tr><td><strong>send_message</strong></td><td>9</td></tr><tr><td><strong>execute_command</strong></td><td>8</td></tr><tr><td><strong>list_tasks</strong></td><td>8</td></tr><tr><td><strong>search_files</strong></td><td>8</td></tr></tbody></table></figure>



<p>Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td>websearch</td><td>brave_web_search</td></tr><tr><td>search-web</td><td>tavily_web_search</td></tr><tr><td>web_search</td><td>google_news_search</td></tr><tr><td>search_web</td><td>google-play-search</td></tr><tr><td>search_webkr</td><td>google_search_parsed</td></tr><tr><td>google_search</td><td>search_google_images</td></tr><tr><td>search_google</td><td>get_webset_search_exa</td></tr><tr><td>ai_web_search</td><td>search_google_scholar</td></tr><tr><td>web_search_exa</td><td>duckduckgo_web_search</td></tr><tr><td>search_web_tool</td><td>google_search_scraper</td></tr><tr><td>web_search_agent</td><td>answer_query_websearch</td></tr><tr><td>batch-web-search</td><td>&nbsp;</td></tr></tbody></table></figure>



<h3 class="wp-block-heading" id="errors-and-error-messages">Errors and error messages</h3>



<p>Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an “IsError” flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string “error: job,” while another tool providing academic search returned “Please retry with 0 or fewer IDs.”</p>



<h3 class="wp-block-heading" id="resource-sharing-conventions">Resource sharing conventions</h3>



<p>Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://modelcontextprotocol.io/specification/2025-06-18/server/tools#resource-links">resource_link<span class="sr-only"> (opens in new tab)</span></a> as a result &#8212; but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).</p>



<p>Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a <em>local</em> PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.</p>



<h2 class="wp-block-heading" id="recommendations">Recommendations</h2>



<p>On balance, along any given dimension, the average MCP server is quite reasonable—but, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.</p>



<h3 class="wp-block-heading" id="protocol-developers">Protocol developers</h3>



<p>We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.</p>



<p>Likewise, we believe&nbsp;MCP should&nbsp;quickly move to&nbsp;provide formal namespaces&nbsp;to eliminate tool name collisions.&nbsp;If namespaces&nbsp;are hierarchical, then this also provides a way of organizing large catalogs&nbsp;of functions&nbsp;into thematically&nbsp;related tool&nbsp;sets.&nbsp;Tool sets, as an organizing principle,&nbsp;are already showing some promise&nbsp;in&nbsp;GitHub MCP Server’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/github/github-mcp-server?tab=readme-ov-file#dynamic-tool-discovery" target="_blank" rel="noopener noreferrer">dynamic tool discovery,<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and VS Code’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://code.visualstudio.com/updates/v1_103#_tool-grouping-experimental" target="_blank" rel="noopener noreferrer">tool grouping (with virtual tools)<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;where agents or users&nbsp;can&nbsp;enable and disable tools&nbsp;as needed.&nbsp;&nbsp;In the future,&nbsp;a standardized mechanism for grouping tools would allow&nbsp;<em>clients</em>&nbsp;to engage in hierarchical tool-calling,&nbsp;where they first select a category, then select a tool, without needing to keep all possible&nbsp;tools in context.</p>



<h3 class="wp-block-heading" id="server-developers">Server developers</h3>



<p>While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.</p>



<h3 class="wp-block-heading" id="client-developers">Client developers</h3>



<p>Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://www.anthropic.com/engineering/multi-agent-research-system">Anthropic recently reported using a tool testing agent<span class="sr-only"> (opens in new tab)</span></a> to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions— for example, generating namespaces from server names—and could reduce token outputs by summarizing or paginating long tool results.</p>



<h3 class="wp-block-heading" id="market-developers">Market developers</h3>



<p>Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/">distributes Python wheels matched to a developer’s operating system or processor<span class="sr-only"> (opens in new tab)</span></a>, an MCP marketplace could serve tool schemas optimized for a developer’s chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users’ clients.</p>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>In summary, the MCP&nbsp;ecosystem offers significant value for AI agent development,&nbsp;despite&nbsp;some&nbsp;early&nbsp;growing pains.&nbsp;Grounded in insights from the&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">MCP Interviewer<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&nbsp;well positioned&nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&nbsp;eliminate&nbsp;collisions, enhancing protocol support for&nbsp;client provided&nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&nbsp;</p>



<p>By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>RenderFormer: How neural networks are reshaping 3D rendering</title>
		<link>https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/</link>
		
		<dc:creator><![CDATA[Yue Dong]]></dc:creator>
		<pubDate>Wed, 10 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149051</guid>

					<description><![CDATA[<p> RenderFormer, from Microsoft Research, is the first model to show that a neural network can learn a complete graphics rendering pipeline. It’s designed to support full-featured 3D rendering using only machine learning—no traditional graphics computation required. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: network node icon, lightbulb-shaped icon with a path tool icon in the center; a monitor icon showing a web browser icon" class="wp-image-1149127" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>3D rendering—the process of converting three-dimensional models into two-dimensional images—is a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.</p>



<p>Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.</p>



<p>Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene—limiting their generalizability.</p>



<h2 class="wp-block-heading" id="renderformer-toward-a-general-purpose-neural-rendering-model">RenderFormer: Toward a general-purpose neural rendering model</h2>



<p>To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML—no traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. <a href="https://www.microsoft.com/en-us/research/publication/renderformer-transformer-based-neural-rendering-of-triangle-meshes-with-global-illumination/">This work</a> has been accepted at SIGGRAPH 2025 and is <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/renderformer" target="_blank" rel="noopener noreferrer">open-sourced on GitHub<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="architecture-overview">Architecture overview</h2>



<p>As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens—each one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2419" height="1008" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png" alt="Figure 1: The figure illustrates the architecture of RenderFormer. It includes a Triangle Mesh Scene with a 3D rabbit model inside a colored cube, a Camera Ray Map grid, a View Independent Transformer (12 layers of Self-Attention and Feed Forward Network), a View Dependent Transformer (6 layers with Cross-Attention and Self-Attention), and a DPT Decoder. Scene attributes—Vertex Normal, Reflectance (Diffuse, Specular, Roughness), Emission, and Position—are embedded into Triangle Tokens via Linear + Norm operations. These tokens and Ray Bundle Tokens (from the Camera Ray Map) are processed by the respective transformers and decoded to produce a rendered image of a glossy rabbit in a colored room." class="wp-image-1149133" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png 2419w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-300x125.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1024x427.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-768x320.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1536x640.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-2048x853.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-240x100.png 240w" sizes="auto, (max-width: 2419px) 100vw, 2419px" /><figcaption class="wp-element-caption">Figure 1. Architecture of RenderFormer</figcaption></figure>



<p>To describe the viewing direction, the model uses ray bundle tokens derived from a ray map—each pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.</p>



<p>The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144027">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-label="AI Testing and Evaluation: Learnings from Science and Industry" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP2-AI-TE_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshots of Daniel Carpenter, Timo Minssen, Chad Atalla, and Kathleen Sullivan for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Testing and Evaluation: Learnings from Science and Industry</h2>
				
								<p id="ai-testing-and-evaluation-learnings-from-science-and-industry" class="large">Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-describedby="ai-testing-and-evaluation-learnings-from-science-and-industry" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="dual-branch-design-for-view-independent-and-view-dependent-effects">Dual-branch design for view-independent and view-dependent effects</h2>



<p>The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.</p>



<ul class="wp-block-list">
<li>The <strong>view-independent transformer</strong> captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.</li>



<li>The <strong>view-dependent transformer</strong> models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.</li>
</ul>



<p>Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.</p>



<p>To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="963" height="509" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png" alt="Table 1: A table comparing the performance of different network variants in an ablation study. The columns are labeled Variant, PSNR (↑), SSIM (↑), LPIPS (↓), and FLIP (↓). Variants include configurations such as "full view-dependent stage," "w/o DPT," "w/o self-attention," and "w/o DPT & w/o self-attention." Each variant is associated with numerical values for the four metrics, showing how removing or altering components affects performance. The full view-dependent stage achieves the highest PSNR and SSIM and lowest LPIPS and FLIP, indicating optimal performance. Additional rows explore configurations involving camera space and world space view-dependent stages with various token and layer setups." class="wp-image-1149129" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png 963w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-300x159.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-768x406.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-240x127.png 240w" sizes="auto, (max-width: 963px) 100vw, 963px" /><figcaption class="wp-element-caption">Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. </figcaption></figure>



<p>To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="943" height="240" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png" alt="Figure 2: The figure displays four 3D-rendered objects showcasing view-independent rendering effects. From left to right: a purple teapot on a green surface, a blue rectangular object on a red surface, an upside-down table casting shadows on a green surface, and a green apple-like object on a blue surface. Each object features diffuse lighting and coarse shadow effects, with distinct highlights and shadows produced by directional light sources." class="wp-image-1149132" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png 943w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-300x76.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-768x195.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-240x61.png 240w" sizes="auto, (max-width: 943px) 100vw, 943px" /><figcaption class="wp-element-caption">Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. </figcaption></figure>



<p>The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall—capturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="947" height="620" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png" alt="Figure 3: The figure contains six panels arranged in two rows and three columns. The top row displays a teapot in a room with red and green walls under three different roughness values: 0.3, 0.7, and 0.99 (left to right). The bottom row shows the corresponding attention outputs for each roughness setting, featuring the teapot silhouette against a dark background with distinct light patterns that vary with roughness." class="wp-image-1149131" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png 947w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-300x196.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-768x503.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-240x157.png 240w" sizes="auto, (max-width: 947px) 100vw, 947px" /><figcaption class="wp-element-caption">Figure 3. Visualization of attention outputs</figcaption></figure>



<h2 class="wp-block-heading" id="training-methodology-and-dataset-design">Training methodology and dataset design</h2>



<p>RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1–3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender’s Cycles renderer, under varied lighting conditions and camera angles.</p>



<p>The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:</p>



<ul class="wp-block-list">
<li>500,000 steps at 256×256 resolution with up to 1,536 triangles</li>



<li>100,000 steps at 512×512 resolution with up to 4,096 triangles</li>
</ul>



<p>The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="805" height="805" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg" alt="Figure 4: The figure presents a 3×3 grid of diverse 3D scenes rendered by RenderFormer. In the top row, the first scene shows a room with red, green, and white walls containing two rectangular prisms; the second features a metallic tree-like structure in a blue-walled room with a reflective floor; and the third depicts a red animal figure, a black abstract shape, and a multi-faceted sphere in a purple container on a yellow surface. The middle row includes three constant width bodies (black, red, and blue) floating above a colorful checkered floor; a green shader ball with a square cavity inside a gray-walled room; and crystal-like structures in green, purple, and red on a reflective surface. The bottom row showcases a low-poly fox near a pink tree emitting particles on grassy terrain; a golden horse statue beside a heart-shaped object split into red and grey halves on a reflective surface; and a wicker basket, a banana and a bottle placed on a white platform." class="wp-image-1149130" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg 805w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-300x300.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-150x150.jpg 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-768x768.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-180x180.jpg 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-360x360.jpg 360w" sizes="auto, (max-width: 805px) 100vw, 805px" /><figcaption class="wp-element-caption">Figure 4. Rendered results of different 3D scenes generated by RenderFormer </figcaption></figure>



<p>RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.</p>



<figure class="wp-block-video aligncenter"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4"></video><figcaption class="wp-element-caption">3D animation sequence rendered by RenderFormer </figcaption></figure>



<h2 class="wp-block-heading" id="looking-ahead-opportunities-and-challenges">Looking ahead: Opportunities and challenges</h2>



<p>RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination—all without any hand-coded graphics computations.</p>



<p>However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI. </p>



<p>Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4" length="58187117" type="video/mp4" />

			</item>
		<item>
		<title>Breaking the networking wall in AI infrastructure </title>
		<link>https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</link>
		
		<dc:creator><![CDATA[Paolo Costa]]></dc:creator>
		<pubDate>Tue, 09 Sep 2025 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false"></guid>

					<description><![CDATA[<p>Datacenter memory and network limits are restraining AI system performance. MOSAIC uses microLEDs and a wide-and-slow optical architecture to deliver faster, longer, more reliable, and energy efficient connections that could transform AI cluster designs.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.</p>



<p>Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(< 2 meters)&nbsp;that&nbsp;restrict their use&nbsp;to within a single&nbsp;GPU&nbsp;rack. Optical&nbsp;fiber&nbsp;links&nbsp;can&nbsp;reach&nbsp;tens of meters,&nbsp;but&nbsp;they&nbsp;consume far more&nbsp;power&nbsp;and fail up to 100 times&nbsp;as often as&nbsp;copper. A&nbsp;team working across&nbsp;Microsoft&nbsp;aims&nbsp;to&nbsp;resolve&nbsp;this trade-off&nbsp;by&nbsp;developing&nbsp;MOSAIC,&nbsp;a novel optical link technology&nbsp;that&nbsp;can provide&nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&nbsp;<em>simultaneously</em>.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;</p>



<p>The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;<em>narrow-and-fast</em>&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;</p>



<p>These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;<em>per rack</em>. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;<em>networking wall</em>&nbsp;akin&nbsp;to the&nbsp;<em>memory wall</em>, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.</p>



<p class="has-text-align-left">A technology offering copper-like power efficiency and reliability over long distances can overcome this networking wall, enabling multi-rack scale-up domains and unlocking new architectures. This is a highly active R&D area, with many candidate technologies currently being developed across the industry. In our recent paper, <em>“<a href="https://www.microsoft.com/en-us/research/publication/mosaic-breaking-the-optics-versus-copper-trade-off-with-a-wide-and-slow-architecture-and-microleds/">MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs</a>”</em>, which received the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://conferences.sigcomm.org/sigcomm/2025/program/papers-info/" target="_blank" rel="noopener noreferrer">Best Paper award at ACM SIGCOMM<span class="sr-only"> (opens in new tab)</span></a>, we present one such promising approach that is the result of a multi-year collaboration between Microsoft Research, Azure, and M365. This work is centered around an optical wide-and-slow architecture, shifting from a small number of high-speed serial channels towards hundreds of parallel low-speed channels. This would be impractical to realize with today’s copper and optical technologies because of i) electromagnetic interference challenges in high-density copper cables and ii) the high cost and power consumption of lasers in optical links, as well as the increase in packaging complexity. MOSAIC overcomes these issues by leveraging directly modulated microLEDs, a technology originally developed for screen displays. </p>



<p>MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm×1 mm&nbsp;silicon&nbsp;die.&nbsp;</p>



<p>MOSAIC’s&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.</p>



<ul class="wp-block-list">
<li>Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.</li>



<li>By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper’s reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.</li>



<li>MicroLEDs’&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;</li>



<li>The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;</li>
</ul>



<p>Further,&nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&nbsp;factor&nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it’s&nbsp;fully compatible with today’s protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;</p>



<p>While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;</p>



<p>Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.</p>



<p>For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today’s&nbsp;large,&nbsp;multi-die packages to&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/good-things-come-in-small-packages-should-we-adopt-lite-gpus-in-ai-infrastructure/">smaller, more cost-effective, ones</a>.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/storage-class-memory-is-dead-all-hail-managed-retention-memory-rethinking-memory-for-the-ai-era/">novel memory technologies</a>.&nbsp;</p>



<p>Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
