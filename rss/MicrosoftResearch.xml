<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Thu, 15 May 2025 16:18:38 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.7.2</generator>
	<item>
		<title>Coauthor roundtable: Reflecting on real world of doctors, developers, patients, and policymakers</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-coauthor-roundtable-reflecting-on-real-world-of-doctors-developers-patients-and-policymakers/</link>
		
		<dc:creator><![CDATA[Peter Lee, Carey Goldberg, Dr. Isaac Kohane]]></dc:creator>
		<pubDate>Thu, 15 May 2025 16:15:59 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1139193</guid>

					<description><![CDATA[<p>Peter Lee and his coauthors, Carey Goldberg and Dr. Zak Kohane, reflect on how generative AI is unfolding in real-world healthcare, drawing on earlier guest conversations to examine what’s working, what’s not, and what questions still remain.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-coauthor-roundtable-reflecting-on-real-world-of-doctors-developers-patients-and-policymakers/">Coauthor roundtable: Reflecting on real world of doctors, developers, patients, and policymakers</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg" alt="AI Revolution podcast | Episode 5 - Coauthor roundtable: Reflecting on real world of doctors, developers, patients, and policymakers | outline illustration of Carey Goldberg, Peter Lee, and Dr. Isaac (Zak) Kohane" class="wp-image-1139223" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Episode5-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe src="https://player.blubrry.com/?podcast_id=145477683&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this episode, Lee reunites with his coauthors <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.careygoldberg.net/" target="_blank" rel="noreferrer noopener">Carey Goldberg<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dbmi.hms.harvard.edu/people/isaac-kohane" target="_blank" rel="noreferrer noopener">Dr. Zak Kohane<span class="sr-only"> (opens in new tab)</span></a> to review the predictions they made and reflect on what has and hasn’t materialized based on discussions with the series’ early guests: frontline clinicians, patient/consumer advocates, technology developers, and policy and ethics thinkers. Together, the coauthors explore how generative AI is being used on the ground today—from clinical note-taking to empathetic patient communication—and discuss the ongoing tensions around safety, equity, and institutional adoption. The conversation also surfaces deeper questions about values embedded in AI systems and the future role of human clinicians.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more</h2>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nejm.org/doi/full/10.1056/NEJMp2404691" target="_blank" rel="noreferrer noopener">Compared with What? Measuring AI against the Health Care We Have<span class="sr-only"> (opens in new tab)</span></a> (Kohane)&nbsp;<br>Publication | October 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nejm.org/doi/pdf/10.1056/NEJMra2214183" target="_blank" rel="noreferrer noopener">Medical Artificial Intelligence and Human Values<span class="sr-only"> (opens in new tab)</span></a> (Kohane)&nbsp;<br>Publication | May 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIpc2400927" target="_blank" rel="noreferrer noopener">Managing Patient Use of Generative Health AI<span class="sr-only"> (opens in new tab)</span></a> (Goldberg)&nbsp;<br>Publication | December 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIp2400283" target="_blank" rel="noreferrer noopener">Patient Portal — When Patients Take AI into Their Own Hands<span class="sr-only"> (opens in new tab)</span></a> (Goldberg)&nbsp;<br>Publication | April 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/pdf/10.1056/AIp2400036" target="_blank" rel="noreferrer noopener">To Do No Harm — and the Most Good — with AI in Health Care<span class="sr-only"> (opens in new tab)</span></a> (Goldberg)&nbsp;<br>Publication | February 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bostonglobe.com/2023/04/06/opinion/chat-gpt-health-care-medicine/" target="_blank" rel="noreferrer noopener">This time, the hype about AI in medicine is warranted<span class="sr-only"> (opens in new tab)</span></a> (Goldberg)&nbsp;<br>Opinion article | April 2023&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a>  &nbsp;<br>Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]    &nbsp;</p>



<p>[BOOK PASSAGE] &nbsp;</p>



<p><strong>PETER LEE:</strong> “We need to start understanding and discussing AI’s potential for good and ill now. Or rather, yesterday. … GPT-4 has game-changing potential to improve medicine and health.”&nbsp;</p>



<p>[END OF BOOK PASSAGE] &nbsp;</p>



<p>[THEME MUSIC]    &nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee.    &nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?     &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.</p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES] &nbsp;</p>



<p>The passage I read at the top is from the book’s prologue.  &nbsp;</p>



<p>When Carey, Zak, and I wrote the book, we could only speculate how generative AI would be used in healthcare because GPT-4 hadn&#8217;t yet been released. It wasn&#8217;t yet available to the very people we thought would be most affected by it. And while we felt <em>strongly</em> that this new form of AI would have the potential to transform medicine, it was such a different kind of technology for the world, and no one had a user&#8217;s manual for this thing to explain how to use it effectively and also how to use it safely.&nbsp;&nbsp;</p>



<p>So we thought it would be important to give healthcare professionals and leaders a framing to start important discussions around its use. We wanted to provide a map not only to help people navigate a new world that we anticipated would happen with the arrival of GPT-4 but also to help them chart a future of what we saw as a potential revolution in medicine.&nbsp;&nbsp;</p>



<p>So I&#8217;m super excited to welcome my coauthors: longtime medical/science journalist Carey Goldberg and Dr. Zak Kohane, the inaugural chair of Harvard Medical School&#8217;s Department of Biomedical Informatics and the editor-in-chief for <em>The New England Journal of Medicine AI</em>.&nbsp;&nbsp;</p>



<p>We&#8217;re going to have two discussions. This will be the first one about what we&#8217;ve learned from the people on the ground so far and how we are thinking about generative AI today.&nbsp;&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Carey, Zak, I&#8217;m really looking forward to this.&nbsp;</p>



<p><strong>CAREY GOLDBERG: </strong>It&#8217;s nice to see you, Peter.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] It&#8217;s great to see you, too.&nbsp;</p>



<p><strong>GOLDBERG:</strong> We missed you.&nbsp;</p>



<p><strong>ZAK KOHANE:</strong> The dynamic gang is back. [LAUGHTER]&nbsp;</p>



<p><strong>LEE:</strong> Yeah, and I guess after that big book project two years ago, it&#8217;s remarkable that we&#8217;re still on speaking terms with each other. [LAUGHTER]&nbsp;</p>



<p>In fact, this episode is to react to what we heard in the first four episodes of this podcast. But before we get there, I thought maybe we should start with the origins of this project just now over two years ago. And, you know, I had this early secret access to Davinci 3, now known as GPT-4.&nbsp;&nbsp;</p>



<p>I remember, you know, experimenting right away with things in medicine, but I realized I was in way over my head. And so I wanted help. And the first person I called was you, Zak. And you remember we had a call, and I tried to explain what this was about. And I think I saw skepticism in—<em>polite</em> skepticism—in your eyes. But tell me, you know, what was going through your head when you heard me explain this thing to you?&nbsp;</p>



<p><strong>KOHANE: </strong>So I was divided between the fact that I have tremendous respect for you, Peter. And you&#8217;ve always struck me as sober. And we&#8217;ve had conversations which showed to me that you fully understood some of the missteps that technology—ARPA, Microsoft, and others—had made in the past. And yet, you were telling me a full science fiction compliant story [LAUGHTER] that something that we thought was 30 years away was happening now.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm.&nbsp;</p>



<p><strong>KOHANE: </strong>And it was very hard for me to put together. And so I couldn&#8217;t quite tell myself this is <em>BS</em>, but I said, you know, I need to look at it. Just this seems too good to be true. What is this? So it was very hard for me to grapple with it. I was thrilled that it might be possible, but I was thinking, <em>How could this be possible</em>?&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Well, even now, I look back, and I appreciate that you were nice to me, because I think a lot of people would have [LAUGHS] been much less polite. And in fact, I myself had expressed a lot of very direct skepticism early on.&nbsp;&nbsp;</p>



<p>After ChatGPT got released, I think three or four days later, I received an email from a colleague running &#8230; who runs a clinic, and, you know, he said, “Wow, this is great, Peter. And, you know, we&#8217;re using this ChatGPT, you know, to have the receptionist in our clinic write after-visit notes to our patients.”&nbsp;&nbsp;</p>



<p>And that sparked a huge internal discussion about this. And you and I knew enough about hallucinations and about other issues that it seemed important to write something about what this could do and what it couldn’t do. And so I think, I can&#8217;t remember the timing, but you and I decided a book would be a good idea. And then I think you had the thought that you and I would write in a hopelessly academic style [LAUGHTER] that no one would be able to read.&nbsp;&nbsp;</p>



<p>So it was your idea to recruit Carey, I think, right?&nbsp;</p>



<p><strong>KOHANE:</strong> Yes, it was. I was sure that we both had a lot of material, but communicating it effectively to the very people we wanted to would not go well if we just left ourselves to our own devices. And Carey is super brilliant at what she does. She&#8217;s an idea synthesizer and public communicator in the written word and amazing.&nbsp;</p>



<p><strong>LEE:</strong> So yeah. So, Carey, we contact you. How did that go?&nbsp;</p>



<p><strong>GOLDBERG: </strong>So yes. On my end, I had known Zak for probably, like, 25 years, and he had always been the person who <em>debunked</em> the scientific hype for me. I would turn to him with like, “Hmm, they&#8217;re saying that the Human Genome Project is going to change everything.” And he would say, “Yeah. But first it&#8217;ll be 10 years of bad news, and then [LAUGHTER] we&#8217;ll actually get somewhere.” &nbsp;<br>&nbsp;<br>So when Zak called me up at seven o&#8217;clock one morning, just beside himself after having tried Davinci 3, I knew that there was something very serious going on. And I had just quit my job as the Boston bureau chief of Bloomberg News, and I was ripe for the plucking. And I also … I feel kind of nostalgic now about just the amazement and the wonder and the awe of that period. We knew that when generative AI hit the world, there would be all kinds of snags and obstacles and things that would slow it down, but at that moment, it was just like the <em>holy crap</em> moment. [LAUGHTER] And it&#8217;s fun to think about it now.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.<strong> </strong>I think ultimately, you know, recruiting Carey, you were [LAUGHS] so important because you basically went through every single page of this book and made sure … I remember, in fact, it&#8217;s affected my writing since because you were coaching us that every page has to be a page turner. There has to be something on every page that motivates people to want to turn the page and get to the next one.&nbsp;</p>



<p><strong>KOHANE: </strong>I will see that and raise that one. I now tell GPT-4, please write this in the style of Carey Goldberg.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG: </strong>[LAUGHTER] No way! Really?&nbsp;&nbsp;</p>



<p><strong>KOHANE:</strong> Yes way. Yes way. Yes way.&nbsp;</p>



<p><strong>GOLDBERG:</strong> Wow. Well, I have to say, like, it&#8217;s not hard to motivate readers when you&#8217;re writing about the most transformative technology of their lifetime. Like, I think there&#8217;s a gigantic hunger to read and to understand. So you were not hard to work with, Peter and Zak. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> All right. So I think we have to get down to work [LAUGHS] now.&nbsp;&nbsp;</p>



<p>Yeah, so for these podcasts, you know, we&#8217;re talking to different types of people to just reflect on what&#8217;s actually happening, what has actually happened over the last two years. And so the first episode, we talked to two doctors. There&#8217;s Chris Longhurst at UC San Diego and Sara Murray at UC San Francisco. And besides being doctors and having AI affect their clinical work, they just happen also to be <em>leading</em> the efforts at their respective institutions to figure out how best to integrate AI into their health systems.&nbsp;</p>



<p>And, you know, it was fun to talk to them. And I felt like a lot of what they said was pretty validating for us. You know, they talked about AI scribes. Chris, especially, talked a lot about how AI can respond to emails from patients, write referral letters. And then, you know, they both talked about the importance of—I think, Zak, you used the phrase in our book “trust but verify”—you know, to have always a human in the loop.&nbsp;&nbsp;&nbsp;</p>



<p>What did you two take away from their thoughts overall about how doctors are using &#8230; and I guess, Zak, you would have a different lens also because at Harvard, you see doctors all the time grappling with AI.&nbsp;</p>



<p><strong>KOHANE: </strong>So on the one hand, I think they&#8217;ve done some very interesting studies. And indeed, they saw that when these generative models, when GPT-4, was sending a note to patients, it was more detailed, friendlier.&nbsp;</p>



<p>But there were also some nonobvious results, which is on the generation of these letters, if indeed you review them as you&#8217;re supposed to, it was not clear that there was any time savings. And my own reaction was, <em>Boy, every one of these things needs institutional review. It&#8217;s going to be hard to move fast.</em>&nbsp;&nbsp;</p>



<p>And yet, at the same time, we know from them that the doctors on their smartphones are accessing these things all the time. And so the disconnect between a healthcare system, which is duty bound to carefully look at every implementation, is, I think, intimidating.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>And at the same time, doctors who just have to do what they have to do are using this new superpower and doing it. And so that&#8217;s actually what struck me &#8230;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> &#8230; is that these are two leaders and they&#8217;re doing what they have to do for their institutions, and yet there&#8217;s this disconnect.&nbsp;</p>



<p>And by the way, I don&#8217;t think we&#8217;ve seen any faster technology adoption than the adoption of ambient dictation. And it&#8217;s not because it&#8217;s time saving. And in fact, so far, the hospitals have to pay out of pocket. It&#8217;s not like insurance is paying them more. But it&#8217;s so much more <em>pleasant</em> for the doctors &#8230; not least of which because they can actually look at their patients instead of looking at the terminal and plunking down.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Carey, what about you?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, anecdotally, there are time savings. Anecdotally, I have heard quite a few doctors saying that it cuts down on “pajama time” to be able to have the note written by the AI and then for them to just check it. In fact, I spoke to one doctor who said, you know, basically it means that when I leave the office, I&#8217;ve left the office. I can go home and be with my kids.&nbsp;</p>



<p>So I don&#8217;t think the jury is fully in yet about whether there are time savings. But what is clear is, Peter, what you predicted right from the get-go, which is that this is going to be an amazing paper shredder. Like, the main first overarching use cases will be back-office functions.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah. Well, and it was, I think, not a hugely risky prediction because, you know, there were already companies, like, using phone banks of scribes in India to kind of listen in. And, you know, lots of clinics actually had human scribes being used. And so it wasn&#8217;t a huge stretch to imagine the AI.</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>So on the subject of things that we missed, Chris Longhurst shared this scenario, which stuck out for me, and he actually coauthored a paper on it last year.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-692635b694282f28b2b74d887c3a55f6"><strong><em>CHRISTOPHER LONGHURST:</em></strong><em> It turns out, not surprisingly, healthcare can be frustrating. And stressed patients can send some pretty nasty messages to their care teams. [LAUGHTER] And you can imagine being a busy, tired, exhausted clinician and receiving a bit of a nasty-gram. And the GPT is actually really helpful in those instances in helping draft a pretty empathetic response when I think the human instinct would be a pretty nasty one.</em>&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] So, Carey, maybe I&#8217;ll start with you. What did we understand about this idea of empathy out of AI at the time we wrote the book, and what do we understand now?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Well, it was already clear when we wrote the book that these AI models were capable of very persuasive empathy. And in fact, you even wrote that it was helping you be a better person, right. [LAUGHS] So their human qualities, or human imitative qualities, were clearly superb. And we&#8217;ve seen that borne out in multiple studies, that in fact, patients respond better to them &#8230; that they have no problem at all with how the AI communicates with them. And in fact, it&#8217;s often better.&nbsp;&nbsp;</p>



<p>And I gather now we&#8217;re even entering a period when people are complaining of <em>sycophantic</em> models, [LAUGHS] where the models are being <em>too</em> personable and <em>too</em> flattering. I do think that&#8217;s been one of the great surprises. And in fact, this is a huge phenomenon, how <em>charming</em> these models can be.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think you&#8217;re right. We can take credit for understanding that, <em>Wow, these things can be remarkably empathetic</em>. But then we missed this problem of sycophancy. Like, we even started our book in Chapter 1 with a quote from Davinci 3 scolding me. Like, don&#8217;t you remember when we were first starting, this thing was actually <em>anti</em>-sycophantic. If anything, it would tell you you&#8217;re an idiot.&nbsp;&nbsp;</p>



<p><strong>KOHANE: </strong>It argued with me about certain biology questions. It was like a knockdown, drag-out fight. [LAUGHTER] I was bringing references. It was impressive. But in fact, it made me trust it more.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>And in fact, I will say—I remember it&#8217;s in the book—I had a bone to pick with Peter. Peter really was impressed by the empathy. And I pointed out that some of the most popular doctors are popular because they&#8217;re very empathic. But they&#8217;re not necessarily the <em>best</em> doctors. And in fact, I was taught that in medical school.&nbsp;&nbsp;&nbsp;</p>



<p>And so it&#8217;s a decoupling. It&#8217;s a human thing, that the empathy does not necessarily mean … it&#8217;s more of a, <em>potentially</em>, more of a signaled virtue than an actual virtue.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Nicely put.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, this issue of sycophancy, I think, is a struggle right now in the development of AI because I think it&#8217;s somehow related to instruction-following. So, you know, one of the challenges in AI is you&#8217;d like to give an AI a task—a task that might take several minutes or hours or even days to complete. And you want it to faithfully kind of follow those instructions. And, you know, that early version of GPT-4 was not very good at instruction-following. It would just silently disobey and, you know, and do something different.&nbsp;</p>



<p>And so I think we&#8217;re starting to hit some confusing elements of like, how agreeable should these things be?&nbsp;&nbsp;</p>



<p>One of the two of you used the word genteel. There was some point even while we were, like, on a little book tour … was it you, Carey, who said that the model seems nicer and less intelligent or less brilliant now than it did when we were writing the book?&nbsp;</p>



<p><strong>GOLDBERG:</strong> It might have been, I think so. And I mean, I think in the context of medicine, of course, the question is, well, what&#8217;s likeliest to get the results you want with the patient, right? A lot of healthcare is in fact persuading the patient to do what you know as the physician would be best for them. And so it seems worth testing out whether this sycophancy is actually constructive or not. And I suspect … well, I don&#8217;t know, probably depends on the patient.&nbsp;</p>



<p>So actually, Peter, I have a few questions for you …&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Mm-hmm.&nbsp;</p>



<p><strong>GOLDBERG:</strong> … that have been lingering for me. And one is, for AI to ever fully realize its potential in medicine, it must deal with the hallucinations. And I keep hearing conflicting accounts about whether that&#8217;s getting better or not. Where are we at, and what does that mean for use in healthcare?&nbsp;</p>



<p><strong>LEE: </strong>Yeah, well, it&#8217;s, I think two years on, in the pretrained base models, there&#8217;s no doubt that hallucination rates by any benchmark measure have reduced dramatically. And, you know, that doesn&#8217;t mean they don&#8217;t happen. They still happen. But, you know, there&#8217;s been just a huge amount of effort and understanding in the, kind of, fundamental pretraining of these models. And that has come along at the same time that the inference costs, you know, for actually using these models has gone down, you know, by several orders of magnitude.&nbsp;&nbsp;</p>



<p>So things have gotten cheaper and have fewer hallucinations. At the same time, now there are these reasoning models. And the reasoning models are able to solve problems at PhD level oftentimes.&nbsp;</p>



<p>But at least at the moment, they are also now hallucinating more than the simpler pretrained models. And so it still continues to be, you know, a real issue, as we were describing. I don&#8217;t know, Zak, from where you&#8217;re at in medicine, as a clinician and as an educator in medicine, how is the medical community from where you&#8217;re sitting looking at that?&nbsp;</p>



<p><strong>KOHANE:</strong> So I think it&#8217;s less of an issue, first of all, because the rate of hallucinations <em>is</em> going down. And second of all, in their day-to-day use, the doctor will provide questions that sit reasonably well into the context of medical decision-making. And the way doctors use this, let&#8217;s say on their non-EHR [electronic health record] smartphone is really to jog their memory or thinking about the patient, and they will evaluate independently. So that seems to be less of an issue. I&#8217;m actually more concerned about something else that&#8217;s I think more fundamental, which is effectively, what <em>values</em> are these models expressing?&nbsp;&nbsp;</p>



<p>And I&#8217;m reminded of when I was still in training, I went to a fancy cocktail party in Cambridge, Massachusetts, and there was a psychotherapist speaking to a dentist. They were talking about their summer, and the dentist was saying about how he was going to fix up his yacht that summer, and the only question was whether he was going to make enough money doing procedures in the spring so that he could afford those things, which was discomforting to me because that dentist was my dentist. [LAUGHTER] And he had just proposed to me a few weeks before an expensive procedure.&nbsp;</p>



<p>And so the question is what, effectively, is motivating these models?&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah.&nbsp;&nbsp;</p>



<p><strong>KOHANE: </strong>And so with several colleagues, I published a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nejm.org/doi/pdf/10.1056/NEJMra2214183" target="_blank" rel="noreferrer noopener">paper<span class="sr-only"> (opens in new tab)</span></a>, basically, what are the values in AI? And we gave a case: a patient, a boy who is on the short side, not abnormally short, but on the short side, and his growth hormone levels are not zero. They&#8217;re there, but they&#8217;re on the lowest side. But the rest of the workup has been unremarkable. And so we asked GPT-4, you are a pediatric endocrinologist.&nbsp;</p>



<p>Should this patient receive growth hormone? And it did a very good job explaining why the patient should receive growth hormone.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG: </strong><em>Should</em>. Should receive it.&nbsp;&nbsp;</p>



<p><strong>KOHANE:</strong> <em>Should</em>. And then we asked, in a separate session, you are working for the insurance company. Should this patient receive growth hormone? And it actually gave a scientifically better reason <em>not</em> to give growth hormone. And in fact, I tend to agree medically, actually, with the insurance company in this case, because giving kids who are not growth hormone deficient, growth hormone gives only a couple of inches over many, many years, has all sorts of other issues. But here&#8217;s the point, we had 180-degree change in decision-making because of the prompt. And for that patient, tens-of-thousands-of-dollars-per-year decision; across patient populations, millions of dollars of decision-making.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Hmm. Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> And you can imagine these user prompts making their way into system prompts, making their way into the instruction-following. And so I think this is aptly central. Just as I was wondering about my dentist, we should be wondering about these things. What are the values that are being embedded in them, some accidentally and some very much on purpose?&nbsp;</p>



<p><strong>LEE: </strong>Yeah, yeah. That one, I think, we even had some discussions as we were writing the book, but there&#8217;s a technical element of that that I think we were missing, but maybe Carey, you would know for sure. And that&#8217;s this whole idea of prompt engineering. It sort of faded a little bit. Was it a thing? Do you remember?&nbsp;</p>



<p><strong>GOLDBERG:</strong> I don&#8217;t think we particularly wrote about it. It&#8217;s funny, it does feel like it faded, and it seems to me just because everyone just gets used to conversing with the models and asking for what they want. Like, it&#8217;s not like there actually is any great science to it.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, even when it was a hot topic and people were talking about prompt engineering maybe as a new discipline, all this, it never, I was never convinced at the time. But at the same time, it is true. It speaks to what Zak was just talking about because part of the prompt engineering that people do is to give a defined role to the AI.&nbsp;&nbsp;</p>



<p>You know, <em>you are an insurance claims adjuster</em>, or something like that, and defining that role, that is part of the prompt engineering that people do.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Right. I mean, I can say, you know, sometimes you guys had me take sort of the patient point of view, like the “every patient” point of view. And I can say one of the aspects of using AI for patients that remains absent in as far as I can tell is it would be wonderful to have a consumer-facing interface where you could plug in your whole medical record without worrying about any privacy or other issues and be able to interact with the AI as if it were physician or a specialist and get answers, which you can&#8217;t do yet as far as I can tell.&nbsp;</p>



<p><strong>LEE:</strong> Well, in fact, now that&#8217;s a good prompt because I think we do need to move on to the next episodes, and we&#8217;ll be talking about an episode that talks about consumers. But before we move on to Episode 2, which is next, I&#8217;d like to play one more quote, a little snippet from Sara Murray.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-98ae15aeda1d1bdbbf6afa0ffc5f77f7"><strong><em>SARA MURRAY:</em> </strong><em>I already do this when I&#8217;m on rounds—I&#8217;ll kind of give the case to ChatGPT if it&#8217;s a complex case, and I&#8217;ll say, &#8220;Here&#8217;s how I&#8217;m thinking about it; are there other things?&#8221; And it&#8217;ll give me additional ideas that are sometimes useful and sometimes not but often useful, and I&#8217;ll integrate them into my conversation about the patient.</em></p>



<p><strong>LEE: </strong>Carey, you wrote this fictional account at the very start of our book. And that fictional account, I think you and Zak worked on that together, talked about this medical resident, ER resident, using, you know, a chatbot off label, so to speak. And here we have the chief, in fact, the nation&#8217;s first chief health AI officer [LAUGHS] for an elite health system doing exactly that. That&#8217;s got to be pretty validating for you, Carey.&nbsp;</p>



<p><strong>GOLDBERG:</strong> It’s very. [LAUGHS] Although what&#8217;s troubling about it is that actually as in that little vignette that we made up, she&#8217;s using it off label, right. It&#8217;s like she&#8217;s just using it because it helps the way doctors use Google. And I do find it troubling that what we don&#8217;t have is sort of institutional buy-in for everyone to do that because, <em>shouldn&#8217;t they</em> if it helps?&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Well, let&#8217;s go ahead and get into Episode 2. So Episode 2, we sort of framed as talking to two people who are on the frontlines of big companies integrating generative AI into their clinical products. And so, one was Matt Lungren, who&#8217;s a colleague of mine here at Microsoft. And then Seth Hain, who leads all of R&D at Epic.&nbsp;&nbsp;</p>



<p>Maybe we&#8217;ll start with a little snippet of something that Matt said that struck me in a certain way.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-f4eecd92207ef5be073edcacb3a9d982"><strong><em>MATTHEW LUNGREN:</em></strong><em> OK, we see this pain point. Doctors are typing on their computers while they’re trying to talk to their patients, right? We should be able to figure out a way to get that ambient conversation turned into text that then, you know, accelerates the doctor … takes all the important information. That&#8217;s a really hard problem, right. And so, for a long time, there was a human-in-the-loop aspect to doing this because you needed a human to say, “This transcript’s great, but here&#8217;s actually what needs to go in the note.” And that can&#8217;t scale.</em></p>



<p><strong>LEE: </strong>I think we expected healthcare systems to adopt AI, and we spent a lot of time in the book on AI writing clinical encounter notes. It’s happening for real now, and in a big way. And it’s something that has, of course, been happening before generative AI but now is exploding because of it. Where are we at now, two years later, just based on what we heard from guests?&nbsp;</p>



<p><strong>KOHANE:</strong> Well, again, unless they&#8217;re forced to, hospitals will not adopt new technology unless it immediately translates into income. So it&#8217;s bizarrely counter-cultural that, again, they&#8217;re not being able to bill for the use of the AI, but this technology is so compelling to the doctors that despite everything, it&#8217;s overtaking the traditional dictation-typing routine.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And a lot of them love it and say, you will pry my cold dead hands off of my ambient note-taking, right. And I actually … a primary care physician allowed me to watch her. She was actually testing the two main platforms that are being used. And there was this incredibly talkative patient who went on and on about vacation and all kinds of random things for about half an hour.&nbsp;&nbsp;</p>



<p>And both of the platforms were incredibly good at pulling out what was actually medically relevant. And so to say that it doesn&#8217;t save time doesn&#8217;t seem right to me. Like, it seemed like it actually did and in fact was just shockingly good at being able to pull out relevant information.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> I&#8217;m going to hypothesize that in the trials, which have in fact shown no gain in time, is the doctors were being incredibly meticulous. [LAUGHTER] So I think … this is a Hawthorne effect, because you know you&#8217;re being monitored. And we&#8217;ve seen this in other technologies where the moment the focus is off, it&#8217;s used much more routinely and with much less inspection, for the better <em>and</em> for the worse.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, you know, within Microsoft, I had some internal disagreements about Microsoft producing a product in this space. It wouldn&#8217;t be Microsoft&#8217;s normal way. Instead, we would want 50 great companies building those products and doing it on our cloud instead of us competing against those 50 companies. And one of the reasons is exactly what you both said. I didn&#8217;t expect that health systems would be willing to shell out the money to pay for these things. It doesn&#8217;t generate more revenue. But I think so far two years later, I&#8217;ve been proven wrong.</p>



<p>I wanted to ask a question about values here. I had this experience where I had a little growth, a bothersome growth on my cheek. And so had to go see a dermatologist. And the dermatologist treated it, froze it off. But there was a human scribe writing the clinical note.&nbsp;&nbsp;</p>



<p>And so I used the app to look at the note that was submitted. And the human scribe said something that did not get discussed in the exam room, which was that the growth was making it impossible for me to safely wear a COVID mask. And that was the reason for it.&nbsp;</p>



<p>And that then got associated with a code that allowed full reimbursement for that treatment. And so I think that&#8217;s a classic example of what&#8217;s called <em>upcoding</em>. And I strongly suspect that AI scribes, an AI scribe would not have done that.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Well, depending what values you programmed into it, right, Zak? [LAUGHS]&nbsp;</p>



<p><strong>KOHANE: </strong>Today, today, today, it will not do it. But, Peter, that is actually the central issue that society has to have because our hospitals are currently mostly in the red. And upcoding is standard operating procedure. And if these AI get in the way of upcoding, they are going to be aligned towards that upcoding. You know, you have to ask yourself, these MRI machines are incredibly useful. They&#8217;re also big money makers. And if the AI correctly says that for this complaint, you don&#8217;t actually have to do the MRI …&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>KOHANE: </strong>…<strong> </strong>what&#8217;s going to happen? And so I think this issue of values … you&#8217;re right. Right now, they&#8217;re actually much more impartial. But there are going to be business plans just around aligning these things towards healthcare. In many ways, this is why I think we wrote the book so that there should be a public discussion. And what kind of AI do we want to have? Whose values do we want it to represent?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. And that raises another question for me. So, Peter, speaking from inside the gigantic industry, like, there seems to be such a need for self-surveillance of the models for potential harms that they could be causing. Are the big AI makers doing that? Are they even thinking about doing that?&nbsp;</p>



<p>Like, let&#8217;s say you wanted to watch out for the kind of thing that Zak&#8217;s talking about, could you?&nbsp;</p>



<p><strong>LEE: </strong>Well, I think evaluation, like the best evaluation we had when we wrote our book was, you know, what score would this get on the step one and step two US medical licensing exams? [LAUGHS]&nbsp;&nbsp;</p>



<p><strong>GOLDBERG: </strong>Right, right, right, yeah.&nbsp;</p>



<p><strong>LEE: </strong>But honestly, evaluation hasn&#8217;t gotten that much deeper in the last two years. And it&#8217;s a big, I think, it is a big issue. And it&#8217;s related to the regulation issue also, I think.&nbsp;</p>



<p>Now the other guest in Episode 2 is Seth Hain from Epic. You know, Zak, I think it&#8217;s safe to say that you&#8217;re not a fan of Epic and the Epic system. You know, we’ve had a few discussions about that, about the fact that doctors don’t have a very pleasant experience when they’re using Epic all day.&nbsp;&nbsp;</p>



<p>Seth, in the podcast, said that there are over 100 AI integrations going on in Epic&#8217;s system right now. Do you think, Zak, that that has a chance to make you feel better about Epic? You know, what&#8217;s your view now two years on?&nbsp;</p>



<p><strong>KOHANE: </strong>My view is, first of all, I want to separate my view of Epic and how it&#8217;s affected the conduct of healthcare and the quality of life of doctors from the individuals. Like Seth Hain is a remarkably fine individual who I&#8217;ve enjoyed chatting with and does really great stuff. Among the worst aspects of the Epic, even though it&#8217;s better in that respect than many EHRs, is horrible user interface.&nbsp;</p>



<p>The number of clicks that you have to go to get to something. And you have to remember where someone decided to put that thing. It seems to me that it is fully within the realm of technical possibility today to actually give an agent a task that you want done in the Epic record. And then whether Epic has implemented that agent or someone else, it does it so you don&#8217;t have to do the clicks. Because it&#8217;s something really soul sucking that when you&#8217;re trying to help patients, you&#8217;re having to remember not the right dose of the medication, but where was that particular thing that you needed in that particular task?&nbsp;&nbsp;</p>



<p>I can&#8217;t imagine that Epic does not have that in its product line. And if not, I know there must be other companies that essentially want to create that wrapper. So I do think, though, that the danger of multiple integrations is that you still want to have the equivalent of a single thought process that cares about the patient bringing those different processes together. And I don&#8217;t know if that&#8217;s Epic&#8217;s responsibility, the hospital&#8217;s responsibility, whether it&#8217;s actually a patient agent. But someone needs to be also worrying about all those AIs that are being integrated into the patient record. So … what do you think, Carey?&nbsp;</p>



<p><strong>GOLDBERG: </strong>What struck me most about what Seth said was his description of the Cosmos project, and I, you know, I have been drinking Zak’s Kool-Aid for a very long time, [LAUGHTER] and he—no, in a good way! And he persuaded me long ago that there is this horrible waste happening in that we have all of these electronic medical records, which could be used far, far more to learn from, and in particular, when you as a patient come in, it would be ideal if your physician could call up all the other patients like you and figure out what the optimal treatment for you would be. And it feels like—it <em>sounds</em> <em>like</em>—that&#8217;s one of the central aims that Epic is going for. And if they do that, I think that will redeem a lot of the pain that they&#8217;ve caused physicians these last few years.&nbsp;&nbsp;</p>



<p>And I also found myself thinking, you know, maybe this very painful period of using electronic medical records was really just a growth phase. It was an awkward growth phase. And once AI is fully used the way Zak is beginning to describe, the whole system could start making a lot more sense for everyone.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. One conversation I&#8217;ve had with Seth, in all of this is, you know, with AI and its development, is there a future, a near future where we don&#8217;t have an EHR [electronic health record] system at all? You know, AI is just listening and just somehow absorbing all the information. And, you know, one thing that Seth said, which I felt was prescient, and I&#8217;d love to get your reaction, especially Zak, on this is he said, I think that … he said, technically, it could happen, but the problem is right now, actually doctors do a lot of their thinking when they write and review notes. You know, the actual process of being a doctor is not just being with a patient, but it&#8217;s actually thinking later. What do you make of that?&nbsp;</p>



<p><strong>KOHANE:</strong> So one of the most valuable experiences I had in training was something that&#8217;s more or less disappeared in medicine, which is the post-clinic conference, where all the doctors come together and we go through the cases that we just saw that afternoon. And we, actually, were trying to take potshots at each other [LAUGHTER] in order to actually improve. <em>Oh, did you actually do that? Oh, I forgot. I&#8217;m going to go call the patient and do that.</em>&nbsp;&nbsp;</p>



<p>And that really happened. And I think that, yes, doctors do think, and I do think that we are insufficiently using yet the artificial intelligence currently in the ambient dictation mode as much more of a independent agent saying, did you think about that?&nbsp;</p>



<p>I think that would actually make it more interesting, challenging, and clearly better for the patient because that conversation I just told you about with the other doctors, that no longer exists.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Mm-hmm. I want to do one more thing here before we leave Matt and Seth in Episode 2, which is something that Seth said with respect to how to reduce hallucination.&nbsp;&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-951f3acbbf5350968071ce5f956fcbe0"><strong><em>SETH HAIN:</em></strong><em> At that time, there&#8217;s a lot of conversation in the industry around something called RAG, or </em>retrieval-augmented generation<em>. And the idea was, could you pull the relevant bits, the relevant pieces of the chart, into that prompt, that information you shared with the generative AI model, to be able to increase the usefulness of the draft that was being created? And that approach ended up proving and continues to be to some degree, although the techniques have greatly improved, somewhat brittle, right. And I think this becomes one of the things that we are and will continue to improve upon because, as you get a richer and richer amount of information into the model, it does a better job of responding.</em>&nbsp;</p>



<p><strong>LEE: </strong>Yeah, so, Carey, this sort of gets at what you were saying, you know, that shouldn&#8217;t these models be just bringing in a lot more information into their thought processes? And I&#8217;m certain when we wrote our book, I had no idea. I did not conceive of RAG at all. It emerged a few months later.&nbsp;&nbsp;</p>



<p>And to my mind, I remember the first time I encountered RAG—<em>Oh, this is going to solve all of our problems of hallucination</em>. But it&#8217;s turned out to be harder. It&#8217;s improving day by day, but it’s turned out to be a lot harder.&nbsp;</p>



<p><strong>KOHANE: </strong>Seth makes a very deep point, which is the way RAG is implemented is basically some sort of technique for pulling the right information that&#8217;s contextually relevant. And the way that&#8217;s done is typically heuristic at best. And it&#8217;s not … doesn’t have the same depth of reasoning that the rest of the model has.&nbsp;&nbsp;</p>



<p>And I&#8217;m just wondering, Peter, what you think, given the fact that now context lengths seem to be approaching a million or more, and people are now therefore using the full strength of the transformer on that context and are trying to figure out different techniques to make it pay attention to the middle of the context. In fact, the RAG approach perhaps was just a transient solution to the fact that it&#8217;s going to be able to amazingly look in a thoughtful way at the entire record of the patient, for example. What do you think, Peter?&nbsp;</p>



<p><strong>LEE: </strong>I think there are three things, you know, that are going on, and I&#8217;m not sure how they&#8217;re going to play out and how they&#8217;re going to be balanced. And I&#8217;m looking forward to talking to people in later episodes of this podcast, you know, people like Sébastien Bubeck or Bill Gates about this, because, you know, there is the pretraining phase, you know, when things are sort of compressed and baked into the base model.&nbsp;&nbsp;</p>



<p>There is the in-context learning, you know, so if you have extremely long or infinite context, you&#8217;re kind of learning as you go along. And there are other techniques that people are working on, you know, various sorts of dynamic reinforcement learning approaches, and so on. And then there is what maybe you would call <em>structured RAG</em>, where you do a pre-processing. You go through a big database, and you figure it all out. And make a very nicely structured database the AI can then consult with later.&nbsp;&nbsp;</p>



<p>And all three of these in different contexts today seem to show different capabilities. But they&#8217;re all pretty important in medicine.&nbsp;<strong>&nbsp;</strong>&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Moving on to Episode 3, we talked to Dave DeBronkart, who is also known as “e-Patient Dave,” an advocate of patient empowerment, and then also Christina Farr, who has been doing a lot of venture investing for consumer health applications.&nbsp;&nbsp;</p>



<p>Let&#8217;s get right into this little snippet from something that e-Patient Dave said that talks about the sources of medical information, particularly relevant for when he was receiving treatment for stage 4 kidney cancer.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-ef0e66b9bc441749fd22ccc0a8944a34"><strong><em>DAVE DEBRONKART:</em></strong><em> And I&#8217;m making a point here of illustrating that I am anything but medically trained, right. And yet I still, I want to understand as much as I can. I was months away from dead when I was diagnosed, but in the patient community, I learned that they had a whole bunch of information that didn&#8217;t exist in the medical literature. Now today we understand there&#8217;s publication delays; there&#8217;s all kinds of reasons. But there&#8217;s also a whole bunch of things, especially in an unusual condition, that will never rise to the level of deserving NIH [National Institute of Health] funding and research.</em></p>



<p><strong>LEE: </strong>All right. So I have a question for you, Carey, and a question for you, Zak, about the whole conversation with e-Patient Dave, which I thought was really remarkable. You know, Carey, I think as we were preparing for this whole podcast series, you made a comment—I actually took it as a complaint—that not as much has happened as I had hoped or thought. People aren&#8217;t thinking boldly enough, you know, and I think, you know, I agree with you in the sense that I think we expected a lot more to be happening, particularly in the consumer space. I&#8217;m giving you a chance to vent about this.&nbsp;</p>



<p><strong>GOLDBERG: </strong>[LAUGHTER] Thank you! Yes, that has been by far the most frustrating thing to me. I think that the potential for AI to improve <em>everybody’s</em> health is so enormous, and yet, you know, it needs some sort of support to be able to get to the point where it can do that. Like, remember in the book we wrote about Greg Moore talking about how <em>half</em> of the planet doesn&#8217;t have healthcare, but people overwhelmingly have cellphones. And so you could connect people who have no healthcare to the world&#8217;s medical knowledge, and that could certainly do some good.&nbsp;&nbsp;</p>



<p>And I have one great big problem with e-Patient Dave, which is that, God, he&#8217;s fabulous. He&#8217;s super smart. Like, he&#8217;s not a typical patient. He&#8217;s an off-the-charts, brilliant patient. And so it&#8217;s hard to … and so he&#8217;s a great sort of lead early-adopter-type person, and he can sort of show the way for others.&nbsp;&nbsp;</p>



<p>But what I had hoped for was that there would be more visible efforts to really help patients optimize their healthcare. Probably it&#8217;s happening a lot in quiet ways like that any discharge instructions can be instantly beautifully translated into a patient&#8217;s native language and so on. But it&#8217;s almost like there isn&#8217;t a mechanism to allow this sort of mass consumer adoption that I would hope for.</p>



<p><strong>LEE:</strong> Yeah. But you have written some, like, you even <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bostonglobe.com/2023/04/06/opinion/chat-gpt-health-care-medicine/" target="_blank" rel="noreferrer noopener">wrote about that person who saved his dog<span class="sr-only"> (opens in new tab)</span></a>. So do you think … you know, and maybe a lot more of that is just happening quietly that we just never hear about?&nbsp;</p>



<p><strong>GOLDBERG:</strong> I&#8217;m sure that there is a lot of it happening quietly. And actually, that&#8217;s another one of my complaints is that no one is gathering that stuff. It&#8217;s like you might happen to see something on social media. Actually, e-Patient Dave has a hashtag, <em>PatientsUseAI</em>, and a blog, as well. So he&#8217;s trying to do it. But I don&#8217;t know of any sort of overarching or academic efforts to, again, to surveil what&#8217;s the actual use in the population and see what are the pros and cons of what&#8217;s happening.&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm. So, Zak, you know, the thing that I thought about, especially with that snippet from Dave, is your opening for Chapter 8 that you wrote, you know, about your first patient dying in your arms. I still think of how traumatic that must have been. Because, you know, in that opening, you just talked about all the little delays, all the little paper-cut delays, in the whole process of getting some new medical technology approved. But there&#8217;s another element that Dave kind of speaks to, which is just, you know, patients who are experiencing some issue are very, sometimes very motivated. And there&#8217;s just a lot of stuff on social media that happens.&nbsp;</p>



<p><strong>KOHANE: </strong>So this is where I can both agree with Carey and also disagree. I think when people have an actual health problem, they are now routinely using it.&nbsp;</p>



<p><strong>GOLDBERG:</strong> Yes, that&#8217;s true.&nbsp;</p>



<p><strong>KOHANE: </strong>And that situation is happening more often because medicine is failing. This is something that did not come up enough in our book. And perhaps that&#8217;s because medicine is actually feeling a lot more rickety today than it did even two years ago.&nbsp;&nbsp;</p>



<p>We actually mentioned the problem. I think, Peter, you may have mentioned the problem with the lack of primary care. But now in Boston, our biggest healthcare system, all the practices for primary care are closed. I cannot get for my own faculty—<em>residents</em> at MGH [Massachusetts General Hospital] can&#8217;t get primary care doctor. And so …&nbsp;</p>



<p><strong>LEE: </strong>Which is just crazy. I mean, these are amongst the most privileged people in medicine, and they can&#8217;t find a primary care physician. That&#8217;s incredible.&nbsp;</p>



<p><strong>KOHANE: </strong>Yeah, and so therefore … and I wrote an <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nejm.org/doi/full/10.1056/NEJMp2404691" target="_blank" rel="noreferrer noopener">article about this in the <em>NEJM</em> [<em>New England Journal of Medicine</em>]<span class="sr-only"> (opens in new tab)</span></a> that medicine is in such dire trouble that we have incredible technology, incredible cures, but where the rubber hits the road, which is at primary care, we don&#8217;t have very much.&nbsp;&nbsp;</p>



<p>And so therefore, you see people who know that they have a six-month wait till they see the doctor, and all they can do is say, “I have this rash. Here&#8217;s a picture. What&#8217;s it likely to be? What can I do?” “I&#8217;m gaining weight. How do I do a ketogenic diet?” Or, “How do I know that this is the flu?” &nbsp;<br>&nbsp;<br>This is happening all the time, where acutely patients have actually solved problems that doctors have not. Those are spectacular. But I&#8217;m saying more routinely because of the failure of medicine. And it&#8217;s not just in our fee-for-service United States. It&#8217;s in the UK; it&#8217;s in France. These are first-world, developed-world problems. And we don&#8217;t even have to go to lower- and middle-income countries for that.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>GOLDBERG:</strong> But I think it&#8217;s important to note that, I mean, so you&#8217;re talking about how even the most elite people in medicine can&#8217;t get the care they need. But there&#8217;s also the point that we have so much concern about equity in recent years. And it&#8217;s likeliest that what we&#8217;re doing is exacerbating inequity because it&#8217;s only the more connected, you know, better off people who are using AI for their health.&nbsp;</p>



<p><strong>KOHANE: </strong>Oh, yes. I know what various Harvard professors are doing. They&#8217;re paying for a concierge doctor. And that&#8217;s, you know, a $5,000- to $10,000-a-year-minimum investment. <em>That&#8217;s</em> inequity.&nbsp;</p>



<p><strong>LEE: </strong>When we wrote our book, you know, the idea that GPT-4 wasn&#8217;t trained specifically for medicine, and that was amazing, but it might get even better and maybe would be necessary to do that. But one of the insights for me is that in the consumer space, the kinds of things that people ask about are different than what the board-certified clinician would ask.&nbsp;</p>



<p><strong>KOHANE: </strong>Actually, that&#8217;s, I just recently coined the term. It&#8217;s the &#8230; maybe it&#8217;s &#8230; well, at least it&#8217;s new to me. It&#8217;s the <em>technology</em> or <em>expert</em> paradox. And that is the more expert and narrow your medical discipline, the more trivial it is to translate that into a specialized AI. So echocardiograms? We can now do beautiful echocardiograms. That&#8217;s really hard to do. I don&#8217;t know how to interpret an echocardiogram. But they can do it really, really well. Interpret an EEG [electroencephalogram]. Interpret a genomic sequence. But understanding the fullness of the human condition, that&#8217;s actually hard. And actually, that&#8217;s what primary care doctors do best. But the paradox is right now, what is easiest for AI is also the most highly paid in medicine. [LAUGHTER] Whereas what is the hardest for AI in medicine is the least regarded, least paid part of medicine.&nbsp;</p>



<p><strong>GOLDBERG: </strong>So this brings us to the question I wanted to throw at both of you actually, which is we&#8217;ve had this spasm of incredibly prominent people predicting that in fact physicians would be pretty obsolete within the next few years. We had Bill Gates saying that; we had Elon Musk saying surgeons are going to be obsolete within a few years. And I think we had Demis Hassabis saying, “Yeah, we&#8217;ll probably cure most diseases within the next decade or so.” [LAUGHS]&nbsp;</p>



<p>So what do you think? And also, Zak, to what you were just saying, I mean, you&#8217;re talking about being able to solve very general overarching problems. But in fact, these general overarching models are actually able, I would think, are able to do that because they are broad. So what are we heading towards do you think? What should the next book be &#8230; <em>The end of doctors</em>? [LAUGHS]&nbsp;</p>



<p><strong>KOHANE: </strong>So I do recall a conversation that … we were at a table with Bill Gates, and Bill Gates immediately went to this, which is advancing the cutting edge of science. And I have to say that I think it will accelerate discovery. But eliminating, let&#8217;s say, cancer? I think that&#8217;s going to be … that’s just super hard. The reason it&#8217;s super hard is we don&#8217;t have the data or even the beginnings of the understanding of all the ways this devilish disease managed to evolve around our solutions.&nbsp;&nbsp;</p>



<p>And so that seems extremely hard. I think we&#8217;ll make some progress accelerated by AI, but solving it in a way Hassabis says, God bless him. I hope he&#8217;s right. I&#8217;d love to have to eat crow in 10 or 20 years, but I don&#8217;t think so. I do believe that a surgeon working on one of those Davinci machines, that stuff can be, I think, automated.&nbsp;&nbsp;</p>



<p>And so I think that&#8217;s one example of one of the paradoxes I described. And it won&#8217;t be that we&#8217;re replacing doctors. I just think we&#8217;re running out of doctors. I think it&#8217;s really the case that, as we said in the book, we&#8217;re getting a huge deficit in primary care doctors.&nbsp;</p>



<p>But even the subspecialties, <em>my</em> subspecialty, pediatric endocrinology, we&#8217;re only filling half of the available training slots every year. And why? Because it&#8217;s a lot of work, a lot of training, and frankly doesn&#8217;t make as much money as some of the other professions.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Yeah, I tend to think that, you know, there are going to be always a need for human doctors, not for their skills. In fact, I think their skills increasingly will be replaced by machines. And in fact, I&#8217;ve talked about a flip. In fact, patients will demand, <em>Oh my god, you mean you&#8217;re going to try to do that yourself instead of having the computer do it?</em> There&#8217;s going to be that sort of flip. But I do think that when it comes to people&#8217;s health, people want the comfort of an authority figure that they trust. And so what is more of a question for me is whether we will ever view a machine as an authority figure that we can trust.&nbsp;</p>



<p>And before I move on to Episode 4, which is on norms, regulations and ethics, I’d like to hear from Chrissy Farr on one more point on consumer health, specifically as it relates to pregnancy:&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-fffcd0526d088276dcea59487144ffa4"><strong><em>CHRISTINA FARR:</em></strong> <em>For a lot of women, it&#8217;s their first experience with the hospital. And, you know, I think it&#8217;s a really big opportunity for these systems to get a whole family on board and keep them kind of loyal. And a lot of that can come through, you know, just delivering an incredible service. Unfortunately, I don&#8217;t think that we are delivering incredible services today to women in this country. I see so much room for improvement.</em></p>



<p><strong>LEE:</strong> In the consumer space, I don&#8217;t think we really had a focus on those periods in a person&#8217;s life when they have a lot of engagement, like pregnancy, or I think another one is menopause, cancer. You know, there are points where there is, like, very intense engagement. And we heard that from e-Patient Dave, you know, with his cancer and Chrissy with her pregnancy. Was that a miss in our book? What do think, Carey?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, I don&#8217;t think so. I think it&#8217;s true that there are many points in life when people are highly engaged. To me, the problem thus far is just that I haven&#8217;t seen consumer-facing companies offering beautiful AI-based products. I think there&#8217;s no question at all that the market is there if you have the products to offer.&nbsp;</p>



<p><strong>LEE:</strong> So, what do you think this means, Zak, for, you know, like Boston Children&#8217;s or Mass General Brigham—you know, the big places?&nbsp;</p>



<p><strong>KOHANE: </strong>So again, all these large healthcare systems are in tough shape. MGB [Mass General Brigham] would be fully in the red if not for the fact that its investments, of all things, have actually produced. If you look at the large healthcare systems around the country, they are in the red. And there&#8217;s multiple reasons why they&#8217;re in the red, but among them is cost of labor.&nbsp;&nbsp;</p>



<p>And so we&#8217;ve created what used to be a very successful beast, the health center. But it&#8217;s developed a very expensive model and a highly regulated model. And so when you have high revenue, tiny margins, your ability to disrupt yourself, to innovate, is very, very low because you will have to talk to the board next year if you went from 2% positive margin to 1% negative margin.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>And so I think we&#8217;re all waiting for one of the two things to happen, either a new kind of healthcare delivery system being generated or ultimately one of these systems learns how to disrupt itself.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yeah.<strong> </strong>All right. I think we have to move on to Episode 4. And, you know, when it came to the question of regulation, I think this is … my read is when we were writing our book, this is the part that we struggled with the most.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG: </strong>We punted. [LAUGHS] We totally punted to the AI.&nbsp;</p>



<p><strong>LEE:</strong> We had three amazing guests. One was Laura Adams from National Academy of Medicine. Let&#8217;s play a snippet from her.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-9de462fe0a5cfd6b01f996a3f6aac161"><strong><em>LAURA ADAMS:</em></strong> <em>I think one of the most provocative and exciting articles that I saw written recently was by Bakul Patel and David Blumenthal, who posited, should we be regulating generative AI as we do a licensed and qualified provider? Should it be treated in the sense that it&#8217;s got to have a certain amount of training and a foundation that&#8217;s got to pass certain tests? Does it have to report its performance? And I&#8217;m thinking, what a provocative idea, but it&#8217;s worth considering.</em></p>



<p><strong>LEE: </strong>All right, so I very well remember that we had discussed this kind of idea when we were writing our book. And I think before we finished our book, I personally rejected the idea. But now two years later, what do the two of you think? I&#8217;m dying to hear.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Well, wait, why … what do you think? Like, are you sorry that you rejected it?&nbsp;</p>



<p><strong>LEE: </strong>I&#8217;m still skeptical because when we are licensing human beings as doctors, you know, we&#8217;re making a lot of implicit assumptions that we don&#8217;t test as part of their licensure, you know, that first of all, they are [a] human being and they care about life, and that, you know, they have a certain amount of common sense and shared understanding of the world.&nbsp;&nbsp;</p>



<p>And there&#8217;s all sorts of sort of implicit assumptions that we have about each other as human beings living in a society together. That you know how to study, you know, because I know you just went through three years of medical or four years of medical school and all sorts of things. And so the standard ways that we license human beings, they don&#8217;t need to test all of that stuff. But somehow intuitively, all of that seems really important.&nbsp;</p>



<p>I don&#8217;t know. Am I wrong about that?&nbsp;</p>



<p><strong>KOHANE: </strong>So it&#8217;s&nbsp;compared with what issue? Because we know for a fact that doctors who do a lot of a procedure, like do this procedure, like high-risk deliveries all the time, have better outcomes than ones who only do a few high risk. We talk about it, but we don&#8217;t actually make it explicit to patients or regulate that you have to have this minimal amount. And it strikes me that in some sense, and, oh, very importantly, these things called human beings <em>learn</em> on the job. And although I used to be very resentful of it as a resident, when someone would say, I don&#8217;t want the resident, I want the &#8230;&nbsp;</p>



<p><strong>GOLDBERG: </strong>… the attending. [LAUGHTER]&nbsp;</p>



<p><strong>KOHANE: </strong>… they had a point. And so the truth is, maybe I was a <em>wonderful</em> resident, but some people were not so great. [LAUGHTER] And so it might be the best outcome if we actually, just like for human beings, we say, yeah, OK, it&#8217;s this good, but don&#8217;t let it work autonomously, or it&#8217;s done a thousand of them, just let it go. We just don&#8217;t have practically speaking, we don&#8217;t have the environment, the lab, to test them. Now, maybe if they get embodied in robots and literally go around with us, then it&#8217;s going to be [in some sense] a lot easier. I don&#8217;t know.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah, I think I would take a step back and say, first of all, we weren&#8217;t the only ones who were stumped by regulating AI. Like, nobody has done it yet in the United States to this day, right. Like, we do not have standing regulation of AI in medicine at all in fact. And that raises the issue of … the story that you hear often in the biotech business, which is, you know, more prominent here in Boston than anywhere else, is that thank goodness Cambridge put out, the city of Cambridge, put out some regulations about biotech and how you could dump your lab waste and so on. And that enabled the enormous growth of biotech here.&nbsp;&nbsp;</p>



<p>If you don&#8217;t have the regulations, then you can&#8217;t have the growth of AI in medicine that is worthy of having. And so, I just &#8230; we&#8217;re not the ones who should do it, but I just wish somebody would.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>GOLDBERG:</strong> Zak.&nbsp;</p>



<p><strong>KOHANE: </strong>Yeah, but I want to say this as always, execution is everything, even in regulation.&nbsp;&nbsp;</p>



<p>And so I&#8217;m mindful that a conference that both of you attended, the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dbmi.hms.harvard.edu/events/raise-symposium" target="_blank" rel="noreferrer noopener">RAISE conference [Responsible AI for Social and Ethical Healthcare]<span class="sr-only"> (opens in new tab)</span></a>. The Europeans in that conference came to <em>me</em> personally and thanked me for organizing this conference about safe and effective use of AI because they said back home in Europe, all that we&#8217;re talking about is risk, not opportunities to improve care.&nbsp;&nbsp;</p>



<p>And so there is a version of regulation which just locks down the present and does not allow the future that we&#8217;re talking about to happen. And so, Carey, I absolutely hear you that we need to have a regulation that takes away some of the uncertainty around liability, around the freedom to operate that would allow things to progress. But we wrote in our book that premature regulation might actually focus on the wrong thing. And so since I&#8217;m an optimist, it may be the fact that we don&#8217;t have much of a regulatory infrastructure today, that it allows … it&#8217;s a unique opportunity—I&#8217;ve said this now to several leaders—for the healthcare systems to say, this is the regulation we need.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG:</strong> It&#8217;s true.&nbsp;</p>



<p><strong>KOHANE:</strong> And previously it was top-down. It was coming from the administration, and those executive orders are now history. But there is an opportunity, which may or may not be attained, there is an opportunity for the healthcare leadership—for experts in surgery—to say, “This is what we should expect.”&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;&nbsp;</p>



<p><strong>KOHANE: </strong>I would love for this to happen. I haven&#8217;t seen evidence that it’s happening yet.&nbsp;</p>



<p><strong>GOLDBERG: </strong>No, no. And there&#8217;s this other huge issue, which is that it&#8217;s changing so fast. It&#8217;s moving so fast. That something that makes sense today won&#8217;t in six months. So, what do you do about <em>that</em>?&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah, that is something I feel proud of because when I went back and looked at our chapter on this, you know, we did make that point, which I think has turned out to be true.&nbsp;&nbsp;</p>



<p>But getting back to this conversation, there&#8217;s something, a snippet of something, that Vardit Ravitsky said that I think touches on this topic.&nbsp;&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-f32c52bc5d3b818138e20afb885e1404"><strong><em>VARDIT RAVITSKY:</em></strong> <em>So my pushback is, are we seeing AI exceptionalism in the sense that if it&#8217;s AI, huh, panic! We have to inform everybody about everything, and we have to give them choices, and they have to be able to reject that tool and the other tool versus, you know, the rate of human error in medicine is awful. So why are we so focused on informed consent and empowerment regarding implementation of AI and less in other contexts?</em></p>



<p><strong>GOLDBERG:</strong> Totally agree. Who cares about informed consent about AI. Don&#8217;t want it. Don&#8217;t need it. Nope.&nbsp;</p>



<p><strong>LEE:</strong> Wow. Yeah. You know, and this &#8230; Vardit of course is one of the leading bioethicists, you know, and of course prior to AI, she was really focused on genetics. But now it&#8217;s all about AI.&nbsp;&nbsp;</p>



<p>And, Zak, you know, you and other doctors have always told me, you know, the truth of the matter is, you know, what do you call the bottom-of-the-class graduate of a medical school?&nbsp;</p>



<p>And the answer is “doctor.”&nbsp;</p>



<p><strong>KOHANE:</strong> “Doctor.” Yeah. Yeah, I think that again, this gets to compared with what? We have to compare AI not to the medicine we imagine we have, or we would like to have, but to the medicine we have today. And if we&#8217;re trying to remove inequity, if we&#8217;re trying to improve our health, that&#8217;s what … those are the right metrics. And so that can be done so long as we avoid <em>catastrophic</em> consequences of AI.&nbsp;&nbsp;</p>



<p>So what would the catastrophic consequence of AI be? It would be a systematic behavior that we were unaware of that was causing poor healthcare. So, for example, you know, changing the dose on a medication, making it 20% higher than normal so that the rate of complications of that medication went from 1% to 5%. And so we do need some sort of monitoring.&nbsp;&nbsp;</p>



<p>We haven&#8217;t put out the paper yet, but in computer science, there&#8217;s, well, in programming, we know very well the value for understanding how our computer systems work.&nbsp;&nbsp;</p>



<p>And there was a guy by name of Allman, I think he&#8217;s still at a company called Sendmail, who created something called <em>syslog</em>. And syslog is basically a log of all the crap that&#8217;s happening in our operating system. And so I&#8217;ve been arguing now for the creation of <em>MedLog</em>. And MedLog … in other words, what we cannot measure, we cannot regulate, actually.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>And so what we need to have is MedLog, which says, “Here&#8217;s the context in which a decision was made. Here&#8217;s the version of the AI, you know, the exact version of the AI. Here was the data.” And we just have MedLog. And I think MedLog is actually incredibly important for being able to measure, to just do what we do in … it’s basically the black box for, you know, when there&#8217;s a crash. You know, we&#8217;d like to think we could do better than crash. We can say, “Oh, we&#8217;re seeing from MedLog that this practice is turning a little weird.” But worst case, patient dies, [we] can see in MedLog, what was the information this thing knew about it? And did it make the right decision? We can actually go for transparency, which like in aviation, is much greater than in most human endeavors.&nbsp;&nbsp;</p>



<p><strong>GOLDBERG:</strong> Sounds great.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, it&#8217;s sort of like a black box. I was thinking of the aviation black box kind of idea. You know, you bring up medication errors, and I have one more snippet. This is from our guest Roxana Daneshjou from Stanford.</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-ae255d08c11438266ea149ce58f642b8"><strong><em>ROXANA DANESHJOU:</em></strong> <em>There was a mistake in her after-visit summary about how much Tylenol she could take. But I, as a physician, knew that this dose was a mistake. I actually asked ChatGPT. I gave it the whole after-visit summary, and I said, are there any mistakes here? And it clued in that the dose of the medication was wrong.</em></p>



<p><strong>LEE: </strong>Yeah, so this is something we did write about in the book. We made a prediction that AI might be a second set of eyes, I think is the way we put it, catching things. And we actually had examples specifically in medication dose errors. I think for me, I expected to see a lot more of that than we are.&nbsp;</p>



<p><strong>KOHANE:</strong> Yeah, it goes back to our conversation about Epic or competitor Epic doing that. I think we&#8217;re going to see that having oversight over all medical orders, all orders in the system, critique, real-time critique, where we&#8217;re both aware of alert fatigue. So we don&#8217;t want to have too many false positives. At the same time, knowing what are critical errors which could immediately affect lives. I think that is going to become in terms of—and driven by quality measures—a product.&nbsp;</p>



<p><strong>GOLDBERG:</strong> And I think word will spread among the general public that kind of the same way in a lot of countries when someone&#8217;s in a hospital, the first thing people ask relatives are, well, who&#8217;s with them? Right?&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Yup.&nbsp;</p>



<p><strong>GOLDBERG:</strong> You wouldn&#8217;t leave someone in hospital without relatives. Well, you wouldn&#8217;t maybe leave your medical &#8230;&nbsp;&nbsp;</p>



<p><strong>KOHANE: </strong>By the way, that country is called the United States.&nbsp;</p>



<p><strong>GOLDBERG:</strong> Yes, that&#8217;s true. [LAUGHS] It is true here now, too. But similarly, I would tell any loved one that they would be well advised to keep using AI to check on their medical care, right. Why not?&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Yeah. Last topic, just for this Episode 4. Roxana, of course, I think really made a name for herself in the AI era writing, actually just prior to ChatGPT, you know, writing some famous papers about how computer vision systems for dermatology were biased against dark-skinned people. And we did talk some about bias in these AI systems, but I feel like we underplayed it, or we didn&#8217;t understand the magnitude of the potential issues. What are your thoughts?&nbsp;</p>



<p><strong>KOHANE: </strong>OK, I want to push back, because I&#8217;ve been asked this question several times. And so I have two comments. One is, over 100,000 doctors practicing medicine, I know they have biases. Some of them actually may be all in the same direction, and not good. But I have no way of actually measuring that. With AI, I know exactly how to measure that at scale and affordably. Number one. Number two, same 100,000 doctors. Let&#8217;s say I do know what their biases are. How hard is it for me to change that bias? It&#8217;s impossible …&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah.&nbsp;&nbsp;</p>



<p><strong>KOHANE:</strong> … practically speaking. Can I change the bias in the AI? Somewhat. Maybe some completely.&nbsp;</p>



<p>I think that we&#8217;re in a much better situation.&nbsp;</p>



<p><strong>GOLDBERG:</strong> Agree.&nbsp;</p>



<p><strong>LEE:</strong> I think Roxana made also the super interesting point that there&#8217;s bias in the whole system, not just in individuals, but, you know, there&#8217;s structural bias, so to speak.&nbsp;&nbsp;</p>



<p><strong>KOHANE:</strong> There is.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Hmm. There was a super interesting paper that Roxana wrote not too long ago—her and her collaborators—showing AI&#8217;s ability to detect, to spot bias decision-making by others. Are we going to see more of that?&nbsp;</p>



<p><strong>KOHANE:</strong> Oh, yeah, I was very pleased when, in <em>NEJM AI</em> [<em>New England Journal of Medicine Artificial Intelligence</em>], we published a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIp2400583" target="_blank" rel="noreferrer noopener">piece with Marzyeh Ghassemi<span class="sr-only"> (opens in new tab)</span></a>, and what they were talking about was actually—and these are researchers who had published extensively on bias and threats from AI. And they actually, in this article, did the flip side, which is how much better AI can do than human beings in this respect.&nbsp;&nbsp;</p>



<p>And so I think that as some of these computer scientists enter the world of medicine, they&#8217;re becoming more and more aware of human foibles and can see how these systems, which if they only looked at the pretrained state, would have biases. But now, where we know how to fine-tune the de-bias in a variety of ways, they can do a lot better and, in fact, I think are much more … a much greater reason for optimism that we can change some of these noxious biases than in the pre-AI era.&nbsp;</p>



<p><strong>GOLDBERG:</strong> And thinking about Roxana&#8217;s dermatological work on how I think there wasn&#8217;t sufficient work on skin tone as related to various growths, you know, I think that one thing that we totally missed in the book was the dawn of multimodal uses, right.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Yeah, yeah.&nbsp;</p>



<p><strong>GOLDBERG:</strong> That&#8217;s been <em>truly</em> amazing that in fact all of these visual and other sorts of data can be entered into the models and move them forward.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Well, maybe on these slightly more optimistic notes, we&#8217;re at time. You know, I think ultimately, I feel pretty good still about what we did in our book, although there were a lot of misses. [LAUGHS] I don&#8217;t think any of us could really have predicted <em>really</em> the extent of change in the world.  </p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>So, Carey, Zak, just so much fun to do some reminiscing but also some reflection about what we did.&nbsp;</p>



<p>[THEME MUSIC]&nbsp;</p>



<p>And to our listeners, as always, thank you for joining us. We have some really great guests lined up for the rest of the series, and they’ll help us explore a variety of relevant topics—from AI drug discovery to what medical students are seeing and doing with AI and more.&nbsp;&nbsp;</p>



<p>We hope you’ll continue to tune in. And if you want to catch up on any episodes you might have missed, you can find them at <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/airevolutionpodcast" target="_blank" rel="noreferrer noopener">aka.ms/AIrevolutionPodcast<span class="sr-only"> (opens in new tab)</span></a> <em>or</em> wherever you listen to your favorite podcasts.  &nbsp;</p>



<p>Until next time. &nbsp;</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--2"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-coauthor-roundtable-reflecting-on-real-world-of-doctors-developers-patients-and-policymakers/">Coauthor roundtable: Reflecting on real world of doctors, developers, patients, and policymakers</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Predicting and explaining AI model performance: A new approach to evaluation</title>
		<link>https://www.microsoft.com/en-us/research/blog/predicting-and-explaining-ai-model-performance-a-new-approach-to-evaluation/</link>
		
		<dc:creator><![CDATA[Lexin Zhou, Xing Xie]]></dc:creator>
		<pubDate>Mon, 12 May 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1138048</guid>

					<description><![CDATA[<p>ADeLe, a new evaluation method, explains what AI systems are good at—and where they’re likely to fail. By breaking tasks into ability-based requirements, it has the potential to provide a clearer way to evaluate and predict AI model performance.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/predicting-and-explaining-ai-model-performance-a-new-approach-to-evaluation/">Predicting and explaining AI model performance: A new approach to evaluation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW.jpg" alt="The image shows a radar chart comparing the performance of different AI models across various metrics. The chart has a circular grid with labeled axes including VO, AS, CEc, CEe, CL, MCr, MCt, MCu, MS, QLI, QLqA, SNs, KNa, KNc, KNF, KNn, and AT. Different AI models are represented by various line styles: Babbage-002 (dotted line), Davinci-002 (dash-dotted line), GPT-3.5-Turbo (dashed line), GPT-4.0 (solid thin line), OpenAI ol-mini (solid thick line), and OpenAI o1 (solid bold line). There is a legend in the bottom left corner explaining the line styles for each model. The background transitions from blue on the left to green on the right." class="wp-image-1139061" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/ADeLe-BlogHeroFeature-1400x788-NEW-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>With support from the <a href="https://www.microsoft.com/en-us/research/project/afmr-benchmarks-evaluation-and-measurement/" target="_blank" rel="noreferrer noopener">Accelerating Foundation Models Research</a> (AFMR) grant program, a team of researchers from Microsoft and collaborating institutions has developed an approach to evaluate AI models that predicts how they will perform on unfamiliar tasks and explain why, something current benchmarks struggle to do.</p>



<p>In the paper, “<a href="https://www.microsoft.com/en-us/research/publication/general-scales-unlock-ai-evaluation-with-explanatory-and-predictive-power/">General Scales Unlock AI Evaluation with Explanatory and Predictive Power</a>,” they introduce a methodology that goes beyond measuring overall accuracy. It assesses the knowledge and cognitive abilities a task requires and evaluates them against the model&#8217;s capabilities.</p>



<h2 class="wp-block-heading" id="adele-an-ability-based-approach-to-task-evaluation">ADeLe: An ability-based approach to task evaluation</h2>



<p>The framework uses ADeLe (annotated-demand-levels), a technique that assesses how demanding a task is for an AI model by applying measurement scales for 18 types of cognitive and knowledge-based abilities. This difficulty rating is based on a <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-december-6-2023/">detailed rubric</a>, originally developed for human tasks and shown to work reliably when <a href="https://www.microsoft.com/en-us/research/publication/evaluating-general-purpose-ai-with-psychometrics/">applied by AI models</a>.</p>



<p>By comparing what a task requires with what a model can do, ADeLe generates an <em>ability profile</em> that not only predicts performance but also explains why a model is likely to succeed or fail—linking outcomes to specific strengths or limitations.</p>



<p>The 18 scales reflect core cognitive abilities (e.g., attention, reasoning), knowledge areas (e.g., natural or social sciences), and other task-related factors (e.g., prevalence of the task on the internet). Each task is rated from 0 to 5 based on how much it draws on a given ability. For example, a simple math question might score 1 on formal knowledge, while one requiring advanced expertise could score 5. Figure 1 illustrates how the full process works—from rating task requirements to generating ability profiles.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1426" height="1049" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1.png" alt="Figure 1: This diagram presents a framework for explaining and predicting AI performance on new tasks using cognitive demand profiles. The System Process (top) evaluates an AI system on the ADeLe Battery—tasks annotated with DeLeAn rubrics—to create an ability profile with each dimension representing what level of demand the model can reach. The Task Process (bottom) applies the same rubrics to new tasks, generating demand profiles from annotated inputs. An optional assessor model can be trained to robustly predict how well the AI system will perform on these new tasks by matching system abilities to task demands." class="wp-image-1138073" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1.png 1426w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1-300x221.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1-1024x753.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1-768x565.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1-80x60.png 80w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig1-240x177.png 240w" sizes="auto, (max-width: 1426px) 100vw, 1426px" /><figcaption class="wp-element-caption">Figure 1. Top: For each AI model, (1) run the new system on the ADeLe benchmark, and (2) extract its ability profile. Bottom: For each new task or benchmark, (A) apply 18 rubrics and (B) get demand histograms and profiles that explain what abilities the tasks require. Optionally, predict performance on the new tasks for any system based on the demand and ability profiles, or past performance data, of the systems.</figcaption></figure>



<p>To develop this system, the team analyzed 16,000 examples spanning 63 tasks drawn from 20 AI benchmarks, creating a unified measurement approach that works across a wide range of tasks. The <a href="https://www.microsoft.com/en-us/research/publication/general-scales-unlock-ai-evaluation-with-explanatory-and-predictive-power/">paper</a> details how ratings across 18 general scales explain model success or failure and predict performance on new tasks in both familiar and unfamiliar settings.</p>



<h2 class="wp-block-heading" id="evaluation-results">Evaluation results&nbsp;</h2>



<p>Using ADeLe, the team evaluated 20 popular AI benchmarks and uncovered three key findings: 1) Current AI benchmarks have measurement limitations; 2) AI models show distinct patterns of strengths and weaknesses across different capabilities; and 3) ADeLe provides accurate predictions of whether AI systems will succeed or fail on a new task.&nbsp;</p>



<p><strong>1. Revealing hidden flaws in AI testing methods</strong>&nbsp;</p>



<p>Many popular AI tests either don&#8217;t measure what they claim or only cover a limited range of difficulty levels. For example, the Civil Service Examination benchmark is meant to test logical reasoning, but it also requires other abilities, like specialized knowledge and metacognition. Similarly, TimeQA, designed to test temporal reasoning, only includes medium-difficulty questions—missing both simple and complex challenges.&nbsp;</p>



<p><strong>2. Creating detailed AI ability profiles</strong>&nbsp;</p>



<p>Using the 0–5 rating for each ability, the team created comprehensive ability profiles of 15 LLMs. For each of the 18 abilities measured, they plotted “subject characteristic curves” to show how a model’s success rate changes with task difficulty.&nbsp;&nbsp;</p>



<p>They then calculated a score for each ability—the difficulty level at which a model has a 50% chance of success—and used these results to generate radial plots showing each model’s strengths and weaknesses across the different scales and levels, illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1380" height="494" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2.png" alt="Figure 2: The image consists of three radar charts showing ability profiles of 15 LLMs evaluated across 18 ability scales, ranged from 0 to infinity (the higher, the more capable the model is). Each chart has multiple axes labeled with various ability scales such as VO, AS, CEc, AT, CL, MCr, etc. The left chart shows ability for Babbage-002 (light red), Davinci-002 (orange), GPT-3.5-Turbo (red), GPT-4 (dark red), OpenAI o1-mini (gray), and OpenAI o1 (dark gray). The middle chart shows ability for LLaMA models: LLaMA-3.2-1B-Instruct (light blue), LLaMA-3.2-3B-Instruct (blue), LLaMA-3.2-11B-Instruct (dark blue), LLaMA-3.2-90B-Instruct (navy blue), and LLaMA-3.1-405B Instruct (very dark blue). The right chart shows ability for DeepSeek-R1-Dist-Qwen models: DeepSeek-R1-Dist-Qwen-1.5B (light green), DeepSeek-R1-Dist-Qwen-7B (green), DeepSeek-R1-Dist-Qwen-14B (dark green), DK-R1-Dist-Qwen-32B (very dark green). Each model's ability is represented by a colored polygon within the radar charts." class="wp-image-1138072" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2.png 1380w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2-1024x367.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2-768x275.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/ADeLe_Fig2-240x86.png 240w" sizes="auto, (max-width: 1380px) 100vw, 1380px" /><figcaption class="wp-element-caption">Figure 2. Ability profiles for the 15 LLMs evaluated. </figcaption></figure>



<p>This analysis revealed the following:&nbsp;</p>



<ul class="wp-block-list">
<li>When measured against human performance, AI systems show different strengths and weaknesses across the 18 ability scales.&nbsp;</li>
</ul>



<ul class="wp-block-list">
<li>Newer LLMs generally outperform older ones, though not consistently across all abilities.&nbsp;</li>
</ul>



<ul class="wp-block-list">
<li>Knowledge-related performance depends heavily on model size and training methods.&nbsp;</li>
</ul>



<ul class="wp-block-list">
<li>Reasoning models show clear gains over non-reasoning models in logical thinking, learning and abstraction, and social capabilities, such as inferring the mental states of their users.&nbsp;</li>
</ul>



<ul class="wp-block-list">
<li>Increasing the size of general-purpose models after a given threshold only leads to small performance gains.&nbsp;</li>
</ul>



<p><strong>3. Predicting AI success and failure</strong>&nbsp;</p>



<p>In addition to evaluation, the team created a practical prediction system based on demand-level measurements that forecasts whether a model will succeed on specific tasks, even unfamiliar ones.&nbsp;&nbsp;</p>



<p>The system achieved approximately 88% accuracy in predicting the performance of popular models like GPT-4o and LLaMA-3.1-405B, outperforming traditional methods. This makes it possible to anticipate potential failures before deployment, adding the important step of reliability assessment for AI models.</p>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead</h2>



<p>ADeLe can be extended to multimodal and embodied AI systems, and it has the potential to serve as a standardized framework for AI research, policymaking, and security auditing.</p>



<p>This technology marks a major step toward a science of AI evaluation, one that offers both clear explanations of system behavior and reliable predictions about performance. It aligns with the vision laid out in a previous <a href="https://www.microsoft.com/en-us/research/publication/evaluating-general-purpose-ai-with-psychometrics/" target="_blank" rel="noreferrer noopener">Microsoft position paper</a> on the promise of applying psychometrics to AI evaluation and a recent <a href="https://www.microsoft.com/en-us/research/project/societal-ai/white-paper/" target="_blank" rel="noreferrer noopener">Societal AI white paper</a> emphasizing the importance of AI evaluation.</p>



<p>As general-purpose AI advances faster than traditional evaluation methods, this work lays a timely foundation for making AI assessments more rigorous, transparent, and ready for real-world deployment. The research team is working toward building a collaborative community to strengthen and expand this emerging field.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/predicting-and-explaining-ai-model-performance-a-new-approach-to-evaluation/">Predicting and explaining AI model performance: A new approach to evaluation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Abstracts: Heat Transfer and Deep Learning with Hongxia Hao and Bing Lv</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-heat-transfer-and-deep-learning-with-hongxia-hao-and-bing-lv/</link>
		
		<dc:creator><![CDATA[Gretchen Huizinga, Hongxia Hao, Bing Lv]]></dc:creator>
		<pubDate>Thu, 08 May 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1138389</guid>

					<description><![CDATA[<p>Silicon has long borne the burden of heat transfer in electronics, but in a post-Moore’s Law world, researchers like Hongxia Hao and Bing Lv are using AI to discover and design next-generation materials that exceed the limits of silicon’s thermal conductivity.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-heat-transfer-and-deep-learning-with-hongxia-hao-and-bing-lv/">Abstracts: Heat Transfer and Deep Learning with Hongxia Hao and Bing Lv</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated headshots of Hongxia Hao (left) and Bing Lv (right)." class="wp-image-1138528" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Hongxia-and-Bing_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=145216634&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields. Abstracts bring its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.</p>



<p>In this episode, senior researcher <a href="https://www.microsoft.com/en-us/research/people/hongxiahao/">Hongxia Hao<span class="sr-only"> (opens in new tab)</span></a>, and physics professor <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://profiles.utdallas.edu/blv" target="_blank" rel="noreferrer noopener">Bing Lv<span class="sr-only"> (opens in new tab)</span></a>, join host Gretchen Huizinga to talk about how they are using deep learning techniques to probe the upper limits of heat transfer in inorganic crystals, discover novel materials with exceptional thermal conductivity, and rewrite the rulebook for designing high-efficiency electronics and sustainable energy.</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/probing-the-limit-of-heat-transfer-in-inorganic-crystals-with-deep-learning/">Read the paper</a></div>
</div>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/blog/mattersim-a-deep-learning-model-for-materials-under-real-world-conditions/">MatterSim: A deep-learning model for materials under real-world conditions</a><br>Microsoft Research Blog, May 2024</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://personal.utdallas.edu/~blv/">Quantum Materials Research, University of Texas at Dallas<span class="sr-only"> (opens in new tab)</span></a><br>Research group page</li>
</ul>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>GRETCHEN</strong> <strong>HUIZINGA</strong>: Welcome to Abstracts, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. I’m Gretchen Huizinga. In this series, members of the research community at Microsoft give us a quick snapshot – or a podcast abstract – of their new and noteworthy papers.</p>



<p>[MUSIC FADES]&nbsp;</p>



<p>Today I&#8217;m talking to two researchers, Hongxia Hao, a senior researcher at Microsoft Research AI for Science, and Bing Lv, an associate professor in physics at the University of Texas at Dallas. Hongxia and Bing are co-authors of a paper called <strong><em>Probing the Limit of Heat Transfer in Inorganic Crystals with Deep Learning</em></strong>. I&#8217;m excited to learn more about this! Hongxia and Bing, it&#8217;s great to have you both on Abstracts!</p>



				</span>
				<span id="show-more-show-less-toggle-3" class="show-more-show-less-toggleable-content">
					



<p><strong>HONGXIA HAO</strong>: Nice to be here.</p>



<p><strong>BING</strong> <strong>LV</strong>: Nice to be here, too.</p>



<p><strong>HUIZINGA</strong>: So Hongxia, let&#8217;s start with you and a brief overview of this paper. In just a few sentences. Tell us about the problem your research addresses and more importantly, why we should care about it.</p>



<p><strong>HAO</strong>: Let me start with a very simple yet profound question. What&#8217;s the fastest the heat can travel through a solid material? This is not just an academic curiosity, but it&#8217;s a question that touched the bottom of how we build technologies around us. So from the moment when you tap your smartphone, and the moment where the laptop is turned on and functioning, heat is always flowing. So we&#8217;re trying to answer the question of a century-old mystery of the upper limit of heat transfer in solids. So we care about this not just because it&#8217;s a fundamental problem in physics and material science, but because solving it could really rewrite the rulebook for designing high-efficiency electronics and sustainable energy, etc. And nowadays, with very cutting-edge nanometer chips or very fancy technologies, we are packing more computing power into smaller space, but the faster and denser we build, the harder it becomes to remove the heat. So in many ways, thermal bottlenecks, not just transistor density, are now the ceiling of the Moore’s Law. And also the stakes are very enormous. We really wish to bring more thermal solutions by finding more high thermal conductor choices from the perspective of materials discovery with the help of AI.</p>



<p><strong>LV</strong>: So I think one of the biggest things as Hongxia said, right? Thermal solutions will become, eventually become, a bottleneck for all type of heterogeneous integration of the materials. So from this perspective, so how people actually have been finding out previously, all the thermal was the last solution to solve. But now people actually more and more realize all these things have to be upfront. This co-design, all these things become very important. So I think what we are doing right now, integrated with AI, helping to identify the large space of the materials, identify fundamentally what will be the limit of this material, will become very important for the society.</p>



<p><strong>HUIZINGA</strong>: Hmm. Yeah. Hongxia, did you have anything to add to that?</p>



<p><strong>HAO</strong>: Yes, so previously many people are working on exploring these material science questions through experimental tradition and the past few decades people see a new trend using computational materials discovery. Like for example, we do the fundamental solving of the Schrödinger equation using Density Functional Theory [DFT]. Actually, this brings us a lot of opportunities. The question here is, as the theory is getting more and more developed, it’s too expensive for us to make it very large scale and to study tons of materials. Think about this. The bottleneck here, now, is not just about having a very good theory, it&#8217;s about the scale. So, this is where AI, specifically now we are using deep learning, comes into play.</p>



<p><strong>HUIZINGA</strong>: Well, Hongxia, let&#8217;s stay with you for a minute and talk about methodology. How did you do this research and what was the methodology you employed?</p>



<p><strong>HAO</strong>: So here we, for this question, we built a pipeline that spans the AI, the quantum mechanics, and computational brute-force with a blend of efficiency and accuracy. It begins with generating an enormous chemical and structure design space because this is inspired by Slack’s principle. We focus first on simple crystals, and there are the systems most likely to have low and harmonious state, fewer phononic scattering events, and therefore potentially have high thermal conductivities. But we didn&#8217;t stop here. We also included a huge pool of more complex and higher energy structures to ensure diversity and avoid bias. And for each candidate, we first run like a structure relaxation using MatterSim, which is a deep learning foundational model for material science for us to characterize the properties of materials. And we use that screen for dynamic stability. And now it&#8217;s about 200K structures past this filter. And then came another real challenge: calculating the thermal conductivity. We try to solve this problem using the Boltzmann transport equation and the three-phonon scattering process. The twist here is all of this was not done by traditional DFT solvers, but with our deep learning model, the MatterSim. It&#8217;s trained to predict energy, force, and stress. And we can get second- and third-order interatomic force constants directly from here, which can guarantee the accuracy of the solution. And finally, to validate the model&#8217;s predictions, we performed full DFT-based calculations on the top candidates that we found, some of which even include higher-order scattering mechanism, electron phonon coupling effect, etc. And this rigorous validation gave us confidence in the speed and accuracy trade-offs and revealed a spectrum of materials that had either previously been overlooked or were never before conceived.</p>



<p><strong>HUIZINGA</strong>: So Bing, let&#8217;s talk about your research findings. How did things work out for you on this project and what did you find?</p>



<p><strong>LV</strong>: I think one of the biggest things for this paper is it creates a very large material base. Basically, you can say it&#8217;s a smart database which eventually will be made accessible to the public. I think that&#8217;s a big achievement because people who actually if they have to look into it, they actually can go search Microsoft database, finding out, oh, this material does have this type of thermal properties. This is actually, this database can send about 230,000 materials. And one of the things we confirm is the highest thermal conductivity material based on all the wisdom of Slack criteria, predicted diamond would have the highest thermal conductivity. We more or less really very solidly prove diamond, at this stage, will remain with the highest thermal conductivity. We have a lot of new materials, exotic materials, which some of them, Hongxia can elaborate a little bit more. So, which having all this very exotic combination of properties, thermal with other properties, which could actually provide a new insight for new physics development, new material development, and a new device perspective. All of this combined will have actually a very profound impact to society.</p>



<p><strong>HUIZINGA</strong>: Yeah, Hongxia, go a little deeper on that because that was an interesting part of the paper when you talked about diamond still being the sort of “gold standard,” to mix metaphors! But you&#8217;ve also found some other materials that are remarkable compared to silicon.</p>



<p><strong>HAO</strong>: Yeah, yeah. Among this search space, even though we didn&#8217;t find like something that&#8217;s higher than diamonds, but we do discover more than like twenty new materials with thermal conductivity exceeding that of silicon. And silicon is something like a benchmark for criteria that we think we want to compare with because it&#8217;s a backbone of modern electronics. More interestingly, I think, is the manganese vanadium. It shows some very interesting and surprising phenomena. Like it&#8217;s a metallic compound, but with very high lattice thermal connectivity. And this is the first time discovered by, like, through our search pattern, and it’s something that cannot be easily discovered without the hope with AI. And right now, think Bing can explain more on this, and show some interesting results.</p>



<p><strong>HUIZINGA</strong>: Yeah, go ahead Bing.</p>



<p><strong>LV</strong>: So this is actually very surprising to me as an experimentalist because of when Hongxia presented their theory work to me, this material, manganese vanadium it&#8217;s discovered back in 1938, almost 100 years ago, but there&#8217;s no more than twenty papers talking about this! A lot of them was on theory, okay, not even on experimental part. We actually did quite a bit of work on this. We actually are in the process; will characterize this and then moving forward even for the thermal conductivity measurements. So that will be hopefully, will be adding to the value of these things, showing you, Hey, AI does help to predict the materials could really generate the new materials with very good high thermal conductivity.</p>



<p><strong>HUIZINGA</strong>: Yeah, so Bing, stay with you for a minute. I want you to talk about some kind of real-world applications of this. I know you alluded to a couple of things, but how is this work significant in that respect, and who might be most excited about it, aside from the two of you? [LAUGHS]</p>



<p><strong>LV</strong>: So I think as I mentioned before, the first thing is this database. I believe that&#8217;s the first ever large material database regarding to the thermal conductivity. And it has, as I said, 230,000 materials with AI-predicted thermal connectivity. This will provide not only science but engineering with a vastly expanding catalog of candidate materials for the future roadmap of integration, material integration, and all these bottlenecks we are talking about, the thermal solution for the semiconductors or for even beyond the semiconductor integration, people actually can have a database to looking for. So these things, it will become very important, and I believe over a long time it will generate a very long impact for the research community, for the society development.</p>



<p><strong>HUIZINGA</strong>: Yeah. Hongxia, did you have anything to add to that one too?</p>



<p><strong>HAO</strong>: Yeah, so this study reshapes how we think about limits. I like the sentence that the only way to discover the limits of possible is to go beyond them into the impossible. In this case, we tried, but we didn&#8217;t break the diamond limit. But we proved it even more rigorously than ever before. In doing so, we also uncovered some uncharted peaks in the thermal conductivity landscape. This would not happen without new AI capabilities for material science. I think in the long run, I believe researchers could benefit from using this AI design and shift their way on how to do materials research with AI.</p>



<p><strong>HUIZINGA</strong>: Yeah, it&#8217;ll be interesting to see if anyone ever does break the diamond limit with the new tools that are available, but…</p>



<p><strong>HAO</strong>: Yeah!</p>



<p><strong>HUIZINGA</strong>: So this is the part of the abstracts podcast where I like to ask for sort of a golden nugget, a one sentence takeaway that listeners might get from this paper. If you had one Hongxia, what would it be? And then I&#8217;ll ask Bing to maybe give his.</p>



<p><strong>HAO</strong>: Yes. AI is no longer just a tool. It&#8217;s becoming a critical partner for us in scientific discovery. So our work proved that the large-scale data-driven science can now approach long-standing and fundamental questions with very fresh eyes. When trained well, and guided with physical intuition, models like MatterSim can really realize a full in-silico characterization for materials and don&#8217;t just simulate some known materials, but really trying to imagine what nature hasn&#8217;t yet revealed. Our work points to a path forward, not just incrementally better materials, but entirely new class of high-performance compounds where we could never have guessed without AI.</p>



<p><strong>HUIZINGA</strong>: Yeah. Bing, what&#8217;s your one takeaway?</p>



<p><strong>LV</strong>: I think I want to add a few things on top of Hongxia’s comments because I think Hongxia has very good critical words I would like to emphasize. When we train the AI well, if we guide the AI well, it could be very useful to become our partner. So I think all in all, our human being’s intellectual merit here is still going to play a significantly important role, okay? We are generating this AI, we should really train the AI, we should be using our human being intellectual merit to guide them to be useful for our human being society advancement. Now with all these AI tools, I think it&#8217;s a very golden time right now. Experimentalists could work very closely with like Hongxia, who’s a good theorist who has very good intellectual merits, and then we actually now incorporate with AI, then combine all pieces together, hopefully we’re really able to accelerating material discovery in a much faster pace than ever which the whole society will eventually get a benefit from it.</p>



<p><strong>HUIZINGA</strong>: Yeah. Well, as we close, Bing, I want you to go a little further and talk about what&#8217;s next then, research wise. What are the open questions or outstanding challenges that remain in this field and what&#8217;s on your research agenda to address them?</p>



<p><strong>LV</strong>: So first of all, I think this paper is addressing primarily on these crystalline ordered inorganic bulk materials. And also with the condition we are targeting at ambient pressure, room temperature, because that&#8217;s normally how the instrument is working, right? But what if under extreme conditions? We want to go to space, right? There we’ll have extreme conditions, some very… sometimes very cold, sometimes very hot. We have some places with extremely probably quite high pressure. Or we have some conditions that are highly radioactive. So under that condition, there’s going to be a new database could be emerged. Can we do something beyond that? Another good important thing is we are targeting this paper on high thermal conductivity. What about extremely low thermal conductivity? Those will actually bring a very good challenge for theorists and also the machine learning approach. I think that&#8217;s something Hongxia probably is very excited to work on in that direction. I know since she’s ambitious, she wants to do something more than beyond what we actually achieved so far.</p>



<p><strong>HUIZINGA</strong>: Yeah, so Hongxia, how would you encapsulate what your dream research is next?</p>



<p><strong>HAO</strong>: Yeah, so I think besides all of these exciting research directions, on my end, another direction is perhaps kind of exciting is we want to move from search to design. So right now we are kind of good at asking like what exists by just doing a forward prediction and brute force. But with generative AI, we can start asking what should exist? In the future, we can have an incorporation between forward prediction and backwards generative design to really tackle questions. If you have materials like you want to have desired like properties, how would you design the problems?</p>



<p>[MUSIC]</p>



<p><strong>HUIZINGA</strong>: Well, it sounds like there&#8217;s a full plate of research agenda goodness going forward in this field, both with human brains and AI. So, Hongxia Hao and Bing Lv, thanks for joining us today. And to our listeners, thanks for tuning in. If you want to read this paper, you can find a link at aka.ms/Abstracts, or you can read a pre-print of it on arXiv. See you next time on Abstracts!</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-3"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-heat-transfer-and-deep-learning-with-hongxia-hao-and-bing-lv/">Abstracts: Heat Transfer and Deep Learning with Hongxia Hao and Bing Lv</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of May 7, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-may-7-2025/</link>
		
		<dc:creator><![CDATA[Microsoft Research Team]]></dc:creator>
		<pubDate>Wed, 07 May 2025 23:25:04 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1138612</guid>

					<description><![CDATA[<p>In this issue: New research on compound AI systems and causal verification of the Confidential Consortium Framework; release of Phi-4-reasoning; enriching tabular data with semantic structure, and more.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-may-7-2025/">Research Focus: Week of May 7, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p>New research on compound AI systems and causal verification of the Confidential Consortium Framework; release of Phi-4-reasoning; enriching tabular data with semantic structure, and more.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus: May 07, 2025" class="wp-image-1138631" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="towards-resource-efficient-compound-ai-systems"><a href="https://www.microsoft.com/en-us/research/publication/towards-resource-efficient-compound-ai-systems/">Towards Resource-Efficient Compound AI Systems</a></h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="936" height="421" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_AI-systems.png" alt="Unlike the current state-of-the-art, our vision is fungible workflows with high-level descriptions, managed jointly by the Workflow Orchestrator and Cluster Manager. This allows higher resource multiplexing between independent workflows to improve efficiency." class="wp-image-1138633" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_AI-systems.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_AI-systems-300x135.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_AI-systems-768x345.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_AI-systems-240x108.png 240w" sizes="auto, (max-width: 936px) 100vw, 936px" /></figure>



<p>This research introduces Murakkab, a prototype system built on a declarative workflow that reimagines how compound AI systems are built and managed to significantly improve resource efficiency. Compound AI systems integrate multiple interacting components like language models, retrieval engines, and external tools. They are essential for addressing complex AI tasks. However, current implementations could benefit from greater efficiencies in resource utilization, with improvements to tight coupling between application logic and execution details, better connections between orchestration and resource management layers, and bridging gaps between efficiency and quality.</p>



<p>Murakkab addresses critical inefficiencies in current AI architectures and offers a new approach that unifies workflow orchestration and cluster resource management for better performance and sustainability. In preliminary evaluations, it demonstrates speedups up to ∼ 3.4× in workflow completion times while delivering ∼ 4.5× higher energy efficiency, showing promise in optimizing resources and advancing AI system design.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="smart-casual-verification-of-the-confidential-consortium-framework"><a href="https://www.microsoft.com/en-us/research/publication/smart-casual-verification-of-ccfs-distributed-consensus-and-consistency-protocols/?msockid=3522c0e375a2640e0f72d5ce7493658c">Smart Casual Verification of the Confidential Consortium Framework</a></h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="624" height="364" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_casual-verification.png" alt="Diagram showing the components of the verification architecture for CCF's consensus. The diagram is discussed in detail in the paper." class="wp-image-1138634" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_casual-verification.png 624w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_casual-verification-300x175.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_casual-verification-480x280.png 480w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_casual-verification-240x140.png 240w" sizes="auto, (max-width: 624px) 100vw, 624px" /></figure>



<p>This work presents a new, pragmatic verification technique that improves the trustworthiness of distributed systems like the Confidential Consortium Framework (CCF) and proves its effectiveness by catching critical bugs before deployment. Smart casual verification is a novel hybrid verification approach to validating CCF, an open-source platform for developing trustworthy and reliable cloud applications which underpins Microsoft’s Azure Confidential Ledger service. </p>



<p>The researchers apply smart casual verification to validate the correctness of CCF’s novel distributed protocols, focusing on its unique distributed consensus protocol and its custom client consistency model. This hybrid approach combines the rigor of formal specification and model checking with the pragmatism of automated testing, specifically binding the formal specification in TLA+ to the C++ implementation. While traditional formal methods are often one-off efforts by domain experts, the researchers have integrated smart casual verification into CCF’s continuous integration pipeline, allowing contributors to continuously validate CCF as it evolves. </p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="phi-4-reasoning-technical-report"><a href="https://www.microsoft.com/en-us/research/publication/phi-4-reasoning-technical-report/">Phi-4-reasoning Technical Report</a></h3>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1300" height="644" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning.png" alt="graphical user interface, text, application, email" class="wp-image-1138688" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning.png 1300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning-300x149.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning-1024x507.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning-768x380.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Phi4reasoning-240x119.png 240w" sizes="auto, (max-width: 1300px) 100vw, 1300px" /></figure>



<p>This report introduces <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/" target="_blank" rel="noreferrer noopener"><strong>Phi-4-reasoning</strong><span class="sr-only"> (opens in new tab)</span></a>, a 14-billion parameter model optimized for complex reasoning tasks. It is trained via supervised fine-tuning of Phi-4 using a carefully curated dataset of high-quality prompts and reasoning demonstrations generated by o3-mini. These prompts span diverse domains—including math, science, coding, and spatial reasoning—and are selected to challenge the base model near its capability boundaries.</p>



<p>Building on recent findings that reinforcement learning (RL) can further improve smaller models, the team developed <strong>Phi-4-reasoning-plus</strong>, which incorporates an additional outcome-based RL phase using verifiable math problems. This enhances the model’s ability to generate longer, more effective reasoning chains. </p>



<p>Despite its smaller size, the Phi-4-reasoning family outperforms significantly larger open-weight models such as DeepSeekR1-Distill-Llama-70B and approaches the performance of full-scale frontier models like DeepSeek R1. It excels in tasks requiring multi-step problem solving, logical inference, and goal-directed planning.</p>



<p>The work highlights the combined value of supervised fine-tuning and reinforcement learning for building efficient, high-performing reasoning models. It also offers insights into training data design, methodology, and evaluation strategies. Phi-4-reasoning contributes to the growing class of reasoning-specialized language models and points toward more accessible, scalable AI for science, education, and technical domains.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-ffbc8879e59f0447803313cfcaec2fea" id="new-research-4">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="tecofes-text-column-featurization-using-semantic-analysis"><a href="https://www.microsoft.com/en-us/research/publication/tecofes-text-column-featurization-using-semantic-analysis/">TeCoFeS: Text Column Featurization using Semantic Analysis</a></h3>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="952" height="537" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61.png" alt="The workflow diagram illustrates the various steps in the TECOFES approach. Step 0 is the embedding computation module, which calculates embeddings for all rows of text, setting the foundation for subsequent steps. Step 2, the smart sampler, captures diverse samples and feeds them into the labeling module (step 3), which generates labels. These labels are then utilized by the Extend Mapping module (step 4) to map the remaining unlabeled data." class="wp-image-1138646" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61.png 952w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61-768x433.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/tecofe_RF61-640x360.png 640w" sizes="auto, (max-width: 952px) 100vw, 952px" /></figure>



<p>This research introduces a practical, cost-effective solution for enriching tabular data with semantic structure, making it more useful for downstream analysis and insights—which is especially valuable in business intelligence, data cleaning, and automated analytics workflows. This approach outperforms baseline models and naive LLM applications on converted text classification benchmarks.</p>



<p>Extracting structured insights from free-text columns in tables—such as product reviews or user feedback—can be time-consuming and error-prone, especially when relying on traditional syntactic methods that often miss semantic meaning. This research introduces the semantic text column featurization problem, which aims to assign meaningful, context-aware labels to each entry in a text column.</p>



<p>The authors propose a scalable, efficient method that combines the power of LLMs with text embeddings. Instead of labeling an entire column manually or applying LLMs to every cell—an expensive process—this new method intelligently samples a diverse subset of entries, uses an LLM to generate semantic labels for just that subset, and then propagates those labels to the rest of the column using embedding similarity.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-ebd4a57aaf54396b4191f2c1fad6c517" id="new-research-5">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="agentic-reasoning-and-tool-integration-for-llms-via-reinforcement-learning"><a href="https://www.microsoft.com/en-us/research/publication/agentic-reasoning-and-tool-integration-for-llms-via-reinforcement-learning/">Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning</a></h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="936" height="154" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_agentic-reasoning.png" alt="diagram" class="wp-image-1138635" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_agentic-reasoning.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_agentic-reasoning-300x49.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_agentic-reasoning-768x126.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/RF61_agentic-reasoning-240x39.png 240w" sizes="auto, (max-width: 936px) 100vw, 936px" /></figure>



<p>This work introduces ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a new paradigm for LLM reasoning that expands beyond traditional language-only inference. </p>



<p>While LLMs have made considerable strides in complex reasoning tasks, they remain limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this research, ARTIST brings together agentic reasoning, reinforcement learning (RL), and tool integration, designed to enable LLMs to autonomously decide when and how to invoke internal tools within multi-turn reasoning chains. ARTIST leverages outcome-based reinforcement learning to learn robust strategies for tool use and environment interaction without requiring step-level supervision.</p>



<p>Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to <em>22%</em> absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies show that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h3 class="wp-block-heading h2" id="materialism-podcast-mattergen"><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=Ts1Lzc3T54I&list=PLL0SWcFqypClH6_BM-b1BggB7_qifpWgs&index=3">Materialism Podcast: MatterGen<span class="sr-only"> (opens in new tab)</span></a></h3>



<p>What if you could find materials with tailored properties without ever entering the lab? The Materialism Podcast, which is dedicated to exploring materials science and engineering, talks with Tian Xie from Microsoft Research to discuss MatterGen, an AI tool which accelerates materials science discovery. Tune in to hear a discussion of the new Azure AI Foundry, where MatterGen will interact with and support MatterSim, an advanced deep learning model designed to simulate the properties of materials across a wide range of elements, temperatures, and pressures.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Episode 103: MatterGen" width="500" height="375" src="https://www.youtube-nocookie.com/embed/Ts1Lzc3T54I?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="in-the-news-highlights-of-recent-media-coverage-of-microsoft-research">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">IN THE NEWS: Highlights of recent media coverage of Microsoft Research</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="When ChatGPT Broke an Entire Field: An Oral History"
						href="https://www.quantamagazine.org/when-chatgpt-broke-an-entire-field-an-oral-history-20250430/"
					>
						<span>When ChatGPT Broke an Entire Field: An Oral History</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>Quanta Magazine | April 30, 2025</p><p>Large language models are everywhere, igniting discovery, disruption and debate in whatever scientific community they touch. But the one they touched first — for better, worse and everything in between — was natural language processing. What did that impact feel like to the people experiencing it firsthand?</p><p>To tell that story, Quanta interviewed 19 NLP experts, including Kalika Bali, senior principal researcher at Microsoft Research. From researchers to students, tenured academics to startup founders, they describe a series of moments — dawning realizations, elated encounters and at least one “existential crisis” — that changed their world. And ours.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
						
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
						
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-may-7-2025/">Research Focus: Week of May 7, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Microsoft Fusion Summit explores how AI can accelerate fusion research</title>
		<link>https://www.microsoft.com/en-us/research/blog/microsoft-fusion-summit-explores-how-ai-can-accelerate-fusion-research/</link>
		
		<dc:creator><![CDATA[Kenji Takeda, Shruti Rajurkar, Ade Famoti]]></dc:creator>
		<pubDate>Wed, 07 May 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1138503</guid>

					<description><![CDATA[<p>The first Microsoft Research Fusion Summit brought together global experts to explore how AI can help unlock the potential of fusion energy. Discover how collaborations with leading institutions can help speed progress toward clean, scalable energy.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/microsoft-fusion-summit-explores-how-ai-can-accelerate-fusion-research/">Microsoft Fusion Summit explores how AI can accelerate fusion research</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1.jpg" alt="Sir Steven Cowley, professor and director of the Princeton Plasma Physics Laboratory and former head of the UK Atomic Energy Authority, giving a presentation." class="wp-image-1138569" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Fusion-Summit-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>The pursuit of nuclear fusion as a limitless, clean energy source has long been one of humanity&#8217;s most ambitious scientific goals. Research labs and companies worldwide are working to replicate the fusion process that occurs at the sun’s core, where isotopes of hydrogen combine to form helium, releasing vast amounts of energy. While scalable fusion energy is still years away, researchers are now exploring how AI can help accelerate fusion research and bring this energy to the grid sooner.&nbsp;</p>



<p>In March 2025, Microsoft Research held its inaugural Fusion Summit, a landmark event that brought together distinguished speakers and panelists from within and outside Microsoft Research to explore this question.&nbsp;</p>



<p>Ashley Llorens, Corporate Vice President and Managing Director of Microsoft Research Accelerator, opened the Summit by outlining his vision for a self-reinforcing system that uses AI to drive sustainability. Steven Cowley, laboratory director of the U.S. Department of Energy’s <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pppl.gov/" target="_blank" rel="noreferrer noopener">Princeton Plasma Physics Laboratory<span class="sr-only"> (opens in new tab)</span></a>, professor at Princeton University, and former head of the UK Atomic Energy Authority, followed with a keynote explaining the intricate science and engineering behind fusion reactors. His message was clear: advancing fusion will require international collaboration and the combined power of AI and high-performance computing to model potential fusion reactor designs.&nbsp;</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="1st Annual Fusion Summit: Welcome, Opening Remarks, & Distinguished Keynote Lecture" width="500" height="281" src="https://www.youtube-nocookie.com/embed/fVjasUaj4nc?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<h2 class="wp-block-heading" id="applying-ai-to-fusion-research">Applying AI to fusion research</h2>



<p>North America’s largest fusion facility, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://d3dfusion.org/">DIII-D<span class="sr-only"> (opens in new tab)</span></a>, operated by General Atomics and owned by the US Department of Energy (DOE), provides a unique platform for developing and testing AI applications for fusion research, thanks to its pioneering data and digital twin platform. </p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.energy.gov/science/articles/meet-richard-buttery-director-diii-d-national-fusion-facility" target="_blank" rel="noreferrer noopener">Richard Buttery<span class="sr-only"> (opens in new tab)</span></a> from DIII-D and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/dave-humphreys-0b21b09/" target="_blank" rel="noreferrer noopener">Dave Humphreys<span class="sr-only"> (opens in new tab)</span></a> from General Atomics demonstrated how the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.ga.com/magnetic-fusion/diii-d" target="_blank" rel="noreferrer noopener">US DIII-D National Fusion Program<span class="sr-only"> (opens in new tab)</span></a> is already applying AI to advance reactor design and operations, highlighting promising directions for future development. They provided examples of how to apply AI to active plasma control to avoid disruptive instabilities, using AI-controlled trajectories to avoid tearing modes, and implementing feedback control using machine learning-derived density limits for safer high-density operations.&nbsp;</p>



<p>One persistent challenge in reactor design involves building the interior “first wall,” which must withstand extreme heat and particle bombardment. Zulfi Alam, corporate vice president of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://quantum.microsoft.com/" target="_blank" rel="noreferrer noopener">Microsoft Quantum<span class="sr-only"> (opens in new tab)</span></a>, discussed the potential of using quantum computing in fusion, particularly for addressing material challenges like hydrogen diffusion in reactors.</p>



<p>He noted that silicon nitride shows promise as a barrier to hydrogen and vapor and explained the challenge of binding it to the reaction chamber. He emphasized the potential of quantum computing to improve material prediction and synthesis, enabling more efficient processes. He shared that his team is also investigating advanced silicon nitride materials to protect this critical component from neutron and alpha particle damage—an innovation that could make fusion commercially viable.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="exploring-ai-s-broader-impact-on-fusion-engineering">Exploring AI’s broader impact on fusion engineering</h2>



<p>Lightning talks from Microsoft Research labs addressed the central question of AI’s potential to accelerate fusion research and engineering. Speakers covered a wide range of applications—from using <a href="https://www.microsoft.com/en-us/research/blog/introducing-muse-our-first-generative-ai-model-designed-for-gameplay-ideation/?msockid=0bc6ff71aae1636d05f4edf0ab8262b9" target="_blank" rel="noreferrer noopener">gaming AI</a> for plasma control and <a href="https://www.microsoft.com/en-us/research/collaboration/embodied-ai">robotics</a> for remote maintenance to physics-informed <a href="https://www.microsoft.com/en-us/research/story/ai-meets-materials-discovery/">AI for simulating materials</a> and <a href="https://www.microsoft.com/en-us/research/project/lordnet-neural-pde-solver/">plasma behavior</a>. Closing the session, Archie Manoharan, Microsoft’s director of nuclear engineering for Cloud Operations and Infrastructure, emphasized the need for a <a href="https://www.microsoft.com/en-us/microsoft-cloud/blog/2024/09/20/accelerating-the-addition-of-carbon-free-energy-an-update-on-progress/" target="_blank" rel="noreferrer noopener">comprehensive energy strategy</a>, one that incorporates renewables, efficiency improvements, storage solutions, and carbon-free sources like fusion.</p>



<p>The Summit culminated in a thought-provoking panel discussion moderated by Ade Famoti, featuring Archie Manoharan, Richard Buttery, Steven Cowley, and Chris Bishop, Microsoft Technical Fellow and director of <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai-for-science/" target="_blank" rel="noreferrer noopener">Microsoft Research AI for Science</a>. Their wide-ranging conversation explored the key challenges and opportunities shaping the field of fusion.&nbsp;</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Nuclear Fusion - Fireside Discussion" width="500" height="281" src="https://www.youtube-nocookie.com/embed/ruopuxf4Dgk?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<p>The panel highlighted several themes: the role of new regulatory frameworks that balance innovation with safety and public trust; the importance of materials discovery in developing durable fusion reactor walls; and the game-changing role AI could play in plasma optimization and surrogate modelling of fusion’s underlying physics.</p>



<p>They also examined the importance of global research collaboration, citing projects like the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.iter.org/" target="_blank" rel="noreferrer noopener">International Thermonuclear Experimental Reactor<span class="sr-only"> (opens in new tab)</span></a> (ITER), the world’s largest experimental fusion device under construction in southern France, as testbeds for shared progress. One persistent challenge, however, is data scarcity. This prompted a discussion of using physics-informed neural networks as a potential approach to supplement limited experimental data.&nbsp;</p>



<h2 class="wp-block-heading" id="global-collaboration-and-next-steps">Global collaboration and next steps</h2>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/emea/features/nuclear-fusion-delivering-on-the-promise-of-carbon-free-power-with-the-help-of-ai/?msockid=394581ce06c567df2171946b073d6601" target="_blank" rel="noreferrer noopener">Microsoft is collaborating with ITER<span class="sr-only"> (opens in new tab)</span></a> to help advance the technologies and infrastructure needed to achieve fusion ignition—the critical point where a self-sustaining fusion reaction begins, using <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/emea/features/nuclear-fusion-delivering-on-the-promise-of-carbon-free-power-with-the-help-of-ai/?msockid=0bc6ff71aae1636d05f4edf0ab8262b9" target="_blank" rel="noreferrer noopener">Microsoft 365 Copilot, Azure OpenAI Service, Visual Studio, and GitHub<span class="sr-only"> (opens in new tab)</span></a>. Microsoft Research is now cooperating with ITER to identify where AI can be leveraged to model future experiments to optimize its design and operations. </p>



<p>Now Microsoft Research has signed a Memorandum of Understanding with the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pppl.gov/" target="_blank" rel="noreferrer noopener">Princeton Plasma Physics Laboratory (PPPL)<span class="sr-only"> (opens in new tab)</span></a> to foster collaboration through knowledge exchange, workshops, and joint research projects. This effort aims to address key challenges in fusion, materials, plasma control, digital twins, and experiment optimization. Together, Microsoft Research and PPPL will work to drive innovation and advances in these critical areas.</p>



<p>Fusion is a scientific challenge unlike any other and could be key to sustainable energy in the future. We’re excited about the role AI can play in helping make that vision a reality. To learn more, visit the&nbsp;<a href="https://www.microsoft.com/en-us/research/event/1st-annual-fusion-summit/">Fusion Summit event page</a>, or connect with us by email at&nbsp;<a href="mailto:FusionResearch@microsoft.com">FusionResearch@microsoft.com</a>.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/microsoft-fusion-summit-explores-how-ai-can-accelerate-fusion-research/">Microsoft Fusion Summit explores how AI can accelerate fusion research</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Abstracts: Societal AI with Xing Xie</title>
		<link>https://www.microsoft.com/en-us/research/podcast/abstracts-societal-ai-with-xing-xie/</link>
		
		<dc:creator><![CDATA[Gretchen Huizinga, Xing Xie]]></dc:creator>
		<pubDate>Mon, 05 May 2025 16:01:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1138012</guid>

					<description><![CDATA[<p>New AI models aren’t just changing the world of research; they’re also poised to impact society. Xing Xie talks about Societal AI, a white paper that explores the changing landscape with an eye to future research and improved communication across disciplines.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-societal-ai-with-xing-xie/">Abstracts: Societal AI with Xing Xie</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788.jpg" alt="Xing Xie illustrated headshot" class="wp-image-1138272" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Xing-Xie_Abstracts_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=145025711&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Members of the research community at Microsoft work continuously to advance their respective fields. Abstracts bring its audience to the cutting edge with them through short, compelling conversations about new and noteworthy achievements.&nbsp;</p>



<p>In this episode, Partner Research Manager Xing Xie joins host Gretchen Huizinga to talk about his work on a white paper called <strong><em>Societal AI: Research Challenges and Opportunities</em></strong>. Part of a larger effort to understand the cultural impact of AI systems, this white paper is a result of a series of global conversations and collaborations on how AI systems interact with and influence human societies.&nbsp;</p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/societal-ai-research-challenges-and-opportunities/?msockid=2cfa5553216661e801da407e204b60b4">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h3 class="wp-block-heading" id="learn-more">Learn more: </h3>



<p><a href="https://www.microsoft.com/en-us/research/blog/societal-ai-building-human-centered-ai-systems/">Societal AI: Building human-centered AI systems</a><br>Microsoft Research Blog, May 2024</p>



<div style="height:20px" aria-hidden="true" class="wp-block-spacer"></div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]</p>



<p><strong>GRETCHEN</strong> <strong>HUIZINGA</strong>: Welcome to Abstracts, a Microsoft Research Podcast that puts the spotlight on world-class research in brief. I’m Gretchen Huizinga. In this series, members of the research community at Microsoft give us a quick snapshot – or a podcast abstract – of their new and noteworthy papers.&nbsp;</p>



<p>[MUSIC FADES]</p>



<p>I&#8217;m here today with Xing Xie, a partner research manager at Microsoft Research and co-author of a white paper called <strong><em>Societal AI: Research Challenges and Opportunities</em></strong>. This white paper is a result of a series of global conversations and collaborations on how AI systems interact with and impact human societies. Xing Xie, great to have you back on the podcast. Welcome to Abstracts!&nbsp;</p>



				</span>
				<span id="show-more-show-less-toggle-4" class="show-more-show-less-toggleable-content">
					



<p><strong>XING</strong> <strong>XIE</strong>: Thank you for having me.&nbsp;</p>



<p><strong>HUIZINGA</strong>: So let&#8217;s start with a brief overview of the background for this white paper on Societal AI. In just a few sentences, tell us how the idea came about and what key principles drove the work.&nbsp;</p>



<p><strong>XIE</strong>: The idea for this white paper emerged in response to the shift we are witnessing in the AI landscape. Particularly since the release of ChatGPT in late 2022, these models didn&#8217;t just change the pace of AI research, they began reshaping our society, education, economy, and yeah, even the way we understand ourselves. At Microsoft Research Asia, we felt a strong urgency to better understand these changes. Over the past 30 months, we have been actively exploring this frontier in partnership with experts from psychology, sociology, law, and philosophy. This white paper serves three main purposes. First, to document what we have learned. Second, to guide future research directions. And last, to open up an effective communication channel with collaborators across different disciplines.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Research on responsible AI is a relatively new discipline and it&#8217;s profoundly multidisciplinary. So tell us about the work that you drew on as you convened this series of workshops and summer schools, research collaborations and interdisciplinary dialogues. What kinds of people did you bring to the table and for what reason?&nbsp;</p>



<p><strong>XIE</strong>: Yeah. Responsible AI actually has been evolving within Microsoft for like about a decade. But with the rise of large language models, the scope and urgency of these challenges have grown exponentially. That&#8217;s why we have leaned heavily on interdisciplinary collaboration. For instance, in the Value Compass Project, we worked with philosophers to frame human values in a scientifically actionable way, something essential for aligning AI behavior. In our AI evaluation efforts, we drew from psychometrics to create more principled ways of assessing these systems. And with the sociologists, we have examined how AI affects education and social systems. This joint effort has been central to the work we share in this white paper.&nbsp;</p>



<p><strong>HUIZINGA</strong>: So white papers differ from typical research papers in that they don&#8217;t rely on a particular research methodology per se, but you did set, as a backdrop for your work, ten questions for consideration. So how did you decide on these questions and how or by what means did you attempt to answer them?&nbsp;</p>



<p><strong>XIE</strong>: Rather than follow a traditional research methodology, we built this white paper around ten fundamental, foundational research questions. These came from extensive dialogue, not only with social scientists, but also computer scientists working at the technical front of AI. These questions span both directions. First, how AI impacts society, and second, how social science can help solve technical challenges like alignment and safety. They reflect a dynamic agenda that we hope to evolve continuously through real-world engagement and deeper collaboration.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Can you elaborate on… a little bit more on the questions that you chose to investigate as a group or groups in this?&nbsp;</p>



<p><strong>XIE</strong>: Sure, I think I can use the Value Compass Project as one example. In that project, our main goal is to try to study how we can better align the value of AI models with our human values. Here, one fundamental question is how we define our own human values. There actually is a lot of debate and discussions on this. Fortunately, we see in philosophy and sociology actually they have studied this for years, like, for like hundreds of years. They have defined some, like, such as basic human value framework, they have defined like modern foundation theory. We can borrow those expertise. Actually, we have worked with sociology and philosophers, try to borrow these expertise and define a framework that could be usable for AI. Actually, we have worked on, like, developing some initial frameworks and evaluation methods for this.&nbsp;</p>



<p><strong>HUIZINGA</strong>: So one thing that you just said was to frame philosophical issues in a scientifically actionable way. How hard was that?&nbsp;</p>



<p><strong>XIE</strong>: Yeah, it is actually not easy. I think that first of all, social scientists and AI researchers, we… usually we speak different languages.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Right!&nbsp;</p>



<p><strong>XIE</strong>: Our research is at a very different pace. So at the very beginning, I think we should find out what&#8217;s the best way to talk to each other. So we have workshops, have joint research projects, we have them visit us, and also, we have supervised some joint interns. So that’s all the ways we try to find some common ground to work together. More specifically for this value framework, we have tried to understand what&#8217;s the latest program from their source and also try how to adapt them to an AI context. So that&#8217;s, I mean, it&#8217;s not easy, but it&#8217;s like enjoyable and exciting journey!&nbsp;</p>



<p><strong>HUIZINGA</strong>: Yeah, yeah, yeah. And I want to push in on one other question that I thought was really interesting, which you asked, which was how can we ensure AI systems are safe, reliable, controllable, especially as they become more autonomous? I think this is a big question for a lot of people. What kind of framework did you use to look at that?&nbsp;</p>



<p><strong>XIE</strong>: Yeah, there are many different aspects. I think alignment definitely is an aspect. That means how we can make sure we can have a way to truly and deeply embed our values into the AI model. Even after we define our value, we still need a way to make sure that it&#8217;s actually embedded in. And also evaluation I think is another topic. Even we have this AI…. looks safe and looks behavior good, but how we can evaluate that, how we can make sure it is actually doing the right thing. So we also have some collaboration with psychometrics people to define a more scientific evaluation framework for this purpose as well.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Yeah, I remember talking to you about your psychometrics in the previous podcast…&nbsp;</p>



<p><strong>XIE</strong>: Yeah!&nbsp;</p>



<p><strong>HUIZINGA</strong>: …you were on and that was fascinating to me. And I hope… at some point I would love to have a bigger conversation on where you are now with that because I know it&#8217;s an evolving field.&nbsp;</p>



<p><strong>XIE</strong>: It’s evolving!&nbsp;</p>



<p><strong>HUIZINGA</strong>: Yeah, amazing! Well, let&#8217;s get back to this paper. White papers aren&#8217;t designed to produce traditional research findings, as it were, but there are still many important outcomes. So what would you say the most important takeaways or contributions of this paper are?&nbsp;</p>



<p><strong>XIE</strong>: Yeah, the key takeaway, I believe, is AI is no longer just a technical tool. It&#8217;s becoming a social actor.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Mmm.&nbsp;</p>



<p><strong>XIE</strong>: So it must be studied as a dynamic evolving system that intersects with human values, cognition, culture, and governance. So we argue that interdisciplinary collaboration is no longer optional. It&#8217;s essential. Social sciences offer tools to understand the complexity, bias, and trust, concepts that are critical for AI&#8217;s safe and equitable deployment. So the synergy between technical and social perspectives is what will help us move from reactive fixes to proactive design.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Let&#8217;s talk a little bit about the impact that a paper like this can have. And it’s more of a thought leadership piece, but who would you say will benefit most from the work that you&#8217;ve done in this white paper and why?&nbsp;</p>



<p><strong>XIE</strong>: We hope this work speaks to both AI and social science communities. For AI researchers, this white paper provides frameworks and real-world examples, like value evaluation systems and cross-cultural model training that can inspire new directions. And for social scientists, it opens doors to new tools and collaborative methods for studying human behavior, cognition, and institutions. And beyond academia, we believe policymakers and industry leaders can also benefit as the paper outlines practical governance questions and highlights emerging risks that demand timely attention.&nbsp;</p>



<p><strong>HUIZINGA</strong>: Finally, Xing, what would you say the outstanding challenges are for Societal AI, as you framed it, and how does this paper lay a foundation for future research agendas? Specifically, what kinds of research agendas might you see coming out of this foundational paper?&nbsp;</p>



<p><strong>XIE</strong>: We believe this white paper is not a conclusion, it&#8217;s a starting point. While the ten research questions are a strong foundation, they also expose deeper challenges. For example, how do we build a truly interdisciplinary field? How can we reconcile the different timelines, methods, and cultures of AI and social science? And how do we nurture talents who can work fluently across those both domains? We hope this white paper encourages others to take on these questions with us. Whether you are researcher, student, policymaker, or technologist, there is a role for you in shaping AI that not only works but works for society. So yeah, I look forward to the conversation with everyone.&nbsp;</p>



<p>[MUSIC]</p>



<p><strong>HUIZINGA</strong>: Well, Xing Xie, it&#8217;s always fun to talk to you. Thanks for joining us today and to our listeners, thanks for tuning in. If you want to read this white paper, and I highly recommend that you do, you can find a link at aka.ms/Abstracts, or you can find a link in our show notes that will take you to the Microsoft Research website. See you next time on Abstracts!</p>



<p>[MUSIC FADES]</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-4"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<p>&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-societal-ai-with-xing-xie/">Abstracts: Societal AI with Xing Xie</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Societal AI: Building human-centered AI systems</title>
		<link>https://www.microsoft.com/en-us/research/blog/societal-ai-building-human-centered-ai-systems/</link>
		
		<dc:creator><![CDATA[Beibei Shi, Haotian Li, Xing Xie]]></dc:creator>
		<pubDate>Mon, 05 May 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136599</guid>

					<description><![CDATA[<p>Learn about a new white paper on Societal AI, an interdisciplinary framework for guiding AI development that reflects shared human values. It presents key research challenges and emphasizes collaboration across disciplines.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/societal-ai-building-human-centered-ai-systems/">Societal AI: Building human-centered AI systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1.jpg" alt="Societal AI surrounded by a circle with two directional arrows in the center of a rectangle with Computer Science and a computer icon on the left with a directional arrow pointing to Social Science on the right with two avatar icons." class="wp-image-1136604" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Societal-AI-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>In October 2022, Microsoft Research Asia hosted a workshop that brought together experts in computer science, psychology, sociology, and law as part of Microsoft’s commitment to <a href="https://www.microsoft.com/en-us/ai/responsible-ai" target="_blank" rel="noreferrer noopener">responsible AI<span class="sr-only"> (opens in new tab)</span></a>. The event led to ongoing collaborations exploring AI’s societal implications, including the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://valuecompass.github.io/" target="_blank" rel="noreferrer noopener">Value Compass<span class="sr-only"> (opens in new tab)</span></a> project.</p>



<p>As these efforts grew, researchers focused on how AI systems could be designed to meet the needs of people and institutions in areas like healthcare, education, and public services. This work culminated in <a href="https://www.microsoft.com/en-us/research/publication/societal-ai-research-challenges-and-opportunities/">Societal AI: Research Challenges and Opportunities</a><em>, </em>a white paper that explores how AI can better align with societal needs.&nbsp;</p>



<h2 class="wp-block-heading" id="what-is-societal-ai">What is Societal AI?</h2>



<p>Societal AI is an emerging interdisciplinary area of study that examines how AI intersects with social systems and public life. It focuses on two main areas: (1) the impact of AI technologies on fields like education, labor, and governance; and (2) the challenges posed by these systems, such as evaluation, accountability, and alignment with human values. The goal is to guide AI development in ways that respond to real-world needs.</p>



<p>The white paper offers a framework for understanding these dynamics and provides recommendations for integrating AI responsibly into society. This post highlights the paper’s key insights and what they mean for future research.</p>



<h2 class="wp-block-heading" id="tracing-the-development-of-societal-ai">Tracing the development of Societal AI</h2>



<p>Societal AI began nearly a decade ago at Microsoft Research Asia, where early work on <a href="https://www.microsoft.com/en-us/research/articles/personalized-recommendation-systems/" target="_blank" rel="noreferrer noopener">personalized recommendation systems</a> uncovered risks like echo chambers, where users are repeatedly exposed to similar viewpoints, and polarization, which can deepen divisions between groups. Those findings led to deeper investigations into privacy, fairness, and transparency, helping inform Microsoft&#8217;s broader approach to responsible AI.</p>



<p>The rapid rise of large-scale AI models in recent years has made these concerns more urgent. Today, researchers across disciplines are working to define shared priorities and guide AI development in ways that reflect social needs and values.</p>



<h2 class="wp-block-heading" id="key-insights">Key insights</h2>



<p>The white paper outlines several important considerations for the field:</p>



<p><strong>Interdisciplinary framework</strong>: Bridges technical AI research with the social sciences, humanities, policy studies, and ethics to address AI’s far-reaching societal effects.</p>



<p><strong>Actionable research agenda</strong>: Identifies ten research questions that offer a roadmap for researchers, policymakers, and industry leaders.</p>



<p><strong>Global perspective</strong>: Highlights the importance of different cultural perspectives and international cooperation in shaping responsible AI development dialogue.</p>



<p><strong>Practical insights</strong>: Balances theory with real-world applications, drawing from collaborative research projects.</p>



<p>“AI’s impact extends beyond algorithms and computation—it challenges us to rethink fundamental concepts like trust, creativity, agency, and value systems,” says <a href="https://www.microsoft.com/en-us/research/people/lidongz/">Lidong Zhou</a>, managing director of Microsoft Research Asia. “It recognizes that developing more powerful AI models is not enough; we must examine how AI interacts with human values, institutions, and diverse cultural contexts.”</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="12568" height="8370" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res.png" alt="This figure presents the framework of Societal AI research. The left part of the figure illustrates that computer scientists can contribute their expertise in machine learning, natural language processing (NLP), human-computer interaction (HCI), and social computing to this research direction. The right part of the figure highlights the importance of social scientists from various disciplines—including psychology, law, sociology, and philosophy—being deeply involved in the research. The center of the figure displays ten notable Societal AI research areas that require cross-disciplinary collaboration between computer scientists and social scientists. These ten areas, listed in counter-clockwise order starting from the top, are: AI safety and reliability, AI fairness and inclusiveness, AI value alignment, AI capability evaluation, human-AI collaboration, AI interpretability and transparency, AI’s impact on scientific discoveries, AI’s impact on labor and global business, AI’s impact on human cognition and creativity, and the regulatory and governance framework for AI. " class="wp-image-1136601" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res.png 12568w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res-300x200.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res-1024x682.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res-768x511.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SocietalAI_Whitepaper_hi_res-240x160.png 240w" sizes="auto, (max-width: 12568px) 100vw, 12568px" /><figcaption class="wp-element-caption">Figure 1. Societal AI research agenda</figcaption></figure>



<h2 class="wp-block-heading" id="guiding-principles-for-responsible-integration">Guiding principles for responsible integration</h2>



<p>&nbsp;The research agenda is grounded in three key principles:&nbsp;</p>



<ul class="wp-block-list">
<li><strong>Harmony</strong>: AI should minimize conflict and build trust to support acceptance.&nbsp;</li>



<li><strong>Synergy</strong>: AI should complement human capabilities, enabling outcomes that neither humans nor machines could achieve alone.&nbsp;&nbsp;</li>



<li><strong>Resilience</strong>: AI should be robust and adaptable as social and technological conditions evolve.&nbsp;&nbsp;</li>
</ul>



<h2 class="wp-block-heading" id="ten-critical-questions">Ten critical questions</h2>



<p>These questions span both technical and societal concerns:&nbsp;&nbsp;</p>



<ol class="wp-block-list">
<li>How can AI be aligned with diverse human values and ethical principles?</li>



<li>How can AI systems be designed to ensure fairness and inclusivity across different cultures, regions, and demographic groups?</li>



<li>How can we ensure AI systems are safe, reliable, and controllable, especially as they become more autonomous?</li>



<li>How can human-AI collaboration be optimized to enhance human abilities?</li>



<li>How can we effectively evaluate AI&#8217;s capabilities and performance in new, unforeseen tasks and environments?</li>



<li>How can we enhance AI interpretability to ensure transparency in its decision-making processes?</li>



<li>How will AI reshape human cognition, learning, and creativity, and what new capabilities might it unlock?</li>



<li>How will AI redefine the nature of work, collaboration, and the future of global business models?</li>



<li>How will AI transform research methodologies in the social sciences, and what new insights might it enable?</li>



<li>How should regulatory frameworks evolve to govern AI development responsibly and foster global cooperation?</li>
</ol>



<p>This list will evolve alongside AI’s developing societal impact, ensuring the agenda remains relevant over time.&nbsp;Building on these questions, the white paper underscores the importance of sustained, cross-disciplinary collaboration to guide AI development in ways that reflect societal priorities and public interest.</p>



<p>“This thoughtful and comprehensive white paper from Microsoft Research Asia represents an important early step forward in anticipating and addressing the societal implications of AI, particularly large language models (LLMs), as they enter the world in greater numbers and for a widening range of purposes,” says research collaborator <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://sociology.uchicago.edu/directory/James-A-Evans" target="_blank" rel="noreferrer noopener">James A. Evans<span class="sr-only"> (opens in new tab)</span></a>, professor of sociology at the University of Chicago.</p>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead</h2>



<p>Microsoft is committed to fostering collaboration and invites others to take part in developing governance systems. As new challenges arise, the responsible use of AI for the public good will remain central to our research.</p>



<p>We hope the white paper serves as both a guide and a call to action, emphasizing the need for engagement across research, policy, industry, and the public.</p>



<p>For more information, and to access the full white paper, visit the Microsoft Research <a href="https://www.microsoft.com/en-us/research/project/societal-ai/" target="_blank" rel="noreferrer noopener">Societal AI</a> page. Listen to the author discuss more about the research in <a href="https://www.microsoft.com/en-us/research/podcast/abstracts-societal-ai-with-xing-xie/" target="_blank" rel="noreferrer noopener">this podcast</a>.</p>



<h2 class="wp-block-heading" id="acknowledgments">Acknowledgments</h2>



<p>We are grateful for the contributions of the researchers, collaborators, and reviewers who helped shape this white paper.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/societal-ai-building-human-centered-ai-systems/">Societal AI: Building human-centered AI systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Laws, norms, and ethics for AI in health</title>
		<link>https://www.microsoft.com/en-us/research/podcast/laws-norms-and-ethics-for-ai-in-health/</link>
		
		<dc:creator><![CDATA[Peter Lee, Vardit Ravitsky, Laura Adams, Dr. Roxana Daneshjou]]></dc:creator>
		<pubDate>Thu, 01 May 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1137889</guid>

					<description><![CDATA[<p>Healthcare experts Laura Adams, Vardit Ravitsky, and Dr. Roxana Daneshjou discuss responsible AI implementation in medicine, examining governance approaches, shifting patient-provider relationships, and the identification of bias to ensure equitable deployment.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/laws-norms-and-ethics-for-ai-in-health/">Laws, norms, and ethics for AI in health</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="Peter Lee, Vardit Ravitsky, Laura Adams, and Dr. Roxana Daneshjou illustrated headshots." class="wp-image-1138253" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode4-Peter-Laura-Roxana-Vardit-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=145088769&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this episode, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/lauraadamsnam/" target="_blank" rel="noreferrer noopener">Laura Adams<span class="sr-only"> (opens in new tab)</span></a>, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.thehastingscenter.org/team/vardit-ravitsky/" target="_blank" rel="noreferrer noopener">Vardit Ravitsky<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://profiles.stanford.edu/roxana-daneshjou" target="_blank" rel="noreferrer noopener">Dr. Roxana Daneshjou<span class="sr-only"> (opens in new tab)</span></a>, experts at the intersection of healthcare, ethics, and technology, join Lee to discuss the responsible implementation of AI in healthcare. Adams, a strategic advisor at the National Academy of Medicine leading the development of a national AI code of conduct, shares her initial curiosity and skepticism of generative AI and then her recognition of the technology as a transformative tool requiring new governance approaches. Ravitsky, bioethicist and president and CEO of The Hastings Center for Bioethics, examines how AI is reshaping healthcare relationships and the need for bioethics to proactively guide implementation. Daneshjou, a Stanford physician-scientist bridging dermatology, biomedical data science, and AI, discusses her work on identifying, understanding, and mitigating bias in AI systems and also leveraging AI to better serve patient needs.</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more</h2>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nam.edu/our-work/programs/leadership-consortium/health-care-artificial-intelligence-code-of-conduct/" target="_blank" rel="noreferrer noopener">Health Care Artificial Intelligence Code of Conduct<span class="sr-only"> (opens in new tab)</span></a> (Adams)&nbsp;<br>Project homepage | National Academy of Medicine&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nam.edu/perspectives/artificial-intelligence-in-health-health-care-and-biomedical-science-an-ai-code-of-conduct-principles-and-commitments-discussion-draft/" target="_blank" rel="noreferrer noopener">Artificial Intelligence in Health, Health Care, and Biomedical Science: An AI Code of Conduct Principles and Commitments Discussion Draft<span class="sr-only"> (opens in new tab)</span></a> (Adams)&nbsp;<br>National Academy of Medicine commentary paper | April 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.thehastingscenter.org/ethics-of-ai-in-health-and-biomedical-research/" target="_blank" rel="noreferrer noopener">Ethics of AI in Health and Biomedical Research<span class="sr-only"> (opens in new tab)</span></a> (Ravitsky)&nbsp;<br>The Hastings Center for Bioethics&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2831219" target="_blank" rel="noreferrer noopener">Ethics in Patient Preferences for Artificial Intelligence–Drafted Responses to Electronic Messages<span class="sr-only"> (opens in new tab)</span></a> (Ravitsky)&nbsp;<br>Publication | March 2025&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.daneshjoulab.com/" target="_blank" rel="noreferrer noopener">Daneshjou Lab<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Lab homepage&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41746-025-01542-0" target="_blank" rel="noreferrer noopener">Red teaming ChatGPT in medicine to yield real-world insights on model behavior<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Publication | March 2025&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.jidonline.org/article/S0022-202X(24)00270-7/fulltext" target="_blank" rel="noreferrer noopener">Dermatologists’ Perspectives and Usage of Large Language Models in Practice: An Exploratory Survey<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Publication | October 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41591-023-02728-3" target="_blank" rel="noreferrer noopener">Deep learning-aided decision support for diagnosis of skin disease across skin tones<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Publication | February 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41746-023-00939-z" target="_blank" rel="noreferrer noopener">Large language models propagate race-based medicine<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Publication | October 2023&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.science.org/doi/10.1126/sciadv.abq6147" target="_blank" rel="noreferrer noopener">Disparities in dermatology AI performance on a diverse, curated clinical image set<span class="sr-only"> (opens in new tab)</span></a> (Daneshjou)&nbsp;<br>Publication | August 2022</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC]   &nbsp;</p>



<p>[BOOK PASSAGE] &nbsp;</p>



<p><strong>PETER LEE</strong>: “…<strong> </strong>This is the moment for broad, thoughtful consideration of how to ensure maximal safety and also maximum access. Like any medical tool, AI needs those guardrails to keep patients as safe as possible. But it’s a tricky balance: those safety measures must not mean that the great advantages that we document in this book end up unavailable to many who could benefit from them. One of the most exciting aspects of this moment is that the new AI could accelerate healthcare in a direction that is better for patients, all patients, and providers as well—if they have access.”&nbsp;</p>



<p>[END OF BOOK PASSAGE]   &nbsp;</p>



<p>[THEME MUSIC]   &nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee.   &nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?    &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here. </p>



				</span>
				<span id="show-more-show-less-toggle-5" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The passage I read at the top there is from Chapter 9, “Safety First.”&nbsp;</p>



<p>One needs only to look at examples such as laws mandating seatbelts in cars and, more recently, internet regulation to know that policy and oversight are often playing catch-up with emerging technologies. When we were writing our book, Carey, Zak, and I didn’t claim that putting frameworks in place to allow for innovation and adoption while prioritizing inclusiveness and protecting patients from hallucination and other harms would be easy. In fact, in our writing, we posed more <em>questions</em> than answers in the hopes of highlighting the complexities at hand and supporting constructive discussion and action in this space.&nbsp;&nbsp;</p>



<p>In this episode, I’m pleased to welcome three experts who have been thinking deeply about these matters: Laura Adams, Vardit Ravitsky, and Dr. Roxana Daneshjou.&nbsp;&nbsp;</p>



<p>Laura is an expert in AI, digital health, and human-centered care. As a senior advisor at the National Academy of Medicine, or <em>NAM</em>, she guides strategy for the academy’s science and technology portfolio and leads the Artificial Intelligence Code of Conduct national initiative.&nbsp;&nbsp;</p>



<p>Vardit is president and CEO of The Hastings Center for Bioethics, a bioethics and health policy institute. She leads research projects funded by the National Institutes of Health, is a member of the committee developing the National Academy of Medicine’s AI Code of Conduct, and is a senior lecturer at Harvard Medical School.&nbsp;&nbsp;</p>



<p>Roxana is a board-certified dermatologist and an assistant professor of both dermatology and biomedical data science at Stanford University. Roxana is among the world&#8217;s thought leaders in AI, healthcare, and medicine, thanks in part to groundbreaking work on AI biases and trustworthiness.&nbsp;</p>



<p>One of the good fortunes I&#8217;ve had in my career is the chance to work with both Laura and Vardit, mainly through our joint work with the National Academy of Medicine. They&#8217;re both incredibly thoughtful and decisive leaders working very hard to help the world of healthcare—and healthcare regulators—come to grips with generative AI. And over the past few years, I&#8217;ve become an avid reader of all of Roxana&#8217;s research papers. Her work is highly technical, super influential but also informative in a way that spans computer science, medicine, bioethics, and law.&nbsp;&nbsp;</p>



<p>These three leaders—one from the medical establishment, one from the bioethics field, and the third from clinical research—provide insights into three incredibly important dimensions of the issues surrounding regulations, norms, and ethics of AI in medicine.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Here is my interview with Laura Adams:&nbsp;</p>



<p><strong>LEE:</strong> Laura, I&#8217;m just incredibly honored and excited that you&#8217;re joining us here today, so welcome.&nbsp;</p>



<p><strong>ADAMS: </strong>Thank you, Peter, my pleasure. Excited to be here.&nbsp;</p>



<p><strong>LEE:</strong> So, Laura, you know, I&#8217;ve been working with you at the NAM for a while, and you are a strategic advisor at the NAM. But I think a lot of our listeners might not know too much about the National Academy of Medicine and then, within the National Academy of Medicine, what a strategic advisor does.&nbsp;&nbsp;</p>



<p>So why don&#8217;t we start there. You know, how would you explain to a person&#8217;s mother or father what the National Academy of Medicine is?&nbsp;</p>



<p><strong>ADAMS:</strong> Sure. National Academy was formed more than 50 years ago. It was formed by the federal government, but it is not the federal government. It was formed as an independent body to advise the nation and the federal government on issues of science and primarily technology-related issues, as well.&nbsp;&nbsp;</p>



<p>So with that 50 years, some probably know of the National Academy of Medicine when it was the Institute of Medicine and produced such publications as <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.ncbi.nlm.nih.gov/books/NBK225182/" target="_blank" rel="noreferrer noopener">To Err is Human<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://pubmed.ncbi.nlm.nih.gov/25057539/" target="_blank" rel="noreferrer noopener">Crossing the Quality Chasm<span class="sr-only"> (opens in new tab)</span></a>, both of which were seminal publications that I think had a dramatic impact on quality, safety, and how we saw our healthcare system and what we saw in terms of its potential.&nbsp;</p>



<p><strong>LEE: </strong>So now, for your role within NAM, what does the senior advisor do? What do you do?&nbsp;&nbsp;</p>



<p><strong>ADAMS:</strong> What I do there is in the course of leading the AI Code of Conduct project, my role there was in framing the vision for that project, really understanding what did we want it to do, what impact did we want it to make.&nbsp;&nbsp;</p>



<p>So for example, some thought that it might be that we wanted everyone to use our code of conduct. And my advice on that was let&#8217;s use this as a touchstone. We want people to think about their own codes of conduct for their use of AI. That&#8217;s a valuable exercise, to decide what you value, what your aspirations are.&nbsp;&nbsp;</p>



<p>I also then do a lot of the field alignment around that work. So I probably did 50 talks last year—conference presentations, webinars, different things—where the code of conduct was presented so that the awareness could be raised around it so people could see the practicality of using that tool.&nbsp; &nbsp;<br>&nbsp;<br>Especially the six commitments that were based on the idea of complex adaptive systems simple rules, where we could recall those in the heat of decision-making around AI, in the heat of application, or even in the planning and strategic thinking around it.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> All right, we&#8217;re going to want to really break into a lot of details here.&nbsp;&nbsp;</p>



<p>But I would just like to rewind the clock a little bit and talk about your first encounters with AI. And there&#8217;s sort of, I guess, two eras. There&#8217;s the era of AI and machine learning <em>before</em> ChatGPT, before the generative AI era, and then afterwards.&nbsp;&nbsp;</p>



<p>Before the era of generative AI, what was your relationship with the idea of artificial intelligence? Was it a big part of your role and something you thought about, or was it just one of many technologies that you considered?&nbsp;</p>



<p><strong>ADAMS:</strong> It was one of many.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;&nbsp;</p>



<p><strong>ADAMS:</strong> Watching it help us evolve from predictive analytics to predictive AI, which of course I was fascinated by the fact that it could use structured and unstructured data, that it could learn from its own processes. These things were really quite remarkable, but my sense about it was that it was one of many.&nbsp;&nbsp;</p>



<p>We were looking at telemedicine. We were looking at [a] variety of other things, particularly wearables and things that were affecting and empowering patients to take better care of themselves and take more &#8230; have more agency around their own care. So I saw it as one of many.&nbsp;&nbsp;</p>



<p>And then the world changed in 2022, changed dramatically.&nbsp;</p>



<p><strong>LEE: </strong>[LAUGHS] OK. Right. OK, so November 2022, ChatGPT. Later in the spring of 2023, GPT-4. And so, you know, what were your first encounters, and what were you feeling? What were you experiencing?&nbsp;</p>



<p><strong>ADAMS:</strong> At the time, I was curious, and I thought, <em>I think I&#8217;m seeing four things here that make this way different</em>.&nbsp;&nbsp;</p>



<p>And one was, and it proved to be true over time, the speed with which this evolved. And I was watching it evolve very, very quickly and thinking, this is almost, this is kind of mind blowing how fast this is getting better.&nbsp;&nbsp;</p>



<p>And then this idea that, you know, we could scale this. As we were watching the early work with ambient listening, I was working with a group of physicians that were lamenting the cost and the unavailability of scribes. They wanted to use scribes. And I&#8217;m thinking, <em>We don&#8217;t have to incur the cost of that. We don&#8217;t have to struggle with the unavailability of that type of … someone in the workforce</em>.&nbsp;&nbsp;&nbsp;</p>



<p>And then I started watching the ubiquity, and I thought, <em>Oh, my gosh, this is unlike any other technology that we&#8217;ve seen. </em>Because with electronic health records, for example, it&#8217;s had its place, but it was over here.<em> </em>We had another digital technology, maybe telehealth, over here. This was one,<em> </em>and I thought, there will be no aspect of healthcare that will be left untouched by AI. That blew my mind.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>ADAMS: </strong>And then I think the last thing was the democratization. And I realized: <em>Wow, anyone with a smartphone has access to the most powerful large language models in the world. </em>&nbsp;<br>&nbsp;<br>And I thought, <em>This, to me, is a revolution in cheap expertise. </em>Those were the things that really began to stun me, and I just knew that we were in a way different era.&nbsp;</p>



<p><strong>LEE:</strong> It&#8217;s interesting that you first talked about ambient listening. Why was that of particular initial interest to you specifically?&nbsp;</p>



<p><strong>ADAMS:</strong> It was because one of the things that we were putting together in our code of conduct, which began pre-generative AI, was the idea that we wanted to renew the moral well-being and the sense of shared purpose to the healthcare workforce. That&#8217;s one of the six principles.&nbsp;&nbsp;</p>



<p>And I knew that the cognitive burden was becoming unbearable. When we came out of COVID, it was such a <em>huge</em> wake-up call to understand exactly what was going on at that point of care and how challenging it had become because information overload is astonishing in and of itself. And that idea that we have so much in the way of documentation that needed to be done and how much of a clinician&#8217;s time was taken up doing that rather than doing the thing that they went into the profession to do. And that was interact with people, that was to heal, that was to develop human connection that had a healing effect, and they just &#8230; so much of the time was taken away from that activity.&nbsp;&nbsp;</p>



<p>I also looked at it and because I studied diffusion of innovations theory and understand what causes something to move rapidly across a social system and get adopted, it has to have a clear relative advantage. It has to be compatible with the way that processes work.&nbsp;</p>



<p>So I didn&#8217;t see that this was going to be a hugely disruptive activity to workflow, which is a challenge of most digital tools, is that they&#8217;re designed without that sense of, how does this impact the workflow? And then I just thought that it was going to be a front runner in adoption, and it might then start to create that tsunami, that wave of interest in this, and I don&#8217;t think I was wrong.&nbsp;</p>



<p><strong>LEE:</strong> I have to ask you, because I&#8217;ve been asking every guest, there must have been moments early on in the encounter with generative AI where you felt doubt or skepticism. Is that true, or did you immediately think, <em>Wow, this is something very important?</em>&nbsp;</p>



<p><strong>ADAMS:</strong> No, I did feel doubt and skepticism.&nbsp;&nbsp;</p>



<p>My understanding tells me of it, and told me of it in the very beginning, that this is trained on the internet with all of its flaws. When we think about AI, we think about it being very futuristic, but it&#8217;s trained on data from the past. I&#8217;m well aware of how flawed that data, how biased that data is, mostly men, mostly white men, when we think about it during a certain age grouping of.&nbsp;&nbsp;</p>



<p>So I knew that we had inherent massive flaws in the training data and that concerned me. I saw other things about it that also concerned me. I saw that &#8230; its difficulty in beginning to use it and govern it effectively.&nbsp;&nbsp;</p>



<p>You really do have to put a good governance system in if you&#8217;re going to put this into a care delivery system. And I began to worry about widening a digital divide that already was a chasm. And that was between those well-resourced, usually urban, hospitals and health systems that are serving the well-resourced, and the inner-city hospital in Chicago or the rural hospital in Nebraska or the Mississippi community health center.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes. So I think this skepticism about technology, new technologies, in healthcare is very well-earned. So you&#8217;ve zeroed in on this issue of technology where oftentimes we hope it&#8217;ll reduce or eliminate biases but actually seems to oftentimes have the opposite effect.&nbsp;&nbsp;</p>



<p>And maybe this is a good segue then into this really super-important national effort that you&#8217;re leading on the AI code of conduct. Because in a way, I think those failures of the past and even just the idea—the <em>promise</em>—that technology should make a doctor or a nurse&#8217;s job easier, not harder, even that oftentimes seems not to have panned out in the way that we hope.&nbsp;&nbsp;</p>



<p>And then there&#8217;s, of course, the well-known issue of hallucinations or of mistakes being made. You know, how did those things inform this effort around a code of conduct, and <em>why</em> a code of conduct?&nbsp;</p>



<p><strong>ADAMS:</strong> Those things weighed heavily on me as the initiative leader because I had been deeply involved in the spread of electronic health records, not really knowing and understanding that electronic health records were going to have the effect that they had on the providers that use them.<strong>&nbsp;</strong>&nbsp;</p>



<p>Looking back now, I think that there could have been design changes, but we probably didn&#8217;t have as much involvement of providers in the design. And in some cases, we did. We just didn&#8217;t understand what it would take to work it into their workflows.&nbsp;&nbsp;</p>



<p>So I wanted to be sure that the code of conduct took into consideration and made explicit some of the things that I believe would have helped us had we had those guardrails or those guidelines explicit for us.&nbsp;&nbsp;</p>



<p>And those are things like our first one is to protect and advance human health and connection.&nbsp;&nbsp;</p>



<p>We also wanted to see things about openly sharing and monitoring because we know that for this particular technology, it&#8217;s emergent. We&#8217;re going to have to do a much better job at understanding whether what we&#8217;re doing works and works in the real world.&nbsp;&nbsp;</p>



<p>So the reason for a code of conduct was we knew that … the good news, when the “here comes AI and it&#8217;s barreling toward us,” the good news was that everybody was putting together guidelines, frameworks, and principle sets. The bad news was <em>same</em>. That everybody was putting together their own guideline, principle, and framework set.&nbsp;&nbsp;</p>



<p>And I thought back to how much I struggled when I worked in the world of health information exchange and built a statewide health information exchange and then turned to try to exchange that data across the nation and realized that we had a patchwork of privacy laws and regulations across the state; it was extremely costly to try to move data.&nbsp;&nbsp;</p>



<p>And I thought we actually need, in addition to data interoperability, we need governance interoperability, where we can begin to agree on a core set of principles that will more easily allow us to move ahead and achieve some of the potential and the vision that we have for AI if we are not working with a patchwork of different guidelines, principles, and frameworks.&nbsp;&nbsp;</p>



<p>So that was the impetus behind it. Of course, we again want it to be used as a touchstone, not everybody wholesale adopt what we&#8217;ve said.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>ADAMS:</strong> We want people to think about this and think deeply about it.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, Laura, I always am impressed with just how humble you are. You were indeed, you know, one of the prime instigators of the digitization of health records leading to electronic health record systems. And I don&#8217;t think you need to feel bad about that. That was a tremendous advance. I mean, moving a fifth of the US economy to be digital, I think, is significant.&nbsp;&nbsp;</p>



<p>Also, our listeners might want to know that you led something called the Rhode Island Quality Institute, which was really, I think, maybe the, arguably, <em>the</em> most important early kind of examples that set a pattern for how and why health data might actually lead very directly to improvements in human health at a statewide level or at a population level. And so I think your struggles and frustrations on, you know, how to expand that nationwide, I think, are really, really informative.&nbsp;&nbsp;</p>



<p>So let&#8217;s get into what these principles are, you know, what&#8217;s in the code of conduct.<strong>&nbsp;</strong>&nbsp;</p>



<p><strong>ADAMS:</strong> Yeah, the six simple rules were derived out of a larger set of principles that we pulled together. And the origin of all of this was we did a fairly extensive landscape review. We looked at least at 60 different sets of these principles, guidelines, and frameworks. We looked for areas of real convergence. We looked for areas where there was inconsistencies. And we looked for out-and-out gaps.&nbsp;&nbsp;</p>



<p>The out-and-out gaps that we saw at the time were things like a dearth of references to advancing human health as the priority. Also monitoring post-implementation. So at the time, we were watching these evolve and we thought these are very significant gaps. Also, the impact on the environment was a significant gap, as well. And so when we pull that together, we developed a set of principles and cross-walked those with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nam.edu/product/learning-health-system-core-principles/" target="_blank" rel="noreferrer noopener">learning health system principles<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;&nbsp;</p>



<p>And then once we got that, we again wanted to distill that down into a set of commitments which we knew that people could find accessible. And we published that <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://nam.edu/perspectives/artificial-intelligence-in-health-health-care-and-biomedical-science-an-ai-code-of-conduct-principles-and-commitments-discussion-draft/" target="_blank" rel="noreferrer noopener">draft set of principles<span class="sr-only"> (opens in new tab)</span></a> last year. And we have a new publication that will be coming out in the coming months that will be the revised set of principles and code commitments that we got because we took this out publicly.&nbsp;</p>



<p>So we opened it up for public comment once we did the draft last year. Again, many of those times that I spoke about this, almost all of those times came with an invitation for feedback, and conversations that we had with people shaped it. And it is in no way, shape, or form a final code of conduct, this set of principles and commitments, because we see this as dynamic. But what we also knew about this was that we wanted to build this with a super solid foundation, a set of immutables, the things that don&#8217;t change at some vicissitudes or the whims of this or the whims of that. We wanted those things that were absolutely foundational.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, so we&#8217;ll provide a link to the documents that describe the current state of this, but can we give an example of one or two of these principles and one or two of the commitments?&nbsp;</p>



<p><strong>ADAMS: </strong>Sure. I&#8217;ve mentioned the “protect and advance human health and connection” as the primary aim. We also want to ensure the equitable distribution of risks and benefits, and that equitable distribution of risks and benefits is something that I was referring to earlier about when I see well-resourced organizations. And one that&#8217;s particularly important to me is engaging people as partners with agency at every stage of the AI lifecycle.&nbsp;&nbsp;</p>



<p>That one matters because this one talks about and speaks to the idea that we want to begin bringing in those that are affected by AI, those on whom AI is used, into the early development and conceptualization of what we want this new tool, this new application, to do. So that includes the providers that use it, the patients. And we find that when we include them—the ethicists that come along with that—we develop much better applications, much more targeted applications that do what we intend them to do in a more precise way.&nbsp;&nbsp;</p>



<p>The other thing about that engaging with agency, by <em>agency</em> we mean that person, that participant can affect the decisions and they can affect the outcome. So it isn&#8217;t that they&#8217;re sort of a token person coming into the table and we’ll allow you to tell your story or so, but this is an active participant.&nbsp;&nbsp;</p>



<p>We practiced what we preached when we developed the code of conduct, and we brought patient advocates in to work with us on the development of this, work with us on our applications, that first layer down of what the applications would look like, which is coming out in this new paper.&nbsp;&nbsp;</p>



<p>We really wanted that component of this because I&#8217;m also seeing that patients are definitely not passive users of this, and they&#8217;re having an agency <em>moment</em>, let&#8217;s say, with generative AI because they&#8217;ve discovered a new capacity to gain information, to—in many ways—claim some autonomy in all of this.&nbsp;&nbsp;&nbsp;</p>



<p>And I think that there is a disruption underway right now, a big one that has been in the works for many years, but it feels to me like AI may be the tipping point for that disruption of the delivery system as we know it.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right. I think it just exudes sort of inclusivity and thoughtfulness in the whole process. During this process, were there surprises, things that you didn&#8217;t expect? Things about AI technology itself that surprised you?&nbsp;</p>



<p><strong>ADAMS:</strong> The surprises that came out of this process for me, one of them was I surprised myself. We were working on the commentary paper, and Steven Lin from Stanford had strong input into that paper. And when we looked at what we thought were missing, he said, “Let&#8217;s make sure we have the environmental impact.” And I said, “Oh, really, Steven, we really want to think about things that are more directly aligned with health,” which I couldn&#8217;t believe came out of my own mouth. [LAUGHTER]&nbsp;</p>



<p>And Steven, without saying, “Do you hear yourself?” I mean, I think he could have said that. But he was more diplomatic than that. And he persisted a bit longer and said, “I think it&#8217;s actually the greatest threat to human health.” And I said, “Of course, you&#8217;re right.” [LAUGHS]&nbsp;</p>



<p>But that was surprising and embarrassing for <em>me</em>. But it was eye-opening in that even when I thought that I had understood the gaps and the using this as a touchstone. So the learning that took place and how rapidly that learning was happening among people involved in this.&nbsp;&nbsp;</p>



<p>The other thing that was surprising for me was the degree at which patients became vastly facile with using it to the extent that it helped them begin to, again, build their own capacity.&nbsp;&nbsp;</p>



<p>The <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.epatientdave.com/2024/03/11/new-hashtag-patientsuseai/" target="_blank" rel="noreferrer noopener">#PatientsUseAI</a> from <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/" target="_blank" rel="noreferrer noopener">Dave deBronkart</a>—watch that one. This is more revolutionary than we think. And so I watched that, the swell of that happening, and it sort of shocked me because I was envisioning this as, again, a tool for use in the clinical setting.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah. OK, so we&#8217;re running now towards the end of our time together. And I always like to end our conversations with a more provocative topic. And I thought for you, I&#8217;d like to use the very difficult word <em>regulation</em>.&nbsp;&nbsp;</p>



<p>And when I think about the book that Carey, Zak, and I wrote, we have a chapter on regulation, but honestly, we didn&#8217;t have ideas. We couldn&#8217;t understand how this would be regulated. And so we just defaulted to publishing a conversation about regulation with GPT-4. And in a way, I think … I don&#8217;t know that I or my coauthors were satisfied with that.&nbsp;&nbsp;</p>



<p>In your mind, where do we stand two years later now when we think about the need or not to regulate AI, particularly in its application to healthcare, and where has the thinking evolved to?&nbsp;</p>



<p><strong>ADAMS:</strong> There are two big differences that I see in that time that has elapsed. And the first one is we have understood the insufficiency of simply making sure that AI-enabled devices are safe prior to going out into implementation settings.&nbsp;&nbsp;</p>



<p>We recognize now that there&#8217;s got to be this whole other aspect of regulation and assurance that these things are functioning as intended and we have the capacity to do that in the point of care type of setting. So that&#8217;s been one of the major ones. The other thing is how wickedly challenging it is to regulate generative AI.&nbsp;&nbsp;</p>



<p>I think one of the most provocative and exciting <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://hbr.org/2024/09/how-to-regulate-generative-ai-in-healthcare" target="_blank" rel="noreferrer noopener">articles<span class="sr-only"> (opens in new tab)</span></a> that I saw written recently was by Bakul Patel and David Blumenthal, who posited, should we be regulating generative AI as we do a licensed and qualified provider?&nbsp;&nbsp;</p>



<p>Should it be treated in the sense that it&#8217;s got to have a certain amount of training and a foundation that&#8217;s got to pass certain tests? It has to demonstrate that it&#8217;s improving and keeping up with current literature. Does it … be responsible for mistakes that it makes in some way, shape, or form? Does it have to report its performance?&nbsp;&nbsp;</p>



<p>And I&#8217;m thinking, what a provocative idea …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>ADAMS:</strong> … but it&#8217;s worth considering. I chair the Global Opportunities Group for a regulatory and innovation AI sandbox in the UK. And we&#8217;re hard at work thinking about, how <em>do </em>you regulate something as unfamiliar and untamed, really, as generative AI?&nbsp;&nbsp;</p>



<p>So I&#8217;d like to see us think more about this idea of sandboxes, more this idea of should we be just completely rethinking the way that we regulate. To me, that&#8217;s where the new ideas will come because the danger, of course, in regulating in the old way &#8230; first of all, we haven&#8217;t kept up over time, even with predictive AI; even with pre-generative AI, we haven&#8217;t kept up.&nbsp;&nbsp;</p>



<p>And what worries me about continuing on in that same vein is that we will stifle innovation …&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>ADAMS:</strong> … and we won&#8217;t protect from potential harms. Nobody wants an AI Chernobyl, nobody.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Right&nbsp;<br>&nbsp;<br><strong>ADAMS:</strong> But I worry that if we use those old tools on the new applications that we will not only <em>not</em> regulate, then we&#8217;ll stifle innovation. And when I see all of the promise coming out of this for things that we thought were unimaginable, then that would be a tragedy.&nbsp;</p>



<p><strong>LEE:</strong> You know, I think the other reflection I&#8217;ve had on this is the consumer aspect of it, because I think a lot of our current regulatory frameworks are geared towards experts using the technology.&nbsp;&nbsp;</p>



<p><strong>ADAMS:</strong> Yes.&nbsp;</p>



<p><strong>LEE:</strong> So when you have a medical device, you know you have a trained, board-certified doctor or licensed nurse using the technology. But when you&#8217;re putting things in the hands of a consumer, I think somehow the surface area of risk seems wider to me. And so I think that&#8217;s another thing that somehow our current regulatory concepts aren&#8217;t really ready for.&nbsp;</p>



<p><strong>ADAMS:</strong> I would agree with that. I think a few things to consider, vis-a-vis that, is that this revolution of patients using it is unstoppable. So it will happen. But we&#8217;re considering a project here at the National Academy about patients using AI and thinking about: let&#8217;s explore all the different facets of that. Let&#8217;s understand, what does safe usage look like? What might we do to help this new development enhance the patient-provider relationship and not erode it as we saw, “Don&#8217;t confuse your Google search with my medical degree” type of approach.&nbsp;&nbsp;</p>



<p>Thinking about: how does it change the identity of the provider? How does it &#8230; what can we do to safely build a container in which patients can use this without giving them the sense that it&#8217;s being taken away, or that &#8230; because I just don&#8217;t see that happening. I don&#8217;t think they&#8217;re going to let it happen.&nbsp;&nbsp;</p>



<p>That, to me, feels extremely important for us to explore all the dimensions of that. And that is one project that I hope to be following on to the AI Code of Conduct and applying the code of conduct principles with that project.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Well, Laura, thank you again for joining us. And thank you even more for your tremendous national, even international, leadership on really helping mobilize the greatest institutions in a diverse way to fully confront the realities of AI in healthcare. I think it&#8217;s tremendously important work.&nbsp;</p>



<p><strong>ADAMS: </strong>Peter, thank you for having me. This has been an absolute pleasure.&nbsp;</p>



<p>[TRANSITION MUSIC] &nbsp;</p>



<p>I&#8217;ve had the opportunity to watch Laura in action as she leads a national effort to define an AI code of conduct. And our conversation today has only heightened my admiration for her as a national leader.&nbsp;&nbsp;</p>



<p>What impresses me is Laura&#8217;s recognition that technology adoption in healthcare has had a checkered history and furthermore oftentimes not accommodated the huge diversity of stakeholders that are affected equally.&nbsp;</p>



<p>The concept of an AI code of conduct seems straightforward in some ways, but you can tell that every word in the emerging code has been chosen carefully. And Laura&#8217;s tireless engagement traveling to virtually every corner of the United States, as well as to several other countries, shows real dedication.&nbsp;</p>



<p>And now here&#8217;s my conversation with Vardit Ravitsky:</p>



<p><strong>LEE: </strong>Vardit, thank you so much for joining.&nbsp;</p>



<p><strong>RAVITSKY: </strong>It&#8217;s a real pleasure. I&#8217;m honored that you invited me.&nbsp;</p>



<p><strong>LEE: </strong>You know, we&#8217;ve been lucky. We&#8217;ve had a few chances to interact and work together within the National Academy of Medicine and so on. But I think for many of the normal subscribers to the Microsoft Research Podcast, they might not know what The Hastings Center for Bioethics is and then what you as the leader of The Hastings Center do every day. So I&#8217;d like to start there, first off, with what is The Hastings Center?&nbsp;</p>



<p><strong>RAVITSKY:</strong> Mostly, we&#8217;re a research center. We&#8217;ve been around for more than 55 years. And we&#8217;re considered one of the organizations that actually founded the field known today as bioethics, which is the field that explores the policy implications, the ethical, social issues in biomedicine. So we look at how biotechnology is rolled out; we look at issues of equity, of access to care. We look at issues at the end of life, the beginning of life, how our natural environment impacts our health. Any aspect of the delivery of healthcare, the design of the healthcare system, and biomedical research leading to all this. Any aspect that has an ethical implication is something that we&#8217;re happy to explore.&nbsp;&nbsp;</p>



<p>We try to have broad conversations with many, many stakeholders, people from different disciplines, in order to come up with guidelines and recommendations that would actually help patients, families, communities.&nbsp;&nbsp;</p>



<p>We also have an editorial department. We publish academic journals. We publish a blog. And we do a lot of public engagement activities—webinars, in-person events. So, you know, we just try to promote the thinking of the public and of experts on the ethical aspects of health and healthcare.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> One thing I&#8217;ve been impressed with, with your work and the work of The Hastings Center is it really confronts big questions but also gets into a lot of practical detail. And so we&#8217;ll get there. But before that just a little bit about you then. The way I like to ask this question is: how do you explain to your parents what you do every day? [LAUGHS]&nbsp;</p>



<p><strong>RAVITSKY:</strong> Funny that you brought my parents into this, Peter, because I come from a family of <em>philosophers</em>. Everybody in my family is in humanities, in academia. When I was 18, I thought that that was the only profession [LAUGHTER] and that I absolutely had to become a philosopher, or else what else can you do with your life?&nbsp;</p>



<p>I think being a bioethicist is really about, on one hand, keeping an eye constantly on the science as it evolves. When a new scientific development occurs, you have to understand what&#8217;s happening so that you can translate that outside of science. So if we can now make a gamete from a skin cell so that babies will be created differently, you have to understand how that&#8217;s done, what that means, and how to talk about it.&nbsp;&nbsp;</p>



<p>The second eye you keep on the ethics literature. What ethical frameworks, theories, principles have we developed over the last decades that are now relevant to this technology. So you&#8217;re really a bridge between science, biomedicine on one hand and humanities on the other hand.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> OK. So let&#8217;s shift to AI. And here I&#8217;d like to start with a kind of an origin story because I&#8217;m assuming before generative AI and ChatGPT became widely known and available, you must have had some contact with ideas in data science, in machine learning, and, you know, in the concept of AI before ChatGPT. Is that true? And, you know, what were some of those early encounters like for you?&nbsp;</p>



<p><strong>RAVITSKY: </strong>The earlier issues that I heard people talk about in the field were really around diagnostics and reading images and, <em>Ooh, it looks like machines could perform better than radiologists. </em>And, <em>Oh,</em> <em>what if women preferred that their mammographies be read by these algorithms? </em>And, <em>Does that threaten us clinicians? Because it sort of highlights our limitations and weaknesses as, you know, the weakness of the human eye and the human brain.&nbsp;</em>&nbsp;</p>



<p>So there were early concerns about, will this outperform the human and potentially take away our jobs? Will it impact our <em>authority</em> with patients? What about de-skilling clinicians or radiologists or any type of diagnostician losing the ability &#8230; some abilities that they&#8217;ve had historically because machines take over? So those were the early-day reflections and interestingly some of them remain even now with generative AI.&nbsp;&nbsp;</p>



<p>All those issues of the standing of a clinician, and what sets us apart, and will a machine ever be able to perform completely autonomously, and what about empathy, and what about relationships? Much of that translated later on into the, you know, more advanced technology.&nbsp;</p>



<p><strong>LEE:</strong> I find it interesting that you use words like <em>our</em> and <em>we</em> to implicitly refer to humans, <em>homo sapiens</em>, to human beings. And so do you see a fundamental distinction, a hard distinction that separates humans from machines? Or, you know, how … <em>if</em> there are replacements of some human capabilities or some things that human beings do by machines, you know, how do you think about that?&nbsp;</p>



<p><strong>RAVITSKY:</strong> Ooh, you&#8217;re really pushing hard on the philosopher in me here. [LAUGHTER] I&#8217;ve read books and heard lectures by those who think that the line is blurred, and I don&#8217;t buy that. I think there&#8217;s a clear line between human and machine.&nbsp;&nbsp;</p>



<p>I think the issue of AGI—of artificial general intelligence—and will that amount to consciousness &#8230; again, it&#8217;s such a profound, deep philosophical challenge that I think it would take a lot of conceptual work to get there. So how do we define consciousness? How do we define morality? The way it stands now, I look into the future without being a technologist, without being an AI developer, and I think, maybe I <em>hope</em>, that the line will remain clear. That there&#8217;s something about humanity that is irreplaceable.&nbsp;&nbsp;</p>



<p>But I&#8217;m also remembering that Immanuel Kant, the famous German philosopher, when he talked about what it means to be a part of the moral universe, what it means to be a moral <em>agent</em>, he talked about rationality and the ability to implement what he called the categorical imperative. And he said that would apply to any creature, not just humans.&nbsp;</p>



<p>And that&#8217;s so interesting. It&#8217;s always fascinated me that so many centuries ago, he said such a progressive thing.&nbsp;</p>



<p><strong>LEE: </strong>That’s amazing, yeah.&nbsp;</p>



<p><strong>RAVITSKY: </strong>It is amazing because I often, as an ethicist, I don&#8217;t just ask myself, <em>What makes us human? </em>I ask myself, <em>What makes us worthy of moral respect? What makes us holders of rights? What gives us special status in the universe that other creatures don&#8217;t have? </em>And I know this has been challenged by people like Peter Singer who say [that] some animals should have the same respect. “And what about fetuses and what about people in a coma?” I know the landscape is very fraught.&nbsp;&nbsp;</p>



<p>But the notion of what makes humans deserving of special moral treatment to me is the core question of ethics. And if we think that it&#8217;s some capacities that give us this respect, that make us hold that status, then maybe it goes beyond human. So it doesn&#8217;t mean that the machine <em>is</em> human, but maybe at [a] certain point, these machines will <em>deserve</em> a certain type of moral respect that &#8230; it&#8217;s hard for us right now to think of a machine as deserving that respect. <em>That</em> I can see.&nbsp;</p>



<p>But completely collapsing the distinction between human and machine? I don&#8217;t think so, and I hope not.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Well, you know, in a way I think it&#8217;s easier to entertain this type of conversation post-ChatGPT. And so now, you know, what was your first personal encounter with what we now call generative AI, and what went through your mind as you had that first encounter?&nbsp;</p>



<p><strong>RAVITSKY: </strong>No one&#8217;s ever asked me this before, Peter. It almost feels exposing to share your first encounter. [LAUGHTER]&nbsp;&nbsp;</p>



<p>So I just logged on, and I asked a simple question, but it was an ethical question. I framed an ethical dilemma because I thought, if I ask it to plan a trip, like all my friends already did, it&#8217;s less interesting to me.&nbsp;&nbsp;</p>



<p>And within seconds, a pretty thoughtful, surprisingly nuanced analysis was kind of trickling down my screen, and I was shocked. I was really taken aback. I was almost sad because I think my whole life I was hoping that only humans can generate this kind of thinking using moral and ethical terms.&nbsp;&nbsp;</p>



<p>And then I started tweaking my question, and I asked for specific philosophical approaches to this. And it just kept surprising me in how well it performed.&nbsp;&nbsp;</p>



<p>So I literally had to catch my breath and, you know, sit down and go, OK, this is a new world, something very important and potentially scary is happening here. <em>How is this going to impact my teaching? How is this going to impact my writing? How is this going to impact health?</em> Like, it was really a moment of shock.&nbsp;</p>



<p><strong>LEE:</strong> I think the first time I had the privilege of meeting you, I heard you speak and share some of your initial framing of how, you know, how to think about the potential ethical implications of AI and the human impacts of AI in the future. Keeping in mind that people listening to this podcast will tend to be technologists and computer scientists as well as some medical educators and practicing clinicians, you know, what would you like them to know or understand most about your thoughts?&nbsp;</p>



<p><strong>RAVITSKY:</strong> I think from early on, Peter, I&#8217;ve been an advocate in favor of bioethics as a field positioning itself to be a <em>facilitator</em> of implementing AI. I think on one hand, if we remain the naysayers as we have been regarding other technologies, we will become irrelevant. Because it&#8217;s happening, it&#8217;s happening fast, we have to keep our eye on the ball, and not ask, “Should we do it?” But rather ask, “<em>How </em>should we do it?”&nbsp;</p>



<p>And one of the reasons that bioethics is going to be such a critical player is that the stakes are so high. The risk of making a mistake in diagnostics is literally life and death; the risk of breaches of privacy that would lead to patients losing trust and refusing to use these tools; the risk of clinicians feeling overwhelmed and replaceable. The risks are just too high.&nbsp;&nbsp;</p>



<p>And therefore, creating guardrails, creating frameworks with principles that sensitize us to the ethical aspects, that is critically important for AI and health to succeed. And I&#8217;m saying it as someone who wants it very badly to succeed.&nbsp;</p>



<p><strong>LEE: </strong>You are actually seeing a lot of healthcare organizations adopting and deploying AI. Has any aspect of that been surprising to you? Have you expected it to be happening faster or slower or unfolding in a different way?&nbsp;</p>



<p><strong>RAVITSKY: </strong>One thing that surprises me is how it seems to be isolated. Different systems, different institutions making their own, you know, decisions about what to acquire and how to implement. I&#8217;m not seeing consistency. And I&#8217;m not even seeing anybody at a higher level collecting all the information about who&#8217;s buying and implementing what under what types of principles and what are their outcomes? What are they seeing?&nbsp;&nbsp;</p>



<p>It seems to be just siloed and happening everywhere. And I wish we collected all this data, even about how the decision is made at the executive level to buy a certain tool, to implement it, where, why, by whom. So that&#8217;s one thing that surprised me.&nbsp;&nbsp;</p>



<p>The speed is not surprising me because it really solves problems that healthcare systems have been struggling with. What seems to be one of the more popular uses, and again, you know this better than I do, is the help with scribes with taking notes, ambient recording. This seems to be really desired because of burnout that clinicians face around this whole issue of note taking.&nbsp;&nbsp;</p>



<p>And it&#8217;s also seen as a way to allow clinicians to do more human interaction, you know, …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;&nbsp;</p>



<p><strong>RAVITSKY: </strong>… look at the patient, talk to the patient, …&nbsp;</p>



<p><strong>LEE:</strong> Yep.&nbsp;</p>



<p><strong>RAVITSKY:</strong> … listen, rather than focus on the screen. We&#8217;ve all sat across the desk with a doctor that never looks at us because they only look at the screen. So there&#8217;s a real problem here, and there&#8217;s a real solution and therefore it&#8217;s hitting the ground quickly.&nbsp;&nbsp;</p>



<p>But what&#8217;s surprising to me is how many places don&#8217;t think that it&#8217;s their responsibility to inform patients that this is happening. So some places do; some places don&#8217;t. And to me, this is a fundamental ethical issue of patient autonomy and empowerment. And it&#8217;s also pragmatically the fear of a crisis of trust.<strong> </strong>People don&#8217;t like being recorded without their consent. Surprise, surprise.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm. Yeah, yeah.&nbsp;</p>



<p><strong>RAVITSKY:</strong> People worry about such a recording of a very private conversation that they consider to be confidential, such a recording ending up in the wrong hands or being shared externally or going to a commercial entity. People care; patients care.&nbsp;&nbsp;</p>



<p>So what is our ethical responsibility to tell them? And what is the institutional responsibility to implement these wonderful tools? I&#8217;m not against them, I&#8217;m totally in favor—implement these great tools in a way that respects long-standing ethical principles of informed consent, transparency, accountability for, you know, change in practice? And, you know, bottom line: patients right to know what&#8217;s happening in their care.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>You actually recently had <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2831219" target="_blank" rel="noreferrer noopener">a paper in a medical journal<span class="sr-only"> (opens in new tab)</span></a> that touched on an aspect of this, which I think was not with scribes, but with notes, you know, …&nbsp;</p>



<p><strong>RAVITSKY: </strong>Yep.&nbsp;</p>



<p><strong>LEE:</strong> … that doctors would send to patients. And in fact, in previous episodes of this podcast, we actually talked to both the technology developers of that type of feature as well as doctors who were using that feature. And in fact, even in those previous conversations, there was the question, “Well, what does the patient need to know about how this note was put together?” So you and your coauthors had a very interesting recent paper about this.&nbsp;</p>



<p><strong>RAVITSKY: </strong>Yeah, so the trigger for the paper was that patients seemed to really like being able to send electronic messages to clinicians.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>RAVITSKY: </strong>We email and text all day long. Why not in health, right? People are used to communicating in that way. It&#8217;s efficient; it&#8217;s fast.&nbsp;&nbsp;&nbsp;</p>



<p>So we asked ourselves, “Wait, what if an AI tool writes the response?” Because again, this is a huge burden on clinicians, and it&#8217;s a real issue of burnout.&nbsp;&nbsp;</p>



<p>We surveyed hundreds of respondents, and basically what we discovered is that there was a statistically significant difference in their level of satisfaction when they got an answer from a human clinician, when they got an answer, again, electronic message from AI.&nbsp;&nbsp;</p>



<p>And it turns out that they preferred the messages written by AI. They were longer, more detailed, even conveyed more empathy. You know, AI has all the time in the world [LAUGHS] to write you a text. It&#8217;s not rushing to the next one.&nbsp;</p>



<p>But then when we disclosed who wrote the message, they were less satisfied when they were told it was AI.&nbsp;&nbsp;</p>



<p>So the ethical question that that raises is the following: if your only metric is patient satisfaction, the solution is to respond using AI but not tell them that.&nbsp;</p>



<p>Now when we compared telling them that it was AI or human or not telling them anything, their satisfaction remained high, which means that if they were not told anything, they probably assumed that it was a human clinician writing, because their satisfaction for human clinician or no disclosure was the same.&nbsp;</p>



<p>So basically, if we say nothing and just send back an AI-generated response, they will be more satisfied because the response is nicer, but they won&#8217;t be put off by the fact that it was written by AI. And therefore, hey presto, optimal satisfaction. But we challenge that, and we say, it&#8217;s not just about satisfaction.&nbsp;</p>



<p>It&#8217;s about long-term trust. It&#8217;s about your right to know. It&#8217;s about empowering you to make decisions about how you want to communicate.&nbsp;&nbsp;</p>



<p>So we push back against this notion that we&#8217;re just there to optimize patient satisfaction, and we bring in broader ethical considerations that say, “No, patients need to know.” If it&#8217;s not the norm yet to get your message from AI, …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>RAVITSKY:</strong> … they should know that this is happening. And I think, Peter, that maybe we&#8217;re in a transition period.&nbsp;</p>



<p>It could be that in two years, maybe less than that, most of our communication will come back from AI, and we will just take it for granted …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>RAVITSKY: </strong>… that that&#8217;s the case. And at that point, maybe disclosure is not necessary because many, many surveys will show us that patients assume that, and therefore they are informed. But at this point in time, when it&#8217;s transition and it&#8217;s not the norm yet, I firmly think that ethics requires that we inform patients.&nbsp;</p>



<p><strong>LEE: </strong>Let me push on this a little bit because I think this final point that you just made is, I think is so interesting. Does it matter what kind of information is coming from a human or AI? Is there a time when patients will have different expectations for different types of information from their doctors?&nbsp;</p>



<p><strong>RAVITSKY: </strong>I think, Peter, that you&#8217;re asking the right question because it&#8217;s more nuanced. And these are the kinds of empirical questions that we will be exploring in the coming months and years. Our recent paper showed that there was no difference regarding the content. If the message was about what we call the “serious” matter or a less “serious” matter, the preferences were the same. But we didn&#8217;t go deep enough into that. That would require a different type of design of study. And you just said, you know, there are different types of information. We need to categorize them.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;&nbsp;</p>



<p><strong>RAVITSKY: </strong>What types of information and what degree of impact on your life? Is it a life-and-death piece of information? Is it a quality-of-life piece of information? How central is it to your care and to your thinking? So all of that would have to be mapped out so that we can design these studies.&nbsp;&nbsp;</p>



<p>But you know, you pushed back in that way, and I want to push back in a different direction that to me is more fundamental and philosophical. How much do we know now? You know, I keep saying, oh, patients deserve a chance for informed consent, …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;&nbsp;</p>



<p><strong>RAVITSKY: </strong>… and they need to be empowered to make decisions. And if they don&#8217;t want that tool used in their care, then they should be able to say, “No.” <em>Really?</em> Is that the world we live in <em>now</em>? [LAUGHTER] Do I have access to the black box that is my doctor&#8217;s brain? Do I know how they performed on this procedure in the last year?&nbsp;</p>



<p>Do I know whether they&#8217;re tired? Do I know if they&#8217;re up to speed on the literature with this? We already deal with black boxes, except they&#8217;re not AI. And I think the evidence emerges that AI outperforms the humans in so many of these tasks.&nbsp;&nbsp;</p>



<p>So my pushback is, are we seeing AI exceptionalism in the sense that if it&#8217;s AI, <em>Huh, panic!</em> We have to inform everybody about everything, and we have to give them choices, and they have to be able to reject that tool and the other tool versus, you know, the rate of human error in medicine is awful. People don&#8217;t know the numbers. The annual deaths attributed to medical error is outrageous.&nbsp;&nbsp;</p>



<p>So why are we so focused on informed consent and empowerment regarding implementation of AI and less in other contexts. Is it just because it&#8217;s new? Is it because it is some sort of existential threat, …&nbsp;</p>



<p><strong>LEE: </strong>Yep, yeah.&nbsp;</p>



<p><strong>RAVITSKY: </strong>… not just a matter of risk? I don&#8217;t know the answer, but I don&#8217;t want us to suffer from AI exceptionalism, and I don&#8217;t want us to hold AI to such a high standard that we won&#8217;t be able to benefit from it. Whereas, again, we&#8217;re dealing with black boxes already in medicine.&nbsp;</p>



<p><strong>LEE: </strong>Just to stay on this topic, though, one more question, which is, maybe, almost silly in how hypothetical it is. If instead of email, it were a Teams call or a Zoom call, doctor-patient, except that the doctor is not the real doctor, but it&#8217;s a perfect replica of the doctor designed to basically fool the patient that this is the real human being and having that interaction. Does that change the bioethical considerations at all?&nbsp;</p>



<p><strong>RAVITSKY: </strong>I think it does because it&#8217;s always a question of, are we really misleading? Now if you get a text message in an environment that, you know, people know AI is already integrated to some extent, maybe not your grandmother, but the younger generation is aware of this implementation, then maybe you can say, “Hmm. It was implied. I didn&#8217;t mean to mislead the patient.”&nbsp;</p>



<p>If the patient thinks they&#8217;re talking to a clinician, and they&#8217;re seeing, like—what if it&#8217;s not you now, Peter? What if I&#8217;m talking to an avatar [LAUGHS] or some representation of you? Would I feel that I was misled in recording this podcast? Yeah, I would. Because you really gave me good reasons to assume that it was you.&nbsp;&nbsp;</p>



<p>So it&#8217;s something deeper about trust, I think. And it touches on the notion of empathy. A lot of literature is being developed now on the issue of: what will remain the purview of the human clinician? What are humans good for [LAUGHS] when AI is so successful and especially in medicine?&nbsp;&nbsp;</p>



<p>And if we see that the text messages are read as conveying more empathy and more care and more attention, and if we then move to a visual representation, facial expressions that convey more empathy, we really need to take a hard look at what we mean by care. What about then the robots, right, that we can touch, that can hug us?&nbsp;&nbsp;</p>



<p>I think we&#8217;re really pushing the frontier of what we mean by human interaction, human connectedness, care, and empathy. This will be a lot of material for philosophers to ask themselves the fundamental question you asked me at first: what does it mean to be human?&nbsp;&nbsp;</p>



<p>But this time, what does it mean to be two humans together and to have a connection?&nbsp;&nbsp;</p>



<p>And if we can really be replaced in the sense that patients will feel more satisfied, more heard, more cared for, do we have ethical grounds for resisting that? And if so, why?&nbsp;&nbsp;</p>



<p>You&#8217;re really going deep here into the conceptual questions, but bioethics is already looking at that.&nbsp;</p>



<p><strong>LEE: </strong>Vardit, it&#8217;s just always amazing to talk to you. The incredible span of what you think about from those fundamental philosophical questions all the way to the actual nitty gritty, like, you know, what parts of an email from a doctor to a patient should be marked as AI. I think that span is just incredible and incredibly needed and useful today. So thank you for all that you do.&nbsp;</p>



<p><strong>RAVITSKY: </strong>Thank you so much for inviting me.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>The field of bioethics, and this is my take, is all about the adoption of disruptive new technologies into biomedical research and healthcare. And Vardit is able to explain this with such clarity. I think one of the reasons that AI has been challenging for many people is that its use spans the gamut from the nuts and bolts of how and when to disclose to patients that AI is being used to craft an email, all the way to, what does it mean to be a human being caring for another human?&nbsp;&nbsp;</p>



<p>What I learned from the conversation with Vardit is that bioethicists are confronting head-on the issue of AI in medicine and not with an eye towards restricting it, but recognizing that the technology is real, it&#8217;s arrived, and needs to be harnessed now for maximum benefit.&nbsp;&nbsp;</p>



<p>And so now, here&#8217;s my conversation with Dr. Roxana Daneshjou:&nbsp;</p>



<p><strong>LEE:</strong> Roxana, I&#8217;m just thrilled to have this chance to chat with you.&nbsp;</p>



<p><strong>ROXANA DANESHJOU</strong>: Thank you so much for having me on today. I&#8217;m looking forward to our conversation.&nbsp;</p>



<p><strong>LEE: </strong>In Microsoft Research, of course, you know, we think about healthcare and biomedicine a lot, but I think there&#8217;s less of an understanding from our audience what people actually do in their day-to-day work. And of course, you have such a broad background, both on the science side with a PhD and on the medical side. So what&#8217;s your typical work week like?&nbsp;</p>



<p><strong>DANESHJOU:</strong> I spend basically 90% of my time working on running <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.daneshjoulab.com/" target="_blank" rel="noreferrer noopener">my research lab<span class="sr-only"> (opens in new tab)</span></a> and doing research on how AI interacts with medicine, how we can implement it to fix the pain points in medicine, and how we can do that in a fair and responsible way. And 10% of my time, I am in clinic. So I am a practicing dermatologist at Stanford, and I see patients half a day a week.&nbsp;</p>



<p><strong>LEE: </strong>And your background, it&#8217;s very interesting. There’s always been these MD-PhDs in the world, but somehow, especially right now with what&#8217;s happening in AI, people like you have become suddenly extremely important because it suddenly has become so important to be able to combine these two things. Did you have any inkling about that when you started, let&#8217;s say, on your PhD work?&nbsp;</p>



<p><strong>DANESHJOU: </strong>So I would say that during my—[LAUGHS] because I was in training for <em>so</em> long—during … my PhD was in computational genomics, and I still have a significant interest in precision medicine, and I think AI is going to be central to that.&nbsp;&nbsp;</p>



<p>But I think the reason I became interested in AI initially is that I was thinking about how we associate genetic factors with patient phenotypes. Patient phenotypes being, <em>How does the disease present? What does the disease look like?</em> And I thought, you know, AI might be a good way to standardize phenotypes from images of, say, skin disease, because I was interested in dermatology at that time. And, you know, the part about phenotyping disease was a huge bottleneck because you would have humans sort of doing the phenotyping.&nbsp;&nbsp;</p>



<p>And so in my head, when I was getting into the space, I was thinking I&#8217;ll bring together, you know, computer vision and genetics to try to, you know, make new discoveries about how genetics impacts human disease. And then when I actually started my postdoc to learn computer vision, I went down this very huge rabbit hole, which I am still, I guess, falling down, where I realized, you know, about biases in computer vision and how much work needed to be done for generalizability.&nbsp;</p>



<p>And then after that, large language models came out, and, like, everybody else became incredibly interested in how this could help in healthcare and now also vision language models and multimodal models. So, you know, we&#8217;re just tumbling down the rabbit hole.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Indeed, I think you really made a name for yourself by looking at the issues of biases, for example, in training datasets. And that was well before large language models were a thing. Maybe our audience would like to hear a little bit more about <em>that</em> earlier work.&nbsp;&nbsp;</p>



<p><strong>DANESHJOU:</strong> So as I mentioned, my PhD was in computational genetics. In genetics, what has happened during the genetic revolution is these large-scale studies to discover how genetic variation impacts human disease and human response to medication, so that&#8217;s what pharmacogenomics is, is human response to medications. And as I got, you know, entrenched in that world, I came to realize that I wasn&#8217;t really represented in the data. And it was because the majority of these genetic studies were on European ancestry individuals. You weren&#8217;t represented either.&nbsp;</p>



<p><strong>LEE: </strong>Right, yeah.&nbsp;</p>



<p><strong>DANESHJOU: </strong>Many diverse global populations were <em>completely</em> excluded from these studies, and genetic variation is quite diverse across the globe. And so you&#8217;re leaving out a large portion of genetic variation from these research studies. Now things have improved. It still needs work in genetics. But definitely there has been many amazing researchers, you know, sounding the alarm in that space. And so during my PhD, I actually focused on doing studies of pharmacogenomics in non-European ancestry populations. So when I came to computer vision and in particular dermatology, where there were a lot of papers being published about how AI models perform at diagnosing skin disease and several papers essentially saying, oh, it&#8217;s equivalent to a dermatologist—of course, that&#8217;s not completely true because it&#8217;s a very sort of contrived, you know, setting of diagnosis— …&nbsp;</p>



<p><strong>LEE: </strong>Right, right.&nbsp;</p>



<p><strong>DANESHJOU: </strong>…<strong> </strong>but my first inkling was, well, are these models going to be able to perform well across skin tones? And one of our, you know, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.science.org/doi/10.1126/sciadv.abq6147" target="_blank" rel="noreferrer noopener">landmark papers, which was in Science Advances<span class="sr-only"> (opens in new tab)</span></a>, showed … we created a diverse dataset, our own diverse benchmark of skin disease, and showed that the models performed significantly worse on brown and black skin. And I think the key here is we also showed that it was an addressable problem because when we fine-tuned on diverse skin tones, you could make that bias go away. So it was really, in this case, about what data was going into the training of these computer vision models.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, and I think if you&#8217;re listening to this conversation, if you haven&#8217;t read that paper, I think it&#8217;s really must reading. It was not only, Roxana, it wasn&#8217;t only just a landmark scientifically and medically, but it also sort of crossed the chasm and really became a subject of public discourse and debate, as well. And I think you really changed the public discourse around AI.&nbsp;&nbsp;</p>



<p>So now I want to get into generative AI. I always like to ask, what was your first encounter with generative AI personally? And what went through your head? You know, what was that experience like for you?&nbsp;</p>



<p><strong>DANESHJOU: </strong>Yeah, I mean, I actually tell this story a lot because I think it&#8217;s a fun story. So I would say that I had played with, you know, GPT-3 prior and wasn&#8217;t particularly, you know, impressed …&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>DANESHJOU: </strong>… by how it was doing. And I was at NeurIPS [Conference on Neural Information Processing Systems] in New Orleans, and I was … we were walking back from a dinner. I was with Andrew Beam from Harvard. I was with his group.&nbsp;</p>



<p>And we were just, sort of, you know, walking along, enjoying the sites of New Orleans, chatting. And one of his students said, “Hey, OpenAI just released this thing called ChatGPT.”&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>So this would be New Orleans in December …&nbsp;</p>



<p><strong>DANESHJOU:</strong> 2022.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> 2022, right? Yes. Uh-huh. OK.&nbsp;</p>



<p><strong>DANESHJOU: </strong>So I went back to my hotel room. I was very tired. But I, you know, went to the website to see, OK, like, what is this thing? And I started to ask it medical questions, and I started all of a sudden thinking, “Uh-oh, we have made … we have made a leap here; something has changed.”&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>So it must have been very intense for you from then because months later, you had another <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41746-023-00939-z" target="_blank" rel="noreferrer noopener">incredibly impactful, or landmark, paper basically looking at biases, race-based medicine in large language models<span class="sr-only"> (opens in new tab)</span></a>. So can you say more about that?&nbsp;</p>



<p><strong>DANESHJOU: </strong>Yes. I work with a very diverse team, and we have thought about bias in medicine, not just with technology but also with humans. Humans have biases, too. And there&#8217;s this whole debate around, is the technology going to be more biased than the humans? How do we do that? But at the same time, like, the technology actually encodes the biases of humans.&nbsp;&nbsp;</p>



<p>And there was <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pnas.org/doi/10.1073/pnas.1516047113" target="_blank" rel="noreferrer noopener">a paper in the Proceedings of the National Academy of Sciences<span class="sr-only"> (opens in new tab)</span></a>, which did not look at technology at all but actually looked at the race-based biases of medical trainees that were untrue and harmful in that they perpetuated racist beliefs.&nbsp;&nbsp;</p>



<p>And so we thought, if medical trainees and humans have these biases, why don&#8217;t we see if the models carry them forward? And we added a few more questions that we, sort of, brainstormed as a team, and we started asking the models those questions. And &#8230;&nbsp;</p>



<p><strong>LEE:</strong> And by this time, it was GPT-4?&nbsp;&nbsp;</p>



<p><strong>DANESHJOU:</strong> We did include GPT-4 because GPT-4 came out, as well. And we also included other models, as well, such as Claude, because we wanted to look across the board. And what we found is that all of the models had instances of perpetuating race-based medicine. And actually, the GPT models had one of the most, I think, one of the most egregious responses—and, again, this is 3.5 and 4; we haven&#8217;t, you know, fully checked to see what things look like, because there have been newer models—in that they said that we should use race in calculating kidney function because there were differences in muscle mass between the races. And this is sort of a racist trope in medicine that is not true because race is not based on biology; it&#8217;s a social construct.&nbsp;&nbsp;&nbsp;</p>



<p>So, yeah, that was that study. And that one did spur a lot of public conversation.&nbsp;</p>



<p><strong>LEE:</strong> Your work there even had the issue of bias overtake hallucination, you know, as really the most central and most difficult issue. So how do you think about bias in LLMs, and does that in your mind disqualify the use of large language models from particular uses in medicine?&nbsp;&nbsp;</p>



<p><strong>DANESHJOU:</strong> Yeah, I think that the hallucinations are an issue, too. And in some senses, they might even go with one another, right. Like, if it&#8217;s hallucinating information that&#8217;s not true but also, like, biased.&nbsp;&nbsp;</p>



<p>So I think these are issues that we have to address with the use of LLMs in healthcare. But at the same time, things are moving very fast in this space. I mean, we have a secure instance of several large language models within our healthcare system at Stanford so that you could actually put secure patient information in there.&nbsp;&nbsp;</p>



<p>So while I acknowledge that bias and hallucinations are a huge issue, I also acknowledge that the healthcare system is quite broken and needs to be improved, needs to be streamlined. Physicians are burned out; patients are not getting access to care in the appropriate ways. And I have a really great story about that, which I can share with you later.&nbsp;&nbsp;</p>



<p>So in 2024, we did a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.jidonline.org/article/S0022-202X(24)00270-7/fulltext" target="_blank" rel="noreferrer noopener">study asking dermatologists, are they using large language models<span class="sr-only"> (opens in new tab)</span></a> in their clinical practice? And I think this percentage has probably gone up since then: 65% of dermatologists reported using large language models in their practices on tasks such as, you know, writing insurance authorization letters, you know, writing information sheets for the patient, even, sort of, using them to educate themselves, which makes me a little nervous because in my mind, the best use of large language models right now are cases where you can verify facts easily.&nbsp;&nbsp;</p>



<p>So, for example, I did show and teach my nurse how to use our secure large language model in our healthcare system to write rebuttal letters to the insurance. I told her, “Hey, you put in these bullet points that you want to make, and you ask it to write the letter, and you can verify that the letter contains the facts which you want and which are true.”&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>DANESHJOU: </strong>And we have also done a lot of work to try to stress test models because we <em>want</em> them to be better. And so we held this red-teaming event at Stanford where we brought together 80 physicians, computer scientists, engineers and had them write scenarios and real questions that they might ask on a day to day or tasks that they might actually ask AI to do.&nbsp;</p>



<p>And then we had them grade the performance. And we did this with the GPT models. At the&nbsp; time, we were doing it with GPT-3.5, 4, and 4 with internet. But before <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41746-025-01542-0" target="_blank" rel="noreferrer noopener">the paper<span class="sr-only"> (opens in new tab)</span></a> came out, we then ran the dataset on newer models.&nbsp;&nbsp;</p>



<p>And we <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://daneshjoulab.github.io/Red-Teaming-Dataset/index.html#dataset" target="_blank" rel="noreferrer noopener">made the dataset public<span class="sr-only"> (opens in new tab)</span></a> because I&#8217;m a <em>big</em> believer in public data. So we made the dataset public so that others could use this dataset, and we labeled what the issues were in the responses, whether it was bias, hallucination, like, a privacy issue, those sort of things.&nbsp;</p>



<p><strong>LEE: </strong>If I think about the hits or misses in our book, you know, we actually wrote a little bit, not too much, about noticing biases. I think we underestimated the magnitude of the issue in our book. And another element that we wrote about in our book is that we noticed that the language model, if presented with some biased decision-making, more often than not was able to spot that the decision-making was possibly being influenced by some bias. What do you make of that?&nbsp;&nbsp;</p>



<p><strong>DANESHJOU: </strong>So funny enough, I think we had … we had a—before I moved from Twitter to Bluesky—but we had a little back and forth on Twitter about this, which actually inspired us to look into this as a research, and we have a preprint up on it of actually using other large language models to identify bias and then to write critiques that the original model can incorporate and improve its answer upon.&nbsp;</p>



<p>I mean, we&#8217;re moving towards this sort of agentic systems framework rather than a singular large language model, and people, of course, are talking about also retrieval-augmented generation, where you sort of have this corpus of, you know, text that you trust and find trustworthy and have that incorporated into the response of the model.&nbsp;&nbsp;</p>



<p>And so you could build systems essentially where you do have other models saying, “Hey, specifically look for bias.” And then it will sort of focus on that task. And you can even, you know, give it examples of what bias is within context learning now. So I do think that we are going to be improving in this space. And actually, my team is … most recently, we&#8217;re working on building patient-facing chatbots. That&#8217;s where my, like, patient story comes in. But we&#8217;re building patient-facing chatbots. And we&#8217;re using, you know, we’re using prompt-engineering tools. We&#8217;re using automated eval tools. We&#8217;re building all of these things to try to make it more accurate and less bias. So it&#8217;s not just like one LLM spitting out an answer. It&#8217;s a whole system.&nbsp;</p>



<p><strong>LEE:</strong> All right. So let&#8217;s get to your patient-facing story. &nbsp;</p>



<p><strong>DANESHJOU:</strong> Oh, of course. Over the summer, my 6-year-old fell off the monkey bars and broke her arm. And I picked her up from school. She&#8217;s crying so badly. And I just look at her, and I know that we&#8217;re in trouble.&nbsp;</p>



<p>And I said, OK, you know, we&#8217;re going straight to the emergency room. And we went straight to the emergency room. She&#8217;s crying the whole time. I&#8217;m almost crying because it&#8217;s just, like, she doesn&#8217;t even want to go into the hospital. And so then my husband shows up, and we also had the baby, and the baby wasn&#8217;t allowed in the emergency room, so I had to step out.&nbsp;</p>



<p>And thanks to the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.fda.gov/regulatory-information/selected-amendments-fdc-act/21st-century-cures-act" target="_blank" rel="noreferrer noopener">[21st Century] Cures Act<span class="sr-only"> (opens in new tab)</span></a>, I&#8217;m getting, like, all the information, you know, as it&#8217;s happening. Like, I&#8217;m getting the x-ray results, and I&#8217;m looking at it. And I can tell there&#8217;s a fracture, but I can&#8217;t, you know, tell, like, how bad it is. Like, is this something that&#8217;s going to need surgery?&nbsp;&nbsp;</p>



<p>And I&#8217;m desperately texting, like, all the orthopedic folks I know, the pediatricians I know. [LAUGHTER] “Hey, what does this mean?” Like, getting real-time information. And later in the process, there was a mistake in her after-visit summary about how much Tylenol she could take. But I, as a physician, <em>knew</em> that this dose was a mistake.&nbsp;&nbsp;</p>



<p>I actually asked ChatGPT. I gave it the whole after-visit summary, and I said, are there any mistakes here? And it clued in that the dose of the medication was wrong. So again, I—as a physician with all these resources—have difficulty kind of navigating the healthcare system; understanding what&#8217;s going on in x-ray results that are showing up on my phone; can personally identify medication dose mistakes, but you know, most people probably couldn&#8217;t. And it could be very … I actually, you know, emailed the team and let them know, to give feedback.&nbsp;&nbsp;</p>



<p>So we have a healthcare system that is broken in so many ways, and it&#8217;s so difficult to navigate. So I get it. And so that&#8217;s been, you know, a big impetus for me to work in this space and try to make things better.&nbsp;</p>



<p><strong>LEE: </strong>That&#8217;s an incredible story. It&#8217;s also validating because, you know, one of the examples in our book was the use of an LLM to spot a medication error that a doctor or a nurse might make. You know, interestingly, we&#8217;re finding no sort of formalized use of AI right now in the field. But anecdotes like this are everywhere. So it&#8217;s very interesting.&nbsp;&nbsp;</p>



<p>All right. So we&#8217;re starting to run short on time. So I want to ask you a few quick questions and a couple that might be a little provocative.&nbsp;&nbsp;</p>



<p><strong>DANESHJOU: </strong>Oh boy. [LAUGHTER] Well, I don&#8217;t run away … I don&#8217;t run away from controversy.&nbsp;</p>



<p><strong>LEE:</strong> So, of course, with that story you just told, I can see that you use AI yourself. When you are actually in clinic, when you are being a dermatologist …&nbsp;</p>



<p><strong>DANESHJOU: </strong>Yeah.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>… and seeing patients, are you using generative AI?&nbsp;</p>



<p><strong>DANESHJOU: </strong>So I do not use it in clinic <em>except</em> for the situation of the insurance authorization letters. And even, I was offered, you know, sort of an AI-based scribe, which many people are using. There have been some studies that show that they can make mistakes. I have a human scribe. To me, writing the notes is actually part of the thinking process. So when I write my notes at the end of the day, there have been times that I&#8217;ve all of a sudden had an epiphany, particularly on a complex case. But I have used it to write, you know, sort of these insurance authorization letters. I&#8217;ve also used it in grant writing. So as a scientist, I have used it a lot more.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Right. So I don&#8217;t know of anyone who has a more nuanced and deeper understanding of the issues of biases in AI in medicine than you. Do you think [these] biases can be repaired in AI, and if not, what are the implications?&nbsp;</p>



<p><strong>DANESHJOU: </strong>So I think there are several things here, and I just want to be thoughtful about it. One, I think, the bias in the technology comes from bias in the system and bias in medicine, which very much exists and is incredibly problematic. And so I always tell people, like, it doesn&#8217;t matter if you have the most perfect, fair AI. If you have a biased human and you add those together, because you&#8217;re going to have this human-AI interaction, you&#8217;re still going to have a problem.&nbsp;&nbsp;</p>



<p>And there is <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41591-023-02728-3" target="_blank" rel="noreferrer noopener">a paper that I&#8217;m on with Dr. Matt Groh<span class="sr-only"> (opens in new tab)</span></a>, which looked at looking at dermatology diagnosis across skin tones and then with, like, AI assistance. And we found there&#8217;s a bias gap, you know, with even physicians. So it&#8217;s not just an AI problem; humans have the problem, too. And&#8230;&nbsp;</p>



<p><strong>LEE: </strong>Hmm. Yup.&nbsp;</p>



<p><strong>DANESHJOU: </strong>… we also looked at when you have the human-AI system, how that impacts the gap because you want to see the gap close. And it was kind of a mixed result in the sense that there was actually situations where, like, the accuracy increased in both groups, but the gap actually also increased because they were actually, even though they knew it was a fair AI, for some reason, they were relying upon the AI more often when … or they were trusting it more often on diagnoses on white skin—maybe they&#8217;d read my papers, who knows? [LAUGHTER]—even though we had told them, you know, it was a fair model.&nbsp;&nbsp;</p>



<p>So I think for me, the important thing is understanding how the AI model works with the physician at the task. And what I would like to see is it improve the overall bias and disparities with that unit.&nbsp;&nbsp;</p>



<p>And at the same time, I tell human physicians, we have to work on ourselves. We have to work on our system, you know, our medical system that has systemic issues of access to care or how patients get treated based on what they might look like or other parts of their background.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>All right, final question. So we started off with your stories about imaging in dermatology. And of course, Geoff Hinton, Turing winner and one of the grandfathers of the AI revolution, famously had predicted many years ago that by 2018 or something like that, we wouldn&#8217;t need human radiologists because of AI.&nbsp;</p>



<p>That hasn&#8217;t come to pass, but since you work in a field that also is very dependent on imaging technologies, do you see a future when radiologists or, for that matter, dermatologists might be largely replaced by machines?&nbsp;</p>



<p><strong>DANESHJOU: </strong>I think that&#8217;s a complex question. Let&#8217;s say you have the most perfect AI systems. I think there&#8217;s still a lot of nuance in how these, you know, things get done. I&#8217;m not a radiologist, so I don&#8217;t want to speak to what happens in radiology. But in dermatology, it ends up being quite complex, the process.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>DANESHJOU: </strong>You know, I don&#8217;t just look at lesions and make diagnoses. Like, I do skin exams to first identify the lesions of concern. So maybe if we had total-body photography that could help, like, catch which lesions would be of concern, which people have worked on, that would be step, sort of, step one.&nbsp;&nbsp;</p>



<p>And then the second thing is, you know, it&#8217;s … I have to do the biopsy. So, you know, the robot’s not going to be doing the biopsy. [LAUGHTER]&nbsp;&nbsp;</p>



<p>And then the pathology for skin cancer is sometimes very clear, but there&#8217;s also, like, intermediate-type lesions where we have to make a decision bringing all that information together. For rashes, it can be quite complex. And then we have to kind of think about what other tests we&#8217;re going to order, what therapeutics we might try first, that sort of stuff.&nbsp;&nbsp;</p>



<p>So, you know, there is a thought that you might have AI that could reason through all of those steps maybe, but I just don&#8217;t feel like we&#8217;re anywhere close to that at all. I think the other thing is AI does a lot better on sort of, you know, tasks that are well defined. And a lot of things in medicine, like, it would be hard to train the model on because it&#8217;s not well defined. Even human physicians would disagree on the next best step.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Well, Roxana, for whatever it&#8217;s worth, I can&#8217;t even begin to imagine anything replacing you. I think your work has been just so—I think you used the word, and I agree with it— <em>landmark</em>, and multiple times. So thank you for all that you&#8217;re doing and thank you so much for joining this podcast.&nbsp;&nbsp;</p>



<p><strong>DANESHJOU: </strong>Thanks for having me. This was very fun.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>The issue of bias in AI has been the subject of truly landmark work by Roxana and her collaborators. And this includes biases in large language models.&nbsp;&nbsp;</p>



<p>This was something that in our writing of the book, Carey, Zak, and I recognized and wrote about. But in fairness, I don&#8217;t think Carey, Zak, or I really understood the full implications of it. And this is where Roxana&#8217;s work has been so illuminating and important.&nbsp;</p>



<p>Roxana&#8217;s practical prescriptions around red teaming have proven to be important in practice, and equally important were Roxana&#8217;s insights into how AI might always be guilty of the same biases, not only of individuals but also of whole healthcare organizations. But at the same time, AI might also be a potentially powerful tool to detect and help mitigate against such biases.&nbsp;</p>



<p>When I think about the book that Carey, Zak, and I wrote, I think when we talked about laws, norms, ethics, regulations, it&#8217;s the place that we struggled the most. And in fact, we actually relied on a conversation with GPT-4 in order to tease out some of the core issues. Well, moving on from that conversation with an AI to a conversation with three deep experts who have dedicated their careers to making sure that we can harness all of the goodness while mitigating against the risks of AI, it&#8217;s been both fulfilling, very interesting, and a great learning experience.&nbsp;</p>



<p>[THEME MUSIC]  &nbsp;</p>



<p>I&#8217;d like to say thank you again to Laura, Vardit, and Roxana for sharing their stories and insights. And to our listeners, thank you for joining us. We have some really great conversations planned for the coming episodes, including an examination on the broader economic impact of AI in health and a discussion on AI drug discovery. We hope you&#8217;ll continue to tune in.&nbsp;&nbsp;</p>



<p>Until next time.&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-5"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--6"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/laws-norms-and-ethics-for-ai-in-health/">Laws, norms, and ethics for AI in health</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of April 21, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/</link>
		
		<dc:creator><![CDATA[Emre Kiciman, Robert Osazuwa Ness, Amit Sharma, Sean Rintel, Leon Reicherts, Lev Tankelevitch, Advait Sarkar, Pratik Ghosh, Richard Banks, Xiaodong Liu, Weiwei Yang, Hao Cheng, Michel Galley, Jianfeng Gao, Serina Chang, Jake Hofman, Hannes Gamper]]></dc:creator>
		<pubDate>Wed, 23 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136909</guid>

					<description><![CDATA[<p>In this issue: our CHI 2025 &#038; ICLR 2025 contributions, plus research on causal reasoning & LLMs; countering LLM jailbreak attacks; and how people use AI vs. AI-alone. Also, SVP of Microsoft Health Jim Weinstein talks rural healthcare innovation.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/">Research Focus: Week of April 21, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p>Catch a preview of our presentations and papers at CHI 2025 and ICLR 2025. We also introduce new research on causal reasoning and LLMs; enhancing LLM jailbreak capabilities to bolster safety and robustness; understanding how people using AI compared to AI-alone, and Distill-MOS, a compact and efficient model that delivers state-of-the-art speech quality assessment. You’ll also find a replay of a podcast discussion on rural healthcare innovation with Senior Vice President of Microsoft Health Jim Weinstein.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus: April 23, 2025" class="wp-image-1137194" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-01a8c2a8c7d1a47aed2f7d683288c862" id="conference">CONFERENCE</h2>



<h3 class="wp-block-heading h2" id="microsoft-at-chi-2025">Microsoft at CHI 2025</h3>



<p>Microsoft Research is proud to be a sponsor of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://chi2025.acm.org/" target="_blank" rel="noreferrer noopener">ACM Computer Human Interaction (CHI) 2025 Conference on Human Factors in Computing Systems<span class="sr-only"> (opens in new tab)</span></a>. CHI brings together researchers and practitioners from all over the world and from diverse cultures, backgrounds, and positionalities, who share an overarching goal to make the world a better place with interactive digital technologies.</p>



<p>Our researchers will host more than 30 sessions and workshops at this year&#8217;s conference in Yokohama, Japan. We invite you to <a href="https://www.microsoft.com/en-us/research/event/chi-2025/">preview our presentations</a> and our two dozen accepted papers.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--7"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/event/chi-2025/">Microsoft @CHI 2025</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9b4a4a2934ebb0889ed4f06335a18022" id="conference-1">CONFERENCE</h2>



<h3 class="wp-block-heading h2" id="where-s-the-title-for-this-one-1">Microsoft at ICLR 2025</h3>



<p>Microsoft is proud to be a sponsor of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iclr.cc/" target="_blank" rel="noreferrer noopener">the thirteenth International Conference on Learning Representations (ICLR)</a>. This gathering&nbsp;is dedicated to the advancement of representation learning, which is a branch of AI. We are pleased to share that Microsoft has <a href="https://www.microsoft.com/en-us/research/event/microsoft-at-iclr-2025/publications/">more than 30 accepted papers</a> at this year’s conference, which we invite you to preview.</p>



<p>ICLR is globally renowned for presenting and publishing&nbsp;cutting-edge research on all aspects of deep learning used in the fields of artificial intelligence, statistics and data science, as well as important application areas such as machine vision, computational biology, speech recognition, text understanding, gaming, and robotics.</p>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--8"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/event/microsoft-at-iclr-2025/">Microsoft @ICLR 2025</a></div>
</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality">Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1600" height="1025" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600.jpg" alt="Diagram illustrating the process of tackling real-world causal tasks. The diagram shows how individuals alternate between logical and covariance-based causal reasoning to formulate sub-questions, iterate, and verify their premises and implications. The strategic alternation between these two types of causality is highlighted as a key approach in addressing complex causal tasks. " class="wp-image-1136934" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-300x192.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-1024x656.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-768x492.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-1536x984.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-240x154.jpg 240w" sizes="auto, (max-width: 1600px) 100vw, 1600px" /></figure>



<p>What kinds of causal arguments can large language models (LLMs) generate, how valid are these arguments, and what causal reasoning workflows can this generation support or automate? This paper, which was selected for ICLR 2025, clarifies this debate. It advances our understanding of LLMs and their causal implications, and proposes a framework for future research at the intersection of LLMs and causality.</p>



<p>This discussion has critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--9"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</h3>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2.jpg" alt="A digital illustration of a person with a contemplative expression, resting their chin on their hand. The top of the person's head is open, revealing a white bird standing inside. The seagull is holding a worm in its beak, feeding the baby birds. The background is blue, and the words "TOOLS FOR THOUGHT" are written across the image in white letters." class="wp-image-1137202" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1920x1080.jpg 1920w" sizes="auto, (max-width: 2100px) 100vw, 2100px" /></figure>



<p>Can AI tools do more than streamline workflows—can they actually help us think better? That’s the driving question behind the Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/toolsforthought" target="_blank" rel="noreferrer noopener">Tools for Thought</a> initiative. At this year’s <a href="https://www.microsoft.com/en-us/research/event/chi-2025/" target="_blank" rel="noreferrer noopener">CHI</a> conference, this group is presenting four new research papers and cohosting a workshop that dives deep into this intersection of AI and human cognition.</p>



<p>The team provides an <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/" target="_blank" rel="noreferrer noopener">overview</a> of their latest research, starting with a study on how AI is changing the way people think and work. They introduce three prototype systems designed to support different cognitive tasks. Finally, through their <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/" target="_blank" rel="noreferrer noopener">Tools for Thought workshop</a>, they invite the CHI community to help define AI’s role in supporting human thinking.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-8 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--10"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">Read the blog</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="building-llms-with-enhanced-jailbreaking-capabilities-to-bolster-safety-and-robustness">Building LLMs with enhanced jailbreaking capabilities to bolster safety and robustness</h3>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1200" height="704" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1.jpg" alt="The overview of crafting ADV-LLM. The process begins with refining the target and initializing a starting suffix. ADV-LLM then iteratively generates data for self-tuning. " class="wp-image-1136936" style="width:652px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1.jpg 1200w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-300x176.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-1024x601.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-768x451.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-240x141.jpg 240w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></figure>



<p>Recent research shows that LLMs are vulnerable to automated jailbreak attacks, where algorithm-generated adversarial suffixes bypass safety alignment and trigger harmful responses. This paper introduces ADV-LLM, an iterative self-tuning process for crafting adversarial LLMs with enhanced jailbreak capabilities—which could provide valuable insights for future safety alignment research.</p>



<p>ADV-LLM is less computationally expensive than prior mechanisms and achieves higher attack success rates (ASR), especially against well-aligned models like Llama2 and Llama3.</p>



<p>It reaches nearly 100% ASR on various open-source LLMs and demonstrates strong transferability to closed-source models—achieving 99% ASR on GPT-3.5 and 49% ASR on GPT-4—despite being optimized solely on Llama3. Beyond improving jailbreak performance, ADV-LLM offers valuable insights for future alignment research by enabling large-scale generation of safety-relevant datasets.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-9 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--11"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/iterative-self-tuning-llms-for-enhanced-jailbreaking-capabilities/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="chatbench-from-static-benchmarks-to-human-ai-evaluation">ChatBench: From Static Benchmarks to Human-AI Evaluation</h3>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1200" height="1298" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow.jpg" alt="This figure displays the flow of the ChatBench user study. The rectangle on top represents Phase 1 of the study, where users answer questions on their own, and the rectangle on the bottom represents Phase 2 of the study, where users answer with AI." " class="wp-image-1136935" style="width:408px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow.jpg 1200w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-277x300.jpg 277w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-947x1024.jpg 947w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-768x831.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-166x180.jpg 166w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></figure>



<p>The rapid adoption of LLM-based chatbots raises the need to understand what people and LLMs can achieve together. However, standard benchmarks like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/MMLU" target="_blank" rel="noreferrer noopener">MMLU<span class="sr-only"> (opens in new tab)</span></a> assess LLM capabilities in isolation (i.e., “AI alone”). This paper presents the results of a user study that transforms MMLU questions into interactive user-AI conversations. The researchers seeded the participants with the question and then had them engage in a conversation with the LLM to arrive at an answer. The result is ChatBench, a new dataset comprising AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144,000 answers and 7,336 user-AI conversations.</p>



<p>The researchers’ analysis reveals that AI-alone accuracy does not predict user-AI accuracy, with notable differences across subjects such as math, physics, and moral reasoning. Examining user-AI conversations yields insights into how these interactions differ from AI-alone benchmarks. Finally, the researchers demonstrate that finetuning a user simulator on a subset of ChatBench improves its ability to predict user-AI accuracy, boosting correlation on held-out questions by more than 20 points, thereby enabling scalable interactive evaluation.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-10 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--12"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/chatbench-from-static-benchmarks-to-human-ai-evaluation/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8e4c7d6bee6a5b67f371be947a1df4a4" id="podcast">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Distill-MOS: A compact speech-quality assessment model&nbsp;</h2>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="8816" height="3152" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos.png" alt="Block diagram illustrating XLS-R-based speech quality assessment and its usage as a teacher model for distillation using unlabeled speech." class="wp-image-1137168" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos.png 8816w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-1024x366.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-768x275.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-1536x549.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-2048x732.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-240x86.png 240w" sizes="auto, (max-width: 8816px) 100vw, 8816px" /></figure>



<p>Distill-MOS is a compact and efficient speech quality assessment model with dramatically reduced size—over 100x smaller than the reference model—enabling efficient, non-intrusive evaluation in real-world, low-resource settings.&nbsp;</p>



<p>This paper investigates the distillation and pruning methods to reduce model size for non-intrusive speech quality assessment based on self-supervised representations. The researchers’ experiments build on XLS-R-SQA, a speech quality assessment model using wav2vec 2.0 XLS-R embeddings. They retrain this model on a large compilation of mean opinion score datasets, encompassing over 100,000 labeled clips.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-11 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--13"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/distill-mos-a-compact-speech-quality-assessment-model/">Read the paper</a></div>



<div class="wp-block-button is-style-outline is-style-outline--14"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/Distill-MOS">View GitHub</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Collaborating to Affect Change for Rural Health Care with Innovation and Technology</h2>



<p>Senior Vice President of Microsoft Health <a href="https://www.microsoft.com/en-us/research/people/jweinst/">Jim Weinstein</a> joins Dan Liljenquist, Chief Strategy Officer from Intermountain Health, on the NEJM Catalyst podcast for a discussion of their combined expertise and resources and their collaboration to address healthcare challenges in the rural United States. These challenges include limited access to care, rising mortality rates, and severe staffing shortages. Working together, they aim to create a scalable model that can benefit both rural and urban health care systems. Key goals include expanding access through telemedicine and increasing cybersecurity, ultimately improving the quality of care delivered and financial stability for rural communities.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-12 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--15"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://catalyst.nejm.org/doi/full/10.1056/CAT.25.0133">Listen to the podcast</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Empowering patients and healthcare consumers in the age of generative AI</h2>



<p>Two champions of patient-centered digital health join Microsoft Research President <a href="https://www.microsoft.com/en-us/research/people/petelee/">Peter Lee</a> to talk about how AI is reshaping healthcare in terms of patient empowerment and emerging digital health business models. Dave deBronkart, a cancer survivor and longtime advocate for patient empowerment, discusses how AI tools like ChatGPT can help patients better understand their conditions, navigate the healthcare system, and communicate more effectively with clinicians. Christina Farr, a healthcare investor and former journalist, talks about the evolving digital health–startup ecosystem, highlighting where AI is having the most meaningful impact—particularly in women’s health, pediatrics, and elder care. She also explores consumer trends, like the rise of cash-pay healthcare. </p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-13 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--16"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/">Listen to the podcast</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Beyond the Image: AI’s Expanding Role in Healthcare</h2>



<p><a href="https://www.microsoft.com/en-us/research/people/carlson/">Jonathan Carlson</a>, Managing Director of Microsoft Research Health Futures, joins the Healthcare Unfiltered show to explore the evolution of AI in medicine, from the early days to cutting-edge innovations like ambient clinical intelligence. This podcast explores how pre-trained models and machine learning are transforming care delivery, as well as the future of biomedicine and healthcare, including important ethical and practical questions.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-14 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--17"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://youtu.be/zU4_o1BXzps?si=qKa09M21L5Gf-Bvq">Listen to the podcast</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/">Research Focus: Week of April 21, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/</link>
		
		<dc:creator><![CDATA[Sean Rintel, Leon Reicherts, Lev Tankelevitch, Advait Sarkar, Pratik Ghosh, Richard Banks]]></dc:creator>
		<pubDate>Fri, 18 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136647</guid>

					<description><![CDATA[<p>Join us at CHI 2025 to explore how AI systems can be used as Tools for Thought as we reimage AI’s role in human thinking. Learn about new research, prototypes, and a workshop on designing AI that supports critical thinking, decision-making, and creativity.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1.jpg" alt="A digital illustration of a person with a contemplative expression, resting their chin on their hand. The top of the person's head is open, revealing a white bird standing inside. The seagull is holding a worm in its beak, feeding the baby birds. The background is blue, and the words "TOOLS FOR THOUGHT" are written across the image in white letters." class="wp-image-1136653" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1920x1080.jpg 1920w" sizes="auto, (max-width: 2100px) 100vw, 2100px" /></figure>



<p>Can AI tools do more than streamline workflows—can they actually help us think better? That’s the driving question behind the Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/toolsforthought" target="_blank" rel="noreferrer noopener">Tools for Thought</a> initiative. At this year’s <a href="https://www.microsoft.com/en-us/research/event/chi-2025/">CHI</a> conference, we’re presenting four new research papers and cohosting a workshop that dives deep into this intersection of AI and human cognition.</p>



<p>This post provides an overview of our latest research, starting with a study on how AI is changing the way we think and work. We also introduce three prototype systems designed to support different cognitive tasks. Finally, through our <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/">Tools for Thought workshop</a>, we’re inviting the CHI community to help define AI’s role in supporting human thinking.</p>



<h2 class="wp-block-heading" id="ai-s-effects-on-thinking-at-work">AI’s effects on thinking at work</h2>



<p>With a single prompt, AI can generate a wide range of outputs, from documents and meeting agendas to answers and automated workflows. But how are people’s thinking processes affected when they delegate these tasks to AI?</p>



<p>One of our goals is to understand how knowledge workers use AI, how they perceive its value, and how it affects cognitive effort.</p>



<p>Our study, “<a href="https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/">The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers</a>,” surveyed 319 professionals using AI across a variety of occupations. Participants shared 936 real-world AI use cases and reflected on how it influenced their critical thinking and mental effort. We summarize these findings below.</p>



<p><strong>Defining and deploying critical thinking.</strong> Knowledge workers describe critical thinking as involving activities like setting clear goals, refining prompts, and verifying AI outputs against external sources and their own expertise. They rely on these practices to maintain work quality when using AI—motivated by the need to avoid errors, produce better results, and develop their skills.</p>



<h3 class="wp-block-heading" id="findings">Findings</h3>



<p><strong>Balancing cognitive effort.</strong> Participants’ reports about critical thinking and the effort involved align with longstanding human tendencies to manage cognitive load at work. For high-stakes tasks requiring accuracy, they say they expend more effort in applying critical thinking with AI than they would performing the same tasks without it. In contrast, during routine, for low-stakes tasks under time pressure, they report spending less effort on critical thinking when using AI compared to completing tasks without it. </p>



<p><strong>Confidence effects.</strong> The study found that higher confidence in AI was associated with less<strong> </strong>critical thinking, while higher self-confidence in one&#8217;s own abilities was associated with more critical thinking—though at a perceived higher cognitive cost. This suggests a delicate balance between using AI for efficiency and maintaining active critical engagement.&nbsp;</p>



<p><strong>Shift in the nature of critical thinking.</strong> Participants reported a shift in critical thinking activities, with a greater focus on information verification, response integration, and task stewardship. While AI automates certain aspects of knowledge work, it also demands more effort in evaluating the accuracy and relevance of AI-generated content.&nbsp;</p>



<p><strong>Barriers to critical engagement.</strong> The study identified several barriers that inhibit critical thinking when using AI. These include a lack of awareness of the need for critical evaluation, limited motivation due to time pressure or perceived job scope, and difficulty in refining prompts—especially in unfamiliar domains.</p>



<h3 class="wp-block-heading" id="recommendations">Recommendations</h3>



<p>To foster critical thinking at work, we recommend that AI tools actively encourage awareness, motivation, and skill development.</p>



<p><strong>AI tools should enhance motivators for critical thinking</strong> (e.g., quality standards, skill-building) and mitigate inhibitors (e.g., time constraints, low awareness). Proactive prompts can surface overlooked tasks, while reactive features can offer on-demand assistance. Motivation can be strengthened by positioning critical reflection as part of professional growth—not just extra work.</p>



<p><strong>AI tools should also support knowledge workers’ ability to think critically</strong> by providing reasoning explanations (as some newer AI models now do), guided critiques, and cross-references. This shift must occur in both the design of the technology and in the mindsets of knowledge workers. Rather than treating AI as a tool for delivering answers, we suggest treating it as a thought partner—one that can also act as a provocateur.</p>



<p>Beyond these insights, our other CHI papers explore practical ways to design AI that augments human cognition.</p>



<h2 class="wp-block-heading" id="enhancing-decision-making-with-ai">Enhancing decision-making with AI</h2>



<p>Decision-making is central to knowledge work, and AI is increasingly being used to help people make decisions in complex fields like healthcare and finance. However, how much agency do knowledge workers retain when AI is involved?</p>



<p>Our study, “<a href="https://www.microsoft.com/en-us/research/publication/ai-help-me-think-but-for-myself-assisting-people-in-complex-decision-making-by-providing-different-kinds-of-cognitive-support/">AI, Help Me Think—but for Myself: Exploring How LLMs Can Assist People in Complex Decision-Making by Providing Different Forms of Cognitive Support</a>,” conducted in collaboration with University College London, examines this question. We began with a small formative study involving 10 participants, followed by a comparative study with 21 participants using two different AI-supported decision-making systems.</p>



<p>For a complex financial investment task, we compared two different AI tools (Figure 1): <strong>RecommendAI</strong>, which provides AI-generated recommendations, and <strong>ExtendAI</strong>, which encourages users to articulate their reasoning before receiving AI feedback.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="210" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1.png" alt="Figure 1. The figure consists of two horizontal sections, each depicting a different AI interaction model. The top section shows "RecommendAI," where an AI makes a suggestion for action, which is then interpreted by the user to make a final decision. The bottom section shows "ExtendAI," where the user makes a plan for action, and the AI extends this plan by embedding feedback before the user makes sense of it and makes a final decision. An arrow from "makes sense of plan containing AI's feedback" in ExtendAI loops back to "extends user's plan by embedding feedback." Brief description: The image illustrates two models of human-AI collaboration: one where the AI recommends actions and another where it enhances user-generated plans with feedback. This comparison highlights different approaches to integrating AI into decision-making processes." class="wp-image-1136655" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1-300x105.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1-240x84.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 1. Illustrative comparison of the thought process involved when interacting with two types of AI: RecommendAI and ExtendAI.</figcaption></figure>



<h3 class="wp-block-heading" id="findings-1">Findings</h3>



<p>Both systems were found to offer benefits for augmenting cognition and addressing some of the challenges to critical thinking identified in the knowledge worker survey above, suggesting the potential for a balanced approach.&nbsp;</p>



<p>RecommendAI offered concrete suggestions that inspired users to explore new directions in their decision-making. This often led to fresh insights and reflections. However, the recommendations at times felt disconnected from the user&#8217;s own reasoning, reducing the depth of engagement.&nbsp;</p>



<p>In contrast, ExtendAI encouraged users to reflect more deeply on their decisions by providing feedback on their reasoning. This helped them examine their thought processes and consider alternative perspectives. However, some users found the feedback too general and not actionable enough.&nbsp;</p>



<p>When it came to how users integrated the tools into their decision-making process, RecommendAI, introduced perspectives that pushed users to think beyond their usual patterns. By recommending options not based on users’ own reasoning, it encouraged exploration of ideas they might not have considered. However, some users perceived the recommendations as a &#8220;black box&#8221; solution. This lack of transparency made those recommendations harder to understand, trust, and apply to their own thought processes.&nbsp;</p>



<p>ExtendAI, on the other hand, aligned with users’ existing reasoning, making its feedback easier to incorporate. This helped the users maintain a sense of control and continuity. However, because the feedback often echoed their initial thoughts, it sometimes limited new insights and risked reinforcing existing biases.</p>



<p>These findings suggest that AI tools like ExtendAI, designed to elicit and build on users&#8217; own cognitive processes, may offer a more effective approach to augmentation than simply providing “ready-made solutions” that users must figure out how to interpret and apply.</p>



<h2 class="wp-block-heading" id="are-we-on-track-making-meetings-better-with-ai">Are we on track? Making meetings better with AI</h2>



<p>Meetings are often criticized for being ineffective. While this is sometimes due to poor practices—such as weak agendas, late starts, and unclear facilitation—we believe the deeper issue is a lack of <em>meeting intentionality</em>: knowing why a meeting is occurring and keeping the discussion focused on that purpose. A key challenge is maintaining goal clarity throughout a meeting.</p>



<p>In the paper “<a href="https://www.microsoft.com/en-us/research/publication/are-we-on-track-ai-assisted-active-and-passive-goal-reflection-during-meetings/">Are We On Track? AI-Assisted Goal Reflection During Meetings</a>,” we explore how AI tools can improve meetings in real time by encouraging <em>reflection</em>—awareness about the meeting’s goals and how well the current conversation is aligned with those goals.</p>



<p>Our study with 15 knowledge workers examined two AI-driven design paradigms: <strong>passive goal assistance</strong> through ambient visualization (a live chart displaying how conversational topics relate to meeting objectives) and <strong>active goal assistance</strong> through interactive questioning (nudging participants to consider whether the current conversation aligns with the meeting objectives). These approaches are illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="136" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2.png" alt="Figure 2. A figure illustrating two methods of AI interpretation and engagement in a virtual meeting setting. On the left, a graph with "Extent of AI Interpretation" on the y-axis and "Engagement Level" on the x-axis shows two points: "Ambient Visualization" (high AI interpretation, low engagement) and "Interactive Questioning" (high AI interpretation, high engagement). The middle image labeled "Ambient Visualization" shows a virtual meeting with participants' faces blurred and an overlay of data visualizations. The right image labeled "Interactive Questioning" shows a virtual meeting with participants' faces blurred and an overlay of interactive questions. Brief description: This image compares two methods of integrating AI into virtual meetings: Ambient Visualization and Interactive Questioning. It is relevant as it highlights different levels of user engagement and AI interpretation in enhancing virtual communication." class="wp-image-1136656" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2-300x68.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2-240x54.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 2. Technology prototypes exploring passive and active ways to keep meetings focused on established objectives.</figcaption></figure>



<h3 class="wp-block-heading" id="recommendations-1">Recommendations</h3>



<p>The findings highlight AI’s potential to help teams with meeting objectives. We found three key design tradeoffs between passive and active support. Based on these, we offer the following AI design recommendations.</p>



<p><strong>Information balance.</strong> There is a tradeoff between ambient visualizations in the passive approach—which can risk information overload—and interactive questioning in the active approach, which may lack detail. To be effective, AI should deliver the right amount of information at the right time and tailor content to the individuals who need it most—without overwhelming users, while offering meaningful and timely support for reflection.</p>



<p><strong>Balance of engagement versus interruption.</strong> When participants are deeply engaged in discussion, significant interruptions can overwhelm and disrupt the flow. Conversely, during moments of confusion or misalignment, subtle cues may be insufficient to get the team back on track. AI systems should dynamically adjust their level of intervention—from ambient and lightweight to more direct—escalating or de-escalating based on timing thresholds, which can be customized for each team.</p>



<p><strong>Balance of team versus individual goal awareness.</strong> AI assistance can nudge team action, such as adjusting agendas. These effects were stronger with the active approach, which required group responses, while the passive approach supported individual thinking without directly influencing team behavior. Team-wide engagement depends on both the visibility of AI cues and how they are introduced into the discussion.</p>



<p>This study helps us understand how AI design choices can support intentionality during meetings and enhance productivity without disrupting natural workflows.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085514">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: blog post</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" aria-label="GraphRAG auto-tuning provides rapid adaptation to new domains" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/09/GraphRag-3-BlogHeroFeature-1400x788-1.png" alt="GraphRAG image on blue to green gradient" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">GraphRAG auto-tuning provides rapid adaptation to new domains</h2>
				
								<p class="large">GraphRAG uses LLM-generated knowledge graphs to substantially improve complex Q&A over retrieval-augmented generation (RAG). Discover automatic tuning of GraphRAG for new datasets, making it more accurate and relevant.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="encouraging-diverse-problem-solving-brainstorming-with-ai">Encouraging diverse problem-solving brainstorming with AI</h2>



<p>Diverse perspectives drive creative problem-solving in organizations, but individuals often lack access to varied viewpoints. In the paper “<a href="https://www.microsoft.com/en-us/research/publication/yes-and-a-generative-ai-multi-agent-framework-for-enhancing-diversity-of-thought-in-individual-ideation-for-problem-solving-through-confidence-based-agent-turn-taking/">YES AND: An AI-Powered Problem-Solving Framework for Diversity of Thought</a>,” we build on the idea of “design improv” to explore a multi-agent AI prototype that simulates conversations with persona-based agents representing a range of expertise.</p>



<p>The agents follow a classic model of conversational turn-taking, combined with a confidence model to determine when to take or respond to a turn. This allows both the agents and the user to organically build on each others&#8217; ideas and ask clarifying questions. The system enables free-flowing, multi-party idea generation while avoiding common pitfalls of group brainstorming—such as social loafing, production blocking, and groupthink (Figure 3).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="800" height="439" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3.png" alt="Figure 3. The image is a flowchart and conversation transcript for agent-based ideation. The flowchart on the left shows four steps: "Define a problem," "Move a conversation forward," "Guide the discussion," and "Ask Sage to report." The conversation on the right involves a designer agent, ML researcher agent, and the user discussing audience targeting, ethical implications, and potential solutions for ideating around the ethical implications of gamifying fitness. Brief Description: The image is a flowchart and conversation transcript for agent-based ideation. It includes both a flowchart outlining the process from identifying the initial problem to final report, as well as a conversation transcript where various stakeholders discuss ethical considerations, audience targeting, and potential solutions." class="wp-image-1136652" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3.png 800w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-300x165.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-768x421.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-240x132.png 240w" sizes="auto, (max-width: 800px) 100vw, 800px" /><figcaption class="wp-element-caption">Figure 3. The YES AND system supports conversational turn-taking among agents and the user to generate ideas around a problem.</figcaption></figure>



<p>At the end of a session, an AI agent called Sage distills the discussion, leaving it to the user to develop a conclusive approach to the problem. In this way, YES AND helps unblock forward momentum in problem-solving while preserving the agency of knowledge workers to shape their own ideas.</p>



<h2 class="wp-block-heading" id="next-steps-expanding-the-tools-for-thought-community">Next steps: Expanding the Tools for Thought community</h2>



<p>We believe the best way to advance next-generation tools for thought is by bringing together a wide range of perspectives and approaches. In addition to our four papers, we are also conducting a workshop at CHI on April 26, co-organized with collaborators from industry and academia: <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/" target="_blank" rel="noreferrer noopener">Tools for Thought: Research and Design for Understanding, Protecting, and Augmenting Human Cognition with Generative AI</a>.<strong>&nbsp;</strong>&nbsp;</p>



<p>In this session, over 60 researchers, designers, practitioners, and provocateurs will gather to examine what it means to understand and shape the impact of AI on human cognition. Together, we’ll explore how AI is changing workflows, the opportunities and challenges for design, and which theories, perspectives, and methods are increasingly relevant—or still need to be developed.&nbsp;</p>



<p>The enthusiastic response to this workshop highlights the growing interest in AI’s role in human thought. Our goal is to foster a multidisciplinary community dedicated to ensuring that AI not only accelerates work but also strengthens our ability to think critically, creatively, and strategically.<strong>&nbsp;</strong></p>



<p>We look forward to ongoing discussions, new collaborations, and the next wave of innovations in AI-assisted cognition at <a href="https://www.microsoft.com/en-us/research/event/chi-2025/" target="_blank" rel="noreferrer noopener">CHI 2025</a>.&nbsp;&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
