<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Thu, 24 Apr 2025 21:25:35 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.7.2</generator>
	<item>
		<title>Research Focus: Week of April 21, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/</link>
		
		<dc:creator><![CDATA[Emre Kiciman, Robert Osazuwa Ness, Amit Sharma, Sean Rintel, Leon Reicherts, Lev Tankelevitch, Advait Sarkar, Pratik Ghosh, Richard Banks, Xiaodong Liu, Weiwei Yang, Hao Cheng, Michel Galley, Jianfeng Gao, Serina Chang, Jake Hofman, Hannes Gamper]]></dc:creator>
		<pubDate>Wed, 23 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136909</guid>

					<description><![CDATA[<p>In this issue: our CHI 2025 &#038; ICLR 2025 contributions, plus research on causal reasoning & LLMs; countering LLM jailbreak attacks; and how people use AI vs. AI-alone. Also, SVP of Microsoft Health Jim Weinstein talks rural healthcare innovation.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/">Research Focus: Week of April 21, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p>Catch a preview of our presentations and papers at CHI 2025 and ICLR 2025. We also introduce new research on causal reasoning and LLMs; enhancing LLM jailbreak capabilities to bolster safety and robustness; understanding how people using AI compared to AI-alone, and Distill-MOS, a compact and efficient model that delivers state-of-the-art speech quality assessment. You’ll also find a replay of a podcast discussion on rural healthcare innovation with Senior Vice President of Microsoft Health Jim Weinstein.</p>



<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus: April 23, 2025" class="wp-image-1137194" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF60-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1401px) 100vw, 1401px" /></figure>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-01a8c2a8c7d1a47aed2f7d683288c862" id="conference">CONFERENCE</h2>



<h3 class="wp-block-heading h2" id="microsoft-at-chi-2025">Microsoft at CHI 2025</h3>



<p>Microsoft Research is proud to be a sponsor of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://chi2025.acm.org/" target="_blank" rel="noreferrer noopener">ACM Computer Human Interaction (CHI) 2025 Conference on Human Factors in Computing Systems<span class="sr-only"> (opens in new tab)</span></a>. CHI brings together researchers and practitioners from all over the world and from diverse cultures, backgrounds, and positionalities, who share an overarching goal to make the world a better place with interactive digital technologies.</p>



<p>Our researchers will host more than 30 sessions and workshops at this year&#8217;s conference in Yokohama, Japan. We invite you to <a href="https://www.microsoft.com/en-us/research/event/chi-2025/">preview our presentations</a> and our two dozen accepted papers.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-1 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--1"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/event/chi-2025/">Microsoft @CHI 2025</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9b4a4a2934ebb0889ed4f06335a18022" id="conference-1">CONFERENCE</h2>



<h3 class="wp-block-heading h2" id="where-s-the-title-for-this-one-1">Microsoft at ICLR 2025</h3>



<p>Microsoft is proud to be a sponsor of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://iclr.cc/" target="_blank" rel="noreferrer noopener">the thirteenth International Conference on Learning Representations (ICLR)</a>. This gathering&nbsp;is dedicated to the advancement of representation learning, which is a branch of AI. We are pleased to share that Microsoft has <a href="https://www.microsoft.com/en-us/research/event/microsoft-at-iclr-2025/publications/">more than 30 accepted papers</a> at this year’s conference, which we invite you to preview.</p>



<p>ICLR is globally renowned for presenting and publishing&nbsp;cutting-edge research on all aspects of deep learning used in the fields of artificial intelligence, statistics and data science, as well as important application areas such as machine vision, computational biology, speech recognition, text understanding, gaming, and robotics.</p>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-2 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--2"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/event/microsoft-at-iclr-2025/">Microsoft @ICLR 2025</a></div>
</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality">Causal Reasoning and Large Language Models: Opening a New Frontier for Causality</h3>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="1600" height="1025" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600.jpg" alt="Diagram illustrating the process of tackling real-world causal tasks. The diagram shows how individuals alternate between logical and covariance-based causal reasoning to formulate sub-questions, iterate, and verify their premises and implications. The strategic alternation between these two types of causality is highlighted as a key approach in addressing complex causal tasks. " class="wp-image-1136934" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600.jpg 1600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-300x192.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-1024x656.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-768x492.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-1536x984.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/causal-frontiers_1600-240x154.jpg 240w" sizes="(max-width: 1600px) 100vw, 1600px" /></figure>



<p>What kinds of causal arguments can large language models (LLMs) generate, how valid are these arguments, and what causal reasoning workflows can this generation support or automate? This paper, which was selected for ICLR 2025, clarifies this debate. It advances our understanding of LLMs and their causal implications, and proposes a framework for future research at the intersection of LLMs and causality.</p>



<p>This discussion has critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-3 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--3"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="causal-reasoning-and-large-language-models-opening-a-new-frontier-for-causality">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</h3>



<figure class="wp-block-image size-full"><img decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2.jpg" alt="A digital illustration of a person with a contemplative expression, resting their chin on their hand. The top of the person's head is open, revealing a white bird standing inside. The seagull is holding a worm in its beak, feeding the baby birds. The background is blue, and the words "TOOLS FOR THOUGHT" are written across the image in white letters." class="wp-image-1137202" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-2-1920x1080.jpg 1920w" sizes="(max-width: 2100px) 100vw, 2100px" /></figure>



<p>Can AI tools do more than streamline workflows—can they actually help us think better? That’s the driving question behind the Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/toolsforthought" target="_blank" rel="noreferrer noopener">Tools for Thought</a> initiative. At this year’s <a href="https://www.microsoft.com/en-us/research/event/chi-2025/" target="_blank" rel="noreferrer noopener">CHI</a> conference, this group is presenting four new research papers and cohosting a workshop that dives deep into this intersection of AI and human cognition.</p>



<p>The team provides an <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/" target="_blank" rel="noreferrer noopener">overview</a> of their latest research, starting with a study on how AI is changing the way people think and work. They introduce three prototype systems designed to support different cognitive tasks. Finally, through their <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/" target="_blank" rel="noreferrer noopener">Tools for Thought workshop</a>, they invite the CHI community to help define AI’s role in supporting human thinking.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-4 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--4"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">Read the blog</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="building-llms-with-enhanced-jailbreaking-capabilities-to-bolster-safety-and-robustness">Building LLMs with enhanced jailbreaking capabilities to bolster safety and robustness</h3>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1200" height="704" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1.jpg" alt="The overview of crafting ADV-LLM. The process begins with refining the target and initializing a starting suffix. ADV-LLM then iteratively generates data for self-tuning. " class="wp-image-1136936" style="width:652px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1.jpg 1200w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-300x176.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-1024x601.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-768x451.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SelfTuningLLM_FIG1-240x141.jpg 240w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></figure>



<p>Recent research shows that LLMs are vulnerable to automated jailbreak attacks, where algorithm-generated adversarial suffixes bypass safety alignment and trigger harmful responses. This paper introduces ADV-LLM, an iterative self-tuning process for crafting adversarial LLMs with enhanced jailbreak capabilities—which could provide valuable insights for future safety alignment research.</p>



<p>ADV-LLM is less computationally expensive than prior mechanisms and achieves higher attack success rates (ASR), especially against well-aligned models like Llama2 and Llama3.</p>



<p>It reaches nearly 100% ASR on various open-source LLMs and demonstrates strong transferability to closed-source models—achieving 99% ASR on GPT-3.5 and 49% ASR on GPT-4—despite being optimized solely on Llama3. Beyond improving jailbreak performance, ADV-LLM offers valuable insights for future alignment research by enabling large-scale generation of safety-relevant datasets.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-5 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--5"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/iterative-self-tuning-llms-for-enhanced-jailbreaking-capabilities/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="chatbench-from-static-benchmarks-to-human-ai-evaluation">ChatBench: From Static Benchmarks to Human-AI Evaluation</h3>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="1200" height="1298" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow.jpg" alt="This figure displays the flow of the ChatBench user study. The rectangle on top represents Phase 1 of the study, where users answer questions on their own, and the rectangle on the bottom represents Phase 2 of the study, where users answer with AI." " class="wp-image-1136935" style="width:408px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow.jpg 1200w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-277x300.jpg 277w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-947x1024.jpg 947w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-768x831.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/chatbench_user_study_flow-166x180.jpg 166w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></figure>



<p>The rapid adoption of LLM-based chatbots raises the need to understand what people and LLMs can achieve together. However, standard benchmarks like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/MMLU" target="_blank" rel="noreferrer noopener">MMLU<span class="sr-only"> (opens in new tab)</span></a> assess LLM capabilities in isolation (i.e., “AI alone”). This paper presents the results of a user study that transforms MMLU questions into interactive user-AI conversations. The researchers seeded the participants with the question and then had them engage in a conversation with the LLM to arrive at an answer. The result is ChatBench, a new dataset comprising AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144,000 answers and 7,336 user-AI conversations.</p>



<p>The researchers’ analysis reveals that AI-alone accuracy does not predict user-AI accuracy, with notable differences across subjects such as math, physics, and moral reasoning. Examining user-AI conversations yields insights into how these interactions differ from AI-alone benchmarks. Finally, the researchers demonstrate that finetuning a user simulator on a subset of ChatBench improves its ability to predict user-AI accuracy, boosting correlation on held-out questions by more than 20 points, thereby enabling scalable interactive evaluation.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-6 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--6"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/chatbench-from-static-benchmarks-to-human-ai-evaluation/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8e4c7d6bee6a5b67f371be947a1df4a4" id="podcast">NEW RESEARCH</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Distill-MOS: A compact speech-quality assessment model&nbsp;</h2>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="8816" height="3152" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos.png" alt="Block diagram illustrating XLS-R-based speech quality assessment and its usage as a teacher model for distillation using unlabeled speech." class="wp-image-1137168" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos.png 8816w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-300x107.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-1024x366.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-768x275.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-1536x549.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-2048x732.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/distillmos-240x86.png 240w" sizes="auto, (max-width: 8816px) 100vw, 8816px" /></figure>



<p>Distill-MOS is a compact and efficient speech quality assessment model with dramatically reduced size—over 100x smaller than the reference model—enabling efficient, non-intrusive evaluation in real-world, low-resource settings.&nbsp;</p>



<p>This paper investigates the distillation and pruning methods to reduce model size for non-intrusive speech quality assessment based on self-supervised representations. The researchers’ experiments build on XLS-R-SQA, a speech quality assessment model using wav2vec 2.0 XLS-R embeddings. They retrain this model on a large compilation of mean opinion score datasets, encompassing over 100,000 labeled clips.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-7 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--7"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/distill-mos-a-compact-speech-quality-assessment-model/">Read the paper</a></div>



<div class="wp-block-button is-style-outline is-style-outline--8"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/Distill-MOS">View GitHub</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Collaborating to Affect Change for Rural Health Care with Innovation and Technology</h2>



<p>Senior Vice President of Microsoft Health <a href="https://www.microsoft.com/en-us/research/people/jweinst/">Jim Weinstein</a> joins Dan Liljenquist, Chief Strategy Officer from Intermountain Health, on the NEJM Catalyst podcast for a discussion of their combined expertise and resources and their collaboration to address healthcare challenges in the rural United States. These challenges include limited access to care, rising mortality rates, and severe staffing shortages. Working together, they aim to create a scalable model that can benefit both rural and urban health care systems. Key goals include expanding access through telemedicine and increasing cybersecurity, ultimately improving the quality of care delivered and financial stability for rural communities.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-8 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--9"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://catalyst.nejm.org/doi/full/10.1056/CAT.25.0133">Listen to the podcast</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Empowering patients and healthcare consumers in the age of generative AI</h2>



<p>Two champions of patient-centered digital health join Microsoft Research President <a href="https://www.microsoft.com/en-us/research/people/petelee/">Peter Lee</a> to talk about how AI is reshaping healthcare in terms of patient empowerment and emerging digital health business models. Dave deBronkart, a cancer survivor and longtime advocate for patient empowerment, discusses how AI tools like ChatGPT can help patients better understand their conditions, navigate the healthcare system, and communicate more effectively with clinicians. Christina Farr, a healthcare investor and former journalist, talks about the evolving digital health–startup ecosystem, highlighting where AI is having the most meaningful impact—particularly in women’s health, pediatrics, and elder care. She also explores consumer trends, like the rise of cash-pay healthcare. </p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-9 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--10"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/">Listen to the podcast</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h2 class="wp-block-heading" id="collaborating-to-affect-change-for-rural-health-care-with-innovation-and-technology">Beyond the Image: AI’s Expanding Role in Healthcare</h2>



<p><a href="https://www.microsoft.com/en-us/research/people/carlson/">Jonathan Carlson</a>, Managing Director of Microsoft Research Health Futures, joins the Healthcare Unfiltered show to explore the evolution of AI in medicine, from the early days to cutting-edge innovations like ambient clinical intelligence. This podcast explores how pre-trained models and machine learning are transforming care delivery, as well as the future of biomedicine and healthcare, including important ethical and practical questions.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-10 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--11"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://youtu.be/zU4_o1BXzps?si=qKa09M21L5Gf-Bvq">Listen to the podcast</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-21-2025/">Research Focus: Week of April 21, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/</link>
		
		<dc:creator><![CDATA[Sean Rintel, Leon Reicherts, Lev Tankelevitch, Advait Sarkar, Pratik Ghosh, Richard Banks]]></dc:creator>
		<pubDate>Fri, 18 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136647</guid>

					<description><![CDATA[<p>Join us at CHI 2025 to explore how AI systems can be used as Tools for Thought as we reimage AI’s role in human thinking. Learn about new research, prototypes, and a workshop on designing AI that supports critical thinking, decision-making, and creativity.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1.jpg" alt="A digital illustration of a person with a contemplative expression, resting their chin on their hand. The top of the person's head is open, revealing a white bird standing inside. The seagull is holding a worm in its beak, feeding the baby birds. The background is blue, and the words "TOOLS FOR THOUGHT" are written across the image in white letters." class="wp-image-1136653" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Thought-at-Microsoft-at-CHI-2025-BlogHeroFeature-1400x788-1-1920x1080.jpg 1920w" sizes="auto, (max-width: 2100px) 100vw, 2100px" /></figure>



<p>Can AI tools do more than streamline workflows—can they actually help us think better? That’s the driving question behind the Microsoft Research <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://aka.ms/toolsforthought" target="_blank" rel="noreferrer noopener">Tools for Thought</a> initiative. At this year’s <a href="https://www.microsoft.com/en-us/research/event/chi-2025/">CHI</a> conference, we’re presenting four new research papers and cohosting a workshop that dives deep into this intersection of AI and human cognition.</p>



<p>This post provides an overview of our latest research, starting with a study on how AI is changing the way we think and work. We also introduce three prototype systems designed to support different cognitive tasks. Finally, through our <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/">Tools for Thought workshop</a>, we’re inviting the CHI community to help define AI’s role in supporting human thinking.</p>



<h2 class="wp-block-heading" id="ai-s-effects-on-thinking-at-work">AI’s effects on thinking at work</h2>



<p>With a single prompt, AI can generate a wide range of outputs, from documents and meeting agendas to answers and automated workflows. But how are people’s thinking processes affected when they delegate these tasks to AI?</p>



<p>One of our goals is to understand how knowledge workers use AI, how they perceive its value, and how it affects cognitive effort.</p>



<p>Our study, “<a href="https://www.microsoft.com/en-us/research/publication/the-impact-of-generative-ai-on-critical-thinking-self-reported-reductions-in-cognitive-effort-and-confidence-effects-from-a-survey-of-knowledge-workers/">The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers</a>,” surveyed 319 professionals using AI across a variety of occupations. Participants shared 936 real-world AI use cases and reflected on how it influenced their critical thinking and mental effort. We summarize these findings below.</p>



<p><strong>Defining and deploying critical thinking.</strong> Knowledge workers describe critical thinking as involving activities like setting clear goals, refining prompts, and verifying AI outputs against external sources and their own expertise. They rely on these practices to maintain work quality when using AI—motivated by the need to avoid errors, produce better results, and develop their skills.</p>



<h3 class="wp-block-heading" id="findings">Findings</h3>



<p><strong>Balancing cognitive effort.</strong> Participants’ reports about critical thinking and the effort involved align with longstanding human tendencies to manage cognitive load at work. For high-stakes tasks requiring accuracy, they say they expend more effort in applying critical thinking with AI than they would performing the same tasks without it. In contrast, during routine, for low-stakes tasks under time pressure, they report spending less effort on critical thinking when using AI compared to completing tasks without it. </p>



<p><strong>Confidence effects.</strong> The study found that higher confidence in AI was associated with less<strong> </strong>critical thinking, while higher self-confidence in one&#8217;s own abilities was associated with more critical thinking—though at a perceived higher cognitive cost. This suggests a delicate balance between using AI for efficiency and maintaining active critical engagement.&nbsp;</p>



<p><strong>Shift in the nature of critical thinking.</strong> Participants reported a shift in critical thinking activities, with a greater focus on information verification, response integration, and task stewardship. While AI automates certain aspects of knowledge work, it also demands more effort in evaluating the accuracy and relevance of AI-generated content.&nbsp;</p>



<p><strong>Barriers to critical engagement.</strong> The study identified several barriers that inhibit critical thinking when using AI. These include a lack of awareness of the need for critical evaluation, limited motivation due to time pressure or perceived job scope, and difficulty in refining prompts—especially in unfamiliar domains.</p>



<h3 class="wp-block-heading" id="recommendations">Recommendations</h3>



<p>To foster critical thinking at work, we recommend that AI tools actively encourage awareness, motivation, and skill development.</p>



<p><strong>AI tools should enhance motivators for critical thinking</strong> (e.g., quality standards, skill-building) and mitigate inhibitors (e.g., time constraints, low awareness). Proactive prompts can surface overlooked tasks, while reactive features can offer on-demand assistance. Motivation can be strengthened by positioning critical reflection as part of professional growth—not just extra work.</p>



<p><strong>AI tools should also support knowledge workers’ ability to think critically</strong> by providing reasoning explanations (as some newer AI models now do), guided critiques, and cross-references. This shift must occur in both the design of the technology and in the mindsets of knowledge workers. Rather than treating AI as a tool for delivering answers, we suggest treating it as a thought partner—one that can also act as a provocateur.</p>



<p>Beyond these insights, our other CHI papers explore practical ways to design AI that augments human cognition.</p>



<h2 class="wp-block-heading" id="enhancing-decision-making-with-ai">Enhancing decision-making with AI</h2>



<p>Decision-making is central to knowledge work, and AI is increasingly being used to help people make decisions in complex fields like healthcare and finance. However, how much agency do knowledge workers retain when AI is involved?</p>



<p>Our study, “<a href="https://www.microsoft.com/en-us/research/publication/ai-help-me-think-but-for-myself-assisting-people-in-complex-decision-making-by-providing-different-kinds-of-cognitive-support/">AI, Help Me Think—but for Myself: Exploring How LLMs Can Assist People in Complex Decision-Making by Providing Different Forms of Cognitive Support</a>,” conducted in collaboration with University College London, examines this question. We began with a small formative study involving 10 participants, followed by a comparative study with 21 participants using two different AI-supported decision-making systems.</p>



<p>For a complex financial investment task, we compared two different AI tools (Figure 1): <strong>RecommendAI</strong>, which provides AI-generated recommendations, and <strong>ExtendAI</strong>, which encourages users to articulate their reasoning before receiving AI feedback.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="210" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1.png" alt="Figure 1. The figure consists of two horizontal sections, each depicting a different AI interaction model. The top section shows "RecommendAI," where an AI makes a suggestion for action, which is then interpreted by the user to make a final decision. The bottom section shows "ExtendAI," where the user makes a plan for action, and the AI extends this plan by embedding feedback before the user makes sense of it and makes a final decision. An arrow from "makes sense of plan containing AI's feedback" in ExtendAI loops back to "extends user's plan by embedding feedback." Brief description: The image illustrates two models of human-AI collaboration: one where the AI recommends actions and another where it enhances user-generated plans with feedback. This comparison highlights different approaches to integrating AI into decision-making processes." class="wp-image-1136655" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1-300x105.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig1-240x84.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 1. Illustrative comparison of the thought process involved when interacting with two types of AI: RecommendAI and ExtendAI.</figcaption></figure>



<h3 class="wp-block-heading" id="findings-1">Findings</h3>



<p>Both systems were found to offer benefits for augmenting cognition and addressing some of the challenges to critical thinking identified in the knowledge worker survey above, suggesting the potential for a balanced approach.&nbsp;</p>



<p>RecommendAI offered concrete suggestions that inspired users to explore new directions in their decision-making. This often led to fresh insights and reflections. However, the recommendations at times felt disconnected from the user&#8217;s own reasoning, reducing the depth of engagement.&nbsp;</p>



<p>In contrast, ExtendAI encouraged users to reflect more deeply on their decisions by providing feedback on their reasoning. This helped them examine their thought processes and consider alternative perspectives. However, some users found the feedback too general and not actionable enough.&nbsp;</p>



<p>When it came to how users integrated the tools into their decision-making process, RecommendAI, introduced perspectives that pushed users to think beyond their usual patterns. By recommending options not based on users’ own reasoning, it encouraged exploration of ideas they might not have considered. However, some users perceived the recommendations as a &#8220;black box&#8221; solution. This lack of transparency made those recommendations harder to understand, trust, and apply to their own thought processes.&nbsp;</p>



<p>ExtendAI, on the other hand, aligned with users’ existing reasoning, making its feedback easier to incorporate. This helped the users maintain a sense of control and continuity. However, because the feedback often echoed their initial thoughts, it sometimes limited new insights and risked reinforcing existing biases.</p>



<p>These findings suggest that AI tools like ExtendAI, designed to elicit and build on users&#8217; own cognitive processes, may offer a more effective approach to augmentation than simply providing “ready-made solutions” that users must figure out how to interpret and apply.</p>



<h2 class="wp-block-heading" id="are-we-on-track-making-meetings-better-with-ai">Are we on track? Making meetings better with AI</h2>



<p>Meetings are often criticized for being ineffective. While this is sometimes due to poor practices—such as weak agendas, late starts, and unclear facilitation—we believe the deeper issue is a lack of <em>meeting intentionality</em>: knowing why a meeting is occurring and keeping the discussion focused on that purpose. A key challenge is maintaining goal clarity throughout a meeting.</p>



<p>In the paper “<a href="https://www.microsoft.com/en-us/research/publication/are-we-on-track-ai-assisted-active-and-passive-goal-reflection-during-meetings/">Are We On Track? AI-Assisted Goal Reflection During Meetings</a>,” we explore how AI tools can improve meetings in real time by encouraging <em>reflection</em>—awareness about the meeting’s goals and how well the current conversation is aligned with those goals.</p>



<p>Our study with 15 knowledge workers examined two AI-driven design paradigms: <strong>passive goal assistance</strong> through ambient visualization (a live chart displaying how conversational topics relate to meeting objectives) and <strong>active goal assistance</strong> through interactive questioning (nudging participants to consider whether the current conversation aligns with the meeting objectives). These approaches are illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="600" height="136" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2.png" alt="Figure 2. A figure illustrating two methods of AI interpretation and engagement in a virtual meeting setting. On the left, a graph with "Extent of AI Interpretation" on the y-axis and "Engagement Level" on the x-axis shows two points: "Ambient Visualization" (high AI interpretation, low engagement) and "Interactive Questioning" (high AI interpretation, high engagement). The middle image labeled "Ambient Visualization" shows a virtual meeting with participants' faces blurred and an overlay of data visualizations. The right image labeled "Interactive Questioning" shows a virtual meeting with participants' faces blurred and an overlay of interactive questions. Brief description: This image compares two methods of integrating AI into virtual meetings: Ambient Visualization and Interactive Questioning. It is relevant as it highlights different levels of user engagement and AI interpretation in enhancing virtual communication." class="wp-image-1136656" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2.png 600w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2-300x68.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig2-240x54.png 240w" sizes="auto, (max-width: 600px) 100vw, 600px" /><figcaption class="wp-element-caption">Figure 2. Technology prototypes exploring passive and active ways to keep meetings focused on established objectives.</figcaption></figure>



<h3 class="wp-block-heading" id="recommendations-1">Recommendations</h3>



<p>The findings highlight AI’s potential to help teams with meeting objectives. We found three key design tradeoffs between passive and active support. Based on these, we offer the following AI design recommendations.</p>



<p><strong>Information balance.</strong> There is a tradeoff between ambient visualizations in the passive approach—which can risk information overload—and interactive questioning in the active approach, which may lack detail. To be effective, AI should deliver the right amount of information at the right time and tailor content to the individuals who need it most—without overwhelming users, while offering meaningful and timely support for reflection.</p>



<p><strong>Balance of engagement versus interruption.</strong> When participants are deeply engaged in discussion, significant interruptions can overwhelm and disrupt the flow. Conversely, during moments of confusion or misalignment, subtle cues may be insufficient to get the team back on track. AI systems should dynamically adjust their level of intervention—from ambient and lightweight to more direct—escalating or de-escalating based on timing thresholds, which can be customized for each team.</p>



<p><strong>Balance of team versus individual goal awareness.</strong> AI assistance can nudge team action, such as adjusting agendas. These effects were stronger with the active approach, which required group responses, while the passive approach supported individual thinking without directly influencing team behavior. Team-wide engagement depends on both the visibility of AI cues and how they are introduced into the discussion.</p>



<p>This study helps us understand how AI design choices can support intentionality during meetings and enhance productivity without disrupting natural workflows.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085514">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: blog post</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" aria-label="GraphRAG auto-tuning provides rapid adaptation to new domains" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/09/GraphRag-3-BlogHeroFeature-1400x788-1.png" alt="GraphRAG image on blue to green gradient" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">GraphRAG auto-tuning provides rapid adaptation to new domains</h2>
				
								<p class="large">GraphRAG uses LLM-generated knowledge graphs to substantially improve complex Q&A over retrieval-augmented generation (RAG). Discover automatic tuning of GraphRAG for new datasets, making it more accurate and relevant.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/blog/graphrag-auto-tuning-provides-rapid-adaptation-to-new-domains/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="GraphRAG auto-tuning provides rapid adaptation to new domains" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="encouraging-diverse-problem-solving-brainstorming-with-ai">Encouraging diverse problem-solving brainstorming with AI</h2>



<p>Diverse perspectives drive creative problem-solving in organizations, but individuals often lack access to varied viewpoints. In the paper “<a href="https://www.microsoft.com/en-us/research/publication/yes-and-a-generative-ai-multi-agent-framework-for-enhancing-diversity-of-thought-in-individual-ideation-for-problem-solving-through-confidence-based-agent-turn-taking/">YES AND: An AI-Powered Problem-Solving Framework for Diversity of Thought</a>,” we build on the idea of “design improv” to explore a multi-agent AI prototype that simulates conversations with persona-based agents representing a range of expertise.</p>



<p>The agents follow a classic model of conversational turn-taking, combined with a confidence model to determine when to take or respond to a turn. This allows both the agents and the user to organically build on each others&#8217; ideas and ask clarifying questions. The system enables free-flowing, multi-party idea generation while avoiding common pitfalls of group brainstorming—such as social loafing, production blocking, and groupthink (Figure 3).</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="800" height="439" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3.png" alt="Figure 3. The image is a flowchart and conversation transcript for agent-based ideation. The flowchart on the left shows four steps: "Define a problem," "Move a conversation forward," "Guide the discussion," and "Ask Sage to report." The conversation on the right involves a designer agent, ML researcher agent, and the user discussing audience targeting, ethical implications, and potential solutions for ideating around the ethical implications of gamifying fitness. Brief Description: The image is a flowchart and conversation transcript for agent-based ideation. It includes both a flowchart outlining the process from identifying the initial problem to final report, as well as a conversation transcript where various stakeholders discuss ethical considerations, audience targeting, and potential solutions." class="wp-image-1136652" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3.png 800w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-300x165.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-768x421.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/tools-for-thought_CHI2025-fig3-240x132.png 240w" sizes="auto, (max-width: 800px) 100vw, 800px" /><figcaption class="wp-element-caption">Figure 3. The YES AND system supports conversational turn-taking among agents and the user to generate ideas around a problem.</figcaption></figure>



<p>At the end of a session, an AI agent called Sage distills the discussion, leaving it to the user to develop a conclusive approach to the problem. In this way, YES AND helps unblock forward momentum in problem-solving while preserving the agency of knowledge workers to shape their own ideas.</p>



<h2 class="wp-block-heading" id="next-steps-expanding-the-tools-for-thought-community">Next steps: Expanding the Tools for Thought community</h2>



<p>We believe the best way to advance next-generation tools for thought is by bringing together a wide range of perspectives and approaches. In addition to our four papers, we are also conducting a workshop at CHI on April 26, co-organized with collaborators from industry and academia: <a href="https://www.microsoft.com/en-us/research/publication/tools-for-thought-research-and-design-for-understanding-protecting-and-augmenting-human-cognition-with-generative-ai/" target="_blank" rel="noreferrer noopener">Tools for Thought: Research and Design for Understanding, Protecting, and Augmenting Human Cognition with Generative AI</a>.<strong>&nbsp;</strong>&nbsp;</p>



<p>In this session, over 60 researchers, designers, practitioners, and provocateurs will gather to examine what it means to understand and shape the impact of AI on human cognition. Together, we’ll explore how AI is changing workflows, the opportunities and challenges for design, and which theories, perspectives, and methods are increasingly relevant—or still need to be developed.&nbsp;</p>



<p>The enthusiastic response to this workshop highlights the growing interest in AI’s role in human thought. Our goal is to foster a multidisciplinary community dedicated to ensuring that AI not only accelerates work but also strengthens our ability to think critically, creatively, and strategically.<strong>&nbsp;</strong></p>



<p>We look forward to ongoing discussions, new collaborations, and the next wave of innovations in AI-assisted cognition at <a href="https://www.microsoft.com/en-us/research/event/chi-2025/" target="_blank" rel="noreferrer noopener">CHI 2025</a>.&nbsp;&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/the-future-of-ai-in-knowledge-work-tools-for-thought-at-chi-2025/">The Future of AI in Knowledge Work: Tools for Thought at CHI 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Empowering patients and healthcare consumers in the age of generative AI</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/</link>
		
		<dc:creator><![CDATA[Peter Lee, Dave deBronkart, Christina Farr]]></dc:creator>
		<pubDate>Thu, 17 Apr 2025 23:10:09 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136554</guid>

					<description><![CDATA[<p>Evangelist for patient empowerment Dave deBronkart and Manatt Heath’s Christina Farr discuss how generative AI is redefining healthcare by empowering patients, challenging traditional care models, and creating new opportunities for innovation and collaboration. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/">Empowering patients and healthcare consumers in the age of generative AI</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2101" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="AI Revolution podcast | Episode 3 - Are patients using generative AI for their own healthcare? | outline illustration of Christina Farr, Peter Lee, and Dave deBronkart" class="wp-image-1136563" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788.jpg 2101w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1536x864.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-2048x1152.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode3-Peter-Christina-Dave-AIRevolution_Hero_Feature_No_Text_1400x788-1920x1080.jpg 1920w" sizes="auto, (max-width: 2101px) 100vw, 2101px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=144710957&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.</p>



<p>In this episode, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.epatientdave.com/about-dave/" target="_blank" rel="noreferrer noopener">Dave deBronkart<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.manatt.com/christina-farr" target="_blank" rel="noreferrer noopener">Christina Farr<span class="sr-only"> (opens in new tab)</span></a>, champions of patient-centered digital health, join Lee to talk about how AI is reshaping healthcare in terms of patient empowerment and emerging digital health business models. DeBronkart, a cancer survivor and longtime advocate for patient empowerment, discusses how AI tools like ChatGPT can help patients better understand their conditions, navigate the healthcare system, and communicate more effectively with clinicians. Farr, a healthcare investor and former journalist, talks about the evolving digital health–startup ecosystem, highlighting where AI is having the most meaningful impact—particularly in women’s health, pediatrics, and elder care. She also explores consumer trends, like the rise of cash-pay healthcare. </p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more:</h2>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.epatientdave.com/" target="_blank" rel="noreferrer noopener">e-Patient Dave<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Patient engagement website&nbsp;&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://patientsuseai.substack.com/" target="_blank" rel="noreferrer noopener">Patients Use AI<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Substack blog&nbsp;&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.ted.com/talks/dave_debronkart_meet_e_patient_dave" target="_blank" rel="noreferrer noopener">Meet e-Patient Dave<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>TED Talk | April 2011&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.epatientdave.com/let-patients-help/" target="_blank" rel="noreferrer noopener">Let Patients Help: A Patient Engagement Handbook<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Book | Dave deBronkart |&nbsp;April 2013&nbsp;&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/" target="_blank" rel="noreferrer noopener">Second Opinion<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Health and tech blog&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/p/there-s-about-to-be-a-lot-of-ai-capital-incineration" target="_blank" rel="noreferrer noopener">There’s about to be a lot of AI capital incineration<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Second Opinion blog post | Christina Farr |&nbsp;December 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/p/a-letter-to-my-kids-about-last-week" target="_blank" rel="noreferrer noopener">A letter to my kids about last week<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Second Opinion blog post |&nbsp;Christina Farr | December 2024</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a> &nbsp;<br>Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC] &nbsp;</p>



<p>[BOOK PASSAGE]  &nbsp;</p>



<p>“In healthcare settings, keeping a human in the loop looks like the solution, at least for now, to GPT-4’s less-than 100% accuracy. But years of bitter experience with ‘Dr. Google’ and the COVID ‘misinfodemic’ show that it matters <em>which</em> humans are in the loop, and that leaving patients to their own electronic devices can be rife with pitfalls. Yet because GPT-4 appears to be such an extraordinary tool for mining humanity’s store of medical information, there’s no question members of the public will want to use it that way—a lot.”&nbsp;</p>



<p>[END OF BOOK PASSAGE]  &nbsp;</p>



<p>[THEME MUSIC] &nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. &nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?  &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here. </p>



				</span>
				<span id="show-more-show-less-toggle-12" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]</p>



<p>The passage I read at the top there is from Chapter 5, “The AI-Augmented Patient,” which Carey wrote.&nbsp;&nbsp;</p>



<p>People have forever turned to the internet and sites like WebMD, Healthline, and so on to find health information and advice. So it wouldn’t be too surprising to witness a significant portion of people refocus those efforts around tools and apps powered by generative AI. Indeed, when we look at our search and advertising businesses here at Microsoft, we find that healthcare is in the top three most common categories of queries by consumers.&nbsp;</p>



<p>When we envision AI’s potential impact on the patient experience, in our book, we suggested that it <em>could</em> potentially be a lifeline, especially for those without easy access to adequate healthcare; a research partner to help people make sense of existing providers and treatments; and even maybe act as a third member of a care team that has traditionally been defined by the doctor-patient relationship. This also could have a huge impact on venture capitalists in the tech sector who traditionally have focused on consumer-facing technologies.&nbsp;&nbsp;</p>



<p>In this episode, I’m pleased to welcome Dave deBronkart and Christina Farr.&nbsp;&nbsp;</p>



<p>Dave, known affectionately online as “e-Patient Dave,” is a world-leading advocate for empowering patients. Drawing on his experience as a survivor of stage 4 cancer, Dave gave a viral TED talk on patient engagement and wrote the highly rated book <em>Let Patients Help! </em>Dave was the Mayo Clinic&#8217;s visiting professor in internal medicine in 2015, has spoken at hundreds of conferences around the globe, and today runs the Patients Use AI blog on Substack.&nbsp;</p>



<p>Chrissy puts her vast knowledge of the emerging digital and health technology landscape to use as a managing director with Manatt Health, a company that works with health systems, pharmaceutical and biotech companies, government policymakers, and other stakeholders to advise on strategy and technology adoption with the goal of improving human health. Previously, she was a health tech reporter and on-air contributor for CNBC, Fast Company, Reuters, and other renowned news organizations and publications.&nbsp;</p>



<p>Hardly a week goes by without a news story about an ordinary person who managed to address their health problems—maybe even save their lives or the lives of their loved ones, including in some cases their pets—through the use of a generative AI system like ChatGPT. And if it’s not doing something as dramatic as getting a second opinion on a severe medical diagnosis, the empowerment that people feel when an AI can help decode an indecipherable medical bill or report or get advice on what to ask a doctor, well, those things are both meaningful and a daily reality in today’s AI world.&nbsp;</p>



<p>And make no mistake—such consumer empowerment could mean business, really big business, and this means that investors in new ventures are smart to be taking a close look at all this.&nbsp;&nbsp;</p>



<p>For these and many other reasons, I am thrilled to pair the perspectives offered by e-Patient Dave and Chrissy Farr together for this episode.</p>



<p>Here is my interview with Dave deBronkart:&nbsp;</p>



<p><strong>LEE:</strong> Dave, it&#8217;s just a thrill and honor to have you join us.&nbsp;</p>



<p><strong>DAVE DEBRONKART:</strong> It&#8217;s a thrill to be alive. I&#8217;m really glad that good medicine saved me, and it is just unbelievable, fun, and exciting and stimulating to be in a conversation with somebody like you.&nbsp;</p>



<p><strong>LEE:</strong> Likewise. Now, we&#8217;re going to want to get into both the opportunities and the challenges that patients face. But before that, I want to talk a little bit and delve a little bit more into you, yourself. I, of course, know you as this amazing speaker and advocate for patients. But you have had actually a pretty long career and history prior to all this. And so can you tell us a little bit about your background?&nbsp;</p>



<p><strong>DEBRONKART:</strong> I&#8217;ll go back all the way to when I first got out of college. I didn&#8217;t know what I wanted to do when I grew up. So I got a job where I &#8230; basically, I used my experience working on the school paper to get a temporary job. It was in type setting, if you can believe that. [LAUGHTER] And, man, a few years later, that became the ultimate lesson in disruptive innovation.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> So you were actually doing movable type? Setting type?&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Oh, no, that was, I was … I&#8217;m not that old, sir! [LAUGHTER] The first place where I worked, they <em>did</em> have an actual Linotype machine and all that.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Wow.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Anyway, one thing led to another. A few years after I got that first job, I was working for the world&#8217;s biggest maker of typesetting machines. And I did product marketing, and I learned how to speak to audiences of all different sorts. And then desktop publishing came along, as I say. And it&#8217;s so funny because, now mind you, this was 10 years before Clay Christensen wrote <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.christenseninstitute.org/book/the-innovators-dilemma/" target="_blank" rel="noreferrer noopener"><em>The Innovator&#8217;s Dilemma</em><span class="sr-only"> (opens in new tab)</span></a>. But I had already lived through that because here we were. We were the journeymen experts in our noble craft that had centuries of tradition as a background. Is this reminding you of anything?&nbsp;</p>



<p>[LAUGHTER] Well, seriously. And then along comes stuff that can be put in the hands of the consumers. And I&#8217;ll tell you what, people like you had no clue how to use fonts correctly. [LAUGHTER] We were like Jack Nicholson, saying “You can&#8217;t handle the Helvetica! You don&#8217;t know what you&#8217;re doing!” But what happened then, and this is really relevant, what happened then is—all of a sudden, the population of users was a hundred times bigger than the typesetting industry had ever been.&nbsp;&nbsp;</p>



<p>The clueless people gained experience, and they also started expressing what <em>they</em> wanted the software to be. The important thing is today everybody uses fonts. It&#8217;s no longer a secret profession. Things are done differently, but there is more power in the hands of the end user.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, I think it&#8217;s so interesting to hear that story. I didn&#8217;t know that about your background. And I think it sheds some light on hopefully what will come out later as you have become such, I would call you a fierce consumer advocate.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Sure, energetic, however, whatever you want to call it, sure. [LAUGHTER] Seriously, Peter, what I always look to do … so this is a mixture of my having been run over by a truck during disruptive innovation, all right, but then also looking at that experience from a marketing perspective: how can I convey what&#8217;s happening in a way that people can hear? Because you really don&#8217;t get much traction as an advocate if you come in and say, you people are messed up.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Right. So, now I know this gets into something fairly personal, but you&#8217;ve actually been remarkably public about this. You became very ill.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> And of course, I suspect some of the listeners to this podcast probably have followed your story, but many have not. So can we go a little bit through that …&nbsp;</p>



<p><strong>DEBRONKART:</strong> Sure.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>… just to give our listeners a sense of how this has formed some of your views about the healthcare system.&nbsp;</p>



<p><strong>DEBRONKART:</strong> So late in 2006, I went in for my annual physical with my deservedly famous primary care physician, Danny Sands at Beth Israel [Deaconess Medical Center] in Boston. And in the process—I had moved away for a few years, so I hadn&#8217;t seen him for a while—I did something unusual. I came into the visit with a preprinted letter with 13 items I wanted to go over with him.&nbsp;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> What made you do that? Why did you do that?&nbsp;</p>



<p><strong>DEBRONKART:</strong> I have always been, even before I knew the term exists, I was an engaged patient, and I also very deeply believe in partnership with my physicians. And I respected his time. I had all these things, because I hadn&#8217;t seen him for three years …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> … all these things I wanted to go through. To me it was just if I walked into a business meeting with a bunch of people that I hadn&#8217;t seen for three years and I want to get caught up, I&#8217;d have an agenda.&nbsp;</p>



<p><strong>LEE:</strong> It&#8217;s so interesting to hear you say this because I&#8217;m very similar to you. I like to do my own research. I like to come in with checklists. And do you ever get a sense like I do that sometimes that makes your doctor a little uncomfortable?&nbsp;</p>



<p><strong>DEBRONKART:</strong> [LAUGHS] Well, you know, so sometimes it does make some doctors uncomfortable and that touches on something that right now is excruciatingly important in the culture change that&#8217;s going on. I&#8217;ve spent a lot of time as I worked on the culture change from the patient side, I want to empathize, understand what&#8217;s going on in the doctor&#8217;s head. Most doctors are not trained in medical school or later, how do you work with a patient who behaves like you or me, you know?&nbsp;&nbsp;</p>



<p>And in the hundreds of speeches that I&#8217;ve given, I&#8217;ve had quite a range of reactions from doctors afterwards. I&#8217;ve had doctors come up to me and say, “This is crap.” I mean, right to my face, right. “I&#8217;ll make the decisions. I&#8217;ll decide what we&#8217;re going to talk about.” And now my thought is, <em>OK, and you&#8217;re not going to be my doctor</em>.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> I want to be responsible for how the time is spent, and I didn&#8217;t want be fumbling for words during the visit.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>DEBRONKART:</strong> So I said, I&#8217;ve got among other things … one of the 13 things was I had a stiff shoulder. So he ordered a shoulder x-ray, and I went and got the shoulder x-ray.&nbsp;&nbsp;</p>



<p>And I will never forget this. Nine o&#8217;clock the next morning, he called me, and I can still—this is burned into my memory—I can see the Sony desk phone with 0900 for the time. He said, “Dave, your shoulder&#8217;s going to be fine. I pulled up the x-ray on my screen at home. It&#8217;s just a rotator cuff thing, but Dave, something else showed up. There&#8217;s something in your lung that shouldn&#8217;t be there.”&nbsp;&nbsp;</p>



<p>And just by total luck, what turned out to be a metastasis of kidney cancer was in my lung next to that shoulder. He immediately ordered a CAT scan. Turned out there were five tumors in both lungs, and I had stage 4 kidney cancer.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Wow.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> And on top of that, back then—so this was like January of 2007—back then, there was much less known about that disease than there is now.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>DEBRONKART:</strong> There were no studies—zero research on people like me—but the best available study said that for somebody with my functional status, my median survival was 24 weeks. Half the people like me would be dead in five and a half months.&nbsp;</p>



<p><strong>LEE:</strong> So that just, you know, I can&#8217;t imagine, you know, how I would react in this situation. And what were your memories of the interaction then between you and your doctor? You know, how did your doctor engage with you at that time?&nbsp;</p>



<p><strong>DEBRONKART:</strong> I have very vivid memories. [LAUGHS] Who was it? I can&#8217;t remember what famous person said, “Nothing focuses the mind like the knowledge that one is to be hanged in a fortnight,” right. But 24 weeks does a pretty good job of it.&nbsp;&nbsp;</p>



<p>And I … just at the end of that phone call where he said I&#8217;m going to order a CAT scan, I said, “Is there anything I should do?” Like I was thinking, like, go home and make sure you don&#8217;t eat this sort of this, this, that, or the other thing.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And what he said was, “Go home and have a glass of wine with your wife.”&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Boy, was that sobering. But then it&#8217;s like, all right, game on. What are we going to do? What are my options? And a really important thing, and this, by the way, this is one reason why I think there ought to be a special department of hell for the people who run hospitals and other organizations where they think all doctors are interchangeable parts. All right. My doctor <em>knew</em> me.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And he knew what was important to me. So when the biopsy came back and said, “All right, this is definitely stage 4, grade 4 renal cell carcinoma.” He knew me enough … he said, “Dave, you&#8217;re an online kind of guy. You might like to join this patient community that I know of.” This was 2007.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> It’s a good quality group. This organization that barely exists.&nbsp;</p>



<p><strong>LEE:</strong> That&#8217;s incredibly progressive, technologically progressive for that time.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Yeah, incredibly progressive. Now, a very important part of the story is this patient community is just a plain old ASCII listserv. You couldn&#8217;t even do boldface, right. And this was when the web was … web 2.0 was just barely being created, but what it was, was a community of people who saw the problems the way I see the problems. God bless the doctors who know all the medical stuff, you know. And they know the pathology and the morphology and whatever it is they all know.&nbsp;&nbsp;</p>



<p>And I&#8217;m making a point here of illustrating that I am anything but medically trained, right. And yet I still, I want to understand as much as I can.&nbsp;&nbsp;</p>



<p>I was months away from dead when I was diagnosed, but in the patient community, I learned that they had a whole bunch of information that didn&#8217;t exist in the medical literature.&nbsp;</p>



<p>Now today we understand there&#8217;s publication delays; there&#8217;s all kinds of reasons. But there&#8217;s also a whole bunch of things, especially in an unusual condition, that will never rise to the level of deserving NIH [National Institute of Health] funding, right …&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> … and research. And as it happens, because of the experience in that patient community, they had firsthand experience at how to survive the often-lethal side effects of the drug that I got. And so I talked with them at length and during my treatment, while I was hospitalized, got feedback from them. And several years later my oncologist, David McDermott, said in the BMJ [British Medical Journal], he said, “You were really sick. I don&#8217;t know if you could have tolerated enough medicine if you hadn&#8217;t been so prepared.”&nbsp;</p>



<p>Now there is a case for action, for being actively involved, and pointing towards AI now, doing what I could to learn what I could despite my lack of medical education.&nbsp;</p>



<p><strong>LEE:</strong> But as you were learning from this patient community these things, there had to be times when that came into conflict with the treatment plan that you&#8217;re under. That must have happened. So first off, did it? And how were those conflicts resolved?&nbsp;</p>



<p><strong>DEBRONKART:</strong> So, yes, it did occasionally because in any large population of people you&#8217;re going to have differences of opinion. Now, before I took any action—and this closely matches the current thought of human in the loop, right—before I took any action based on the patient community, I checked with my clinicians.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Were there times when there were things that … advice you were getting from the patient community that you were very committed to, personally, but your official, formal caregivers disagreed with?&nbsp;</p>



<p><strong>DEBRONKART:</strong> No, I can&#8217;t think of a single case like that. Now, let me be clear. My priority was: save my ass, keep me alive, you know? And if I thought a stranger at the other end of an internet pipe had a different opinion from the geniuses at my hospital—who the whole patient community had said, this is maybe the best place in the world for your disease—&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> I was not going to go off and have some philosophical debate about epistemology and all of that stuff. And remember, the clock was ticking.&nbsp;</p>



<p><strong>LEE:</strong> Well, in fact, there&#8217;s a reason why I keep pressing on this point. It&#8217;s a point of curiosity because in the early days of GPT-4, there was an episode that my colleague and friend Greg Moore, who&#8217;s a neuroradiologist, had with a friend of his that became very ill with cancer.&nbsp;&nbsp;</p>



<p>And she went in for treatment and the treatment plan was a specific course of chemotherapy, but she disagreed with that. She wanted a different type of, more experimental immunotherapy. And that disagreement became intractable to the point that the cancer specialists that were assigned to treat her asked Greg, “Can you talk to her and explain, you know, why we think our decision is best?”&nbsp;&nbsp;</p>



<p>And the thing that was remarkable is Greg decided to use that case as one of the tests in the early development days of GPT-4 and had a conversation to explain the situation. They went back and forth. GPT-4 gave some very useful advice to Greg on what to say and how to frame it.&nbsp;&nbsp;</p>



<p>And then, when Greg finally said, “You know, thank you for the help.” What floored both me and Greg is GPT-4 said, “You&#8217;re welcome. But, Greg, what about you? Are you getting all the support that you need? Here are some resources.”&nbsp;&nbsp;</p>



<p>And, you know, I think we can kind of take that kind of behavior for granted today, and there have been some published studies about the seeming empathy of generative AI.&nbsp;</p>



<p>But in those early days, it was eerie, it was awe-inspiring, it was disturbing—you know, all of these things at once. And that&#8217;s essentially why I&#8217;m so curious about your experiences along these lines.&nbsp;</p>



<p><strong>DEBRONKART:</strong> That&#8217;s like, that&#8217;s the flip side of the famous <em>New York Times</em> reporter who got into a late-night discussion …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Oh, Kevin Roose, yes. [LAUGHTER]&nbsp;</p>



<p><strong>DEBRONKART:</strong> <em>You say you&#8217;re happy in your marriage, but I think you&#8217;re not.</em>&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>DEBRONKART:</strong> It&#8217;s like, whoa, this is creepy. But you know, it&#8217;s funny because one of the things that&#8217;s always intrigued me, partly because of my professional experience at explaining technology to people, is the early messaging around LLMs [large language models], which I still hear people … The people who say, “Well, wait a minute, these things hallucinate, so don&#8217;t trust them.” Or they say, “Look, all it&#8217;s doing is predicting the next word.”&nbsp;&nbsp;</p>



<p>But there are loads of nuances, …&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> …<strong> </strong>and that&#8217;s, I mean, it takes an extraordinary amount of empathy, not just for the other person&#8217;s feelings, but for their thought process …&nbsp;</p>



<p><strong>LEE:</strong> Hmm, yes. Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> … to be able to express that. Honestly, that is why I&#8217;m so excited about the arriving future. One immensely important thing … as I said earlier, I really respect my doctors’ time—“doctors” <em>plural</em>—and it breaks my heart that the doctors who did all this work to get license and all that stuff are quitting the field because the economic pressures are so great. I can go home and spend as many hours as I want asking <em>it</em> questions.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> All right. I&#8217;ve recently learned a thing to do after I have one of these hours-long sessions, I&#8217;ll say to it, “All right, so if I wanted to do this in a single-shot prompt, how would you summarize this whole conversation?&#8221; So having explored with no map, I end up with a perspective that it just helps me see the whole thing &#8230;&nbsp;</p>



<p><strong>LEE:</strong> Yes. Yeah, that&#8217;s brilliant.&nbsp;</p>



<p><strong>DEBRONKART:</strong> &#8230; without spending a moment of the doctor&#8217;s time.</p>



<p><strong>LEE</strong>: Yeah, yeah. So when was the first time that you used, you know, generative AI?</p>



<p><strong>DEBRONKART:</strong> It had to be February or March of whatever the first year was.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah. And was it the <em>New York Times</em> article that piqued your interest?&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Oh absolutely.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. And so what did you think? Were you skeptical? Were you amazed? What went through your mind?&nbsp;</p>



<p><strong>DEBRONKART:</strong> Oh, no, no, no. It blew my <em>mind</em>. And I say that as somebody who emerged from the 1960s and ’70s, one of the original people who knew what it was to have your mind blown back in the psychedelic era. [LAUGHTER] No, it blew my mind. And it wasn&#8217;t just the things it said; it was the implications of the fact that it could do that.&nbsp;&nbsp;</p>



<p>I did my first programming with BASIC or Fortran. I don&#8217;t know, something in the mid-&#8217;60s, when I was still in high school. So I understand, well, you know, you got to tell it exactly what you want it to do or it&#8217;ll do the wrong thing. So, yeah, for this to be doing something indistinguishable from thinking—indistinguishable from<em> thinking</em>—was completely amazing. And that immediately led me to start thinking about what this would mean in the hands of a sick person. And, you know, my particular area of fascination in medicine—everything I use it for these days is mundane—but the future of a new world of medicine and healthcare is one where I can explore and not be limited to things where you can read existing answers online.&nbsp;</p>



<p><strong>LEE:</strong> Right. So if you had GPT-4 back in 2006, 2007, when you were first diagnosed with your renal cancer, how would things have been different for you? Would things have been different for you?&nbsp;</p>



<p><strong>DEBRONKART:</strong> Oh, boy, oh, boy, oh, boy. This is going to have to be just a swag because, I mean, for it to—you mean, if it had just dropped out of thin air?&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes. [LAUGHS]&nbsp;</p>



<p><strong>DEBRONKART:</strong> Ah, well, that&#8217;s &#8230; that&#8217;s even weirder. First thing we in the patient community would have to do is figure out what this thing does …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> … before we can start asking it questions.&nbsp;&nbsp;</p>



<p>Now, Peter, a large part of my evangelism, you know, there&#8217;s a reason why <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.epatientdave.com/let-patients-help/" target="_blank" rel="noreferrer noopener">my book<span class="sr-only"> (opens in new tab)</span></a> and my <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.ted.com/talks/dave_debronkart_meet_e_patient_dave" target="_blank" rel="noreferrer noopener">TED talk<span class="sr-only"> (opens in new tab)</span></a> were titled “Let Patients Help.”&nbsp;</p>



<p>I really am interested in planting a thought in people&#8217;s minds, and it&#8217;s not covert. I come right out and say it in the title of the book, right, planting a thought that, with the passage of time, will hold up as a reasonable thing to do. And same thing is true with AI. So … and I&#8217;ve been thinking about it that way from the very beginning. I never closed the loop on my cancer story. I was diagnosed in January, and I had my last drop of high-dose interleukin—experimental immunotherapy, right—in July. And that was it. By September, they said, looks like you beat it. And I was all done.&nbsp;&nbsp;</p>



<p>And there&#8217;s the question: how could it be that I didn&#8217;t die? How could it be that valuable information could exist and not be in the minds of most doctors? Not be in the pages of journals?&nbsp;&nbsp;</p>



<p>And if you think of it that way, along the way, I became a fan of Thomas Kuhn&#8217;s famous book, <em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://press.uchicago.edu/ucp/books/book/chicago/S/bo13179781.html" target="_blank" rel="noreferrer noopener">The Structure of Scientific Revolutions<span class="sr-only"> (opens in new tab)</span></a></em>.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> When something that the paradigm says could not happen <em>does </em>happen, then responsible thinkers have to say, the paradigm must be wrong. That&#8217;s the stage of science that he called a crisis. So if something came along back in 2006, 2007, I would have to look at it and say, “This means we&#8217;ve got to rethink our assumptions.”&nbsp;</p>



<p><strong>LEE:</strong> Yes. You know, now with the passage of time, you know, over the last two years, we&#8217;ve seen so many stories like this, you know, where people have consulted AI for a second opinion, …&nbsp;</p>



<p><strong>DEBRONKART: </strong>Sure.&nbsp;</p>



<p><strong>LEE: </strong>… maybe uploaded their labs and so on and gotten a different diagnosis, a different treatment suggestion. And in several cases that have been reported, both in medical journals and in the popular press, it&#8217;s saved, it has saved lives. And then your point about communities, during COVID pandemic, even doctors form communities to share information. A very famous example are doctors turning to Facebook and Twitter to share that if they had a COVID patient in severe respiratory distress, sometimes they could avoid intubation by &#8230;&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Pronation. Yeah.&nbsp;</p>



<p><strong>LEE:</strong> … pronation. And things like this end up being, in a way, I think the way you&#8217;re couching it, ways to work around the restrictions in the more formal healthcare system.&nbsp;</p>



<p><strong>DEBRONKART:</strong> The traditional flow. Yes. And there is nothing like a forest fire, an emergency, an unprecedented threat to make people drop the usual formal pathways.&nbsp;</p>



<p><strong>LEE:</strong> So, I&#8217;d like to see if we can impart from your wisdom and experience some advice for specific stakeholders. So, what do you say to a patient? What do you say to a doctor? What do you say to the executive in charge of a healthcare system? And then finally, what do you say to policymakers and regulators? So, let&#8217;s start with patients.&nbsp;</p>



<p><strong>DEBRONKART:</strong> So if you&#8217;ve got a problem that or a question where you really want to understand more than you&#8217;ve been able to, then give a try to these things. Ask some questions. And it&#8217;s not just the individual question and answer. The famous, amazing patient advocate, Hugo Campos, …&nbsp;</p>



<p><strong>LEE:</strong> Hmm, yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> … said something that I call “Hugo&#8217;s Law.” He said, “Dave, I don&#8217;t ask it for answers. I use it to help me think.”&nbsp;</p>



<p><strong>LEE:</strong> Yes, absolutely.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> So you get an answer and you say, “Well, I don&#8217;t understand this. What about that? Well, what if I did something different instead?” And never forget, you can come back three months later and say, “By the way, I just thought of something. What about that,” right.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah, fantastic.&nbsp;</p>



<p><strong>DEBRONKART:</strong> So be focused on what <em>you</em> want to understand.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>So now let&#8217;s go to a doctor or a nurse. What&#8217;s the advice there?&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Please try to imagine a world … I know that most people today are not as activated as I am in wanting to be engaged in their health. But to a very large extent, people, a lot of people, family and friends, have said they don&#8217;t want to do this because they don&#8217;t want to offend the doctors and nurses. Now, even if the doctor or nurse is not being a paternal jerk, all right, the patients have a fear of this. Dr. Sands handles this brilliantly. I mentioned it in the book. He proactively asks, are there any websites you&#8217;ve found useful?&nbsp;&nbsp;</p>



<p>And you can do the same thing with AI. Have you done anything useful with ChatGPT or something like that?&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> That actually suggests some curricular changes in medical schools in order to train doctors.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Absolutely. In November, I attended a retreat on rethinking medical education. I couldn&#8217;t believe it, Peter. They were talking about how AI can be used in doing medical education. And I was there saying, “Well, hello. As long as we&#8217;re here, let&#8217;s rethink how you teach doctors, medical students to deal with somebody like me.” Cause what we do not want &#8230;&nbsp;&nbsp;</p>



<p>There was just a study in Israel where it said 18% of adults use AI regularly for medical questions, which matches other studies in the US.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yep.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> But it&#8217;s 25% for people under 25. We do not want 10 years from now to be minting another crop of doctors who tells patients to stay off of the internet and AI.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> You know, it&#8217;s such an important point. Students, you know, entering into college to go on to medical school and then a residency and then finally into practice. I think you&#8217;re thinking about the year 2035 or thereabouts. And when you think of that, at least in tech industry terms, we&#8217;re going to be on Mars, we&#8217;re going to have flying cars, we&#8217;re going to have AGI [artificial general intelligence], and you really do need to think ahead.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Well, you know, healthcare, and this speaks to the problems that health system executives are facing: y&#8217;all better watch out or you&#8217;re going to be increasingly irrelevant, all right.&nbsp;&nbsp;</p>



<p>One of the key use cases, and I&#8217;m not kidding &#8230; I mean, I don&#8217;t mean that if I have stage 4 kidney cancer, I&#8217;m going to go have a talk with my robot. But one of the key use cases that makes people sit down and try to solve a problem on their own with an LLM is if they can&#8217;t get an appointment.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Well, so let&#8217;s figure out, can the health system, can physicians and patients learn to work together in some modified way? Nobody I know wants to stop seeing a doctor, but they do need to have their problems solved.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And there is one vitally important thing I want to … I insist that we get into this, Peter. In order for the AI to perform to the best of its contribution, it needs to know all the data.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>DEBRONKART:</strong> Well, and so does the patient. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://patientsuseai.substack.com/p/field-note-using-ai-in-healthcare" target="_blank" rel="noreferrer noopener">Another super-patient, James Cummings, has two rare-genetic-mutation kids.<span class="sr-only"> (opens in new tab)</span></a> He goes to four Epic-using hospitals. Those doctors can&#8217;t see each other&#8217;s data. So he compiles it, and he shows … the patient brings in the consolidated data.&nbsp;</p>



<p><strong>LEE:</strong> Yes. Well, and I know this is something that you&#8217;ve really been passionate about, and you&#8217;ve really testified before Congress on. But maybe then that leads to this fourth category of people who need advice, which are policymakers and regulators. What would you tell them?&nbsp;</p>



<p><strong>DEBRONKART:</strong> It&#8217;s funny, in our current political environment, there&#8217;s lots of debates about regulation, more regulation, less regulation. I&#8217;m heavily in favor of the regulations that say, yeah, I gotta be able to see and download my <em>damn data</em>, as I&#8217;m famous for calling it. But what we need to do if we were to have any more regulations is just mandate that you can&#8217;t keep the data away from people who need it. You can&#8217;t when …&nbsp;</p>



<p><strong>LEE:</strong> Yep.&nbsp;</p>



<p><strong>DEBRONKART:</strong> OK, consider <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://patientsuseai.substack.com/p/three-years-17-doctors-suffering" target="_blank" rel="noreferrer noopener">one of the most famous AI-using patients is this incredible woman, Courtney Hofmann, whose son saw 17 doctors over three years<span class="sr-only"> (opens in new tab)</span></a>, and she finally sat down one night and typed it all into GPT. She has created a startup to try to automate the process of gathering everyone&#8217;s data.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes, yes. Yeah.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And I know people who have been trying to do this and it&#8217;s just really hard. Policy people should say, look, I mean, we know that American healthcare is unsustainable economically.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And one way to take the pressure off the system—because it ain&#8217;t the doctors’ fault, because they&#8217;re burned out and quitting—one way to take the pressure off is to put more data in the hands of the patients so that entrepreneurs can make better tools.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. All right. So, we&#8217;ve run out of time, but I want to ask one last provocative question to send us off. Just based on your life&#8217;s experience, which I think is just incredible and also your personal generosity in sharing your stories with such a wide audience, I think is incredible. It&#8217;s just doing so much good in the world. Do you see a future where AI effectively replaces human doctors? Do you think that&#8217;s a world that we&#8217;re heading towards?&nbsp;</p>



<p><strong>DEBRONKART:</strong> No, no, no, no. People are always asking me this. I do imagine an increasing base, an increasing if … maybe there&#8217;s some Venn diagram or something, where the number of things that I can resolve on my own will increase.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm. Yes.&nbsp;</p>



<p><strong>DEBRONKART:</strong> And in particular, as the systems get more useful, and as I gain more savvy at using them and so on, there will be cases where I can get it resolved good enough before I can get an appointment, right. But I cannot imagine a world without human clinicians. Now, I don&#8217;t know what that&#8217;s going to look like, right.</p>



<p><strong>LEE:</strong> Yes. [LAUGHS]</p>



<p><strong>DEBRONKART:</strong> I mean, who knows what it&#8217;s going to be. But I keep having … Hugo blogged this incredible vision of where <em>his</em> agentic AI will be looking at one of these consolidated blob medical records things, and so will his doctor&#8217;s agentic AI. </p>



<p><strong>LEE:</strong> Yes. Well, I think I totally agree with you. I think there&#8217;ll always be a need and a desire for the human connection. Dave, this has been an incredible, really at times, riveting conversation. And as I said before, thank you for being so generous with your personal stories and with all the activism and advocacy that you do for patients.&nbsp;</p>



<p><strong>DEBRONKART:</strong> Well, thank you. I&#8217;m, as I said at the beginning, I&#8217;m glad to be alive and I&#8217;m really, really, really grateful to be given a chance to share my thoughts with your audience because I really like super smart nerds. &nbsp;<br>&nbsp;<br>[LAUGHTER] No, well, no kidding. In preparing for this, I listened to a bunch of back podcast episodes, “Microsoft Research,” “NEJM AI.” They talk about things I do not comprehend and don&#8217;t get me started on quantum, right? [LAUGHTER] But I&#8217;m grateful and I hope I can contribute some guidance on how to solve the problem of the person for whom the industry exists.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, you absolutely have done that. So thank you.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>E-Patient Dave is so much fun to talk to. His words and stories are dead serious, including his openness about his struggles with cancer. But he just has a way of engaging with the world with such activism and positivity. The conversation left me at least with a lot of optimism about what AI will mean for the consumer.&nbsp;&nbsp;</p>



<p>One of the key takeaways for me is Dave&#8217;s point that sometimes informal patient groups have more up-to-date knowledge than doctors. One wonders whether AI will make these sorts of communities even more effective in the near future. It sure looks like it.&nbsp;&nbsp;</p>



<p>And as I listen to Dave&#8217;s personal story about his bout with cancer, it&#8217;s a reminder that it can be lifesaving to do your own research, but ideally to do so in a way that also makes it possible to work with your caregivers. Healthcare, after all, is fundamentally a collaborative activity today.&nbsp;</p>



<p>Now, here&#8217;s my conversation with Christina Farr:&nbsp;</p>



<p><strong>LEE: </strong>Chrissy, welcome. I&#8217;m just thrilled that you&#8217;ve joined us here.&nbsp;</p>



<p><strong>CHRISTINA FARR: </strong>Peter, I&#8217;m so excited to be here. Thanks for having me on.&nbsp;</p>



<p><strong>LEE:</strong> One thing that our listeners should know is you have a blog called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/" target="_blank" rel="noreferrer noopener">Second Opinion<span class="sr-only"> (opens in new tab)</span></a>. And it&#8217;s something that I read religiously. And <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/p/there-s-about-to-be-a-lot-of-ai-capital-incineration" target="_blank" rel="noreferrer noopener">one of the things you wrote<span class="sr-only"> (opens in new tab)</span></a> a while ago expressed some questions about as an investor or as a founder of a digital health company, if you don&#8217;t use the words AI prominently, you will struggle to gain investment. And you were raising some questions about this. So maybe we start there. And, you know, what are you seeing right now in the kind of landscape of emerging digital health tech companies? What has been both the positive and negative impact of the AI craziness that we have in the world today on that?&nbsp;</p>



<p><strong>FARR: </strong>Yeah, I think the title of that was something around the great AI capital incineration [LAUGHTER] that we were about to see. But I, you know, stand by it. I do think that we&#8217;ve sort of gone really deep into this hype curve with AI, and you see these companies really just sucking up the lion&#8217;s share of venture capital investment.&nbsp;</p>



<p>And what worries me is that these are, you know, it&#8217;s really hard, and we know this from just like decades of being in the space that tools are very hard to monetize in healthcare. Most of healthcare still today and where really the revenue is, is in, still in services. It&#8217;s still in those kind of one-to-one interactions. And what concerns me is that we are investing in a lot of these AI <em>tools</em> that, you know, are intended to sell into the system. But the system doesn&#8217;t yet know how to buy them and then, beyond that, how to really integrate them into the workflow.&nbsp;&nbsp;</p>



<p>So where I feel more enthusiastic, and this is a little bit against the grain of what a lot of VCs [venture capitalists] think, but I actually really like care delivery businesses that are fully virtual or hybrid and really using AI as part of their stack. And I think that improves really the style of medicine that they&#8217;re delivering and makes it far more efficient. And you start to see, you know, a real improvement in the metrics, like the gross margins of these businesses beyond what you would see in really traditional kind of care delivery. And because they are the ones that own the stack, they&#8217;re the ones delivering the actual care, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>FARR:</strong> … they can make the decision to incorporate AI, and they can bring in the teams to do that. And I feel like in the next couple of years, we&#8217;re going to see more success with that strategy than just kind of more tools that the industry doesn&#8217;t know what to do with.&nbsp;</p>



<p><strong>LEE:</strong> You know, I think one thing that I think I kind of learned or I think I had an inkling of it, but it was really reinforced reading your writings, as a techie, I and I think my colleagues tend to be predisposed to looking for silver bullets. You know, technology that really just solves a problem completely.&nbsp;&nbsp;</p>



<p>And I think in healthcare delivery in particular, there probably aren&#8217;t silver bullets. And what you need to do is to really look holistically at things and your emphasis on looking for those metrics that measure those end-to-end outcomes. So at the same time,<strong> </strong>if I could still focus on your blog, you do highlight companies that seem to be succeeding that way.&nbsp;&nbsp;</p>



<p>Just, in preparation for this discussion, I re-read your post about <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://flo.health/" target="_blank" rel="noreferrer noopener">Flo<span class="sr-only"> (opens in new tab)</span></a> being the first kind of unicorn women&#8217;s health digital tech startup. And there is actually a lot of very interesting AI technology involved there. So it can happen. How do you think about that?&nbsp;</p>



<p><strong>FARR: </strong>Yeah, I mean, I see a lot of AI across the board. And it&#8217;s real with some of these companies, whether it&#8217;s, you know, a consumer health app like Flo that, you know, is really focused on kind of period tracking. And AI is very useful there in helping women just predict things like their optimal fertility windows. And it&#8217;s very much kind of integrated very deeply into that solution. And they have really sophisticated technology.&nbsp;&nbsp;</p>



<p>And you see that now as well with the kind of craze around these longevity companies, that there is a lot of AI kind of underlying these companies, as well, especially as they&#8217;re doing, you know, a lot of health tests and pulling in new data and providing access to that data in a way that, you know, historically patients haven&#8217;t had access to.&nbsp;&nbsp;</p>



<p>And then I also see it with, you know, like I spoke about with these care delivery companies. I recently spent some time with a business called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.theoriginway.com/" target="_blank" rel="noreferrer noopener">Origin<span class="sr-only"> (opens in new tab)</span></a>, for instance, which is in, you know, really in kind of women&#8217;s health, MSK [musculoskeletal], and that beachhead is in pelvic floor PT [physical therapy].&nbsp;&nbsp;</p>



<p>And for them, you know, it&#8217;s useful in the back office for … a lot of their PT providers are getting great education through AI. And then it&#8217;s also useful on the patient-facing side as they provide kind of more and more content for you to do exercises at home. A lot of that can be delivered through AI. So for some of these companies, you know, they look across the whole stack of what they&#8217;re providing, and they&#8217;re just seeing opportunities in so many different places for AI. And I think that&#8217;s really exciting, and it&#8217;s very, very real. And it&#8217;s really to me like where I&#8217;m seeing kind of the first set of really kind of promising AI applications. There are definitely some really compelling AI tools, as well.&nbsp;</p>



<p>I think companies like Nuance and like Abridge and that whole category of really kind of replacing human scribes with AI, like to me, that is a … that has been so successful because it literally is the pain point. It&#8217;s the pain point. You&#8217;re solving <em>the</em> pain point for health systems and physicians.&nbsp;&nbsp;</p>



<p>Burnout is a huge problem. Documentation is a huge problem. So, you know, to say we&#8217;ve got this kind of AI solution, everybody&#8217;s basically on board—you know, as long as it works—[LAUGHTER] from the first meeting. And then the question becomes, which one do you choose? You know, that said, you know, to me, that&#8217;s sort of a standout area. I&#8217;m not seeing that everywhere.</p>



<p><strong>LEE:</strong> So there are like a bunch of things to delve into there. You know, since you mentioned the Nuance, the Dragon Copilot, and Abridge, and they are doing extremely well. But even for them, and this is another thing that you write about extensively, health systems have a hard time justifying investing in these technologies. It&#8217;s not like they&#8217;re swimming in cash. And so on that element of things, is there advice to companies that are trying to make technologies to sell into health systems?&nbsp;</p>



<p><strong>FARR:</strong> Yeah, I mean, I&#8217;ll give you something really practical on that just example specifically. So I spend a lot of time chatting with a lot of the health system CMIOs [chief medical informatics officers] trying to, you know, just really understand kind of their take. And they often tell me, “Look, you know, these technologies are not inexpensive, and we&#8217;ve already spent a boatload of money on REHR [regional electronic health records], which continues to be expensive. And so we just don&#8217;t have a lot of budget.” And for them, I think the question becomes, you know, who within the clinical organization would benefit most from these tools?&nbsp;&nbsp;</p>



<p>There are going to be progressive physicians that will jump on these on day one and start using them and really integrating them into the workflow. And there will be a subset that just wants to do things the way they always have done things. And you don&#8217;t want to pay for seats for everybody when there&#8217;s a portion that will not be using it. So I think that&#8217;s maybe something that I would kind of share with the startup crowd is just, like, don&#8217;t try to sell to every clinician within the organization. Not everybody is going to be, you know, a technology early adopter. Work with the health systems to figure out that cohort that&#8217;s likely to jump on board first and then kind of go from there.&nbsp;</p>



<p><strong>LEE: </strong>So now let me get back to specifically to women&#8217;s health. I think your investing strategy has, I think it&#8217;s fair to say has had some emphasis on women&#8217;s health. And I would say for me, that has always made sense because if there&#8217;s one thing the tech industry knows how to do in any direct-to-consumer business is to turn engagement into dollars.&nbsp;&nbsp;</p>



<p>And when you think about healthcare, there are very few moments in a person&#8217;s life when they have a lot of engagement with their own healthcare. But women have many. You mentioned period tracking, pregnancy, menopause. There are so many areas where you could imagine that technology could be good. At least that&#8217;s way I would think about it, but does that make any sense to you, or do you have a different thought process?&nbsp;&nbsp;</p>



<p><strong>FARR: </strong>Oh, my god, I&#8217;ve been, I&#8217;m just nodding right now because I&#8217;ve been saying the same thing for years, [LAUGHS] that like, I think the, you know, the moments of what I call naturally high engagement are most interesting to me. And I think it&#8217;s why it&#8217;s been such a struggle with some of these companies that are looking at, you know, areas like or conditions like type two diabetes.&nbsp;&nbsp;</p>



<p>I mean, it&#8217;s just so hard to try to change somebody&#8217;s behavior, especially through technology. You know, we&#8217;ve not kind of proven out that these nudges are really changing anybody&#8217;s mind about, you know, their day-to-day lifestyles. Whereas, you know, in these moments, like you said, of just like naturally high engagement &#8230; like it&#8217;s, you know, women&#8217;s health, you&#8217;re right, there&#8217;s a lot of them. Like if you&#8217;re pregnant, you&#8217;re very engaged. If you&#8217;re going through menopause, you&#8217;re very engaged. And I think there are other examples like this, you know, such as oncology. You get a cancer diagnosis, you&#8217;re very engaged.&nbsp;</p>



<p>And so, to me, that&#8217;s really kind of where I see the most interesting opportunities for technology and for digital health.&nbsp;&nbsp;</p>



<p>And, you know, one example I&#8217;ll give you in women&#8217;s health, I&#8217;m not invested in this company, sadly. They are called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.joinmidi.com/" target="_blank" rel="noreferrer noopener">Midi Health<span class="sr-only"> (opens in new tab)</span></a>. And they&#8217;re really everywhere in the menopause area now, like, you know, the visit volume that they are seeing is just insane. You know, this is a population that is giant. It&#8217;s, like, one in two people are women. At some point, we pretty much all go through menopause, some people earlier, some later.&nbsp;</p>



<p>And for a lot of us, it&#8217;s a really painful, disruptive thing to experience. And we tend to experience it at a moment when we actually have spending money. So it just ticks all the boxes. And yet I think because of the bias that we see, you know, in the venture land and in the startup world, we just couldn&#8217;t get on this opportunity for a really long time. So I&#8217;ve been very excited to see companies like that really have breakout success.&nbsp;</p>



<p><strong>LEE: </strong>First off, you know, I think in terms of hits and misses from our book. One hit is we did think a lot about the idea that patients directly would be empowered by AI. And, you know, we had a whole chapter on this, and it was something that I think has really turned out to be true, and I think it will become more true. But one big miss is we actually didn&#8217;t think about what we were just talking about, about like <em>who</em> and <em>when</em> would this happen? And the specific focus on women, women&#8217;s health, I think is something that we missed.&nbsp;&nbsp;</p>



<p>And I think one of the reasons I sought you out for this conversation is if I remember your own personal history, you essentially transitioned from journalism to venture investing at about the same time that you yourself were having a very intense period of engagement with health because of your own pregnancy. And so if you don&#8217;t mind, I&#8217;d like to get into your own experience with healthcare through pregnancy, your own experiences raising children, and how that has informed your relationship with digital health and the investing and advising that you do today.&nbsp;</p>



<p><strong>FARR:</strong> Yeah, it&#8217;s great question. And I actually was somebody who, you know, wrote a lot while I was kind of on maternity leave about this experience because it was such a profound one. You know, I think the reason that pregnancy is so interesting to healthcare companies and systems is because really for a lot of women, it&#8217;s their first experience with the hospital.&nbsp;&nbsp;</p>



<p>Most of us have never stayed in the hospital for any period of time until that moment. Both times I had C-sections, so I was there for a good three or four days. And, you know, I think it&#8217;s a really big opportunity for these systems, even if they lose money, many of them lose money on pregnancy, which is a whole different topic, but there is an opportunity to get a whole family on board and keep them kind of loyal. And a lot of that can come through, you know, just delivering an incredible service.&nbsp;&nbsp;</p>



<p>Unfortunately, I don&#8217;t think that we are delivering incredible services today to women in this country. I see so much room for improvement. You know, you see, just look at the data. You see women, you know, still dying in childbirth in this country where in many other developed nations, that&#8217;s just no longer the case.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah. And what are, in your view, the prime opportunities or needs? What do we need to do if we have a focus on technology to improve that situation?&nbsp;&nbsp;</p>



<p><strong>FARR:</strong> Yeah, I mean, I think there&#8217;s definitely an opportunity for, you know, just digital technologies and for remote patient monitoring and just other forms of monitoring. I do think we should look at what other countries have done and really consider things like, you know, three days post-discharge, somebody comes to your home, you know, whether it&#8217;s to check on you from a healthcare perspective, both, you know, physical and mental health, but then also make sure that the environment is safe for both the mother and the baby. Simple things like that, that don&#8217;t even really require any technology.&nbsp;&nbsp;</p>



<p>And then there&#8217;s certainly opportunities for new forms of, you know, diagnostic tests for things like preeclampsia, postpartum preeclampsia. We could definitely use some new therapeutics in this area. Then, you know, would love to kind of also touch on the opportunity in pediatrics because there I think is an ideal use case for AI. And that&#8217;s definitely my reality now.&nbsp;</p>



<p><strong>LEE:</strong> Well, fact, yeah, in fact, I hope I&#8217;m not delving into too many personal issues here. But I do remember, I think with your first child, which you had during the height of the COVID pandemic, that your child actually had COVID and actually even lost sense of taste and smell for a period. And, in our book, we had sort of theorized that people would turn possibly to AI for advice to understand what was going on.&nbsp;&nbsp;</p>



<p>When you look broadly at the kinds of queries that come into a search engine or into something like ChatGPT or Copilot, you do see things along those lines. But at the same time, I had always thought people wouldn&#8217;t just use a raw chat bot for these things. People would want an app, perhaps powered by AI, that would be really designed for this. And yet somehow that seems not to be as widespread.&nbsp;&nbsp;</p>



<p><strong>FARR:</strong> Yeah. And I think the word <em>app</em> is a great one that I&#8217;d love to, you know, maybe interrogate a little bit because I think that we have been overly reliant on apps. I&#8217;ll give you an example. So in a pediatric space, I am a user of an app called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://app.summerhealth.com/signup/welcome" target="_blank" rel="noreferrer noopener">Summer Health<span class="sr-only"> (opens in new tab)</span></a> or it&#8217;s not an app. Sorry. It&#8217;s a text messaging service. [LAUGHTER] And this is the genius. So I just pick up my phone, and I text “Summer” and a pediatrician responds within a matter of minutes. And sometimes it&#8217;s a pediatric nurse, but it&#8217;s somebody who responds to me. And they say, oh, what&#8217;s going on? And I might say, OK, well, this week we had the norovirus. So these are the symptoms. And they might say, you know, I&#8217;d love to see an image or a video. And I can text that to them.&nbsp;&nbsp;</p>



<p>And if a prescription is required, then that goes to a pharmacy near me through another digital application that&#8217;s really cool called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.photon.health/" target="_blank" rel="noreferrer noopener">Photon Health<span class="sr-only"> (opens in new tab)</span></a>, where my script is portable, so I can move it around based on what&#8217;s open.&nbsp;&nbsp;</p>



<p>So, through this, I&#8217;m getting an incredible experience that&#8217;s the most convenient …&nbsp;</p>



<p><strong>LEE:</strong> Wow.&nbsp;</p>



<p><strong>FARR:</strong> I could ever ask for, and there is no app. [LAUGHS] And you could imagine the potential for AI. You know, a company like this is probably getting so many questions about a norovirus or COVID or RSV [Respiratory Syncytial Virus], and is, I&#8217;m sure, starting to think about kind of ways in which AI could be very useful in this regard. And you don&#8217;t need a pediatrician or pediatric nurse answering every question. Perhaps there&#8217;s like sophisticated triaging to determine which questions should go to the human expert.&nbsp;&nbsp;</p>



<p>But, you know, again, back to this app question, like, I think we have too many. Like, it&#8217;s just … like from a user experience perspective, just having to find the app, log into the app. Sometimes there&#8217;s just layers of authentication. Then you have to remember your password. [LAUGHTER] And it&#8217;s just, you know, it&#8217;s just too many steps. And then there&#8217;s like 50 of them for all kinds of different things.&nbsp;</p>



<p><strong>LEE:</strong> Yes. Well, and you have to also go to an app store, download the thing.&nbsp;&nbsp;</p>



<p><strong>FARR:</strong> Go to the app store down. It&#8217;s just too many steps.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>FARR:</strong> So, like, I, you know, I recognize that HIPAA exists. If there is any kind of claim involved, then, you know, you need an app because you got privacy to think about and compliance, but like, in<strong> </strong>this wave of consumerization of healthcare, there&#8217;s a lot more that&#8217;s possible. And so I&#8217;d love to see people experimenting a bit more with the form factor. And I think once we do that, we could open up a lot more interesting applications with AI, because you&#8217;ll see so much more usage day to day than you will if you require any of this kind of gatekeeping with an app.&nbsp;</p>



<p><strong>LEE:</strong> It&#8217;s so interesting to hear you say this because one thing that I&#8217;ve thought—and I&#8217;ve actually even expressed publicly in some venues—is one logical endpoint for AI as we understand it today <em>is</em> that apps become unnecessary. We might still have machines that, you know, you hold in the palm of your hand, but it&#8217;s just a machine that does what you want it to do.&nbsp;&nbsp;</p>



<p>Of course, the business model implications are pretty profound. So for that particular text messaging service, do you understand what their business model is? You know, how are they sustaining themselves?&nbsp;</p>



<p><strong>FARR:</strong> Consumer, it&#8217;s all cash pay. It&#8217;s cash pay. You just pay a subscription. And, you know, there are certainly kind of privacy requirements, you know, related to kind of federal and state, but you could consent to be able to do something like this. And, you know, companies like this have teams of lawyers that kind of think through how do you make something like this happen. But it&#8217;s possible because of this cash pay element that really underlies that. And I think that is a growing trend.&nbsp;&nbsp;</p>



<p>You know, I was literally sitting with a benefits consultant a few weeks ago, and he was saying to me, like, “I tell all my friends and family, just don&#8217;t use your insurance at all, unless it&#8217;s for like a very high price thing, like a medical procedure that&#8217;s expensive or a surgery.” He said, for everything else, I just pay cash. I pay cash for all my primary care. I pay cash for, you know, basic generic, you know, prescription medications that, you know, it&#8217;s like a few cents to manufacture.&nbsp;&nbsp;</p>



<p>And I&#8217;m sort of getting there, too, where I just kind of increasingly am relying on cash pay. And I think that sort of opens up a world of opportunity for just innovation related to user experience that could really bring us to this place that you mentioned where there is no app. You literally just text or, you know, you use your voice, and you say, “I need a restaurant reservation,” and it&#8217;s done.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm. Yeah.&nbsp;</p>



<p><strong>FARR:</strong> And it&#8217;s that simple, right? And the sort of <em>appification</em> of everything, you know, was a important kind of evolution or moment in technology that is undeniable. But I totally agree with you that I think we might be moving past that.&nbsp;</p>



<p><strong>LEE: </strong>On this idea of cash, there is a little bit of a fatigue, on the other hand, with—for consumers; let me just speak as a consumer—I can&#8217;t keep track anymore of all the subscriptions I have. And so are we just trading one form of, you know, friction for another?&nbsp;</p>



<p><strong>FARR:</strong> Yeah, that&#8217;s a great point. But there are things that, you know, I think there are those moments where you continue to pay a subscription because it&#8217;s just something that&#8217;s chronic. You know, it&#8217;s just relevant to you. You know, pediatrics is a great example. At some point, like I won&#8217;t need a pediatrician on demand, which is what I have now, maybe when my kids are a little older, and we&#8217;re not just a cesspool of various kind of viruses at home. [LAUGHTER] But again, back to your point about, you know, the sort of moments of just, like, natural engagement, I think there&#8217;s also a moment there … there are areas or parts of our lives where, like primary care, where it&#8217;s just more longitudinal.&nbsp;&nbsp;</p>



<p>And it makes sense to pay on a kind of subscription basis. Like our system is messed up because there’s just messed up incentives, right. And a subscription to me is very pure. [LAUGHTER] Like it&#8217;s you&#8217;re just saying, “I&#8217;m paying for a service that I want and need.” And then the company is saying, “OK, let me make this service as efficient and great and affordable for you as I possibly can.” And to me, that&#8217;s like a very, like refreshing trade. And I feel the same way, by the way, in my media business, which, you know, definitely has a subscription element. And it just means a lot when someone&#8217;s willing to say like this content&#8217;s worth paying for.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>FARR: </strong>It doesn&#8217;t work for everything, but I think it works for things that, you know, have that long-term payoff.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I really love that. And if I have one regret about the chapter on kind of the consumer experience from our book—I think all of this seems obvious in retrospect—you know, I wish we had tried to understand, you know, this aspect of the consumer experience, that people might actually have just online experiences that they would pay a monthly fee or an annual fee for. Because it also hits on another aspect of consumer, which is this broad—it&#8217;s actually now a national issue in healthcare—about price transparency.&nbsp;&nbsp;</p>



<p>And this is another thing that I think you&#8217;ve thought about and written about, both the positives and negatives of this. I remember one blog post you made that talked about the issue of churn in digital health. And if I remember correctly, you weren&#8217;t completely certain that this was a good thing for the emerging digital health ecosystem. Can you say more about this idea of churn?&nbsp;</p>



<p><strong>FARR:</strong> Yeah, I mean, you know, I&#8217;ve been writing for a long time and thinking for a long time about the buyers of a lot of these kind of digital health companies, like who are the customers? And there was a long period where it was, it was really the self-insured employer, like Microsoft, being a sort of customer of these solutions because they wanted to provide a great array of health benefits for their own employees.&nbsp;&nbsp;</p>



<p>And that was, you know, for a long time, like 10 or 15 years, you know, big companies that have now gone public, and it seemed like a faster timeline to be able to sell relative to health systems and, you know, health plans and other groups. And I&#8217;ve now kind of been on the forefront of saying that this channel is kind of dead. And one of the big reasons is just, you know, there&#8217;s no difference, I would say to what you see kind of in the payer lane, which is that churn is a big problem. People used to stay at jobs for 20, 30, 40 years, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>FARR: </strong>… and then you&#8217;d retire and have great benefits. And so it kind of made sense that your company was responsible for the healthcare that you received. And now I think the last time I looked at the Bureau of Labor Statistics, it&#8217;s around four years, a little bit less than four years. So what can you do in four years? [LAUGHS]&nbsp;</p>



<p>I just read an interesting analysis on GLP-1s, these medications now that obviously are everywhere in tackling type two diabetes, and obesity is kind of the main, seems to be the hot use case. But, you know, I&#8217;m reading analysis around ROI that it&#8217;s 15, over 15 years, to see an ROI if you are, you know, a system or a plan or employer that chooses to pay for this. So how does that equate when you don&#8217;t keep an employee around for more than four?&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yep.&nbsp;</p>



<p><strong>FARR: </strong>So I think it&#8217;s just left employers in a really bad place of having to make a bunch of tradeoffs and, you know, employees are demanding, we want access to these things. And they&#8217;re saying, well, our healthcare costs just keep going up and up and up. You know, we have inflation to contend with and we&#8217;re not seeing, you know, the analysis that it necessarily makes sense for us to do so. So that&#8217;s what I have, you know, been sort of harping on about with this churn issue that I&#8217;m seeing.&nbsp;</p>



<p><strong>LEE:</strong> Well, I have to tell you, it really, when I first started reading about this from you, it really had a profound impact on my thinking, my thought process. Because one of the things that we dream about is this idea that&#8217;s been present actually for decades in the healthcare world of this concept of real-world evidence, <em>RWE</em>. And that is this dream that now that we&#8217;ve digitized so much health experience, we should be able to turn all that digital data from people&#8217;s health experiences into new medical knowledge.&nbsp;&nbsp;</p>



<p>But the issue of churn that I think that I would credit you introducing me to calls that into question because you&#8217;re right. Over a four-year period, you don&#8217;t get the longitudinal view of a person&#8217;s health that gives you the ability to get those medical insights. And so something needs to change there. But it&#8217;s very much tied to what consumers want to do. Consumers move around; they change jobs.&nbsp;&nbsp;</p>



<p><strong>FARR: </strong>Yes.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> If it&#8217;s cash-based, they&#8217;ll be shopping based on all sorts of things. And so it &#8230;&nbsp;</p>



<p><strong>FARR: </strong>And so the natural end of all this, it&#8217;s two words: single payer. [LAUGHS] But we don&#8217;t want to go there as a country. So, you know, it sort of left us in this kind of murky middle. And I think a lot about, kind of, what kind of system we&#8217;ll end up having. What I don&#8217;t think is possible is that this current one is sustainable.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> You know, I do think in terms of the payer of CMS [Centers for Medicare and Medicaid Services], Medicare and Medicaid services, the amount of influence that they exert on health spending in the US has been increasing steadily year by year. And in a sense, you could sort of squint and view that as a slow drift towards some element of single payer. But it&#8217;s definitely not so intentional or organized right now.&nbsp;&nbsp;</p>



<p>While we&#8217;re talking about these sorts of trends, of course, another big trend is the <em>graying</em> of America. And we&#8217;re far from alone, China, and much of the Orient, Europe, UK, people are getting older. And from the consumer-patient perspective, this brings up the challenge, I think, that many people have in caring for elderly loved ones.&nbsp;&nbsp;</p>



<p>And this seems to me, like women&#8217;s health, to be another area where if I were starting a new digital health company, I would think very seriously about that space because that&#8217;s another space where there can be extreme intensity of engagement with the healthcare system. Do you as both a human being and consumer but also as an investor, do you think about that space at all?&nbsp;</p>



<p><strong>FARR:</strong> Oh, yes, all the time. And I do think there&#8217;s incredible opportunity here.&nbsp;&nbsp;</p>



<p>And it&#8217;s probably because of the same kind of biases that exist that, you know, didn&#8217;t allow us to see the menopause opportunity, I think we&#8217;re just not seeing this as being as big as it is. And like you said, it&#8217;s not just an American problem. It&#8217;s being felt across the world.&nbsp;&nbsp;</p>



<p>And I do think that there are some, you know, I&#8217;ve seen some really interesting stuff lately. Was recently spending some time with a company called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.cherishhealth.com/" target="_blank" rel="noreferrer noopener">Cherish Health<span class="sr-only"> (opens in new tab)</span></a> out of Boston, and they&#8217;re using AI and radar-based sensing technologies to just be able to stick a device and like really anywhere in the person&#8217;s home. And it just like passively is able to detect falls and also kind of monitor kind of basic health metrics. And because it&#8217;s radar, it can operate through walls. So even if you&#8217;re in the bathroom, it still works, which has been a big problem with a lot of these devices in the past.&nbsp;&nbsp;</p>



<p>And then, you have to have really advanced kind of AI and, you know, this sort of technology to be able to glean whether it&#8217;s a true fall or, you know, that&#8217;s really, you need help or it&#8217;s, you know, just the person sitting down on the floor to play with their grandchild. So things like this are, they&#8217;re still early, but I think really exciting. And we&#8217;re going to see a lot more of that in addition to, you know, some really interesting companies that are trying to think more about sort of social needs that are not healthcare needs, but you know, this, this population needs care, like outside of just, you know, medical treatment. They oftentimes may be experiencing homelessness, they might experience food insecurity, there might be a lack of just caregivers in their life. And so, you know, there are definitely some really interesting businesses there, as well.&nbsp;&nbsp;</p>



<p>And then kind of a, you know, another trend that I think we&#8217;ll see a lot more is that, you know, countries are freaking out about the lack of babies being born, which you need to be able to … you know, I recognize climate change is a huge issue, but you also need babies to be born to support this aging population. So I think we&#8217;re going to see, you know, a lot more interest from these administrations around, you know, both like child tax credits and various policies to support parents but then also IVF [in vitro fertilization] and innovation around technology in the fertility space.<strong>&nbsp;</strong>&nbsp;</p>



<p><strong>LEE:</strong> All right. So we&#8217;re starting to run towards the end of our time together. So I&#8217;d like to get into maybe a couple more provocative or, you know, kinds of questions. So first, and there&#8217;s one that&#8217;s a little bit dark and another that&#8217;s much lighter. So let me start with the darker one so we can have a chance to end on a lighter note. I think one of the most moving pieces I&#8217;ve read from you recently was the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://secondopinion.media/p/a-letter-to-my-kids-about-last-week" target="_blank" rel="noreferrer noopener">open letter to your kids about the assassination of Brian Thompson<span class="sr-only"> (opens in new tab)</span></a>, who&#8217;s a senior executive of UnitedHealth Group. And so I wonder if you&#8217;re willing to share, first off, what you wrote there and then why you felt it was important to do that.&nbsp;</p>



<p><strong>FARR: </strong>Yeah. So, you know, I thought about just not saying anything. That was my original intention because it was just, you know, that moment that it happened, it was just so hot button. And a lot of people have opinions, and Twitter was honestly a scary place, just with the things that people were saying about this individual, who, you know, I think just like had a family and friends and a lot of my network knew him and felt really personally impacted by this. And I, you know, it was just a really sad moment, I think, for a lot of reasons.&nbsp;&nbsp;</p>



<p>And then I just kind of sat down one evening and I wrote this letter to my kids that basically tried to put a lot of this in context. Like what … why are people feeling this way about our healthcare system? You know, why was all this sort of vitriol being really focused on this one individual? And then, you know, I think one of the things I sort of argued in this letter was that there&#8217;s lots of ways to approach innovation in the space. You can do it from the outside in, or you can do it from the inside out.&nbsp;&nbsp;</p>



<p>And I&#8217;ll tell you that a lot of like, I got a lot of emails that week from people who were working at health plans, like UnitedHealth employees, some of them in their 20s, you know, they were recent kind of grads who&#8217;d gone to work at this company. And they said, you know, I felt like I couldn&#8217;t tell my friends, kind of, where I worked that week. And I emailed back and said, “Look, you&#8217;re learning healthcare. You are in an incredible position right now. Like whether you choose to stay your current company or you choose to leave, like you, you understand like the guts and the bowels of healthcare because you&#8217;re working at the largest healthcare company in the world. So you&#8217;re in an enviable position. And I think you are going to be able to effect change, like, more so than anyone else.” And that was part of what I wrote in this letter, that, you know, we should all agree that the system is broken, and we could do better. Nothing about what happened was OK. And also, like, let&#8217;s admire our peers and colleagues that are going into the trenches to learn because I genuinely believe those are the people that, you know, have the knowledge and the contacts and the network to be able to really kind of get change moving along, such desperately needed change.&nbsp;</p>



<p><strong>LEE:</strong> All right. So now one thing I&#8217;ve been asking every guest is about the origin story with respect to your first encounter with generative AI. How did that happen, and what were your first sort of experiences like? You know, what emotionally, intellectually, what went through your mind?&nbsp;</p>



<p><strong>FARR:</strong> So probably my first experience was I was really struggling with the title for my book. And I told ChatGPT what my book was about and what I wanted the title to evoke and asked it for recommendations. And then, I thought the first, like, 20 were actually pretty good. And I was able to say, can you make it a bit more witty? Can you make it more funny? And it spat back out some quite decent titles. And then what was interesting is that it just got worse and worse, like, over time and just ended up, like, deeply cheesy. [LAUGHTER]&nbsp;</p>



<p>And so it sort of both like made me think that this could be a really useful prompt for just brainstorming. But then either it does seem to be some weird thing with AI where, like the more you push it on the same question, it just, like, it doesn&#8217;t &#8230; it seems to have sparked the most creativity in the first few tries, and then it just gets worse. And maybe you know more about this than I would. You certainly know more about this than I do. But that&#8217;s been my kind of general experience of it thus far.&nbsp;</p>



<p><strong>LEE:</strong> Mm-hmm. But would you say you were more skeptical or awe-inspired? What were the emotions at that moment?&nbsp;</p>



<p><strong>FARR: </strong>Um, you know, it was better than, like, a lot of my ideas. [LAUGHTER] So I definitely felt like it was from that perspective very impressive. But then, you know, it seemed to have the same human, like I said, we all kind of run out of ideas at some point and, you know, it turns out, so do the machines.&nbsp;&nbsp;</p>



<p>So that was interesting in and of itself. And I ended up picking, I think a title that was like sort of, you know, inspired by the AI suggestions, but was definitely had its own twist that was my own.&nbsp;</p>



<p><strong>LEE:</strong> Well, Chrissy, I&#8217;ve never known you as someone who runs out of ideas, but this has been just great. As always, I always learn a lot when I have a chance to interact with you or read your writings. And so, thank you again for joining. Just really, really appreciate it.&nbsp;</p>



<p><strong>FARR: </strong>Of course, and next time I want to have you on <em>my</em> podcast because I have a million questions for you, too.&nbsp;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Sure, anytime.&nbsp;</p>



<p><strong>FARR: </strong>Amazing. OK, I&#8217;ll hold you to that. Thanks so much for having me on.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>LEE:</strong> I&#8217;ve always been impressed not only with Chrissy&#8217;s breadth and depth of experience with the emerging tech trends that affect the health industry, but she&#8217;s also a connector to key decision-makers in nearly <em>every</em> sector of healthcare. This experience, plus her communication abilities, make it no surprise that she&#8217;s sought out for help in a range of go-to-market, investor relations, social media, content development, and communications issues.&nbsp;</p>



<p>Maybe it shouldn&#8217;t be a surprise, but one thing I learned from our conversation is that the business of direct-to-consumer health is still emerging. It&#8217;s far from mature. And you can see that Chrissy and her venture-investing colleagues are still trying to figure out what works. Her discussion, for example, on cash-only health delivery and the idea that consumers might not want another app on their phones were indicative of that.&nbsp;&nbsp;</p>



<p>Another takeaway is that some areas, such as pre- and postnatal care, menopause, elder care, and other types of what the health industry might call subacute care are potentially areas where not only AI might find the most impact but also where there&#8217;s sufficient engagement by consumers to make it possible to sustain the business.&nbsp;</p>



<p>When Carey, Zak, and I started writing our book, one of the things that we started off with was based on a story that Zak had written concerning his 90-year-old mother. And of course, as I had said in an <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-an-introduction/" target="_blank" rel="noreferrer noopener">earlier episode of this podcast</a>, that was something that really touched me because I was having a similar struggle with my father, who at the time was 89 years old.&nbsp;</p>



<p>One of the things that was so difficult about caring for my father is that he was living in Los Angeles, and I was living up in the Pacific Northwest. And my two sisters also lived far away from Los Angeles, being in Pittsburgh and in Phoenix.&nbsp;&nbsp;</p>



<p>And so as the three of us, my two sisters and I, tried to navigate a fairly complex healthcare system involving a primary care physician for my father plus two specialists, I have to say over a long period of illness, a lot of things happen, including the fraying of relationships between three siblings. What was so powerful for us, and this is where this idea of patient empowerment comes in, is when we could give all of the data, all of the reports from the specialist, from the primary care physician, other information, give it to GPT-4 and then just ask the question, “We&#8217;re about to have a 15-minute phone call with one of the specialists. What are the most important two or three things we should ask about?” Doing that just brings down the temperature, eliminates a potential source of conflict between siblings who are all just wanting to take care of their father.&nbsp;</p>



<p>And so as we think about the potential of AI in medicine, this concept of patient empowerment, while we&#8217;ve learned in this episode, is still emerging, I think in the long run could be the most important long-term impact of this new age of AI.&nbsp;</p>



<p>[THEME MUSIC]&nbsp;&nbsp;</p>



<p>I&#8217;d like to say thank you again to Dave and Chrissy for sharing their stories and insights. And to our listeners, thank you for joining us. We have some really great conversations planned for the coming episodes, including a discussion on regulations, norms, and ethics developing around AI and health. We hope you&#8217;ll continue to tune in.&nbsp;&nbsp;</p>



<p>Until next time.&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-12"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--13"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-empowering-patients-and-healthcare-consumers-in-the-age-of-generative-ai/">Empowering patients and healthcare consumers in the age of generative AI</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project</title>
		<link>https://www.microsoft.com/en-us/research/blog/engagement-user-expertise-and-satisfaction-key-insights-from-the-semantic-telemetry-project/</link>
		
		<dc:creator><![CDATA[Amber Hoak, Scott Counts, Kate Lytvynets, David Tittsworth, Siddharth Suri, Nirupama Chandrasekaran, Ben Cutler, Weiwei Yang]]></dc:creator>
		<pubDate>Mon, 14 Apr 2025 19:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136235</guid>

					<description><![CDATA[<p>Semantic Telemetry Project data show that people who use AI for more professional and complex tasks are more likely to keep using the tool and to use it more often. Novice AI users engage in simpler tasks, but their usage is becoming more complex.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/engagement-user-expertise-and-satisfaction-key-insights-from-the-semantic-telemetry-project/">Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1.jpg" alt="The image features four white icons on a gradient background that transitions from blue on the left to green on the right. The first icon is a network or molecule structure with interconnected nodes. The second icon is a light bulb, symbolizing an idea or innovation. The third icon is a checklist with three items and checkmarks next to each item. The fourth icon consists of two overlapping speech bubbles, representing communication or conversation." class="wp-image-1136384" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Semantic-Telemetry-2-BlogHeroFeature-1400x788-1-1920x1080.jpg 1920w" sizes="auto, (max-width: 2100px) 100vw, 2100px" /></figure>



<p>The <a href="https://www.microsoft.com/en-us/research/project/semantic-telemetry/" target="_blank" rel="noreferrer noopener">Semantic Telemetry Project</a> aims to better understand complex, turn-based human-AI interactions in Microsoft Copilot using a new data science approach.&nbsp;</p>



<p>This understanding is crucial for recognizing how individuals utilize AI systems to address real-world tasks. It provides actionable insights, enhances key use cases , and identifies opportunities for system improvement.</p>



<p>In a recent <a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/?msockid=0f3bfc3f6f3464c71ab7e9906e196520">blog post</a>, we shared our approach for classifying chat log data using large language models (LLMs), which allows us to analyze these interactions at scale and in near real time. We also introduced two of our LLM-generated classifiers: Topics and Task Complexity. </p>



<p>This blog post will examine how our suite of LLM-generated classifiers can serve as early indicators for user engagement and highlight how usage and satisfaction varies based on AI and user expertise.</p>



<p><strong>The key findings from our research are:&nbsp;</strong></p>



<ul class="wp-block-list">
<li>When users engage in more professional, technical, and complex tasks, they are more likely to continue utilizing the tool and increase their level of interaction with it.&nbsp;</li>



<li>Novice users currently engage in simpler tasks, but their work is gradually becoming more complex over time.&nbsp;</li>



<li>More expert users are satisfied with AI responses only where AI expertise is on par with their own expertise on the topic, while novice users had low satisfaction rates regardless of AI expertise.&nbsp;</li>
</ul>



<p>Read on for more information on these findings. Note that all analyses were conducted on anonymous Copilot in Bing interactions containing no personal information.&nbsp;</p>



<div style="padding-bottom:0; padding-top:0" class="wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner wp-block-msr-immersive-section__inner--narrow">
			<hr class="wp-block-separator has-alpha-channel-opacity"/>



<p><em>Classifiers mentioned in article:&nbsp;</em></p>



<p><strong>Knowledge work classifier</strong>: Tasks that involve creating artifacts related to information work typically requiring creative and analytical thinking. Examples include strategic business planning, software design, and scientific research.&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/?msockid=0f3bfc3f6f3464c71ab7e9906e196520" target="_blank" rel="noreferrer noopener"><strong>Task complexity classifier</strong></a>: Assesses the cognitive complexity of a task if a user performs it without the use of AI. We group into two categories: <em>low complexity</em> and <em>high complexity</em>.&nbsp;</p>



<p><strong>Topics classifier</strong>: A single label for the primary topic of the conversation.</p>



<p><strong>User expertise: </strong>Labels the user’s expertise on the primary topic within the conversation as one of the following categories: Novice (no familiarity with the topic), Beginner (little prior knowledge or experience), Intermediate (some basic knowledge or familiarity with the topic), Proficient (can apply relevant concepts from conversation), and Expert (deep and comprehensive understanding of the topic).&nbsp;</p>



<p><strong>AI expertise: </strong>Labels the AI agent expertise based on the same criteria as user expertise above.&nbsp;</p>



<p><strong>User satisfaction: </strong>A 20-question satisfaction/dissatisfaction rubric that the LLM evaluates to create an aggregate score for overall user satisfaction.&nbsp;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>		</div>
	</div>

	</div>



<h2 class="wp-block-heading" id="what-keeps-bing-chat-users-engaged">What keeps Bing Chat users engaged?&nbsp;</h2>



<p>We conducted a study of a random sample of 45,000 anonymous Bing Chat users during May 2024. The data was grouped into three cohorts based on user activity over the course of the month:&nbsp;</p>



<ul class="wp-block-list">
<li><em>Light </em>(1 active chat session per week)&nbsp;</li>



<li><em>Medium</em> (2-3 active chat sessions per week)&nbsp;</li>



<li><em>Heavy</em> (4+ active chat sessions per week)&nbsp;</li>
</ul>



<p><strong>The key finding is that </strong><strong><em>heavy users</em></strong><strong> are doing more professional, complex work.</strong>&nbsp;</p>



<p>We utilized our <strong>knowledge work classifier</strong> to label the chat log data as relating to knowledge work tasks. What we found is knowledge work tasks were higher in all cohorts, with the highest percentage in <em>heavy users</em>.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2308" height="1429" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work.png" alt="Bar chart illustrating knowledge work distribution across three engagement cohorts: light, medium, and heavy. The chart shows that all three cohorts engage in more knowledge work compared to the 'Not knowledge work' and 'Both' categories, with heavy users performing the most knowledge work. " class="wp-image-1136247" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work.png 2308w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-300x186.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-1024x634.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-768x476.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-1536x951.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-2048x1268.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_1_Engagement-Knowledge-Work-240x149.png 240w" sizes="auto, (max-width: 2308px) 100vw, 2308px" /><figcaption class="wp-element-caption"><em>Figure 1: Knowledge work based on engagement cohort</em></figcaption></figure>



<p>Analyzing task complexity, we observed that users with higher engagement frequently perform the highest number of tasks with high complexity, while users with lower engagement performed more tasks with low complexity.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1798" height="1087" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity.png" alt="Bar chart illustrating task complexity distribution across three engagement cohorts: light, medium, and heavy. The chart shows all three cohorts perform more high complexity tasks than low complexity tasks, with heavy users performing the greatest number of high complexity tasks. " class="wp-image-1136248" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity.png 1798w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity-300x181.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity-1024x619.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity-768x464.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity-1536x929.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_2_Engagement-task-complexity-240x145.png 240w" sizes="auto, (max-width: 1798px) 100vw, 1798px" /><figcaption class="wp-element-caption"><em>Figure 2: High complexity and low complexity tasks by engagement cohort+</em>&nbsp;</figcaption></figure>



<p>Looking at the overall data, we can filter on heavy users and see higher numbers of chats where the user was performing <em>knowledge work</em> tasks. Based on task complexity, we see that most <em>knowledge work </em>tasks seek to apply a solution to an existing problem, primarily within <em>programming and scripting</em>. This is in line with our top overall topic, <em>technology</em>, which we discussed in the previous post.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2161" height="1219" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view.png" alt="Tree diagram illustrating how heavy users are engaging with Bing Chat. The visual selects the most common use case for heavy users: knowledge work, “apply” complexity and related topics.  " class="wp-image-1136249" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view.png 2161w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-1024x578.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-768x433.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-1536x866.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-2048x1155.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-1066x600.png 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_3_Engagement-heavy-users-tree-view-640x360.png 640w" sizes="auto, (max-width: 2161px) 100vw, 2161px" /><figcaption class="wp-element-caption"><em>Figure 3: Heavy users tree diagram</em>&nbsp;</figcaption></figure>



<p>In contrast, <em>light users</em> tended to do more <em>low complexity </em>tasks (<a href="https://www.microsoft.com/en-us/research/blog/semantic-telemetry-understanding-how-users-interact-with-ai-systems/?msockid=061a2f481be2670621233a8a1a5866f2">“Remember”</a>), using Bing Chat like a traditional search engine and engaging more in topics like <em>business and finance </em>and <em>computers and electronics.</em></p>



<figure class="wp-block-image aligncenter size-large"><img loading="lazy" decoding="async" width="1024" height="580" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-1024x580.png" alt="Tree diagram illustrating how light users are engaging with Bing Chat. The visual selects the most common use case for light users: knowledge work, “remember” complexity and related topics. " class="wp-image-1136250" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-1024x580.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-300x170.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-768x435.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-1536x870.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-2048x1161.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_4_Engagement-light-users-tree-view-240x136.png 240w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption"><em>Figure 4: Light users tree diagram</em>&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="novice-queries-are-becoming-more-complex">Novice queries are becoming more complex&nbsp;</h2>



<p>We looked at Bing Chat data from January through August 2024 and we classified chats using our <strong>User Expertise</strong> classifier. When we looked at how the different user expertise groups were using the tool for professional tasks, we discovered that proficient and expert users tend to do more professional tasks with high complexity in topics like <em>programming and scripting, professional writing and editing</em>, <em>and physics and chemistry</em>.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1576" height="1135" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics.png" alt="Bar chart illustrating top topics for proficient and expert users with programming and scripting (18.3%), professional writing and editing (10.4%), and physics and chemistry (9.8%) as top three topics. " class="wp-image-1136251" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics.png 1576w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics-300x216.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics-1024x737.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics-768x553.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics-1536x1106.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_5_Proficent-Expert-top-topics-240x173.png 240w" sizes="auto, (max-width: 1576px) 100vw, 1576px" /><figcaption class="wp-element-caption"><em>Figure 5: Top topics for proficient/expert users</em>&nbsp;</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1329" height="667" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity.png" alt="Bar chart showing task complexity for proficient and expert users. The chart shows a greater number of high complexity chats than low complexity chats, with the highest percentage in categories “Understand” (30.8%) and “Apply” (29.3%). " class="wp-image-1136252" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity.png 1329w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity-300x151.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity-1024x514.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity-768x385.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_6_Proficent-expert-task-complexity-240x120.png 240w" sizes="auto, (max-width: 1329px) 100vw, 1329px" /><figcaption class="wp-element-caption"><em>Figure 6: Task complexity for proficient/expert</em>&nbsp;</figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1882" height="1386" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics.png" alt="Bar chart illustrating top topics for novice users with business and finance (12.5%), education and learning (10.0%), and computers and electronics (9.8%) as top three topics. " class="wp-image-1136253" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics.png 1882w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-300x221.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-1024x754.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-768x566.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-1536x1131.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-80x60.png 80w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_7_Novice-top-topics-240x177.png 240w" sizes="auto, (max-width: 1882px) 100vw, 1882px" /><figcaption class="wp-element-caption"><em>Figure 7: Top topics for novices</em>&nbsp;</figcaption></figure>



<p>In contrast, novice users engaged more in professional tasks relating to <em>business and finance</em> and <em>education and learning, </em>mainly using the tool to recall information.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1588" height="799" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity.png" alt="Bar chart showing task complexity for novice users. The chart shows a greater number of low complexity chats than high complexity chats, with the highest percentage in categories “Remember” (48.6%). " class="wp-image-1136254" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity.png 1588w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity-300x151.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity-1024x515.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity-768x386.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity-1536x773.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_8_novice-task-complexity-240x121.png 240w" sizes="auto, (max-width: 1588px) 100vw, 1588px" /><figcaption class="wp-element-caption"><em>Figure 8: Task complexity for novices</em>&nbsp;</figcaption></figure>



<p>However, novices are targeting increasingly more complex tasks over time. Over the eight-month period, we see the percentage of high complexity tasks rise from about 36% to 67%, revealing that novices are learning and adapting quickly (see Figure 9).&nbsp;</p>



<figure class="wp-block-image aligncenter size-large"><img loading="lazy" decoding="async" width="1024" height="672" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-1024x672.png" alt="Line chart showing weekly percentage of high complexity chats for novice users from January-August 2024. The line chart starts at 35.9% in January and ends at 67.2% in August.  " class="wp-image-1136255" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-1024x672.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-300x197.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-768x504.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-1536x1007.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice-240x157.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_9_weekly-high-complexity-novice.png 1807w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption"><em>Figure 9: High complexity for novices Jan-Aug 2024</em>&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="how-does-user-satisfaction-vary-according-to-expertise">How does user satisfaction vary according to expertise?&nbsp;</h2>



<p>We classified both the user expertise and AI agent expertise for anonymous interactions in Copilot in Bing. We compared the level of user and AI agent expertise with our <strong>user satisfaction classifier</strong>.&nbsp;</p>



<p><strong>The key takeaways are:</strong>&nbsp;</p>



<ul class="wp-block-list">
<li>Experts and proficient users are only satisfied with AI agents with similar expertise (expert/proficient).&nbsp;</li>



<li>Novices are least satisfied, regardless of the expertise of the AI agent.&nbsp;</li>
</ul>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2548" height="859" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI.png" alt="Table illustrating user satisfaction based on expertise level of user and agent. Each row if the table is the user expertise group (novice, beginner, intermediate, proficient, expert) and on the columns is AI expertise group (novice, beginner, intermediate, proficient, expert). The table illustrates that novice users are least satisfied overall and expert/proficient users are satisfied with AI expertise of proficient/expert.  " class="wp-image-1136256" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI.png 2548w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-300x101.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-1024x345.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-768x259.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-1536x518.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-2048x690.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Figure_10_Sat-by-User-AI-240x81.png 240w" sizes="auto, (max-width: 2548px) 100vw, 2548px" /><figcaption class="wp-element-caption"><em>Figure 10: Copilot in Bing satisfaction intersection of AI expertise and User expertise (August-September 2024)</em>&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>Understanding these metrics is vital for grasping user behavior over time and relating it to real-world business indicators. Users are finding value from complex professional knowledge work tasks, and novices are quickly adapting to the tool and finding these high value use-cases. By analyzing user satisfaction in conjunction with expertise levels, we can tailor our tools to better meet the needs of different user groups. Ultimately, these insights can help improve user understanding across a variety of tasks.&nbsp;&nbsp;</p>



<p>In our next post, we will examine the engineering processes involved in LLM-generated classification.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/engagement-user-expertise-and-satisfaction-key-insights-from-the-semantic-telemetry-project/">Engagement, user expertise, and satisfaction: Key insights from the Semantic Telemetry Project</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Debug-gym: an environment for AI coding tools to learn how to debug code like programmers</title>
		<link>https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers/</link>
		
		<dc:creator><![CDATA[Eric Yuan, Morgane Moss, Charbel Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, Marc-Alexandre Côté]]></dc:creator>
		<pubDate>Thu, 10 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136154</guid>

					<description><![CDATA[<p>Developers spend a lot of time debugging code. Learn how debug-gym can equip AI agents to help, enabling them to set breakpoints, navigate the codebase, and print runtime variable values on demand, so they better understand the code and its execution flow.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers/">Debug-gym: an environment for AI coding tools to learn how to debug code like programmers</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2100" height="1182" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1.jpg" alt="A graphic with a gradient background transitioning from blue on the left to pink on the right. The graphic features a white outline of a computer monitor with code brackets on the screen, an arrow pointing downwards into the monitor, and another arrow curving around to point upwards towards a magnifying glass with a bug icon inside it." class="wp-image-1136365" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1.jpg 2100w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/NEWDeBug-BlogHeroFeature-1400x788-1-1920x1080.jpg 1920w" sizes="auto, (max-width: 2100px) 100vw, 2100px" /></figure>



<p>The ongoing proliferation of AI coding tools is not only boosting developers’ efficiency, it also signals a future where AI will generate a growing share of all new code. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.freethink.com/robots-ai/github-copilot" target="_blank" rel="noreferrer noopener">GitHub CEO Thomas Dohmke<span class="sr-only"> (opens in new tab)</span></a> predicted as much in 2023, when he said that &#8220;sooner than later, 80% of the code is going to be written by Copilot.&#8221;&nbsp;&nbsp;</p>



<p>Both large and small software companies are already heavily using AI to generate code. <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=coojA-odaTk&t=861s" target="_blank" rel="noreferrer noopener">Y Combinator’s Garry Tan<span class="sr-only"> (opens in new tab)</span></a> noted that 95% of code for a quarter of Y Combinator’s latest batch of startups was written by large language models.</p>



<p>In fact, <strong>most developers spend the majority of their time debugging code,</strong> not writing it. As maintainers of popular open-source repositories, this resonates with us. But what if an AI tool could propose fixes for hundreds of open issues, and all we had to do was approve them before merging? This was what motivated us to maximize the potential time savings from AI coding tools by teaching them to debug code.&nbsp;</p>



<p>By debugging we mean the interactive, iterative process to fix code. Developers typically hypothesize why their code crashed, then gather evidence by stepping through the program and examining variable values. They often use debugging tools like pdb (Python debugger) to assist in gathering information. This process is repeated until the code is fixed.</p>



<p>Today&#8217;s AI coding tools boost productivity and excel at suggesting solutions for bugs based on available code and error messages. However, unlike human developers, these tools don&#8217;t seek additional information when solutions fail, leaving some bugs unaddressed, as you can see in this simple <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/debug-gym/" target="_blank" rel="noreferrer noopener">demo of how a mislabeled column stumps today’s coding tools<span class="sr-only"> (opens in new tab)</span></a>. This may leave users feeling like AI coding tools don’t understand the full context of the issues they are trying to solve.&nbsp;</p>



<h2 class="wp-block-heading" id="introducing-debug-gym">Introducing debug-gym</h2>



<p>A natural research question emerges: <strong>to what degree can LLMs use interactive debugging tools such as pdb?</strong> To explore this question, we released <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/debug-gym/" target="_blank" rel="noreferrer noopener"><strong>debug-gym</strong><span class="sr-only"> (opens in new tab)</span></a> – an environment that allows code-repairing agents to access tools for active information-seeking behavior. Debug-gym expands an agent’s action and observation space with feedback from tool usage, enabling setting breakpoints, navigating code, printing variable values, and creating test functions. Agents can interact with tools to investigate code or rewrite it, if confident. We believe interactive debugging with proper tools can empower coding agents to tackle real-world software engineering tasks and is central to LLM-based agent research. The fixes proposed by a coding agent with debugging capabilities, and then approved by a human programmer, will be grounded in the context of the relevant codebase, program execution and documentation, rather than relying solely on guesses based on previously seen training data.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1503" height="488" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2.png" alt="Figure 1: Diagram demonstrating the code-repairing process in outline. Left: conventional code-repairing system; right: additional tools enabled by debug-gym." class="wp-image-1136156" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2.png 1503w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2-300x97.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2-1024x332.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2-768x249.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_intro_diagram_v2-240x78.png 240w" sizes="auto, (max-width: 1503px) 100vw, 1503px" /><figcaption class="wp-element-caption">Figure 1: Diagram demonstrating the code-repairing process in outline. In most existing approaches (shown in <strong>black</strong>), an agent rewrites its code conditioned on error messages obtained from executing the code. debug-gym equips the agent with additional tools such as pdb (shown in <strong>red</strong>), so it can interactively seek necessary information from the semantic space hidden behind the code and therefore have better code-repairing performance.</figcaption></figure>



<p>Debug-gym is designed and developed to:</p>



<ul class="wp-block-list">
<li><strong>Handle repository-level information</strong>: the full repository is available to agents in debug-gym, allowing them to navigate and edit files.</li>



<li><strong>Be robust and safe</strong>: to safeguard both the system and the development process, debug-gym runs code within sandbox Docker containers. This isolates the runtime environment, preventing harmful actions while still allowing thorough testing and debugging.&nbsp;&nbsp;</li>



<li><strong>Be easily extensible</strong>: debug-gym was conceived with extensibility in mind and provides practitioners with the possibility of easily adding new tools.&nbsp;&nbsp;</li>



<li><strong>Be text-based</strong>: debug-gym represents observation information in structured text (e.g., JSON format) and defines a simple syntax for text actions, making the environment fully compatible with modern LLM-based agents.</li>
</ul>



<p>With debug-gym, researchers and developers can specify a folder path to work with any custom repository to evaluate their debugging agent&#8217;s performance. Additionally, debug-gym includes three coding benchmarks to measure LLM-based agents’ performance in interactive debugging: Aider for simple function-level code generation, Mini-nightmare for short, hand-crafted buggy code examples, and SWE-bench for real-world coding problems requiring a comprehensive understanding of a large codebase and a solution in the format of a GitHub pull request.</p>



<p>To learn more about debug-gym and start using it to train your own debugging agents, please refer to the <a href="https://www.microsoft.com/en-us/research/publication/debug-gym-a-text-based-environment-for-interactive-debugging/" target="_blank" rel="noreferrer noopener">technical report<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/debug-gym" target="_blank" rel="noreferrer noopener">GitHub<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<h2 class="wp-block-heading" id="early-experimentation-promising-signal">Early experimentation: promising signal</h2>



<p>For our initial attempt to validate that LLMs perform better on coding tests when they have access to debugging tools, we built a simple prompt-based agent and provided it with access to the following debug tools: eval, view, pdb, rewrite, and listdir. We used nine different LLMs as the backbone for our agent. Detailed results can be found in the <a href="https://www.microsoft.com/en-us/research/publication/debug-gym-a-text-based-environment-for-interactive-debugging/">technical report<span class="sr-only"> (opens in new tab)</span></a><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/2503.21557" target="_blank" rel="noreferrer noopener">.<span class="sr-only"> (opens in new tab)</span></a></p>



<p>Even with debugging tools, our simple prompt-based agent rarely solves more than half of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.swebench.com/lite.html" target="_blank" rel="noreferrer noopener">SWE-bench <span class="sr-only"> (opens in new tab)</span></a>Lite issues. We believe this is due to the scarcity of data representing sequential decision-making behavior (e.g., debugging traces) in the current LLM training corpus. However, the significant performance improvement (as shown in the most promising results in the graph below) validates that this is a promising research direction.&nbsp;</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1934" height="974" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart.png" alt="Figure 2: The success rate represents the percentage of the 300 SWE-bench Lite issues resolved, comparing between agents with and without debugging tools." class="wp-image-1136159" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart.png 1934w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart-300x151.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart-1024x516.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart-768x387.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart-1536x774.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/DeBug_froggy_bar_chart-240x121.png 240w" sizes="auto, (max-width: 1934px) 100vw, 1934px" /><figcaption class="wp-element-caption">Figure 2: The success rate represents the percentage of the 300 SWE-bench Lite issues resolved. The green bars indicate the performance of the agent with debugging tools, while the gray bars show the performance of the agent without debugging tools. Note that both agents use the same backbone LLM to make decisions and propose code edits.</figcaption></figure>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1085520">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft research podcast</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/podcast/collaborators-silica-in-space-with-richard-black-and-dexter-greene/" aria-label="Collaborators: Silica in space with Richard Black and Dexter Greene" data-bi-cN="Collaborators: Silica in space with Richard Black and Dexter Greene" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/09/Richard-and-Dexter_Collaborators_Hero_Feature_No_Text_1400x788-1.jpg" alt="Headshots of Richard Black and Dexter Greene for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Collaborators: Silica in space with Richard Black and Dexter Greene</h2>
				
								<p class="large">College freshman Dexter Greene and Microsoft research manager Richard Black discuss how technology that stores data in glass is supporting students as they expand earlier efforts to communicate what it means to be human to extraterrestrials.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/podcast/collaborators-silica-in-space-with-richard-black-and-dexter-greene/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Listen now" data-bi-cN="Collaborators: Silica in space with Richard Black and Dexter Greene" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="future-work">Future work</h2>



<p>We believe that training or fine-tuning LLMs can enhance their interactive debugging abilities. This requires specialized data, such as trajectory data that records agents interacting with a debugger to gather information before suggesting a fix. Unlike conventional reasoning problems, interactive debugging involves generating actions at each step that trigger feedback from the environment. This feedback helps the agent make new decisions, requiring dense data like the problem description and the sequence of actions leading to the solution.&nbsp;</p>



<p>Our plan is to fine-tune an info-seeking model specialized in gathering the necessary information to resolve bugs. The goal is to use this model to actively build relevant context for a code generation model. If the code generation model is large, there is an opportunity to build a smaller info-seeking model that can provide relevant information to the larger one, e.g., a generalization of retrieval augmented generation (RAG), thus saving AI inference costs. The data collected during the reinforcement learning loop to train the info-seeking model can also be used to fine-tune larger models for interactive debugging.</p>



<p>We are open-sourcing debug-gym to facilitate this line of research. We encourage the community to help us advance this research towards building interactive debugging agents and, more generally, agents that can seek information by interacting with the world on demand.</p>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We thank Ruoyao Wang for their insightful discussion on building interactive debugging agents, Chris Templeman and Elaina Maffeo for their team coaching, Jessica Mastronardi and Rich Ciapala for their kind support in project management and resource allocation, and Peter Jansen for providing valuable feedback for the technical report.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/debug-gym-an-environment-for-ai-coding-tools-to-learn-how-to-debug-code-like-programmers/">Debug-gym: an environment for AI coding tools to learn how to debug code like programmers</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of April 7, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-7-2025/</link>
		
		<dc:creator><![CDATA[Microsoft Research Team]]></dc:creator>
		<pubDate>Wed, 09 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1136066</guid>

					<description><![CDATA[<p>In this issue: We introduce a new dataset designed to assist renewable energy infrastructure planners, a new method for denoising MRI imagery, and an AI tool for analyzing distant galaxies. Check out our latest research and other updates. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-7-2025/">Research Focus: Week of April 7, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p>We introduce a new dataset designed to assist renewable energy infrastructure planners, a new method for denoising MRI imagery, and an AI tool for analyzing distant galaxies. Check out our latest research and other updates.&nbsp;</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus -- Week of April 7" class="wp-image-1136071" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/RF59-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="global-renewables-watch-a-temporal-dataset-of-solar-and-wind-energy-derived-from-satellite-imagery">Global Renewables Watch: A Temporal Dataset of Solar and Wind Energy Derived from Satellite Imagery</h3>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="899" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-scaled.jpg" alt="A 2-panel figure. The left panel shows a global map with the distribution of 86,410 solar PV installations points and 375,197 onshore windmills points detected by our models in 2024 Q2. The right panel shows satellite imagery with annotated solar and wind installations over the village of Farmsum in the Dutch province of Groningen. " class="wp-image-1136208" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-300x105.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-1024x360.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-768x270.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-1536x540.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-2048x719.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Global-Renewables-Watch-FIG1-1-240x84.jpg 240w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /></figure>



<p>Siting renewable energy infrastructure requires careful consideration of the potential impact on ecosystems, cultural and historical resources, agriculture, and scenic landscapes. To help policymakers, researchers, and other stakeholders assess strategies for deployment, researchers from Microsoft, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.org/en-us/" target="_blank" rel="noreferrer noopener">The Nature Conservancy<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.planet.com/" target="_blank" rel="noreferrer noopener">Planet<span class="sr-only"> (opens in new tab)</span></a> present a comprehensive global temporal dataset of commercial solar photovoltaic (PV) farms and onshore wind turbines.</p>



<p>The researchers built the dataset by training deep learning-based segmentation models on high-resolution satellite imagery and then deploying them on over 13 trillion pixels of images covering the world. The final spatial dataset includes 375,197 individual wind turbines and 86,410 solar photovoltaic installations. For each detected feature, they estimate the construction date and the preceding land use type, and aggregate their findings to the country level, along with estimates of total power capacity.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-12 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--14"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/global-renewables-watch-a-temporal-dataset-of-solar-and-wind-energy-derived-from-satellite-imagery/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="snraware-improved-deep-learning-mri-denoising-with-snr-unit-training-and-g-factor-map-augmentation">SNRAware: Improved Deep Learning MRI Denoising with SNR Unit Training and G-factor Map Augmentation</h3>



<p>This research proposes a new training method, SNRAware, to improve the ability of deep learning models to denoise—or remove unwanted random variations—from MRI images. MRI images can suffer from high levels of noise when scanning is accelerated with parallel imaging or when data are acquired using lower cost, low-field MRI systems. &nbsp;</p>



<p>The researchers tested SNRAware on 14 different models, including ones based on transformer and convolutional architectures. The proposed training scheme improved the performance of all the tested models. This broad applicability means that the method is flexible and can be applied to different kinds of models without redesigning them. The testing showed SNRAware significantly improves the quality and clinical utility of MRI images while preserving important diagnostic details.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1152" height="648" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/SNRAware.gif" alt="The movies correspond to the example in Figure 1b. The ground-truth clean 
image is the single one on the left.  The first row are the noisy samples. The second 
row are the SNR images. " class="wp-image-1136115"/></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-13 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--15"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/snraware-improved-deep-learning-mri-denoising-with-snr-unit-training-and-g-factor-map-augmentation/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="can-ai-unlock-the-mysteries-of-the-universe">Can AI unlock the mysteries of the universe? </h3>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1200" height="627" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1.jpg" alt="An astronomer’s workflow involves using a space telescope to observe a large number galaxies. Astronomers identify “interesting” phenomena and attempt to explain them through a series of physical models." class="wp-image-1136077" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1.jpg 1200w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1-300x157.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1-1024x535.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1-768x401.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/galaxies_analysis-TWLIFB-1200x627-1-240x125.jpg 240w" sizes="auto, (max-width: 1200px) 100vw, 1200px" /></figure>



<p>Analyzing the physical properties of individual galaxies is a fundamental skill in astronomy. It requires a thorough understanding of galaxy formation theories and the ability to interpret vast amounts of observational data. However, even for seasoned astronomers, this process can be time-consuming and labor-intensive. To help astronomers accelerate this fundamental process, researchers from Microsoft and external colleagues introduce <a href="https://www.microsoft.com/en-us/research/publication/interpreting-multi-band-galaxy-observations-with-large-language-model-based-agents/">Mephisto,</a> research designed to analyze extremely distant galaxies observed by the James Webb Space Telescope (JWST).</p>



<p>Mephisto analyzes photometric data from distant galaxies, proposing physical models and interacting with <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cigale.lam.fr/" target="_blank" rel="noreferrer noopener">Code Investigating Galaxy Emission<span class="sr-only"> (opens in new tab)</span></a>, a commonly used galaxy spectral simulation program. Mephisto can detect discrepancies between models and observational data, identifies potential instrumental errors or limitations in the models, iteratively adjusts parameters, and generates multiple explanations for the observational data.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-14 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--16"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/articles/can-ai-unlock-the-mysteries-of-the-universe/">Read the article</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-7f9ceb7f278a1e69211971cf8e80d961" id="applied-ai">APPLIED AI</h2>



<h3 class="wp-block-heading h2" id="japan-airlines-new-ai-app-will-make-it-easier-for-cabin-attendants-to-report-inflight-events-with-microsoft-s-phi-4-small-language-model">Japan Airlines’ new AI app will make it easier for cabin attendants to report inflight events with Microsoft’s Phi-4 small language model</h3>



<p>Japan Airlines (JAL) is using technology developed by Microsoft Research to deploy an AI app that helps flight crews communicate more effectively with ground staff when something unexpected comes up during a flight.</p>



<p>The JAL-AI Report is being developed using Microsoft’s Phi-4 small language model (SLM), which requires less computing power than the large language models (LLMs) most generative AI tools run on, so it can be used offline on a device for specific tasks.</p>



<p>Cabin attendants who have tried it say it can slash the time for writing operation reports by up to two thirds, say, from one hour to 20 minutes, or from 30 minutes to 10 for simpler cases.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-15 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--17"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://news.microsoft.com/source/asia/features/japan-airlines-new-ai-app-will-make-it-easier-for-cabin-attendants-to-report-inflight-events-with-microsofts-phi-4-small-language-model/" target="_blank" rel="noreferrer noopener">Read the story</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>



<div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="microsoft-research-in-case-you-missed-it">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">Microsoft Research | In case you missed it</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="AI weather forecast project eyes access through desktop computers"
						href="https://www.ft.com/content/73492128-5822-4bb2-b953-64217eb303e4"
					>
						<span>AI weather forecast project eyes access through desktop computers</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>Financial Times | March 20, 2025</p><p>Aardvark Weather uses AI to deliver accurate forecasts in just minutes from a desktop computer. Developed by scientists at the University of Cambridge, with support from the Alan Turing Institute, Microsoft Research, and the European Centre for Medium-Range Weather Forecasts, this technology is tens of times faster than existing methods and requires only a fraction of the computing power.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Director of Microsoft Research talks AI for science (what it really means)"
						href="https://www.youtube.com/watch?v=rfyS_rLOKUc"
					>
						<span>Director of Microsoft Research talks AI for science (what it really means)</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>The Deep View | March 11, 2025</p><p>Chris Bishop, Director, AI for Science, Microsoft Research, discusses what AI is doing for science. This interview dives into how AI is accelerating discovery of new techniques and findings, the benefits of foundation models like Aurora, MatterGen’s capabilities, and AI’s impact on scientists.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Microsoft’s Christopher Bishop: Scientific discovery is AI’s killer application"
						href="https://archive.ph/I6K88#selection-1571.0-1571.79"
					>
						<span>Microsoft’s Christopher Bishop: Scientific discovery is AI’s killer application</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>Financial Times | April 3, 2025</p><p>Christopher Bishop runs Microsoft’s AI for Science research unit, which applies the powerful technology to the natural sciences. Bishop sees the mission of the lab, which was founded in 2022, as accelerating scientific discovery using the technology.</p><p>In this conversation with the Financial Times’ AI editor Madhumita Murgia, he explains why he believes scientific discovery will prove to be the single most important application of the technology.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Innovation to Impact (ft. Dr M - DGTL Voices with Ed Marx)"
						href="https://podcasts.apple.com/us/podcast/innovation-to-impact-ft-dr-matthew-lungren-dr-jonathan/id1552066046?i=1000698934775"
					>
						<span>Innovation to Impact (ft. Dr M &#8211; DGTL Voices with Ed Marx)</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>DGTL Voices with Ed Marx | March 12, 2025</p><p>Matthew Lungren, Chief Scientific Officer, Microsoft Health and Life Sciences, and Jonathan Carlson, Managing Director, Microsoft Health Futures, discuss AI&#8217;s transformative impact on radiology and the importance of collaboration in research and product development. They highlight how healthcare organizations can leverage Microsoft&#8217;s resources for innovation, emphasizing Microsoft&#8217;s progress in developing radiology-specific multimodal models and its broader work in healthcare.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Tech Life - The doctor will see you now"
						href="https://www.bbc.co.uk/sounds/play/w3ct5wnm"
					>
						<span>Tech Life &#8211; The doctor will see you now</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>BBC Sounds | March 4, 2025</p><p>An update from the live trials in Ghana of Microsoft Research&#8217;s Holoportation 3D telemedicine technology. BBC&#8217;s Tech Life speaks to lead researcher Spencer Fowers, as well as a patient and doctor benefiting from the portable kit.</p><p>Related video: <a href="https://newsroom.ap.org/editorial-photos-videos/detail?itemid=997c14f31e214dd1b9c7a09af8a79d27" target="_blank" rel="noreferrer noopener">3D telemedicine offers help to sick Ghanaians in remote locations</a></p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Microsoft Unveils New AI Model to Edit Video Games"
						href="https://spectrum.ieee.org/ai-video-games"
					>
						<span>Microsoft Unveils New AI Model to Edit Video Games</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>IEEE Spectrum | March 11, 2025</p><p>Lead researcher Katja Hoffman discusses Microsoft&#8217;s Muse, a transformer model with 1.6 billion parameters trained on 500,000 hours of player data that can generate gameplay examples from a single screenshot.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="National University of Singapore collaborates with Microsoft Research Asia to advance AI research and cultivate computing talent"
						href="https://news.nus.edu.sg/nus-microsoft-research-asia-advance-ai-research-cultivate-computing-talent/"
					>
						<span>National University of Singapore collaborates with Microsoft Research Asia to advance AI research and cultivate computing talent</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>NUS News | April 2, 2025</p><p>The National University of Singapore (NUS) has signed a five-year collaboration agreement with Microsoft Research Asia for a Joint PhD Supervision Program, bringing together NUS&#8217;s academic and research excellence with Microsoft Research Asia’s global leadership in AI, computing research, and industrial applications to cultivate talent. As part of this collaboration, NUS and Microsoft Research Asia will nurture PhD students through the Industrial Postgraduate Program, supported by the Singapore Economic Development Board (EDB). This initiative will help to cultivate interdisciplinary, high-caliber tech professionals and drive the integration of AI technology across industries.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="How Microsoft made it through 50 years"
						href="https://www.theverge.com/microsoft/643246/microsoft-50-business-model-cloud-ai"
					>
						<span>How Microsoft made it through 50 years</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>The Verge | April 4, 2025</p><p>A lot has changed since Microsoft was founded, but in many ways, the company’s core business model and ethos remain the same: make software that everyone needs and get it installed everywhere. Adapting to change, including the ongoing AI transformation, has always played an important role in the company&#8217;s success.</p>				</div>
			
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-april-7-2025/">Research Focus: Week of April 7, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Real-world healthcare AI development and deployment—at scale</title>
		<link>https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/</link>
		
		<dc:creator><![CDATA[Peter Lee, Matthew Lungren, Seth Hain]]></dc:creator>
		<pubDate>Thu, 03 Apr 2025 13:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135527</guid>

					<description><![CDATA[<p>Microsoft’s Dr. Matthew Lungren and Epic’s Seth Hain discuss the challenges and opportunities of leveraging generative AI for enhanced patient care and improved clinical documentation and recordkeeping at scale—plus what’s next for the technology in the field.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/">Real-world healthcare AI development and deployment—at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg" alt="AI Revolution podcast | Episode 2 - Real-world healthcare AI development and deployment—at scale | outline illustration of Seth Hain, Peter Lee, Dr. Matthew Lungren" class="wp-image-1135817" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/04/Episode2-Peter-Seth-Matt-AIRevolution_Hero_Feature_No_Text_1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=144098908&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this episode, <a href="https://www.microsoft.com/en-us/research/people/mlungren/" target="_blank" rel="noreferrer noopener">Dr. Matthew Lungren<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/seth-hain-12760647/" target="_blank" rel="noreferrer noopener">Seth Hain<span class="sr-only"> (opens in new tab)</span></a>, leaders in the implementation of healthcare AI technologies and solutions at scale, join Lee to discuss the latest developments. Lungren, the chief scientific officer at Microsoft Health and Life Sciences, explores the creation and deployment of generative AI for automating clinical documentation and administrative tasks like clinical note-taking. Hain, the senior vice president of R&D at the healthcare software company Epic, focuses on the opportunities and challenges of integrating AI into electronic health records at global scale, highlighting AI-driven workflows, decision support, and Epic’s Cosmos project, which leverages aggregated healthcare data for research and clinical insights.&nbsp;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more">Learn more:</h2>



<p><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/03/03/meet-microsoft-dragon-copilot-your-new-ai-assistant-for-clinical-workflow/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">Meet Microsoft Dragon Copilot: Your new AI assistant for clinical workflow</a>&nbsp;<br>Microsoft Industry Blog | March 2025&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2024/10/10/unlocking-next-generation-ai-capabilities-with-healthcare-ai-models/" target="_blank" rel="noreferrer noopener">Unlocking next-generation AI capabilities with healthcare AI models</a>&nbsp;<br>Microsoft Industry Blog | October 2024&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/articles/multimodal-generative-ai-the-next-frontier-in-precision-health/" target="_blank" rel="noreferrer noopener">Multimodal Generative AI: the Next Frontier in Precision Health</a>&nbsp;<br>Microsoft Research Forum | March 2024&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/learning/an-introduction-to-how-generative-ai-will-transform-healthcare" target="_blank" rel="noreferrer noopener">An Introduction to How Generative AI Will Transform Healthcare with Dr. Matthew Lungren<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>LinkedIn Learning&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/video/ai-for-precision-health/" target="_blank" rel="noreferrer noopener">AI for Precision Health</a>&nbsp;<br>Video | July 2023&nbsp;</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/chexnet-radiologist-level-pneumonia-detection-on-chest-x-rays-with-deep-learning/" target="_blank" rel="noreferrer noopener">CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning</a>&nbsp;<br>Publication | December 2017&nbsp;</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cosmos.epic.com/" target="_blank" rel="noreferrer noopener">Epic Cosmos<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Homepage</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a>&nbsp;<br>Book | April 2023</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC] &nbsp;</p>



<p>[BOOK PASSAGE]  &nbsp;</p>



<p><strong>PETER LEE:</strong> “It&#8217;s hard to convey the huge complexity of today&#8217;s healthcare system. Processes and procedures, rules and regulations, and financial benefits and risks all interact, evolve, and grow into a giant edifice of paperwork that is well beyond the capability of any one human being to master. This is where the assistance of an AI like GPT-4 can be not only useful—but crucial.”&nbsp;&nbsp;&nbsp;</p>



<p>[END OF BOOK PASSAGE] &nbsp;</p>



<p>[THEME MUSIC] &nbsp;</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. &nbsp;</p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong?  &nbsp;</p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.</p>



				</span>
				<span id="show-more-show-less-toggle-18" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The passage I read at the top there is from Chapter 7 of the book, “The Ultimate Paperwork Shredder.”&nbsp;&nbsp;</p>



<p>Paperwork plays a particularly important role in healthcare. It helps convey treatment information that supports patient care, and it’s also used to help demonstrate that providers are meeting regulatory responsibilities, among other things. But if we’re being honest, it’s taxing—for everyone—and it’s a big contributor to the burnout our clinicians are experiencing today. Carey, Zak, and I identified this specific pain point as one of the best early avenues to pursue as far as putting generative AI to good work in the healthcare space. &nbsp;</p>



<p>In this episode, I’m excited to welcome Dr. Matt Lungren and Seth Hain to talk about matching technological advancements in AI to clinical challenges, such as the paperwork crisis, to deliver solutions in the clinic and in the health system back office. &nbsp;</p>



<p>Matt is the chief scientific officer for Microsoft Health and Life Sciences, where he focuses on translating cutting-edge technology, including generative AI and cloud services, into innovative healthcare applications. He&#8217;s a clinical interventional radiologist and a clinical machine learning researcher doing collaborative research and teaching as an adjunct professor at Stanford University. His scientific work has led to more than 200 publications, including work on new computer vision and natural language processing approaches for healthcare. &nbsp;</p>



<p>Seth is senior vice president of research and development at Epic, a leading healthcare software company specializing in electronic health record systems, also known as <em>EHR</em>, as well as other solutions for connecting clinicians and patients. During his 19 years at Epic, Seth has worked on enhancing the core analytics and other technologies in Epic&#8217;s platforms as well as their applications across medicine,<strong> </strong>bringing together his graduate training in mathematics and his dedication to better health. &nbsp;</p>



<p>I&#8217;ve had the pleasure of working closely with both Matt and Seth. Matt, as a colleague here at Microsoft, really focused on our health and life sciences business. And Seth, as a collaborator at Epic, as we embark on the questions of how to integrate and deploy generative AI into clinical applications at scale.  &nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Here&#8217;s my conversation with Dr. Matt Lungren: &nbsp;</p>



<p><strong>LEE:</strong> Matt, welcome. It&#8217;s just great to have you here.&nbsp;</p>



<p><strong>MATTHEW LUNGREN:</strong> Thanks so much, Peter. Appreciate being here.&nbsp;</p>



<p><strong>LEE:</strong> So, I&#8217;d like to just start just talking about you. You know, I had mentioned your role as the chief scientific officer for Microsoft Health and Life Sciences. Of course, that&#8217;s just a title. So, what the heck is that? What is your job exactly? And, you know, what does a typical day at work look like for you?&nbsp;</p>



<p><strong>LUNGREN: </strong>So, really what you could boil my work down to is essentially cross collaboration, right. We have a very large company, lots of innovation happening all over the place, lots of partners that we work with and then obviously this sort of healthcare mission.</p>



<p>And so, what innovations, what kind of advancements are happening that can actually solve clinical problems, right, and sort of kind of direct that. And we can go into some examples, you know, later. But then the other direction, too, is important, right. So, identifying problems that may benefit from a technologic application or solution and kind of translating that over into the, you know, pockets of innovation saying, “Hey, if you kind of tweaked it this way, this is something that would really help, you know, the clinical world.”&nbsp;&nbsp;</p>



<p>And so, it&#8217;s really a bidirectional role. So, my day to day is … every day is a little different, to be honest with you. Some days it&#8217;s very much in the science and learning about new techniques. On the other side, though, it can be very much in the clinic, right. So, what are the pain points that we&#8217;re seeing? Where are the gaps in the solutions that we&#8217;ve already rolled out? And, you know, again, what can we do to make healthcare better broadly?&nbsp;</p>



<p><strong>LEE:</strong> So, you know, I think of you as a technologist, and, Matt, you and I actually are colleagues working together here at Microsoft. But you also do spend time in the clinic still, as well, is that right?&nbsp;</p>



<p><strong>LUNGREN:</strong> You know, initially it was kind of a … very much a non-negotiable for me … in sort of taking an industry role. I think like a lot of, you know, physicians, you know, we&#8217;re torn with the idea of like, hey, I spent 20 years training. I love what I do, you know, with a lot of caveats there in terms of some of the administrative burden and some of the hassle sometimes. But for the most part, I love what I do, and there&#8217;s no greater feeling than using something that you trained years to do and actually see the impact on a human life. It&#8217;s unbelievable, right.&nbsp;&nbsp;</p>



<p>So, I think part of me was just, like, I didn&#8217;t want to let that part of my identity go. And frankly, as I often say, to this day, I walk by a fax machine in our office <em>today</em>, like in 2025.&nbsp;&nbsp;</p>



<p>So just to be extra clear, it really grounds me in, like, yes, I love the possibilities. I love thinking about what we can do. But also, I have a very stark understanding of the reality on the ground, both in terms of the technology but also the burnout, right. The challenges that we&#8217;re facing in taking care of patients has gotten, you know, much, much more difficult in the last few years, and, you know, I like to think it keeps my perspective, yeah.&nbsp;</p>



<p><strong>LEE: </strong>You know, I think some listeners to this podcast might be surprised that we have doctors on staff in technical roles at Microsoft. How do you explain that to people?&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Yeah, no, yeah, it is interesting. I would say that, you know, from, you know, the legacy Nuance <a href="#ftn_1">[1]</a> world, it wasn&#8217;t so far-fetched that you have physicians that were power users and eventually sort of, you know, became, “Hey, listen, I think this is a strategic direction; you should take it&#8221; or whatever. And certainly maybe in the last, I want to say, five years or so, I&#8217;ve seen more and more physicians who have, you know, taken the time, sometimes on their own, to learn some of the AI capabilities, learn some of the principles and concepts; and frankly, some are, you know, even coding solutions and leading companies.</p>



<p>So, I do think that that has shifted a bit in terms of like, “Hey, doctor, this is your lane, and over here, you know, here&#8217;s a technical person.” And I think that&#8217;s fused quite a bit more.&nbsp;&nbsp;</p>



<p>But yeah, it is an unusual thing, I think, in sort of how we&#8217;ve constructed what at least my group does. But again, I can&#8217;t see any other way around some of the challenges.&nbsp;&nbsp;</p>



<p>I think, you know, an anecdote I’d like to tell you, when I was running the AIMI [Artificial Intelligence in Medicine and Imaging] Center, you know, we were bringing the medical school together with the computer science department, right, at Stanford. And I remember one day a student, you know, very smart, came into my office, you know, a clinical day or something, and he&#8217;s like, is there just, like, a book or something where I can just learn medicine? Because, like, I feel like there&#8217;s a lot of, like, translation you have to do for me.&nbsp;&nbsp;</p>



<p>It really raised an important insight, which is that you can learn the, you know, medicine, so to speak. You know, go to med school; you know, take the test and all that. But it really … you don&#8217;t really understand the practice of medicine until you are doing that.&nbsp;&nbsp;</p>



<p>And in fact, I even push it a step further to say after training those first two or three years of … <em>you</em> are the responsible person; you can turn around, and there&#8217;s no one there. Like, you are making a decision. Getting used to that and then having a healthy respect for that actually I think provides the most educational value of anything in healthcare.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>You know, I think what you&#8217;re saying is so important because as I reflect on my own journey. Of course, I&#8217;m a computer scientist. I don&#8217;t have medical training, although at this point, I feel confident that I could pass a Step 1 medical exam.&nbsp;&nbsp;</p>



<p><strong>LUNGREN: </strong>I have no doubt. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> But I think that the tech industry, because of people like you, have progressed tremendously in having a more sophisticated and nuanced understanding of what actually goes on in clinic and also what goes on in the boardrooms of healthcare delivery organizations. And of course, at the end of the day, I think that&#8217;s really been your role.&nbsp;&nbsp;</p>



<p>So roughly speaking, your job as an executive at a big tech company has been to understand what the technology platforms need to be, particularly with respect to machine learning, AI, and cloud computing, to best support healthcare. And so maybe let&#8217;s start <em>pre</em>-GPT-4, <em>pre</em>-ChatGPT, and tell us a little bit, you know, about maybe some of your proudest moments in getting advanced technologies like AI into the clinic.&nbsp;</p>



<p><strong>LUNGREN:</strong> You know, when I first started, so remember, like you go all the way back to about 2013, right, my first faculty job, and, you know, we&#8217;re building a clinical program and I, you know, I had a lot of interest in public health and building large datasets for pop [population] health, etc. But I was doing a lot of that, you know, sort of labeling to get those insights manually, right. So, like, I was the person that you&#8217;d probably look at now and say, “What are you doing?” Right?&nbsp;&nbsp;</p>



<p>So … but I had a complete random encounter with Andrew Ng, who I didn&#8217;t know at the time, at Stanford. And I, you know, went to one of the seminars that he was holding at the Gates building, and, you know, they were talking about their performance on ImageNet. You know, cat and dog and, you know, tree, bush, whatever. And I remember sitting in kind of the back, and I think I maybe had my scrubs on at the time and just kind of like, what? Like, why … like, this … we could use this in healthcare, you know. [LAUGHS]&nbsp;&nbsp;</p>



<p>But for me, it was a big moment. And I was like, this is huge, right. And as you remember, the deep learning really kind of started to show its stuff with, you know, Fei-Fei Li&#8217;s ImageNet stuff.</p>



<p>So anyway, we started the collaboration that actually became <strong>a NIDUS</strong>. And one of the first things we worked on, we just said, “Listen, one of the most common medical imaging examinations in the world is the chest x-ray.” Right? Two, three billion are done every year in the world, and so is that not a great place to start?</p>



<p>And of course, we had a very democratizing kind of mission. As you know, Andrew has done a lot of work in that space, and I had similar ambitions. And so, we really started to focus on bringing the, you know, the sort of the clinical and the CS together and see what could be done.&nbsp;&nbsp;</p>



<p>So, we did <a href="https://www.microsoft.com/en-us/research/publication/chexnet-radiologist-level-pneumonia-detection-on-chest-x-rays-with-deep-learning/" target="_blank" rel="noreferrer noopener">CheXNet</a>. And this is, remember this is around the time when, like, Geoffrey Hinton was saying things like we should stop training radiologists, and all this stuff was going on. [LAUGHTER] So there&#8217;s a lot of hype, and this is the narrow AI days just to remind the audience.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> How did you feel about that since you <em>are</em> a radiologist?&nbsp;</p>



<p><strong>LUNGREN:</strong> Well, it was so funny. So, Andrew is obviously very prolific on social media, and I was, who am I, right? So, I remember he tagged me. Well, first he said, “Matt, you need to get a Twitter account.” And I said OK. And he tagged me on the very first post of our, what we call, CheXNet that was kind of like the “Hello, World!” for this work.&nbsp;&nbsp;</p>



<p>And I remember it was a clinical day. I had set my phone, as you do, outside the OR. I go in. Do my procedure. You know, hour or so, come back, my phone&#8217;s dead. I&#8217;m like, oh, that&#8217;s weird. Like I had a decent charge. So, you know, I plug it in. I turn it on. I had like hundreds of thousands of notifications because Andrew had tweeted out to his millions or whatever about CheXNet.&nbsp;&nbsp;</p>



<p>And so, then of course, as you point out, I go to RSNA that year, which is our large radiology conference, and that Geoffrey Hinton quote had come out. And everyone&#8217;s looking at me like, “What are you doing, Matt?” You know, like, are you coming after our specialty? I&#8217;m like, “No, no,” that&#8217;s, [LAUGHS] you know, it&#8217;s a way to interpret it, but you have to take a much longer horizon view, right.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Well, you know, we&#8217;re going to, just as an enticement for listeners to this podcast to listen to the very end, I&#8217;m going to pin you down toward the end on your assessment of whether Geoffrey Hinton will eventually be proven right or not. [LAUGHTER] But let&#8217;s take our time to get there.&nbsp;&nbsp;</p>



<p>Now let&#8217;s go ahead and enter the generative AI era. When we were first exposed to what we now know of as GPT-4—this was before it was disclosed to the world—a small number of people at Microsoft and Microsoft Research were given access in order to do some technical assessment.&nbsp;&nbsp;</p>



<p>And, Matt, you and I were involved very early on in trying to assess what might this technology mean for medicine.<strong> </strong>Tell us, you know, what was the first encounter with this new technology like for you?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> It was the weirdest thing, Peter. Like … I joined that summer, so the summer before, you know, the actual GPT came out. I had literally no idea what I was getting into.&nbsp;&nbsp;</p>



<p>So, I started asking it questions, you know, kind of general stuff, right. Just, you know, I was like, oh, all right, it&#8217;s pretty good. And so, then I would sort of go a little deeper. And eventually I got to the point where I&#8217;m asking questions that, you know, maybe there&#8217;s three papers on it in my community, and remember I&#8217;m a sub-sub specialist, right, <em>pediatric interventional radiology</em>. And the things that we do in vascular malformations and, you know, rare cancers are really, really strange and not very commonly known.&nbsp;&nbsp;</p>



<p>And I kind of walked away from that—first I said, can I have this thing, right? [LAUGHS]&nbsp;&nbsp;</p>



<p>But then I, you know, I don&#8217;t want to sound dramatic, but I didn&#8217;t sleep that well, if I&#8217;m being honest, for the first few nights. Partially because I couldn&#8217;t tell anybody, except for the few that I knew were involved, and partially because I just couldn&#8217;t wrap my head around how we went from what I was doing in LSTMs [long short-term memory networks], right, which was state of the artish at the time for NLP [natural language processing].&nbsp;&nbsp;</p>



<p>And all of a sudden, I have this thing that is broadly, you know, domain experts, you know, representations of knowledge that there&#8217;s no way you could think of it would be in distribution for a normal approach to this.&nbsp;&nbsp;</p>



<p>And so, I really struggled with it, honestly. Interpersonally, like, I would be like, uh, well, let&#8217;s not work on that. They&#8217;re like, why not? You were just excited about it last week. I&#8217;m like, I don&#8217;t know. I think that we could think of another approach later.<strong> </strong>[LAUGHS]&nbsp;&nbsp;</p>



<p>And so yeah, when we were finally able to really look at some of the capabilities and really think clearly, it was really clear that we had a massive opportunity on our hands to impact healthcare in a way that was never possible before.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, and at that time you were still a part of Nuance. Nuance, I think, was in the process of being acquired by Microsoft. Is that right?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> That’s right.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> And so, of course, this was also a technology that would have profound and very direct implications for Nuance. How did you think about that?&nbsp;</p>



<p><strong>LUNGREN:</strong> Nuance, for those in the audience who don&#8217;t know, for 25 years was, sort of, <em>the</em> medical speech-to-text thing that all, you know, physicians used. But really the brass ring had always been … and I want to say going back to like 2013, 2014, Nuance had tried to figure out, OK, we see this pain point. Doctors are typing on their computers while they&#8217;re trying to talk to their patients, right.&nbsp;&nbsp;</p>



<p>We should be able to figure out a way to get that ambient conversation turned into text that then, you know, accelerates the doctor … takes all the important information. That&#8217;s a really hard problem, right. You&#8217;re having a conversation with a patient about their knee pain, but you&#8217;re also talking about, you know, their cousin&#8217;s wedding and their next vacation and their dog is sick or whatever and all that gets recorded, right.&nbsp;&nbsp;</p>



<p>And so, then you have to have the intelligence/context to be able to tease out what&#8217;s important for a note. And then it has to be at the performance level that a physician who, again, 20 years of training and education plus a huge, huge amount of, you know, need to get through his cases efficiently, that&#8217;s a really difficult problem.&nbsp;&nbsp;</p>



<p>And so, for a long time, there was a human-in-the-loop aspect to doing this because you needed a human to say, “This transcript&#8217;s great, but here&#8217;s actually what needs to go on the note.” And that can&#8217;t scale, as you know.&nbsp;&nbsp;</p>



<p>When the GPT-4, you know, model kind of, you know, showed what it was capable of, I think it was an immediate light bulb because there was no … you can ask any physician in your life, anyone in the audience, you know, what are your … what is the biggest pain point when you go to see your doctor? Like, “Oh, they don&#8217;t talk to me. They don&#8217;t look me in the eye. They&#8217;re rushing around trying to finish a note.”&nbsp;&nbsp;</p>



<p>If we could get that off their plate, that&#8217;s a huge unlock, Peter. And I think that, again, as you know, it&#8217;s now led to so much more. But that was kind of the initial, I think, reaction.&nbsp;</p>



<p><strong>LEE: </strong>And so, maybe that gets us into our next set of questions, our next topic, which is about the book and all the predictions we made in the book. Because Carey, Zak, and I—actually we did make a prediction that this technology would have a huge impact on this problem of clinical note-taking.&nbsp;&nbsp;</p>



<p>And so, you&#8217;re just right in the middle of that. You&#8217;re directly hands-on creating, I think, what is probably the most popular early product for doing exactly that. So, were we right? Were we wrong? What else do we need to understand about this?&nbsp;</p>



<p><strong>LUNGREN:</strong> No, you were right on. I think in the book, I think you called it like a paper shredder or something. I think you used a term like that. That&#8217;s exactly where the activity is right now and the opportunity.&nbsp;&nbsp;</p>



<p>I&#8217;ve even taken that so far as to say that when folks are asking about what the technology is capable of doing, we say, well, listen, it&#8217;s going to save time before it saves lives. It&#8217;ll do both. But right now, it&#8217;s about saving time.&nbsp;&nbsp;</p>



<p>It&#8217;s about peeling back the layers of the onion that if you, you know, put me in where I started medicine in 2003, and then fast-forward and showed me a day in the life of 2025, I would be shocked at what I was doing that <em>wasn&#8217;t</em> related to patient care, right. So, all of those layers that have been stacked up over the years, we can start finding ways to peel that back. And I think that&#8217;s exactly what we&#8217;re seeing.</p>



<p>And to your point, I think you mentioned this, too, which is, well, sure, we can do this transcript, and we can turn a note, but then we can do other things, right. We can summarize that in the patient&#8217;s language or education level of choice. We can pend orders. We can eventually get to a place of decision support. So, “Hey, did you think about this diagnosis, doctor?” Like those kinds of things.&nbsp;&nbsp;</p>



<p>And all those things, I think you highlighted beautifully, and again, it sounds like with, you know, a lot of, right, just kind of guesswork and prediction, but those things are actually happening every single day <em>right now</em>.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Well, so now, you know, in this episode, we&#8217;re really trying to understand, you know, where the technology industry is in delivering these kinds of things. And so from your perspective, you know, in the business that you&#8217;re helping to run here at Microsoft, you know, what are the things that are actually shipping as product versus things that clinicians are doing, let&#8217;s say, off label, just by using, say, ChatGPT on their personal mobile devices, and then what things aren&#8217;t happening?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah. I&#8217;ll start with the shipping part because I think you, again, you know my background, right. Academic clinician, did a lot of research, hadn&#8217;t had a ton of <em>product</em> experience.&nbsp;&nbsp;</p>



<p>In other words, like, you know, again, I&#8217;m happy to show you what benchmarks we beat or a new technique or, you know, get a grant to do all this, or even frankly, you know, talk about startups. But to actually have an audience that is accustomed to a certain level of performance for the solutions that they use, to be able to deliver something new at that same level of expectation, wow, that&#8217;s a big deal.&nbsp;&nbsp;</p>



<p>And again, this is part of the learning by, you know, kind of being around this environment that we have, which is we have this, you know, incredibly focused, very experienced clinical product team, right.</p>



<p>And then I think on the other side, to your point about the general-purpose aspect of this, it&#8217;s no secret now, right, that, you know, this is a useful technology in a lot of different medical applications. And let&#8217;s just say that there&#8217;s a lot of knowledge that can be used, particularly by the physician community. And I think the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://informatics.bmj.com/content/31/1/e101102" target="_blank" rel="noreferrer noopener">most recent survey I saw</a> was from the British Medical Journal, which said, hey, you know, which doctors are using … are you willing to tell us, you know, what you&#8217;re doing? And it turns out that folks are, what, 30% or so said that they were using it regularly in clinic <a href="#ftn_2">[2]</a>. And again, this is the general, this is the API or whatever off the shelf.</p>



<p>And then frankly, when they ask what they&#8217;re using it for, tends to be things like, “Hey, differential, like, help me fill in my differential or suggest … ” and to me, I think what that created, at least—and you&#8217;re starting to see this trend really accelerate in the US especially—is, well, listen, we can&#8217;t have everybody pulling out their laptops and potentially exposing, you know, patient information by accident or something to a public API.&nbsp;&nbsp;</p>



<p>We have to figure this out, and so brilliantly, I think NYU [New York University] was one of the first. Now I think there&#8217;s 30 plus institutions that said, listen, “OK, we know this is useful to the entire community in the healthcare space.” Right?<strong> </strong>We know the administrators and nurses and everybody thinks this is great.&nbsp;&nbsp;</p>



<p>We can&#8217;t allow this sort of to be a very loosey-goosey approach to this, right, given this sort of environment. So, what we&#8217;ll do is we&#8217;ll set up a HIPAA-compliant instance to allow anyone in the community—you know, in the health system—to use the models, and then whatever, the newest model comes, it gets hosted, as well.&nbsp;&nbsp;</p>



<p>And what&#8217;s cool about that—and that&#8217;s happened now a lot of places—is that at the high level … first of all, people get to use it and experiment and learn. But at the high level, they&#8217;re actually seeing what are the common use cases. Because you could ask 15 people and you might get super long lists, and it may not help you decide what to operationalize in your health system.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>But let me ask you about that. When you observe that, are there times when you think, “Oh, some specific use cases that we&#8217;re observing in that sort of organic way need to be taken into specialized applications and made into products?” Or is it best to keep these things sort of, you know, open-chat-interface types of general-purpose platform?&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> Honestly, it&#8217;s both, and that&#8217;s exactly what we&#8217;re seeing. I&#8217;m most familiar with Stanford, kind of, the work that Nigam Shah leads on this. But he, he basically, … you know, there&#8217;s a really great paper that is coming out in JAMA, but basically saying, “Here&#8217;s what our workforce is using it for. Here are the things in the literature that would suggest what would be popular.”&nbsp;&nbsp;</p>



<p>And some of those line up, like helping with a clinical diagnosis or documentation, but some of them don&#8217;t. But for the most part, the stuff that flies to the top, those are opportunities to operationalize and productize, etc. And I think that&#8217;s exactly what we&#8217;re seeing.&nbsp;</p>



<p><strong>LEE: </strong>So, let&#8217;s get into some of the specific predictions. We&#8217;ve, I think, beaten note-taking to death here. But there&#8217;s other kinds of paperwork, like filling out prior authorization request forms or referral letters, an after-visit note or summary to give instructions to patients, and so on. And these were all things that we were making guesses in our book might be happening. What&#8217;s the reality there?&nbsp;</p>



<p><strong>LUNGREN:</strong> I&#8217;ve seen every single one of those. In fact, I&#8217;ve probably seen a dozen startups too, right, doing exactly those things. And, you know, we touched a little bit on translation into the actual clinic. And that&#8217;s actually another thing that I used to kind of underappreciate, which is that, listen, you can have a computer scientist and a physician or nurse or whatever, like, give the domain expertise, and you think you&#8217;re ready to build something.&nbsp;&nbsp;</p>



<p>The health IT [LAUGHS] is another part of that Venn diagram that&#8217;s so incredibly critical, and then exactly how are you going to bring that into the system. That&#8217;s a whole new ballgame.&nbsp;</p>



<p>And so I do want to do a callout because the collaboration that we have with Epic is monumental because here, you have the system of record that most physicians, at least in the US, use. And they&#8217;re going to use an interface and they&#8217;re going to have an understanding of, hey, we know these are pain points, and so I think there&#8217;s some really, really cool, you know, new innovations that are coming out of the relationship that we have with Epic. And certainly the audience may be familiar with those, that I think will start to knock off a lot of the things that you predicted in your book relatively soon.&nbsp;</p>



<p><strong>LEE: </strong>I think most of the listeners to this podcast will know what Epic is. But for those that are unfamiliar with the health industry, and especially the technology foundation, Epic is probably the largest provider of electronic health record systems. And, of course, in collaboration with you and your team, they&#8217;ve been integrating generative AI quite a bit. Are there specific uses that Epic is making and deploying that get you particularly excited?&nbsp;</p>



<p><strong>LUNGREN:</strong> First of all, the ambient note generation, by the way, is integrated into Epic now. So like, you know, it&#8217;s not another screen, another thing for physicians. So that&#8217;s a huge, huge unlock in terms of the translation.</p>



<p>But then Epic themselves, so they have, I guess, on the last roadmap that they talked [about], more than 60, but the one that&#8217;s kind of been used now is this inbox response.&nbsp;</p>



<p>So again, maybe someone might not be familiar with, why is it such a big deal? Well, if you&#8217;re a physician, you already have, you know, 20 patients to see that day and you got all those notes to do, and then Jevons paradox, right. So if you give me better access to my doctor, well, maybe I won&#8217;t make an appointment. I&#8217;m just going to send him a note and this is kind of this inbox, right.&nbsp;&nbsp;</p>



<p>So then at the end of my day, I got to get all my notes done. And then I got to go through all the inbox messages I&#8217;ve received from all of my patients and make sure that they&#8217;re not like having chest pain and they&#8217;re blowing it off or something.&nbsp;&nbsp;</p>



<p>Now that&#8217;s a lot of work and the cold start problem of like, OK, I to respond to them. So Epic has leveraged this system to say, “Let me just draft a note for you,” understanding the context of, you know, what&#8217;s going on with the patient, etc. And you can edit that and sign it, right. So you can accelerate some of those … so that&#8217;s probably one I&#8217;m most excited about. But there&#8217;s so many right now.&nbsp;</p>



<p><strong>LEE: </strong>Well, I think I need to let you actually state the name of the clinical note-taking product that you&#8217;re associated with. Would you like to do that? [LAUGHS]&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Sure. Yeah, it&#8217;s called DAX Copilot <a href="#ftn_3">[3]</a>. And for the record, it is the fastest-growing copilot in the Microsoft ecosystem. We&#8217;re very proud of that. Five hundred institutions already are using it, and millions of notes have already been created with it. And the feedback has been tremendous.</p>



<p><strong>LEE: </strong>So, you sort of referred to this a little bit, you know, this idea of AI being a second set of eyes. So, doctor makes some decisions in diagnosis or kind of working out potential treatments or medication decisions. And in the book, you know, we surmise that, well, AI might not replace the doctor doing those things. It could but might not. But AI could possibly reduce errors if doctors and nurses are making decisions by just looking at those decisions and just checking them out. Is that happening at all, and what do you see the future there?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, I would say, you know, that&#8217;s kind of the jagged edge of innovation, right, where sometimes the capability gets ahead of the ability to, you know, operationalize that. You know, part of that is just related to the systems. The evidence has been interesting on this. So, like, you know this, our colleague Eric Horvitz has been doing a lot of work in sort of looking at physician, physician with GPT-4, let&#8217;s say, and then GPT-4 alone for a whole variety of things. You know, we&#8217;ve been saying to the world for a long time, particularly in the narrow AI days, that AI plus human is better than either alone. We&#8217;re not really seeing that bear out really that well yet in some of the research.&nbsp;&nbsp;</p>



<p>But it is a signal to me and to the use case you&#8217;re suggesting, which is that if we let this system, in the right way, kind of handle a lot of the safety-net aspects of what we do but then also potentially take on some of the things that maybe are not that challenging or at least somewhat simple.&nbsp;&nbsp;</p>



<p>And of course, this is really an interesting use case in my world, in the vision world, which is that we know these models are multimodal, right. They can process images and text. And what does that look like for pathologists or radiologists, where we do have a certain percentage of the things we look at in a given day are normal, right? Or as close to normal as you can imagine. So is there a way to do that? And then also, by the way, have a safety net.&nbsp;&nbsp;</p>



<p>And so I think that this is an extremely active area right now. I don&#8217;t think we&#8217;ve figured out exactly how to have the human and AI model interact in this space yet. But I know that there&#8217;s a lot of attempts at it right now.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think, you know, this idea of a true copilot, you know, a true collaborator, you know, I think is still something that&#8217;s coming. I think we&#8217;ve had a couple of decades of people being trained to think of computers as question-answering machines. Ask a question, get an answer. Provide a document, get a summary. And so on.&nbsp;&nbsp;</p>



<p>But the idea that something might actually be this second set of eyes just assisting you all day continuously, I think, is a new mode of interaction. And we haven&#8217;t quite figured that out.&nbsp;&nbsp;</p>



<p>Now, in preparation for this podcast, Matt, you said that you actually used AI to assist you in getting ready. [LAUGHS] Would you like to share what you learned by doing that?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, it&#8217;s very funny. So, like, you may have heard <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.oneusefulthing.org/p/detecting-the-secret-cyborgs" target="_blank" rel="noreferrer noopener">this term coined by Ethan Mollick called the “secret cyborg,”<span class="sr-only"> (opens in new tab)</span></a> which is sort of referring to the phenomena of folks using GPT, realizing it can actually help them a ton in all kinds of parts of their work, but not necessarily telling anybody that they&#8217;re using it, right.&nbsp;&nbsp;</p>



<p>And so in a similar secret cyborgish way, I was like, “Well, listen, you know, I haven&#8217;t read your book in like a year. I recommend it to everybody. And [I need] just a refresher.” So what I did was I took your book, I put it into GPT-4, OK, and asked it to sort of talk about the predictions that you made.&nbsp;&nbsp;</p>



<p>And then I took that and put it in the stronger reasoning model—in this case, the “deep research” that you may have just seen or heard of and the audience from OpenAI—and asked it to research all the current papers, you know, and blogs and whatever else and tell me like what was right, what was wrong in terms of the predictions. [LAUGHS]&nbsp;&nbsp;</p>



<p>So it, actually, it was an incredible thing. It&#8217;s a, like, what, six or seven pages. It probably would have taken me two weeks, frankly, to do this amount of work.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>I&#8217;ll be looking forward to reading that in the New England Journal of Medicine shortly.&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] That&#8217;s right. Yeah, no, don&#8217;t, before this podcast comes out, I&#8217;ll submit it as an opinion piece. No. [LAUGHS] But, yeah, but I think on balance, incredibly insightful views. And I think part of that was, you know, your team that got together really had a lot of different angles on this. But, you know, and I think the only area that was, like, which I&#8217;ve observed as well, it&#8217;s just, man, this can do a lot for education.&nbsp;&nbsp;</p>



<p>We haven&#8217;t seen … I don&#8217;t think we&#8217;re looking at this as a tutor. To your point, we&#8217;re kind of looking at it as a transactional in and out. But as we&#8217;ve seen in all kinds of data, both in low-, middle-income countries and even in Harvard, using this as a tutor can really accelerate your knowledge and in profound ways.&nbsp;&nbsp;</p>



<p>And so that is probably one area where I think your prediction was maybe slightly even further ahead of the curve because I don&#8217;t think folks have really grokked that opportunity yet.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, and for people who haven&#8217;t read the book, you know, the guess was that you might use this as a training aid if you&#8217;re an aspiring doctor. For example, you can ask GPT-4 to pretend to be a patient that presents a certain way and that you are the doctor that this patient has come to see. And so you have an interaction. And then when you say end of encounter, you ask GPT-4 to assess how well you did. And we thought that this might be a great training aid, and to your point, it seems not to have materialized.&nbsp;&nbsp;</p>



<p><strong>LUNGREN:</strong> There&#8217;s some sparks. You know, with, like, communication, end-of-life conversations that no physician loves to have, right. It&#8217;s very, very hard to train someone in those. I&#8217;ve seen some work done, but you&#8217;re right. It&#8217;s not quite hit mainstream yet.&nbsp;</p>



<p><strong>LEE:</strong> On the subject of things that we missed, one thing that you&#8217;ve been very, very involved in in the last several months has been in shipping products that are multimodal. So that was something I think that we missed completely. What is the current state of affairs for multimodal, you know, healthcare AI, medical AI?&nbsp;</p>



<p><strong>LUNGREN:</strong> Yeah, the way I like to explain it—and first of all, no fault to you, but this is not an area that, like, we were just so excited about the text use cases that I can&#8217;t fault you. But yeah, I mean, so if we look at healthcare, right, how we take care of patients today, as you know, the vast majority of the data in terms of just data itself is actually not in text, right. It&#8217;s going be in pathology and genomics and radiology, etc.&nbsp;&nbsp;</p>



<p>And it seems like an opportunity here to watch this huge curve just goes straight up in the general reasoning and frankly medical competency and capabilities of the models that are coming and continue to come but then to see that it&#8217;s not as proficient for medical-specific imaging and video and, you know, other data types. And that gap is, kind of, what I describe as the multimodal medical AI gap.&nbsp;&nbsp;</p>



<p>We&#8217;re probably in GPT-2 land, right, for this other modality types versus the, you know, we&#8217;re now at o3, who knows where we&#8217;re going to go. At least in our view, we can innovate in that space.&nbsp;&nbsp;</p>



<p>How do we help bring those innovations to the broader community to close that gap and see some of these use cases really start to accelerate in the multimodal world?&nbsp;&nbsp;</p>



<p>And I think we&#8217;ve taken a pretty good crack at that. A lot of that is credit to the innovative work. I mean, MSR [Microsoft Research] was two or three years ahead of everyone else on a lot of this. And so how do we package that up in a way that the community can actually access and use? And so, we took a lot of what your group had done in, let&#8217;s just say, radiology or pathology in particular, and say, “OK, well, let&#8217;s put this in an ecosystem of other models.” Other groups can participate in this, but let&#8217;s put it in a platform where maybe I&#8217;m really competent in radiology or pathology. How do I connect those things together? How do I bring the general reasoner knowledge into a multimodal use case?&nbsp;&nbsp;</p>



<p>And I think that&#8217;s what we&#8217;ve done pretty well so far. We have a lot of work to do still, but this is very, very exciting. We&#8217;re seeing just such a ton of interest in building with the tools that we put out there.&nbsp;</p>



<p><strong>LEE:</strong> Well, I think how rapidly that&#8217;s advancing has been a surprise to me. So I think we&#8217;re running short on time. So two last questions to wrap up this conversation. The first one is, as we think ahead on AI in medicine, what do you think will be the biggest changes or make the biggest differences two years from now, five years from now, 10 years from now?</p>



<p><strong>LUNGREN:</strong> This is really tough. OK. I think the two-year timeframe, I think we will have some autonomous agent-based workflows for a lot of the &#8230; what I would call undifferentiated heavy lifting in healthcare.&nbsp;&nbsp;</p>



<p>And this is happening in, you know, the pharmaceutical industry, the payer … every aspect is sort of looking at their operations at a macro level: where are these big bureaucratic processes that largely involve text and where can we shrink those down and really kind of unlock a lot of our workforce to do things that might be more meaningful to the business? I think that&#8217;s my safe one.&nbsp;&nbsp;</p>



<p>Going five years out, you know, I have a really difficult time grappling with this seemingly shrinking timeline to AGI [artificial general intelligence] that we hear from people who I would respect and certainly know more than me. And in that world, I think there&#8217;s only been <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/abs/10.1056/AIp2400559" target="_blank" rel="noreferrer noopener">one paper that I&#8217;ve seen that has attempted to say, what does that mean in healthcare<span class="sr-only"> (opens in new tab)</span></a> when we have this?&nbsp;&nbsp;</p>



<p>And the fact is, I actually don&#8217;t know. [LAUGHS] I wonder whether there&#8217;ll still be a gap in some modalities. Maybe there&#8217;ll be the ability to do new science, and all kinds of interesting things will come of that.&nbsp;&nbsp;</p>



<p>But then if you go all the way to your 10-year, I do feel like we&#8217;re going to have systems that are acting autonomously in a variety of capacities, if I&#8217;m being honest.&nbsp;&nbsp;</p>



<p>What I would like to see if I have any influence on some of this is, can we start to celebrate the closing of hospitals instead of opening them? Meaning that, can we actually start to address—at a personal, individual level—care? And maybe that&#8217;s outside the home, maybe that&#8217;s, you know, in a way that doesn&#8217;t have to use so many resources and, frankly, really be very reactive instead of proactive.&nbsp;&nbsp;</p>



<p>I really want to see that. That&#8217;s been the vision of precision medicine for, geez, 20-plus years. I feel like we&#8217;re getting close to that being something we can really tackle.&nbsp;</p>



<p><strong>LEE: </strong>So, we talked about Geoff Hinton and his famous prediction that we would soon not have human radiologists. And of course, maybe he got the date wrong. So, let&#8217;s reset the date to 2028. So, Matt, do you think Geoff is right or wrong?&nbsp;</p>



<p><strong>LUNGREN:</strong> [LAUGHS] Yeah, so the way … I&#8217;m not going to dodge the question, but let me just answer this a different way.&nbsp;&nbsp;</p>



<p>We have a clear line of sight to go from images to draft reports. That is unmistakable. And that&#8217;s now in 2025. How it will be implemented and what the implications of that will be, I think, will be heavily dependent on the health system or the incentive structure for where it&#8217;s deployed.&nbsp;&nbsp;</p>



<p>So, if I&#8217;m trying to take a step back, back to my global health days, man, that can&#8217;t come fast enough. Because, you know, you have entire health systems, you know, in fact entire countries that have five, you know, medical imaging experts for the whole country, but they still need this to you know take care of patients.&nbsp;&nbsp;</p>



<p>Zooming in on today&#8217;s crisis in the US, right, we have the burnout crisis just as much as the doctors who are seeing patients and write notes. We can&#8217;t keep up with the volume. In fact, we&#8217;re not training folks fast enough, so there is a push pull; there may be a flip to your point of autonomous reads across some segments of what we do.&nbsp;&nbsp;</p>



<p>By 2028, I think that&#8217;s a reasonable expectation that we&#8217;ll have some form of that. Yes.&nbsp;</p>



<p><strong>LEE: </strong>I tend to agree, and I think things get reshaped, but it seems very likely that even far into the future we&#8217;ll have humans wanting to take care of other humans and be taken care of by humans.&nbsp;&nbsp;</p>



<p>Matt, this has been a fantastic conversation, and, you know, I feel it&#8217;s always a personal privilege to have a chance to work with someone like you so keep it up.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>LUNGREN: </strong>Thank you so much, Peter. Thanks for having me.&nbsp;</p>



<p><strong>LEE:</strong> I&#8217;m always so impressed when I talk to Matt, and I feel lucky that we get a chance to work together here at Microsoft. You know, one of the things that always strikes me whenever I talk to him is just how disruptive generative AI has been to a business like Nuance. Nuance has had clinical note-taking as part of their product portfolio for a long, long time. And so, you know, when generative AI comes along, it&#8217;s not only an opportunity for them, but also a threat because in a sense, it opens up the possibility of almost anyone being able to make clinical note-taking capabilities into products.&nbsp;&nbsp;</p>



<p>It&#8217;s really interesting how Matt&#8217;s product, DAX Copilot, which since the time that we had our conversation has expanded into a full healthcare workflow product called <a href="https://www.microsoft.com/en-us/health-solutions/clinical-workflow/dragon-copilot">Dragon Copilot</a>, has really taken off in the marketplace and how many new competing AI products have also hit the market, and all in just two years, because of generative AI.&nbsp;&nbsp;</p>



<p>The other thing, you know, that I always think about is just how important it is for these kinds of systems to work together and especially how they integrate into the electronic health record systems. This is something that Carey, Zak, and I didn&#8217;t really realize fully when we wrote our book. But you know, when you talk to both Matt and Seth, of course, we see how important it is to have that integration.&nbsp;&nbsp;</p>



<p>Finally, what a great example of yet another person who is both a surgeon and a tech geek. [LAUGHS] People sometimes think of healthcare as moving very slowly when it comes to new technology, but people like Matt are actually making it happen much more quickly than most people might expect.&nbsp;&nbsp;</p>



<p>Well, anyway, as I mentioned, we also had a chance to talk to Seth Hain, and so here&#8217;s my conversation with Seth:</p>



<p><strong>LEE:</strong> Seth, thank you so much for joining.&nbsp;&nbsp;</p>



<p><strong>SETH HAIN:</strong> Well, Peter, it&#8217;s such an exciting time to sit down and talk about this topic. So much has changed in the last two years. Thanks for inviting me.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yeah, in fact, I think in a way both of our lives have been upended in many ways by the emergence of AI. [LAUGHTER]&nbsp;&nbsp;</p>



<p>The traditional listeners of the Microsoft Research Podcast, I think for the most part, aren&#8217;t steeped in the healthcare industry. And so maybe we can just start with two things. One is, what is Epic, really? And then two, what is your job? What does the senior vice president for R&D at Epic do every day?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, well, let&#8217;s start with that first question. So, what is Epic? Most people across the world experience Epic through something we call MyChart. They might use it to message their physician. They might use it to check the lab values after they&#8217;ve gotten a recent test. But it&#8217;s an app on their phone, right, for connecting in with their doctors and nurses and really making them part of the care team.&nbsp;&nbsp;</p>



<p>But the software we create here at Epic goes beyond that. It&#8217;s what runs in the clinic, what runs at the bedside, in the back office to help facilitate those different pieces of care, from collecting vital information at the bedside to helping place orders if you&#8217;re coming in for an outpatient visit, maybe with a kiddo with an earache, and capturing that note and record of what happened during that encounter, all the way through back-office encounters, back-office information for interacting with payers as an example.&nbsp;&nbsp;</p>



<p>And so, we provide a suite of software that health systems and increasingly a broader set of the healthcare ecosystem, like payers and specialty diagnostic groups, use to connect with that patient at the center around their care.&nbsp;</p>



<p>And my job is to help our applications across the company take advantage of those latest pieces of technology to help improve the efficiency of folks like clinicians in the exam room when you go in for a visit. We&#8217;ll get into, I imagine, some use cases like ambient conversations, capturing that conversation in the exam room to help drive some of that documentation.&nbsp;&nbsp;</p>



<p>But then providing that platform for those teams to build those and then strategize around what to create next to help both the physicians be efficient and also the health systems. But then ultimately continuing to use those tools to advance the science of medicine.&nbsp;</p>



<p><strong>LEE:</strong> Right. You know, one thing that I explain to fellow technologists is that I think today health records are almost entirely digital. I think the last figures I saw is well over 99% of all health records are digital.&nbsp;&nbsp;</p>



<p>But in the year 2001, fewer than 15% of health records were digital. They were literally in folders on paper in storerooms, and if you&#8217;re old enough, you might even remember seeing those storerooms.&nbsp;&nbsp;</p>



<p>So, it&#8217;s been quite a journey. Epic and Epic&#8217;s competitors—though I think Epic is really the most important company—have really moved the entire infrastructure of record keeping and other communications in healthcare to a digital foundation.&nbsp;&nbsp;</p>



<p>And I think one thing we&#8217;ll get into, of course, one of the issues that has really become, I think, a problem for doctors and nurses is the kind of clerical or paperwork, record-keeping, burden. And for that reason, Epic and Epic systems end up being a real focus of attention. And so, we&#8217;ll get into that in a bit here.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> And I think that hits, just to highlight it, on both sides. There is both the need to capture documentation; there&#8217;s also the challenge in reviewing it.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;&nbsp;</p>



<p><strong>HAIN: </strong>The average medical record these days is somewhere between the length of <em>Fahrenheit 451</em> and <em>To Kill a Mockingbird</em>. [LAUGHTER] So there&#8217;s a fair amount of effort going in on that review side, as well.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, indeed. So much to get into there. But I would like to talk about encounters with AI. So obviously, I think there are two eras here: before the emergence of ChatGPT and what we now call of as generative AI and afterwards. And so, let&#8217;s take the former.&nbsp;&nbsp;</p>



<p>Of course, you&#8217;ve been thinking about machine learning and health data probably for decades. Do you have a memory of how you got into this? Why did you get an interest in data analytics and machine learning in the first place?&nbsp;</p>



<p><strong>HAIN: </strong>Well, my background, as you noted, is in mathematics before I came to Epic. And the sort of patterns and what could emerge were always part of what drove that. Having done development and kind of always been around computers all my life, it was a natural transition as I came here.&nbsp;&nbsp;</p>



<p>And I started by really focusing on, how do we scale systems for the very largest organizations, making sure they are highly available and also highly responsive? Time is critical in these contexts in regards to rapidly getting information to doctors and nurses.&nbsp;&nbsp;</p>



<p>And then really in the, say, in the 2010s, there started to be an emergence of capabilities from a storage and compute perspective where we could begin to build predictive analytics models. And these were models that were very focused, right. It predicted the likelihood somebody would show up for an appointment. It predicted the likelihood that somebody may fall during an inpatient stay, as an example.&nbsp;&nbsp;</p>



<p>And I think a key learning during that time period was thinking through the full workflow. What information was available at that point in time, right? At the moment somebody walks into the ED [emergency department], you don&#8217;t have a full picture to predict the likelihood that they may deteriorate during an inpatient encounter.&nbsp;&nbsp;</p>



<p>And in addition to what information was available was, what can you do about it? And a key part of that was how do we help get the right people in the right point in time at the bedside to make an assessment, right? It was a human-in-the-loop type of workflow where, for example, you would predict deterioration in advance and have a nurse come to the bedside or a physician come to the bedside to assess.&nbsp;&nbsp;</p>



<p>And I think that combination of narrowly focused predictive models with an understanding that to have them make an impact you had to think through the full workflow of where a human would make a decision was a key piece.&nbsp;</p>



<p><strong>LEE: </strong>Obviously there is a positive human impact. And so, for sure, part of the thought process for these kinds of capabilities comes from that.<strong>&nbsp;</strong>&nbsp;</p>



<p>But Epic is also a business, and you have to worry about, you know, what are doctors and clinics and healthcare systems willing to buy. And so how do you balance those two things, and do those two things ever come into conflict as you&#8217;re imagining what kinds of new capabilities and features and products to create?&nbsp;</p>



<p><strong>HAIN:</strong> Two, sort of, two aspects I think really come to mind. First off, generally speaking, we see analytics and AI as a <em>part</em> of the application. So, in that sense, it&#8217;s not something we license separately. We think that those insights and those pieces of data are part of what makes the application meaningful and impactful.&nbsp;&nbsp;</p>



<p>At the scale that many of these health systems operate and the number of patients that they care for, as well as having tens of thousands of users in the system daily, one needs to think about the compute overhead …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>HAIN:</strong> … that these things cause. And so, in that regard, there is always a ROI assessment that is taking place to some degree around, what happens if this runs at full scale? And in a way, that really got accelerated as we went into the generative AI era.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right. OK. So, you mentioned generative AI. What was the first encounter, and what was that experience for you?</p>



<p><strong>HAIN:</strong> So, in the winter of ’22 and into 2023, I started experimenting alongside you with what we at that time called DV3, or Davinci 3, and eventually became GPT-4. And immediately, a few things became obvious. The tool was highly general purpose. One was able to, in putting in a prompt, have it sort of convert into the framing and context of a particular clinical circumstance and reason around that context. But I think the other thing that started to come to bear in that context was there was a fair amount of latent knowledge inside of it that was very, very different than anything we&#8217;d seen before. And, you know, there&#8217;s some examples from the <a href="https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/?msockid=12da2addb70263b40f2e3f57b6c56288" target="_blank" rel="noreferrer noopener">Sparks of AGI paper from Microsoft Research</a>, where a series of objects end up getting stacked together in the optimal way to build height. Just given the list of objects, it seems to have a understanding of physical space that it intuited from the training processes we hadn&#8217;t seen anywhere. So that was an entirely new capability that programmers now had access to.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Well in fact, you know, I think that winter of 2022, and we&#8217;ll get into this, one of your projects that you&#8217;ve been running for quite a few years is something called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://cosmos.epic.com/" target="_blank" rel="noreferrer noopener">Cosmos<span class="sr-only"> (opens in new tab)</span></a>, which I find exceptionally interesting. And I was motivated to understand whether this type of technology could have an impact there.&nbsp;&nbsp;</p>



<p>And so, I had to receive permission from both OpenAI and Microsoft to provide you with early access.&nbsp;&nbsp;</p>



<p>When I did first show this technology to you, you must have had an emotional response, either skepticism or … I can&#8217;t imagine you just trusted, you know, trusted me to the extent of believing everything I was telling you.&nbsp;</p>



<p><strong>HAIN:</strong> I think there&#8217;s always a question of, what is it actually, right? It&#8217;s often easy to create demos. It&#8217;s often easy to show things in a narrow circumstance. And it takes getting your hands on it and really spending your 10,000 hours digging in and probing it in different ways to see just how general purpose it was.&nbsp;&nbsp;</p>



<p>And so, the skepticism was really around, how applicable can this be broadly? And I think the second question—and we&#8217;re starting to see this play out now in some of the later models—was, is this just a language thing? Is it narrowly only focused on that? Or can we start to imagine other modalities really starting to factor into this? How will it impact basic sciences? Those sorts of things.</p>



<p>On a personal note, I mean, I had, at that point, now they&#8217;re now 14 and 12, two kids that I wondered, what did this mean for them? What is the right thing for them to be studying? And so I remember sleepless nights on that topic, as well.&nbsp;</p>



<p><strong>LEE: </strong>OK, so now you get early access to this technology; you&#8217;re able to do some experimentation. I think one of the things that impressed me is just less than four months later at the major health tech industry conference, HIMSS, which also happened timing-wise to take place just after the public disclosure of GPT-4, Epic showed off some early prototype applications of generative AI. And so, describe what those were, and how did you choose what to try to do there?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, and we were at that point, we actually had the very first users live on that prototype, on that early version.&nbsp;&nbsp;</p>



<p>And the key thing we&#8217;d focused on—we started this development in very, very late December, January of 2023—was a problem that its origins really were during the pandemic.&nbsp;&nbsp;</p>



<p>So, during the pandemic, we started to see patients increasingly messaging their providers, nurses, and clinicians through MyChart, that patient portal I mentioned with about 190 million folks on it. And as you can imagine, that was a great opportunity in the context of COVID to limit the amount of direct contact between providers and patients while still getting their questions answered.&nbsp;&nbsp;</p>



<p>But what we found as we came out of the pandemic was that folks preferred it regardless. And that messaging volume had stayed very, very high and was a time-consuming effort for folks.&nbsp;&nbsp;</p>



<p>And so, the first use case we came out with was a draft message in the context of the message from the patient and understanding of their medical history using that medical record that we talked about.&nbsp;&nbsp;</p>



<p>And the nurse or physician using the tool had two options. They could either click to start with that draft and edit it and then hit send, or they could go back to the old workflow and start with a blank text box and write it from their own memory as they preferred.</p>



<p>And so that was that very first use case. There were many more that we had started from a development perspective, but, yeah, we had that rolling out right in March of 2023 there with the first folks.&nbsp;</p>



<p><strong>LEE: </strong>So, I know from our occasional discussions that some things worked very well. In fact, this is a real product now for Epic. And it seems to be really a very, very popular feature now. I know from talking to you that a lot of things have been harder. And so, I&#8217;d like to dive into that. As a developer, tech developer, you know, what&#8217;s been easy, what&#8217;s been hard, what&#8217;s in your mind still is left to do in terms of the development of AI?&nbsp;</p>



<p><strong>HAIN: </strong>Yeah. You know, the first thing that comes to mind sort of starting foundationally, and we hinted at this earlier in our conversation, was at that point in time, it was kind of per a message, rather compute-intensive to run these. And so, there were always trade-offs we were making in regards to how many pieces of information we would send into the model and how much would we request back out of it.&nbsp;&nbsp;</p>



<p>The result of that was that while kind of theoretically or even from a research perspective, we could achieve certain outcomes that were quite advanced, one had to think about, where you make those trade-offs from a scalability perspective as you wanted to roll that out to lot of folks. So …&nbsp;</p>



<p><strong>LEE:</strong> Were you charging your customers more money for this feature?&nbsp;</p>



<p><strong>HAIN:</strong> Yeah, essentially the way that we handle that is there&#8217;s compute that&#8217;s required. As I mentioned, the feature is just part of our application. So, it&#8217;s just what they get with an upgrade.&nbsp;&nbsp;</p>



<p>But that compute overhead is something that we needed to pass through to them. And so, it was something, particularly given both the staffing challenges, but also the margin pressures that health systems are feeling today, we wanted to be very cautious and careful about.&nbsp;</p>



<p><strong>LEE:</strong> And let&#8217;s put that on the stack because I do want to get into, from the selling perspective, that challenge and how you perceive health systems as a customer making those trade-offs. But let&#8217;s continue on the technical side here.&nbsp;</p>



<p><strong>HAIN:</strong> Yeah. On the technical side, it was a consideration, right. We needed to be thoughtful about how we used them. But going up a layer in the stack, at that time, there&#8217;s a lot of conversation in the industry around something called RAG, or <em>retrieval-augmented generation</em>.&nbsp;&nbsp;</p>



<p>And the idea was, could you pull the relevant bits, the relevant pieces of the chart, into that prompt, that information you shared with the generative AI model, to be able to increase the usefulness of the draft that was being created? And that approach ended up proving and continues to be to some degree, although the techniques have greatly improved, somewhat brittle, right. You have a general-purpose technology that is drafting the response.&nbsp;</p>



<p>But in many ways, you needed to, for a variety of pragmatic reasons, have somewhat brittle capability in regards to what you pulled into that approach. It tended to be pretty static. And I think this becomes one of the things that, looking forward, as these models have gotten a lot more efficient, we are and will continue to improve upon because, as you get a richer and richer amount of information into the model, it does a better job of responding.&nbsp;&nbsp;</p>



<p>I think the third thing, and I think this is going to be something we&#8217;re going to continue to work through as an industry, was helping users understand and adapt to these circumstances. So many folks when they hear AI think, it will just magically do everything perfectly.&nbsp;&nbsp;</p>



<p>And particularly early on with some of those challenges we&#8217;re talking about, it doesn&#8217;t. You know, if it&#8217;s helpful 85% of the time, that&#8217;s great, but it&#8217;s not going to be 100% of the time. And it&#8217;s interesting as we started, we do something we call immersion, where we always make sure that developers are right there elbow to elbow with the users of the software.&nbsp;</p>



<p>And one of the things that I realized through that experience with some of the very early organizations like UCSD [UC San Diego] or University of Wisconsin here in Madison was that even when I&#8217;m responding to an email or a physician is responding to one of these messages from a patient, depending on the patient and depending on the person, they respond differently.&nbsp;&nbsp;</p>



<p>In that context, there&#8217;s opportunity to continue to mimic that behavior as we go forward more deeply. And so, you learn a lot about, kind of, human behavior as you&#8217;re putting these use cases out into the world.&nbsp;</p>



<p><strong>LEE: </strong>So, you know, this increasing burden of electronic communications between doctors, nurses, and patients is centered in one part of Epic. I think that&#8217;s called your in-basket application, if I understand correctly.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> That&#8217;s correct.&nbsp;</p>



<p><strong>LEE:</strong> But that also creates, I think, a reputational risk and challenge for Epic because as doctors feel overburdened by this and they&#8217;re feeling burnt out—and as we know, that&#8217;s a big issue—then they point to, you know, “Oh, I&#8217;m just stuck in this Epic system.”&nbsp;&nbsp;</p>



<p>And I think a lot of the dissatisfaction about the day-to-day working lives of doctors and nurses then focuses on Epic. And so, to what extent do you see technologies like generative AI as, you know, a solution to that or contributing either positively or negatively to this?&nbsp;</p>



<p><strong>HAIN:</strong> You know, earlier I made the comment that in December, as we started to explore this technology, we realized there were a class of problems that now might have solutions that never did before.&nbsp;&nbsp;</p>



<p>And as we&#8217;ve started to dig into those—and we now have about 150 different use cases that are under development, many of which are live across … we&#8217;ve got about 350 health systems using them—one of the things we&#8217;ve started to find is that physicians, nurses, and others start to react to saying it&#8217;s helping them move forward with their job.&nbsp;&nbsp;</p>



<p>And examples of this, obviously the draft of the in-basket message response is one, but using ambient voice recognition as a kind of new input into the software so that when a patient and a physician sit down in the exam room, the physician can start a recording and that conversation then ends up getting translated or summarized, if you will, including using medical jargon, into the note in the framework that the physician would typically write.&nbsp;&nbsp;</p>



<p>Another one of those circumstances where they then review it, don&#8217;t need to type it out from scratch, for example, …&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>HAIN:</strong> … and can quickly move forward.&nbsp;&nbsp;</p>



<p>I think looking forward, you know, you brought up Cosmos earlier. It&#8217;s a suite of applications, but at its core is a dataset of about 300 million de-identified patients. And so using generative AI, we built research tools on top of it. And I bring that up because it’s a precursor of how that type of deep analytics can be put into context at the point of care. That&#8217;s what we see this technology more deeply enabling in the future.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, when you are creating … so you said there are about 150 sort of integrations of generative AI going into different parts of Epic&#8217;s software products.&nbsp;&nbsp;</p>



<p>When you are doing those developments and then you&#8217;re making a decision that something is going to get deployed, one thing that people might worry about is, well, these AI systems hallucinate. They have biases. There are unclear accountabilities, you know, maybe patient expectations.&nbsp;&nbsp;</p>



<p>For example, if there&#8217;s a note drafted by AI that&#8217;s sent to a patient, does the patient have a right to know what was written by AI and what was written by the human doctor? So, can we run through how you have thought about those things?&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> I think one thing that is important context to set here for folks, and I think it’s often a point of confusion when I&#8217;m chatting with folks in public, is that their interaction with generative AI is typically through a chatbot, right. It&#8217;s something like ChatGPT or Bing or one of these other products where they&#8217;re essentially having a back-and-forth conversation.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>HAIN: </strong>And that is a dramatically different experience than how we think it makes sense to embed into an enterprise set of applications.&nbsp;&nbsp;</p>



<p>So, an example use case may be in the back office, there are folks that are <em>coding</em> encounters. So, when a patient comes in, right, they have the conversation with the doctor, the doctor documents it, that encounter needs to be billed for, and those folks in the back-office associate to that encounter a series of codes that provide information about how that billing should occur.</p>



<p>So, one of the things we did from a workflow perspective was add a selector pane to the screen that uses generative AI to suggest a likely code. Now, this suggestion runs the risk of hallucination. So, the question is, how do you build into the workflow additional checks that can help the user do that?&nbsp;&nbsp;</p>



<p>And so in this context, we always include a citation back to the part of the medical record that justifies or supports that code. So quickly on hover, the user can see, does this make sense before selecting it? And it&#8217;s those types of workflow pieces that we think are critical to using this technology as an aid to helping people make decisions faster, right. It&#8217;s similar to drafting documentation that we talked about earlier.&nbsp;&nbsp;</p>



<p>And it&#8217;s interesting because there&#8217;s a series of patterns that are … going back to the <em>AI Revolution</em> book you folks wrote two years ago. Some of these are really highlighted there, right. This idea of things like a universal translator is a common pattern that we ended up applying across the applications. And in my mind, translation, this may sound a little bit strange, but summarization is an example of translating a very long series of information in a medical record into the context that an ED physician might care about, where they have three or four minutes to quick review that very long chart.&nbsp;&nbsp;</p>



<p>And so, in that perspective, and back to your earlier comment, we added the summary into the workflow but always made sure that the full medical record was available to that user, as well. So, a lot of what we&#8217;ve done over the last couple of years has been to create a series of repeatable techniques in regards to both how to build the backend use cases, where to pull the information, feed it into the generative AI models.&nbsp;&nbsp;</p>



<p>But then I think more importantly are the user experience design patterns to help mitigate those risks you talked about and to maintain consistency across the integrated suite of applications of how those are deployed.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> You might remember from our book, we had a whole chapter on reducing paperwork, and I think that&#8217;s been a lot of what we&#8217;ve been talking about. I want to get beyond that, but before transitioning, let&#8217;s get some numbers.&nbsp;&nbsp;</p>



<p>So, you talked about messages drafted to patients, to be sent to patients. So, give a sense of the volume of what&#8217;s happening right now.&nbsp;</p>



<p><strong>HAIN:</strong> Oh, we are seeing across the 300 and, I think it&#8217;s, 48 health systems that are now using generative AI—and to be clear, we have about 500 health systems we have the privilege of working with, each with many, many hospitals—there are tens of thousands of physicians and nurses using the software. That includes drafting million-plus, for example, notes a month at this point, as well as helping to generate in a similar ballpark that number of responses to patients.&nbsp;&nbsp;</p>



<p>The thing I&#8217;m increasingly excited about is the broader set of use cases that we&#8217;re seeing folks starting to deploy now. One of my favorites has been … it&#8217;s natural that as part of, for example, a radiology workflow, in studying that image, the radiologist made note that it would be worth double checking, say in six to eight months, that the patient have this area scanned of their chest. Something looks a little bit fishy there, but there&#8217;s not &#8230;&nbsp;</p>



<p><strong>LEE:</strong> There&#8217;s not a definitive finding yet.&nbsp;</p>



<p><strong>HAIN:</strong> … there&#8217;s not a definitive finding at that point. Part of that workflow is that the patient&#8217;s physician place an order for that in the future. And so, we&#8217;re using generative AI to note that back to the physician. And with one click, allow them to place that order, helping that patient get better care.&nbsp;&nbsp;</p>



<p>That&#8217;s one example of dozens of use cases that are now live, both to help improve the care patients are getting but also help the workforce. So going back to the translation-summarization example, a nurse at the end of their shift needs to write up a summary of that shift for the next nurse for each …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>HAIN: </strong>… each patient that they care for. Well, they&#8217;ve been documenting information in the chart over those eight or 12 hours, right.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>Yep, yep.&nbsp;</p>



<p><strong>HAIN:</strong> So, we can use that information to quickly draft that end-of-shift note for the nurse. They can verify it with those citations we talked about and make any additions or edits that they need and then complete their end of day far more efficiently.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> Right. OK. So now let&#8217;s get to Cosmos, which has been one of these projects that I think has been your baby for many years and has been something that has had a profound impact on my thinking about possibilities. So first off, what is Cosmos?&nbsp;</p>



<p><strong>HAIN: </strong>Well, just as an aside, I appreciate the thoughtful comments. There is a whole team of folks here that are really driving these projects forward. And a large part of that has been, as you brought up, both Cosmos as a foundational capability but then beginning to integrate it into applications. And that&#8217;s what those folks spend time on.&nbsp;&nbsp;</p>



<p>Cosmos is this effort across hundreds of health systems that we have the privilege of working with to build out a de-identified dataset with today—and it climbs every day—but 300 million unique patient records in it.&nbsp;&nbsp;</p>



<p>And one of the interesting things about that structure is that, for example, if I end up in a hospital in Seattle and have that encounter documented at a health system in Seattle, I still—a <em>de-identified version of me</em>—still only shows up once in Cosmos, stitching together both my information from here in Madison, Wisconsin, where Epic is at, with that extra data from Seattle. The result is these 300 million unique longitudinal records that have a deep history associated with them.&nbsp;&nbsp;</p>



<p><strong>LEE: </strong>And just to be clear, a patient record might have hundreds or even thousands of individual, I guess what you would call, clinical records or elements.&nbsp;</p>



<p><strong>HAIN: </strong>That&#8217;s exactly right. It&#8217;s the breadth of information from orders and allergies and blood pressures collected, for example, in an outpatient setting to cancer staging information that might have come through as part of an oncology visit. And it&#8217;s coming from a variety of sources. We exchange information about 10 million times a day between different health systems. And that full picture is available within Cosmos in that way of the patient.&nbsp;</p>



<p><strong>LEE:</strong> So now why? Why Cosmos?&nbsp;</p>



<p><strong>HAIN:</strong> Why Cosmos? Well, the real ultimate aim is to put a deeply informed in-context perspective at the point of care. So, as a patient, if I&#8217;m in the exam room, it&#8217;s helpful for the physician and me to know what have similar patients like me experienced in this context. What was the result of that line of treatment, for example?&nbsp;</p>



<p>Or as a doctor, if I&#8217;m looking and working through a relatively rare or strange case to me, I might be able to connect with—this as an example workflow we built called Look-Alikes—with another physician who has seen similar patients or within the workflow see a list of likely diagnoses based on patients that have been in a similar context. And so, the design of Cosmos is to put those insights into the point of care in the context of the patient.&nbsp;&nbsp;</p>



<p>To facilitate those steps there, the first phase was building out a set of research tooling. So, we see dozens of papers a year being published by the health systems that we work with. Those that participate in Cosmos have access to it to do research on it. And so they use both a series of analytical and data science tools to do that analysis and then publish research. So, building up trust that way.&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> The examples you gave are, like with Look-Alikes, it&#8217;s very easy, I think, for people outside of the healthcare world to imagine how that could be useful. So now why is GPT-4 or any generative AI relevant to this?&nbsp;</p>



<p><strong>HAIN:</strong> Well, so a couple of different pieces, right. Earlier we talked about—and I think this is the most important—how generative AI is able to cast things into a specific context. And so, in that way, we can use these tools to help both identify a cohort of patients similar to you when you&#8217;re in the exam room. And then also help present that information back in a way that relates to other research and understandings from medical literature to understand what are those likely outcomes.&nbsp;&nbsp;</p>



<p>I think more broadly, these tools and generative AI techniques in the transformer architecture envision a deeper understanding of sequences of events, sequences of words. And that starts to open up broader questions about what can really be understood about patterns and sequences of events in a patient&#8217;s journey.&nbsp;&nbsp;</p>



<p>Which if you didn&#8217;t know, the name Epic, just like a great long nation&#8217;s journey is told through an epic story, is a patient&#8217;s story. So that&#8217;s where it came from.&nbsp;</p>



<p><strong>LEE:</strong> So, we&#8217;re running up against our time together. And I always like to end with a more provocative question.&nbsp;&nbsp;</p>



<p><strong>HAIN:</strong> Certainly.&nbsp;</p>



<p><strong>LEE:</strong> And for you, I wanted to raise a question that I think we had asked ourselves in the very earliest days that we were sharing Davinci 3, what we now know of as GPT-4, with each other, which is, is there a world in the future because of AI where we don&#8217;t need electronic health records anymore? Is there a world in the future without EHR?&nbsp;</p>



<p><strong>HAIN:</strong> I think it depends on how you define EHR. I see a world coming where we need to manage a hybrid workforce, where there is a combination of humans and something folks are sometimes calling <em>agents</em> working in concert together to care for more and more of our … of the country and of the world. And there is and will need to be a series of tools to help orchestrate that hybrid workforce. And I think things like EHRs will transform into helping that operate … be operationally successful.&nbsp;&nbsp;</p>



<p>But as a patient, I think there&#8217;s a very different opportunity that starts to be presented. And we&#8217;ve talked about kind of understanding things deeply in context. There&#8217;s also a real acceleration happening in science right now. And the possibility of bringing that second- and third-order effects of generative AI to the point of care, be that through the real-world evidence we were talking about with Cosmos or maybe personalized therapies that really are well matched to that individual. These generative AI techniques open the door for that, as well as the full lifecycle of managing that from a healthcare perspective all the way through monitoring after the fact.&nbsp;&nbsp;</p>



<p>And so, I think we&#8217;ll still be recording people&#8217;s stories. Their stories are relevant to them, and they can help inform the bigger picture. But I think the real question is, how do you put those in a broader context? And these tools open the door for a lot more.&nbsp;</p>



<p><strong>LEE:</strong> Well, that&#8217;s really a great vision for the future.&nbsp;&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Seth, I always really learn so much talking to you, and thank you so much for this great chat.&nbsp;</p>



<p><strong>HAIN:</strong> Thank you for inviting me.&nbsp;&nbsp;&nbsp;</p>



<p><strong>LEE:</strong> I see Seth as someone on the very leading frontier of bringing generative AI to the clinic and into the healthcare back office and at the full scale of our massive healthcare system. It&#8217;s always impressive to me how thoughtful Seth has had to be about how to deploy generative AI into a clinical setting.&nbsp;&nbsp;</p>



<p>And, you know, one thing that sticks out—and he made such a point of this—is, you know, generative AI in the clinical setting isn&#8217;t just a chatbot. They&#8217;ve had to really think of other ways that will guarantee that the human stays in the loop. And that&#8217;s of course exactly what Carey, Zak, and I had predicted in our book. In fact, we even had a full chapter of our book entitled “Trust but Verify,” which really spoke to the need in medicine to always have a human being directly involved in overseeing the process of healthcare delivery.&nbsp;</p>



<p>One technical point that Carey, Zak, and I completely missed, on the other hand, in our book, was the idea of something that Seth brought up called RAG, which is <em>retrieval-augmented generation</em>. That&#8217;s the idea of giving AI access to a database of information and allowing it to use that database as it constructs its answers. And we heard from Seth how fundamental RAG is to a lot of the use cases that Epic is deploying.&nbsp;</p>



<p>And finally, I continue to find Seth&#8217;s project called Cosmos to be a source of inspiration, and I&#8217;ve continued to urge every healthcare organization that has been collecting data to consider following a similar path.&nbsp;</p>



<p>In our book, we spent a great deal of time focusing on the possibility that AI might be able to reduce or even eliminate a lot of the clerical drudgery that currently exists in the delivery of healthcare. We even had a chapter entitled “The Paperwork Shredder.” And we heard from both Matt and Seth that that has indeed been the early focus of their work.&nbsp;&nbsp;</p>



<p>But we also saw in our book the possibility that AI could provide diagnoses, propose treatment options, be a second set of eyes to reduce medical errors, and in the research lab be a research assistant. And here in Epic’s Cosmos, we are seeing just the early glimpses that perhaps generative AI can actually provide new research possibilities in addition to assistance in clinical decision making and problem solving. On the other hand, that still seems to be for the most part in our future rather than something that&#8217;s happening at any scale today.&nbsp;</p>



<p>But looking ahead to the future, we can still see the potential of AI helping connect healthcare delivery experiences to the advancement of medical knowledge. As Seth would say, the ability to connect bedside to the back office to the bench. That&#8217;s a pretty wonderful future that will take a lot of work and tech breakthroughs to make it real. But the fact that we now have a credible chance of making that dream happen for real, I think that&#8217;s pretty wonderful.&nbsp;</p>



<p>[MUSIC TRANSITIONS TO THEME]&nbsp;</p>



<p>I&#8217;d like to say thank you again to Matt and Seth for sharing their experiences and insights. And to our listeners, thank you for joining us. We have some really great conversations planned for the coming episodes, including a look at how patients are using generative AI for their own healthcare, as well as an episode on the laws, norms, and ethics developing around AI and health, and more. We hope you&#8217;ll continue to tune in.</p>



<p>Until next time.</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-18"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div style="height:30px" aria-hidden="true" class="wp-block-spacer"></div>



<p id="ftn_1">[1] A provider of conversational, ambient, and generative AI, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/2022/03/04/microsoft-completes-acquisition-of-nuance-ushering-in-new-era-of-outcomes-based-ai/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">Nuance was acquired by Microsoft in March 2022<span class="sr-only"> (opens in new tab)</span></a>. Nuance solutions and capabilities are now part of Microsoft Cloud for Healthcare.</p>



<p id="ftn_2">[2] According to the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://informatics.bmj.com/content/31/1/e101102" target="_blank" rel="noreferrer noopener">survey<span class="sr-only"> (opens in new tab)</span></a>, of the 20% of respondents who said they use generative AI in clinical practice, 29% reported using the technology for patient documentation and 28% said they use it for differential diagnosis.</p>



<p id="ftn_3">[3] A month after the conversation was recorded, <a href="https://www.microsoft.com/en-us/health-solutions/clinical-workflow/dragon-copilot" target="_blank" rel="noreferrer noopener">Microsoft Dragon Copilot</a> was unveiled. Dragon Copilot combines and extends the capabilities of DAX Copilot and Dragon Medical One.</p>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-end-mark"/>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--19"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/the-ai-revolution-in-medicine-revisited-real-world-healthcare-ai-development-and-deployment-at-scale/">Real-world healthcare AI development and deployment—at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>VidTok introduces compact, efficient tokenization to enhance AI video processing</title>
		<link>https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/</link>
		
		<dc:creator><![CDATA[Tianyu He, Junliang Guo, Jiang Bian]]></dc:creator>
		<pubDate>Wed, 02 Apr 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135315</guid>

					<description><![CDATA[<p>The VidTok method can enable AI systems to process and generate videos more effectively. Compact tokenization reduces computational costs while maintaining video quality across a potentially diverse set of applications.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/">VidTok introduces compact, efficient tokenization to enhance AI video processing</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1024x576.png" alt="Diagram showing an overview of how video tokenizers work with stages labeled as Input, Encoder, Regularizer (Latent Space), Decoder, and Output. " class="wp-image-1135333" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1024x576.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-300x169.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-768x432.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1066x600.png 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-655x368.png 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-240x135.png 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-640x360.png 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-960x540.png 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1-1280x720.png 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok-BlogHeroFeature-1400x788-1.png 1400w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /></figure>



<p>Every day, countless videos are uploaded and processed online, putting enormous strain on computational resources. The problem isn’t just the sheer volume of data—it’s how this data is structured. Videos consist of raw pixel data, where neighboring pixels often store nearly identical information. This redundancy wastes resources, making it harder for systems to process visual content effectively and efficiently.</p>



<p>To tackle this, we’ve developed a new approach to compress visual data into a more compact and manageable form. In our paper “<a href="https://www.microsoft.com/en-us/research/publication/vidtok-a-versatile-and-open-source-video-tokenizer/" target="_blank" rel="noreferrer noopener">VidTok: A Versatile and Open-Source Video Tokenizer</a>,” we introduce a method that converts video data into smaller, structured units, or <em>tokens</em>. This technique provides researchers and developers in visual world modeling—a field dedicated to teaching machines to interpret images and videos—with a flexible and efficient tool for advancing their work.&nbsp;</p>



<h2 class="wp-block-heading" id="how-vidtok-works">How VidTok works<strong></strong></h2>



<p>VidTok is a technique that converts raw video footage into a format that AI can easily work with and understand, a process called <em>video tokenization</em>. This process converts complex visual information into compact, structured tokens, as shown in Figure 1.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="692" height="213" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1.png" alt="Diagram showing an overview of how video tokenizers work with stages labeled as Input, Encoder, Regularizer (Latent Space), Decoder, and Output. " class="wp-image-1135323" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1.png 692w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1-300x92.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-1-240x74.png 240w" sizes="auto, (max-width: 692px) 100vw, 692px" /><figcaption class="wp-element-caption">Figure 1. An overview of how video tokenizers work, which form the basis of VidTok.</figcaption></figure>



<p>By simplifying videos into manageable chunks, VidTok can enable AI systems to learn from, analyze, and generate video content more efficiently. VidTok offers several potential advantages over previous solutions:</p>



<p><strong>Supports both discrete and continuous tokens.</strong> Not all AI models use the same “language” for video generation. Some perform best with continuous tokens—ideal for high-quality diffusion models—while others rely on discrete tokens, which are better suited for step-by-step generation, like language models for video. VidTok is a tokenizer that has demonstrated seamless support for both, making it adaptable across a range of AI applications.</p>



<p><strong>Operates in both causal and noncausal modes. </strong>In some scenarios, video understanding depends solely on past frames (causal), while in others, it benefits from access to both past and future frames (noncausal). VidTok can accommodate both modes, making it suitable for real-time use cases like robotics and video streaming, as well as for high-quality offline video generation.</p>



<p><strong>Efficient training with high performance.</strong> AI-powered video generation typically requires substantial computational resources. VidTok can reduce training costs by half through a two-stage training process—delivering high performance and lowering costs.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1116360">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft Research Blog</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/story/microsoft-research-2024-a-year-in-review/" aria-label="Research at Microsoft 2024: Meeting the challenge of a changing world" data-bi-cN="Research at Microsoft 2024: Meeting the challenge of a changing world" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/12/Year-in-review-2024_Stories_Hero_Feature-1400x788-1.jpg" alt="Research at Microsoft 2024 - Year in Review" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Research at Microsoft 2024: Meeting the challenge of a changing world</h2>
				
								<p class="large">In this new AI era, technology is changing even faster than before, and the transition from research to reality, from concept to solution, now takes days or weeks rather than months or years.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/microsoft-research-2024-a-year-in-review/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Research at Microsoft 2024: Meeting the challenge of a changing world" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="architecture">Architecture</h2>



<p>The VidTok framework builds on a classic 3D encoder-decoder structure but introduces 2D and 1D processing techniques to handle spatial and temporal information more efficiently. Because 3D architectures are computationally intensive, VidTok combines them with less resource-intensive 2D and 1D methods to reduce computational costs while maintaining video quality.</p>



<p><strong>Spatial processing.</strong> Rather than treating video frames solely as 3D volumes, VidTok applies 2D convolutions—pattern-recognition operations commonly used in image processing—to handle spatial information within each frame more efficiently.</p>



<p><strong>Temporal processing.</strong> To model motion over time, VidTok introduces the <em>AlphaBlender</em> operator, which blends frames smoothly using a learnable parameter. Combined with 1D convolutions—similar operations applied over sequences—this approach captures temporal dynamics without abrupt transitions.</p>



<p>Figure 2 illustrates VidTok’s architecture in detail.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1269" height="523" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2.png" alt="A diagram illustrating VidTok’s architecture, which integrates 2D+1D operations instead of relying solely on 3D techniques. The left side represents the encoder pathway, starting with a 3D InputBlock, followed by multiple 2D+1D DownBlocks and AlphaBlender Temporal DownBlocks. The right side shows the decoder pathway, mirroring the encoder with 2D+1D UpBlocks and AlphaBlender Temporal UpBlocks before reaching the 3D OutputBlock. A Regularizer module is connected at the bottom.  This approach strikes a balance between computational speed and high-quality video output. " class="wp-image-1135324" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2.png 1269w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-300x124.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-1024x422.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-768x317.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/VidTok_Fig2-240x99.png 240w" sizes="auto, (max-width: 1269px) 100vw, 1269px" /><figcaption class="wp-element-caption">Figure 2. VidTok’s architecture. It uses a combination of 2D and 1D operations instead of solely relying on 3D techniques, improving efficiency. For smooth frame transitions, VidTok employs the AlphaBlender operator in its temporal processing modules. This approach strikes a balance between computational speed and high-quality video output.</figcaption></figure>



<h2 class="wp-block-heading" id="quantization">Quantization</h2>



<p>To efficiently compress video data, AI systems often use quantization to reduce the amount of information that needs to be stored or transmitted. A traditional method for doing this is vector quantization (VQ), which groups values together and matches them to a fixed set of patterns (known as a codebook). However, this can lead to an inefficient use of patterns and lower video quality.</p>



<p>For VidTok, we use an approach called finite scalar quantization (FSQ). Instead of grouping values, FSQ treats each value separately. This makes the compression process more flexible and accurate, helping preserve video quality while keeping the file size small. Figure 3 shows the difference between the VQ and FSQ approaches.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="831" height="179" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17.png" alt="A diagram comparing Vector Quantization (VQ) and Finite Scalar Quantization (FSQ). VQ maps input z to a learned codebook, selecting the closest entry, while FSQ quantizes z using fixed sets independently for each value. FSQ simplifies optimization and improves training stability. " class="wp-image-1135319" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17.png 831w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-300x65.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-768x165.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/image-17-240x52.png 240w" sizes="auto, (max-width: 831px) 100vw, 831px" /><figcaption class="wp-element-caption">Figure 3. VQ (left) relies on learning a codebook, while FSQ (right) simplifies the process by independently grouping values into fixed sets, making optimization easier. VidTok adopts FSQ to enhance training stability and reconstruction quality.</figcaption></figure>



<h2 class="wp-block-heading" id="training">Training</h2>



<p>Training video tokenizers requires significant computing power. VidTok uses a two-stage process:</p>



<ol class="wp-block-list">
<li>It first trains the full model on low-resolution videos.</li>



<li>Then, it fine-tunes only the decoder using high-resolution videos.</li>
</ol>



<p>This approach cuts training costs in half—from 3,072 to 1,536 GPU hours—while maintaining video quality. Older tokenizers, trained on full-resolution videos from the start, were slower and more computationally intensive.&nbsp;</p>



<p>VidTok&#8217;s method allows the model to quickly adapt to new types of videos without affecting its token distribution. Additionally, it trains on lower-frame-rate data to better capture motion, improving how it represents movement in videos.</p>



<h2 class="wp-block-heading" id="evaluating-vidtok">Evaluating VidTok</h2>



<p>VidTok&#8217;s performance evaluation using the MCL-JCV benchmark—a comprehensive video quality assessment dataset—and an internal dataset demonstrates its superiority over existing state-of-the-art models in video tokenization. The assessment, which covered approximately 5,000 videos of various types, employed four standard metrics to measure video quality:</p>



<ol class="wp-block-list">
<li>Peak Signal-to-Noise Ratio (PSNR)</li>



<li>Structural Similarity Index Measure (SSIM)</li>



<li>Learned Perceptual Image Patch Similarity (LPIPS)</li>



<li>Fréchet Video Distance (FVD)</li>
</ol>



<p>The following table and Figure 4 illustrate VidTok’s performance:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1651" height="680" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1.png" alt="Result table showing VidTok's performance compared to other models (MAGVIT-v2, OmniTokenizer, Cosmos-DV, CV-VAE, Open-Sora-v1.2, Open-Sora-Plan-v1.2, CogVideoX, Cosmos-CV) on two datasets (MCL-JCV and Internal-Val) with metrics including PSNR, SSIM, LPIPS, and FVD." class="wp-image-1135326" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1.png 1651w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-300x124.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-1024x422.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-768x316.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-1536x633.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/table-1-240x99.png 240w" sizes="auto, (max-width: 1651px) 100vw, 1651px" /><figcaption class="wp-element-caption">Table 1</figcaption></figure>



<p>The results indicate that VidTok outperforms existing models in both discrete and continuous tokenization scenarios. This improved performance is achieved even when using a smaller model or a more compact set of reference patterns, highlighting VidTok&#8217;s efficiency.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1181" height="372" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2.png" alt="Radar charts comparing the performance of discrete and continuous tokenization methods in VidTok and state-of-the-art methods using four metrics: PSNR, SSIM, LPIPS, and FVD. Larger chart areas indicate better overall performance. " class="wp-image-1135327" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2.png 1181w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-300x94.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-1024x323.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-768x242.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/figure-2-240x76.png 240w" sizes="auto, (max-width: 1181px) 100vw, 1181px" /><figcaption class="wp-element-caption">Figure 4. Quantitative comparison of discrete and continuous tokenization performance in VidTok and state-of-the-art methods, evaluated using four metrics: PSNR, SSIM, LPIPS, and FVD. Larger chart areas indicate better overall performance.</figcaption></figure>



<h2 class="wp-block-heading" id="looking-ahead">Looking ahead</h2>



<p>VidTok represents a significant development in video tokenization and processing. Its innovative architecture and training approach enable improved performance across various video quality metrics, making it a valuable tool for video analysis and compression tasks. Its capacity to model complex visual dynamics could improve the efficiency of video systems by enabling AI processing on more compact units rather than raw pixels.</p>



<p>VidTok serves as a promising foundation for further research in video processing and representation. The code for VidTok is available on <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/VidTok" target="_blank" rel="noreferrer noopener">GitHub<span class="sr-only"> (opens in new tab)</span></a>, and we invite the research community to build on this work and help advance the broader field of video modeling and generation.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/vidtok-introduces-compact-efficient-tokenization-to-enhance-ai-video-processing/">VidTok introduces compact, efficient tokenization to enhance AI video processing</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Ideas: Accelerating Foundation Models Research: AI for all</title>
		<link>https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/</link>
		
		<dc:creator><![CDATA[Gretchen Huizinga, Evelyne Viegas, Muhammed Idris, Cesar Torres Jr.]]></dc:creator>
		<pubDate>Mon, 31 Mar 2025 13:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1134446</guid>

					<description><![CDATA[<p>Innovative AI research often depends on access to resources. Microsoft wants to help. Technical Advisor Evelyne Viegas and distinguished faculty from two Minority Serving Institutions discuss the benefits of Microsoft’s Accelerating Foundation Models Research program in their lives and research.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/">Ideas: Accelerating Foundation Models Research: AI for all</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788.jpg" alt="Microsoft Research Podcast | Ideas: Evelyne Viegas, Muhammed Idris, Cesar Torres" class="wp-image-1134454" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/AFMR_Ideas_Hero_Feature_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=143858112&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>Behind every emerging technology is a great idea propelling it forward. In the Microsoft Research Podcast series <em>Ideas</em>, members of the research community at Microsoft discuss the beliefs that animate their research, the experiences and thinkers that inform it, and the positive human impact it targets.&nbsp;</p>



<p>In this episode, host Gretchen Huizinga talks with three researchers about <a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/">Accelerating Foundation Models Research (AFMR)<span class="sr-only"> (opens in new tab)</span></a>, a global research network and resource platform that allows members of the larger academic community to push the boundaries of AI foundation models and explore exciting and unconventional collaborations across disciplines and institutions. <a href="https://www.microsoft.com/en-us/research/people/evelynev/">Evelyne Viegas<span class="sr-only"> (opens in new tab)</span></a>, a technical advisor at Microsoft Research, shares her vision for the program from the Microsoft perspective, while <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.uta.edu/academics/faculty/profile?user=cearto" target="_blank" rel="noreferrer noopener">Cesar Torres<span class="sr-only"> (opens in new tab)</span></a>, an assistant professor of computer science at the University of Texas at Arlington, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.msm.edu/about_us/FacultyDirectory/Medicine/MuhammedIdris/index.php" target="_blank" rel="noreferrer noopener">Muhammed Idris<span class="sr-only"> (opens in new tab)</span></a>, an assistant professor in the departments of medicine and public health at the Morehouse School of Medicine, tell their stories of how access to state-of-the-art foundation models is helping creative practitioners find inspiration from both their physical and virtual environments and making cancer-related health information more accessible and culturally congruent. The three recount their research journeys, including both frustrations and aspirations, and relate how AFMR resources have provided game-changing opportunities for Minority Serving Institutions and the communities they serve.&nbsp;</p>



<p> &nbsp;</p>



<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<p><a href="https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/">Accelerating Foundation Models Research</a><br>Collaboration homepage</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://hybridatelier.uta.edu/" target="_blank" rel="noreferrer noopener">The Hybrid Atelier<span class="sr-only"> (opens in new tab)</span></a><br>Homepage, The University of Texas at Arlington</p>



<p><a href="https://www.microsoft.com/en-us/research/blog/announcing-recipients-of-the-afmr-minority-serving-institutions-grant/" target="_blank" rel="noreferrer noopener">Announcing recipients of the AFMR Minority Serving Institutions grant</a><br>Microsoft Research Blog, January 30, 2024</p>



<p>&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/source/features/ai/ai-for-all-how-access-to-new-models-is-advancing-academic-research-from-astronomy-to-education/?msockid=35739e94ab6c69d41b738b93aa076831" target="_blank" rel="noreferrer noopener">AI ‘for all’: How access to new models is advancing academic research, from astronomy to education<span class="sr-only"> (opens in new tab)</span></a><br>Microsoft Blog, March 12, 2024</p>



<p><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.press.jhu.edu/newsroom/morehouse-model-how-one-school-medicine-revolutionized-community-engagement-and-health-equity" target="_blank" rel="noreferrer noopener">The Morehouse Model: How One School of Medicine Revolutionized Community Engagement and Health Equity<span class="sr-only"> (opens in new tab)</span></a>&nbsp;<br>Book, July 10, 2020&nbsp;</p>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[TEASER]&nbsp;</p>



<p>[MUSIC PLAYS UNDER DIALOG]&nbsp;&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> So AFMR is really a program which enabled us to provide access to foundation models, but it&#8217;s also a global network of researchers. And so for us, I think when we started that program, it was making sure that AI was made available to anyone and not just the few, right? And really important to hear from our academic colleagues, what they were discovering and covering and what were those questions that we&#8217;re not even really thinking about, right? So that&#8217;s how we started with AFMR.</p>



<p><strong>CESAR TORRES:</strong> One of the things that the AFMR program has allowed me to see is this kind of ability to better visualize the terrain of creativity. And it&#8217;s a little bit of a double-edged sword because when we talk about disrupting creativity and we think about tools, it&#8217;s typically the case that the tool is making something easier for us. So my big idea is to actually think about tools that are purposely making us slower, that have friction, that have errors, that have failures. To say that maybe the easiest path is not the most advantageous, but the one that you can feel the most fulfillment or agency towards.</p>



<p><strong>MUHAMMED IDRIS:</strong> For me, I think what programs like AFMR have enabled us to do is really start thinking outside the box as to how will these or how can these emerging technologies revolutionize public health? What truly would it take for an LLM to understand context? And really, I think for the first time, we can truly, truly achieve personalized, if you want to use that term, health communication.&nbsp;</p>



<p>[TEASER ENDS]&nbsp;</p>



<p>[MUSIC PLAYS]&nbsp;</p>



<p><strong>GRETCHEN HUIZINGA:</strong> You’re listening to Ideas, a Microsoft Research podcast that dives deep into the world of technology research and the profound questions behind the code. I&#8217;m Gretchen Huizinga. In this series, we&#8217;ll explore the technologies that are shaping our future and big ideas that propel them forward.</p>



				</span>
				<span id="show-more-show-less-toggle-20" class="show-more-show-less-toggleable-content">
					



<p>[MUSIC FADES]&nbsp;</p>



<p>I&#8217;m excited to share the mic today with three guests to talk about a really cool program called Accelerating Foundation Models Research, or AFMR for short. With me is Cesar Torres, an assistant professor of computer science at the University of Texas, Arlington, and the director of a program called The Hybrid Atelier. More on that soon. I&#8217;m also joined by Muhammed Idris, an assistant professor of medicine at the Morehouse School of Medicine. And finally, I welcome Evelyne Viegas, a technical advisor at Microsoft Research. Cesar, Muhammed, Evelyne, welcome to Ideas!&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> Pleasure.&nbsp;</p>



<p><strong>CESAR TORRES:</strong> Thank you.&nbsp;</p>



<p><strong>MUHAMMED IDRIS:</strong> Thank you.&nbsp;</p>



<p><strong>HUIZINGA: </strong>So I like to start these episodes with what I&#8217;ve been calling the “research origin story” and since there are three of you, I&#8217;d like you each to give us a brief overview of your work. And if there was one, what big idea or larger than life person inspired you to do what you&#8217;re doing today? Cesar let&#8217;s start with you and then we&#8217;ll have Muhammed and Evelyne give their stories as well. </p>



<p><strong>CESAR TORRES:</strong> Sure, thanks for having me. So, I work at the frontier of creativity especially thinking about how technology could support or augment the ways that we manipulate our world and our ideas. And I would say that the origin of why I happened into this space can really come back down to a “bring your kid to work” day. [LAUGHTER] My dad, who worked at Maquiladora, which is a factory on the border, took me over – he was an accountant – and so he first showed me the accountants and he&#8217;s like look at the amazing work that these folks are doing. But the reality is that a lot of what they do is hidden behind spreadsheets and so it wasn&#8217;t necessarily the most engaging. Suffice to say I did not go into accounting like my dad! [LAUGHTER] But then he showed us the chemical engineer in the factory, and he would tell me this chemical engineer holds the secret formula to the most important processes in the entire company. But again, it was this black box, right? And I got a little bit closer when I looked at this process engineer who was melting metal and pulling it out of a furnace making solder and I thought wow, that&#8217;s super engaging but at the same time it&#8217;s like it was hidden behind machinery and heat and it was just unattainable. And so finally I saw my future career and it was a factory line worker who was opening boxes. And the way that she opened boxes was incredible. Every movement, every like shift of weight was so perfectly coordinated. And I thought, here is the peak of human ability. [LAUGHTER] This was a person who had just like found a way to leverage her surroundings, to leverage her body, the material she was working with. And I thought, this is what I want to study. I want to study how people acquire skills. And I realized … that moment, I realized just how important the environment and visibility was to being able to acquire skills. And so from that moment, everything that I&#8217;ve done to this point has been trying to develop technologies that could get everybody to develop a skill in the same way that I saw that factory line worker that day. </p>



<p><strong>HUIZINGA:</strong> Wow, well, we&#8217;ll get to the specifics on what you&#8217;re doing now and how that&#8217;s relevant in a bit. But thank you for that. So Muhammed, what&#8217;s the big idea behind your work and how did you get to where you are today?&nbsp;</p>



<p><strong>MUHAMMED IDRIS:</strong> Yeah, no. First off, Cesar, I think it&#8217;s a really cool story. I wish I had an origin story [LAUGHTER] from when I was a kid, and I knew exactly what my life&#8217;s work was going to be. Actually, my story, I figured out my “why” much later. Actually, my background was in finance. And I started my career in the hedge fund space at a company called BlackRock, really large financial institution you might have heard of. Then I went off and I did a PhD at Penn State. And I fully intended on going back. I was going to basically be working in spreadsheets for the rest of my life. But actually during my postdoc at the time I was living in Montreal, I actually had distant relatives of mine who were coming to Montreal to apply for asylum and it was actually in helping them navigate the process, that it became clear to me, you know, the role, it was very obvious to me, the role that technology can play in helping people help themselves. And kind of the big idea that I realized is that, you know, oftentimes, you know, the world kind of provides a set of conditions, right, that strip away our rights and our dignity and our ability to really fend for ourselves. But it was so amazing to see, you know, 10-, 12-year-old kids who, just because they had a phone, were able to help their families navigate what shelter to go to, how to apply for school, and more importantly, how do they actually start the rest of their lives? And so actually at the time, I, you know, got together a few friends, and, you know, we started to think about, well, you know, all of this information is really sitting on a bulletin board somewhere. How can we digitize it? And so we put together a pretty, I would say, bad-ass team, interdisciplinary team, included developers and refugees, and we built a prototype over a weekend. And essentially what happened was we built this really cool platform called Atar. And in many ways, I would say that it was the first real solution that leveraged a lot of the natural language processing capabilities that everyone is using today to actually help people help themselves. And it did that in three really important ways. The first way is that people could essentially ask what they needed help with in natural language. And so we had some algorithms developed that would allow us to identify somebody&#8217;s intent. Taking that information then, we had a set of models that would then ask you a set of questions to understand your circumstances and determine your eligibility for resources. And then from that, we&#8217;d create a customized checklist for them with everything that they needed to know, where to go, what to bring, and who to talk to in order to accomplish that thing. And it was amazing to see how that very simple prototype that we developed over a weekend really became a lifeline for a lot of people. And so that&#8217;s really, I think, what motivated my work in terms of trying to combine data science, emerging technologies like AI and machine learning, with the sort of community-based research that I think is important for us to truly identify applications where, in my world right now, it&#8217;s really studying health disparities. </p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, tell us how you got into doing what you&#8217;re doing as a technical advisor. What&#8217;s the big idea behind what you do and how you got here?&nbsp;</p>



<p><strong>EVELYNE VIEGAS:</strong> So as a technical advisor in Microsoft Research, I really look for ideas out there. So ideas can come from anywhere. And so think it of scanning the horizon to look for some of those ideas out there and then figuring out, are there scientific hypotheses we should be looking at? And so the idea here is, once we have identified some of those ideas, the goal is really to help nurture a healthy pipeline for potential big bets. What I do is really about “subtle science and exact art” and we discover as we do and it involves a lot of discussions and conversations working with our researchers here, our scientists, but of course with the external research community. And how I got here … well first I will say that I am so excited to be alive in a moment where AI has made it to industry because I&#8217;ve looked and worked in AI for as long as I can remember with very different approaches. And actually as important, importantly for me is really natural languages which have enabled this big evolution. People sometimes also talk about revolution in AI, via the language models. Because when I started, so I was very fortunate growing up in an environment where my family, my extended family spoke different languages, but then it was interesting to see the different idioms in those natural languages. Just to give you an example, in English you say, it rains cats and dogs. Well, in France, in French it doesn&#8217;t mean anything, right? In French, actually, it rains ropes, right? Which probably doesn&#8217;t mean anything in English. [LAUGHTER] And so I was really curious about natural languages and communication. When I went to school, being good at math, I ended up doing math, realizing very quickly that I didn&#8217;t want to do a career in math. You know, proofs all that is good in high school, doing a full career, was not my thing, math. You know, proofs, all that. It’s good in high school, but doing a full career, it was not my thing, math. But there was that class I really, really enjoyed, which was mathematical logic. And so little by little, I started discovering people working in that field. And at the same time, I was still restless with natural languages. And so I also took some classes in linguistics on the humanity university in Toulouse in France. And I stumbled on those people who were actually working in … some in linguistics, some in computer science, and then there was this lab doing computational linguistics. And then that was it for me. I was like, that&#8217;s, you know, so that&#8217;s how I ended up doing my PhD in computational linguistics. And the last aspect I&#8217;ll talk about, because in my role today, the aspect of working with a network of people, with a global network, is still so important to me, and I think for science as a whole. At the time, there was this nascent field of computational lexical semantics. And for me, it was so important to bring people together because I realized that we all had different approaches, different theories, not even in France, but across the world, and actually, I worked with somebody else, and we co-edited the first book on computational lexical semantics, where we started exposing what it meant to do lexical semantics and the relationships between words within a larger context, with a larger context of conversations, discourse, and all those different approaches. And that&#8217;s an aspect which for me to this day is so important and that was also really important to keep as we develop what we&#8217;re going to talk about today, Accelerating Foundation Models Research program.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, this is fascinating because I didn&#8217;t even know all of these stories. I just knew that there were stories here and this is the first time I&#8217;m hearing them. So it&#8217;s like this discovery process and the sort of pushing on a door and having it be, well, that&#8217;s not quite the door I want. [LAUGHTER] Let&#8217;s try door number two. Let&#8217;s try door number three. Well, let&#8217;s get onto the topic of Accelerating Foundation Models Research and unpack the big idea behind that. Evelyne, I want to stay with you on this for a minute because I&#8217;m curious as to how this initiative even came to exist and what it hopes to achieve. So, maybe start out with a breakdown of the title. It might be confusing for some people, Accelerating Foundation Models Research. What is it?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, thank you for the question. So I think I&#8217;m going to skip quickly on accelerate research. I think people can understand it&#8217;s just like to bring …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Make it faster …&nbsp;</p>



<p><strong>VIEGAS:</strong> … well, faster and deeper advances. I mean, there are some nuances there, but I think the terms like foundation models, maybe that&#8217;s where I&#8217;ll start here. So when we talk about foundation models, just think about any model which has been trained on broad data, and which actually enables you to really do any task. That&#8217;s, I think, the simplest way to talk about it. And indeed, actually people talk a lot about large language models or language models. And so think of language models as just one part, right, for those foundation models. The term was actually coined at Stanford when people started looking at GPTs, the generative pre-trained transformers, this new architecture. And so that term was coined like to go not just talk about language models, but foundation models, because actually it&#8217;s not just language models, but there are also vision models. And so there are other types of models and modalities really. And so when we started with Accelerating Foundation Models Research and from now on, I will say AFMR if that&#8217;s okay.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Not to be confused with ASMR, which is that sort of tingly feeling you get in your head when you hear a good sound, but AFMR, yes.&nbsp;</p>



<p><strong>VIEGAS:</strong> So with the AFMR, so actually I need to come a little bit before that and just remind us that actually that this is not just new. The point I was making earlier about it’s so important to engage with the external research community in academia. So Microsoft Research has been doing it for as long as I&#8217;ve been at Microsoft and I&#8217;ve been 25 years, I just did 25 in January.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Congrats!&nbsp;</p>



<p><strong>VIEGAS:</strong> And so, I … thank you! &#8230;  and so, it&#8217;s really important for Microsoft Research, for Microsoft. And so we had some programs even before the GPT, ChatGPT moment where we had engaged with the external research community on a program called the Microsoft Turing Academic Program where we provided access to the Turing model, which was a smaller model than the one then developed by OpenAI. But at that time, it was very clear that we needed to be responsible, to look at safety, to look at trustworthiness of those models. And so we cannot just drink our own Kool-Aid and so we really had to work with people externally. And so we were already doing that. But that was an effort which we couldn&#8217;t scale really because to scale an effort and having multiple people that can have access to the resources, you need more of a programmatic way to be able to do that and rely on some platform, like for instance, Azure, which has security and privacy, confidentiality which enables to scale those type of efforts. And so what happens as we&#8217;re developing this program on the Turing model with a small set of academic people, then there was this ChatGPT moment in November 2022, which was the moment like the “aha moment,” I think, as I mentioned, for me, it&#8217;s like, wow, AI now has made it to industry. And so for us, it became very clear that we could not with this moment and the amount of resources needed on the compute side, access to actually OpenAI that new that GPT, at the beginning of GPT-3 and then 4 and then … So how could we build a program? First, should we, and was there interest? And academia responded “Yes! Please! Of course!” right? [LAUGHTER] I mean, what are you waiting for? So AFMR is really a program which enabled us to provide access to foundation models, but it&#8217;s also a global network of researchers. And so for us, I think when we started that program, it was making sure that AI was made available to anyone and not just the few, right? And really important to hear from our academic colleagues, what they were discovering and covering and what were those questions that we were not even really thinking about, right? So that&#8217;s how we started with AFMR. </p>



<p><strong>HUIZINGA:</strong> This is funny, again, on the podcast, you can&#8217;t see people shaking their heads, nodding in agreement, [LAUGHTER] but the two academic researchers are going, yep, that&#8217;s right. Well, Muhammed, let&#8217;s talk to you for a minute. I understand AFMR started a little more than a year ago with a pilot project that revolved around health applications, so this is a prime question for you. And since you&#8217;re in medicine, give us a little bit of a “how it started, how it&#8217;s going” from your perspective, and why it&#8217;s important for you at the Morehouse School of Medicine.&nbsp;</p>



<p><strong>IDRIS:</strong> For sure. You know, it&#8217;s something as we mentioned that really, I remember vividly is when I saw my first GPT-3 demo, and I was absolutely blown away. This was a little bit before the ChatGPT moment that Evelyne was mentioning, but just the possibilities, oh my God, were so exciting! And again, if I tie that back to the work that we were doing, where we were trying to kind of mimic what ChatGPT is today, there were so many models that we had to build, very complex architectures, edge cases that we didn&#8217;t even realize. So you could imagine when I saw that, I said, wow, this is amazing. It&#8217;s going to unlock so many possibilities. But at the same time, this demo was coming out, I actually saw a tweet about the inherent biases that were baked into these models. And I&#8217;ll never forget this. I think it was at the time he was a grad student at Stanford, and they were able to show that if you asked the model to complete a very simple sentence, a sort of joke, “Two Muslims walk into a bar …” what is it going to finish? And it was scary.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Wow.&nbsp;</p>



<p><strong>IDRIS:</strong> Two thirds, it was about 66% of the time, the responses referenced some sort of violence, right? And that really was an “aha moment” for me personally, of course, not being that I&#8217;m Muslim, but beyond that, that there are all of these possibilities. At the same time, there&#8217;s a lot that we don&#8217;t know about how these models might operate in the real world. And of course, the first thing that this made me do as a researcher was wonder how do these emerging technologies, how may they unintentionally lead to greater health disparities? Maybe they do. Maybe they don&#8217;t. The reality is that we don&#8217;t know.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>IDRIS: </strong>Now I tie that back to something that I&#8217;ve been fleshing out for myself, given my time here at Morehouse School of Medicine. And kind of what I believe is that, you know, the likely outcome, and I would say this is the case for really any sort of emerging technology, but let&#8217;s specifically talk about AI, machine learning, large language models, is that if we&#8217;re not intentional in interrogating how they perform, then what&#8217;s likely going to happen is that despite overall improvements in health, we&#8217;re going to see greater health disparities, right? It&#8217;s almost kind of that trickle-down economics type model, right? And it&#8217;s really this addressing of health disparities, which is at the core of the mission of Morehouse School of Medicine. It is literally the reason why I came here a few years ago. Now, the overarching goal of our program, without getting too specific, is really around evaluating the capabilities of foundation models. And those, course, as Evelyne mentioned, are large language models. And we&#8217;re specifically working on facilitating accessible and culturally congruent cancer-related health information. And specifically, we need to understand that communities that are disproportionately impacted have specific challenges around trust. And all of these are kind of obstacles to taking advantage of things like cancer screenings, which we know significantly reduce the likelihood of mortality. And it&#8217;s going very well. We have a pretty amazing interdisciplinary team. And I think we&#8217;ve been able to develop a pretty cool research agenda, a few papers and a few grants. I&#8217;d be happy to share about a little bit later.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, that&#8217;s awesome. And I will ask you about those because your project is really interesting. But I want Cesar to weigh in here on sort of the goals that are the underpinning of AFMR, which is aligning AI with human values, improving AI-human interaction, and accelerating scientific discovery. Cesar, how do these goals, writ large, align with the work you&#8217;re doing at UT Arlington and how has this program helped?&nbsp;</p>



<p><strong>TORRES:</strong> Yeah, I love this moment in time that everybody&#8217;s been talking about, that GPT or large language model exposure. Definitely when I experienced it, the first thing that came to my head was, I need to get this technology into the hands of my students because it is so nascent, there&#8217;s so many open research questions, there&#8217;s so many things that can go wrong, but there&#8217;s also so much potential, right? And so when I saw this research program by Microsoft I was actually surprised. I saw that, hey, they are actually acknowledging the human element. And so the fact that there was this call for research that was looking at that human dimension was really refreshing. So like what Muhammad was saying, one of the most exciting things about these large language models is you don&#8217;t have to be a computer scientist in order to use them. And it reminded me to this moment in time within the arts when digital media started getting produced. And we had this crisis. There was this idea that we would lose all the skills that we have learned from working traditionally with physical materials and having to move into a digital canvas.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right.&nbsp;</p>



<p><strong>TORRES:</strong> And it&#8217;s kind of this, the birth of a new medium. And we&#8217;re kind of at this unique position to guide how this medium is produced and to make sure that people develop that virtuosity in being able to use that medium but also understand its limitations, right? And so one of the fun projects that we&#8217;ve done here has been around working with our glass shop. Specifically, we have this amazing neon-bending artists here at UTA, Jeremy Scidmore and Justin Ginsberg. We&#8217;ve been doing some collaborations with them, and we&#8217;ve been essentially monitoring how they bend glass. I run an undergraduate research program here and I’ve had undergrads try to tackle this problem of how do you transfer that skill of neon bending? And the fact is that because of AFMR, here is just kind of a way to structure that undergraduate research process so that people feel comfortable to ask those dumb questions exactly where they are. But what I think is even more exciting is that they start to see that questions like skill acquisition is still something that our AI is not able to do. And so it&#8217;s refreshing to see; it&#8217;s like the research problems have not all been solved. It just means that new ones have opened and ones that we previously thought were unattainable now have this groundwork, this foundation in order to be researched, to be investigated. And so it&#8217;s really fertile ground. And I really thank AFMR … the AFMR program for letting us have access to those grounds.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. I&#8217;m really eager to get into both your projects because they&#8217;re both so cool. But Evelyne, I want you to just go on this “access” line of thought for a second because Microsoft has given grants in this program, AFMR, to several Minority Serving Institutions, or MSIs, as they&#8217;re called, including Historically Black Colleges and Universities and Hispanic Serving Institutions, so what do these grants involve? You&#8217;ve alluded to it already, but can you give us some more specifics on how Microsoft is uniquely positioned to give these and what they&#8217;re doing?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yes. So the grant program, per se, is really access to resources, actually compute and API access to frontier models. So think about Azure, OpenAI … but also now actually as the program evolves, it&#8217;s also providing access to even our research models, so Phi, I mean if you … like smaller models …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, P-H-I.&nbsp;</p>



<p><strong>VIEGAS:</strong> Yes, Phi! [LAUGHTER] OK! So, so it&#8217;s really about access to those resources. It&#8217;s also access to people. I was talking about this global research network and the importance of it. And I&#8217;ll come back to that specifically with the Minority Serving Institutions, what we did. But actually when we started, I think we started a bit in a naive way, thinking … we did an open call for proposals, a global one, and we got a great response. But actually at the beginning, we really had no participation from MSIs. [LAUGHTER] And then we thought, why? It&#8217;s open … it’s … and I think what we missed there, at the beginning, is like we really focused on the technology and some people who were already a part of the kind of, this global network, started approaching us, but actually a lot of people didn&#8217;t even know, didn&#8217;t think they could apply, right? And so we ended up doing a more targeted call where we provided not only access to the compute resources, access to the APIs to be able to develop applications or validate or expand the work which is being done with foundation models, but also we acknowledged that it was important, with MSIs, to also enable the students of the researchers like Cesar, Muhammed, and other professors who are part of the program so that they could actually spend the time working on those projects because there are some communities where the teaching load is really high compared to other communities or other colleges. So we already had a good sense that one size doesn&#8217;t fit all. And I think what came also with the MSIs and others, it&#8217;s like also one culture doesn&#8217;t fit all, right? So it&#8217;s about access. It&#8217;s about access to people, access to the resources and really co-designing so that we can really, really make more advances together.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Cesar let&#8217;s go over to you because big general terms don&#8217;t tell a story as well as specific projects with specific people. So your project is called, and I&#8217;m going to read this, <em>AI-Enhanced Bricolage: Augmenting Creative Decision Making in Creative Practices</em>. That falls under the big umbrella of Creativity and Design. So tell our audience, and as you do make sure to explain what bricolage is and why you work in a Hybrid Atelier, terms I&#8217;m sure are near and dear to Evelyne&#8217;s heart … the French language. Talk about that, Cesar. </p>



<p><strong>TORRES:</strong> So at UTA, I run a lab called The Hybrid Atelier. And I chose that name because “lab” is almost too siloed into thinking about scientific methods in order to solve problems. And I wanted something that really spoke to the ethos of the different communities of practice that generate knowledge. And so The Hybrid Atelier is a space, it&#8217;s a makerspace, and it&#8217;s filled with the tools and knowledge that you might find in creative practices like ceramics, glass working, textiles, polymer fabrication, 3D printing. And so every year I throw something new in there. And this last year, what I threw in there was GPT and large language models. And it has been exciting to see how it has transformed. But speaking to this specific project, I think the best way I can describe bricolage is to ask you a question: what would you do if you had a paperclip, duct tape, and a chewing gum wrapper? What could you make with that, right? [LAUGHTER] And so some of us have these MacGyver-type mentalities, and that is what Claude Lévi-Strauss<strong> </strong>kind of terms as the “bricoleur,” a person who is able to improvise solutions with the materials that they have at hand. But all too often, when we think about bricolage, it&#8217;s about the physical world. But the reality is that we very much live in a hybrid reality where we are behind our screens. And that does not mean that we cannot engage in these bricoleur activities. And so this project that I was looking at, it&#8217;s both a vice and an opportunity of the human psyche, and it&#8217;s known as “functional fixation.” And that is to say, for example, if I were to give you a hammer, you would see everything as a nail. And while this helps kind of constrain creative thought and action to say, okay, if I have this tool, I&#8217;m going to use it in this particular way. At the same time, it limits the other potential solutions, the ways that you could use a hammer in unexpected ways, whether it&#8217;s to weigh something down or like jewelers to texturize a metal piece or, I don&#8217;t know, even to use it as a pendulum &#8230; But my point here is that this is where large language models can come in because they can, from a more unbiased perspective, not having the cognitive bias of functional fixation say, hey, here is some tool, here&#8217;s some material, here&#8217;s some machine. Here are all the ways that I know people have used it. Here are other ways that it could be extended. And so we have been exploring, you know, how can we alter the physical and virtual environment in such a way so that this information just percolates into the creative practitioner’s mind in that moment when they&#8217;re trying to have that creative thought? And we&#8217;ve had some fun with it. I did a workshop at an event known as OurCS here at DFW. It&#8217;s a research weekend where we bring a couple of undergrads and expose them to research. And we found that it&#8217;s actually the case that it&#8217;s not AI that does better, and it&#8217;s also not the case that the practitioner does better! [LAUGHTER] It&#8217;s when they hybridize that you really kind of lock into the full kind of creative thought that could emerge. And so we&#8217;ve been steadily moving this project forward, expanding from our data sets, essentially, to look at the corpus of video tutorials that people have published all around the web to find the weird and quirky ways that they have extended and shaped new techniques and materials to advance creative thought. So …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Wow.&nbsp;&nbsp;</p>



<p><strong>TORRES:</strong> … it&#8217;s been an exciting project to say the least.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Okay, again, my face hurts because I&#8217;m grinning so hard for so long. I have to stop. No, I don&#8217;t because it&#8217;s amazing. You made me think of that movie Apollo 13 when they&#8217;re stuck up in space and this engineer comes in with a box of, we&#8217;ll call it bricolage, throws it down on the table and says, we need to make this fit into this using this, go. And they didn&#8217;t have AI models to help them figure it out, but they did a pretty good job. Okay, Cesar, that&#8217;s fabulous. I want Muhammed&#8217;s story now. I have to also calm down. It&#8217;s so much fun. [LAUGHTER]&nbsp;</p>



<p><strong>IDRIS:</strong> No, know I love it. I love it and actually to bring it back to what Evelyne was mentioning earlier about just getting different perspectives in a room, I think this is a perfect example of it. Actually, Cesar, I never thought of myself as being a creative person but as soon as you said a paperclip and was it the gum wrapper …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Duct tape.&nbsp;</p>



<p><strong>IDRIS:</strong> … duct tape or gum wrapper, I thought to myself, my first internship I was able to figure out how to make two paper clips and a rubber band into a … this was of course before AirPods, right? But something that I could wrap my wires around and it was perfect! [LAUGHTER] I almost started thinking to myself, how could I even scale this, or maybe get a patent on it, but it was a paper clip … yeah. Uh, so, no, no, I mean, this is really exciting stuff, yeah.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, Muhammed, let me tee you up because I want to actually … I want to say your project out loud …&nbsp;</p>



<p><strong>IDRIS:</strong> Please.&nbsp;</p>



<p><strong>HUIZINGA:</strong> … because it&#8217;s called <em>Advancing Culturally Congruent Cancer Communication with Foundation Models</em>. You might just beat Cesar&#8217;s long title with yours. I don&#8217;t know. [LAUGHTER] You include alliteration, which as an English major, that makes my heart happy, but it’s positioned under the Cognition and Societal Benefits bucket, whereas Cesar&#8217;s was under Creativity and Design, but I see some crossover. Evelyne&#8217;s probably grinning too, because this is the whole thing about research is how do these things come together and help? Tell us, Muhammed, about this cultury … culturally … Tell us about your project! [LAUGHTER]&nbsp;</p>



<p><strong>IDRIS:</strong> So, you know, I think again, whenever I talk about our work, especially the mission and the “why” of Morehouse School of Medicine, everything really centers around health disparities, right? And if you think about it, health disparities usually comes from one of many, but let&#8217;s focus on kind of three potential areas. You might not know you need help, right? If you know you need help, you might not know where to go. And if you end up there, you might not get the help that you need. And if you think about it, a lot of like the kind of the through line through all of these, it really comes down to health communication at the end of the day. It&#8217;s not just what people are saying, it&#8217;s how people are saying it as well. And so our project focuses right now on language and text, right? But we are, as I&#8217;ll talk about in a second, really exploring the kind of multimodal nature of communication more broadly and so, you know, I think another thing that&#8217;s important in terms of just background context is that for us, these models are more than just tools, right? We really do feel that if we&#8217;re intentional about it that they can be important facilitators for public health more broadly. And that&#8217;s where this idea of our project fitting under the bucket at benefiting society as a whole. Now, you know, the context is that over the past couple of decades, how we&#8217;ve talked about cancer, how we&#8217;ve shared health information has just changed dramatically. And a lot of this has to do with the rise, of course, of digital technologies more broadly, social media, and now there&#8217;s AI. People have more access to health information than ever before. And despite all of these advancements, of course, as I keep saying over and over again, not everyone&#8217;s benefiting equally, especially when it comes to cancer screening. Now, breast and cervical cancer, that&#8217;s what we&#8217;re focusing on specifically, are two of the leading causes of cancer-related deaths in women worldwide. And actually, black and Hispanic women in the US are at particular risk and disproportionately impacted by not just lower screening rates, but later diagnoses, and of course from that, higher mortality rates as well. Now again, an important part of the context here is COVID-19. I think there are, by some estimates, about 10 million cancer screenings that didn&#8217;t happen. And this is also happening within a context of just a massive amount of misinformation. It&#8217;s actually something that the WHO termed as an infodemic. And so our project is trying to kind of look for creative emerging technologies-based solutions for this. And I think we&#8217;re doing it in a few unique ways. Now the first way is that we&#8217;re looking at how foundation models like the GPTs but also open-source models and those that are, let&#8217;s say, specifically fine-tuned on medical texts, how do they perform in terms of their ability to generate health information? How accurate are they? How well is it written? And whether it&#8217;s actually useful for the communities that need it the most. We developed an evaluation framework, and we embedded within that some qualitative dimensions that are important to health communications. And we just wrapped up an analysis where we compared the general-purpose models, like a ChatGPT, with medical and more science-specific domain models and as you&#8217;d expect, the general-purpose models kind of produced information that was easier to understand, but that was of course at the risk of safety and more accurate responses that the medically tuned models were able to produce. Now a second aspect of our work, and I think this is really a unique part of not what I&#8217;ve called, but actually literally there&#8217;s a book called <em>The Morehouse Model</em>, is how is it that we could actually integrate communities into research? And specifically, my work is thinking about how do we integrate communities into the development and evaluation of language models? And that&#8217;s where we get the term “culturally congruent.” That these models are not just accurate, but they&#8217;re also aligned with the values, the beliefs, and even the communication styles of the communities that they&#8217;re meant to serve. One of the things that we&#8217;re thinking, you know, quite a bit about, right, is that these are not just tools to be published on and maybe put in a GitHub, you know, repo somewhere, right? That these are actually meant to drive the sort of interventions that we need within community. So of course, implementation is really key. And so for this, you know, not only do you need to understand the context within which these models will be deployed, the goal here really is to activate you and prepare you with information to be able to advocate for yourself once you actually see your doctor, right? So that again, I think is a good example of that. But you also have to keep in mind Gretchen that, you know, our goal here is, we don&#8217;t want to create greater disparities between those who have and those who don&#8217;t, right? And so for example, thinking about accessibility is a big thing and that&#8217;s been a part of our project as well. And so for example, we&#8217;re leveraging some of Azure API services for speech-to-text and we&#8217;re even going as far as trying to leverage some of the text-to-image models to develop visuals that address health literacy barriers and try to leverage these tools to truly, truly benefit health. </p>



<p><strong>HUIZINGA:</strong> One of the most delightful and sometimes surprising benefits of programs like AFMR is that the technologies developed in conjunction with people in minority communities have a big impact for people in majority communities as well, often called the Curb Cut Effect. Evelyne, I wonder if you&#8217;ve seen any of this happen in the short time that AFMR has been going?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, so, I&#8217;m going to focus a bit more maybe on education and examples there where we&#8217;ve seen, as Cesar was also talking about it, you know for scaling and all that. But we&#8217;ve seen a few examples of professors working with their students where English is not the first language.&nbsp;&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah …&nbsp;</p>



<p><strong>VIEGAS:</strong> Another one I would mention is in the context of domains. So for domains, what I mean here is application domains, like not just in CS, but we&#8217;ve been working with professors who are, for instance, astronomers, or lawyers, or musicians working in universities. So they started looking actually at these LLMs as more of the “super advisor” helping them. And so it&#8217;s another way of looking at it. And actually they started focusing on, can we actually build small astronomy models, right? And I&#8217;m thinking, okay, that could … maybe also we learn something which could be potentially applied to some other domain. So these are some of the things we are seeing.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yes.&nbsp;</p>



<p><strong>VIEGAS:</strong> But I will finish with something which may, for me, kind of challenges this Curb Cut Effect to certain extent, if I understand the concept correctly, is that I think, with this technology and the way AI and foundation models work compared to previous technologies, I feel it&#8217;s kind of potentially the opposite. It&#8217;s kind of like the tail catching up with the head. But here I feel that with the foundation models, I think it&#8217;s a different way to find information and gain some knowledge. I think that actually when we look at that, these are really broad tools that now actually can be used to help customize your own curb, as it were! So kind of the other way around.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Oh, interesting …&nbsp;</p>



<p><strong>VIEGAS:</strong> So I think it&#8217;s maybe there are two dimensions. It&#8217;s not just I work on something small, and it applies to everyone. I feel there is also a dimension of, this is broad, this is any tasks, and it enables many more people. I think Cesar and Muhammed made that point earlier, is you don&#8217;t have to be a CS expert or rocket scientist to start using those tools and make progress in your field. So I think that maybe there is this dimension of it.&nbsp;</p>



<p><strong>HUIZINGA:</strong> I love the way you guys are flipping my questions back on me. [LAUGHTER] So, and again, that is fascinating, you know, a custom curb, not a curb cut. Cesar, Muhammad, do you, either of you, have any examples of how perhaps this is being used in your work and you&#8217;re having accidental or serendipitous discoveries that sort of have a bigger impact than what you might&#8217;ve thought?&nbsp;</p>



<p><strong>TORRES:</strong> Well, one thing comes to mind. It&#8217;s a project that two PhD students in my lab, Adam Emerson and Shreyosi Endow have been working on. It&#8217;s around this idea of communities of practice and that is to say, when we talk about how people develop skills as a group, it&#8217;s often through some sort of tiered structure. And I&#8217;m making a tree diagram with my hands here! [LAUGHTER] And so we often talk about what it&#8217;s like for an outsider to enter from outside of the community, and just how much effort it takes to get through that gate, to go through the different rungs, through the different rites of passage, to finally be a part of the inner circle, so to speak. And one of the projects that we&#8217;ve been doing, we started to examine these known communities of practice, where they exist. But in doing this analysis, we realized that there&#8217;s a couple of folks out there that exist on the periphery. And by really focusing on them, we could start to see where the field is starting to move. And these are folks that have said, I&#8217;m neither in this community or another, I&#8217;m going to kind of pave my own way. While we&#8217;re still seeing those effects of that research go through, I think being able to monitor the communities at the fringe is a really telling sign of how we&#8217;re advancing as a society. I think shining some light into these fringe areas, it&#8217;s exactly how research develops, how it&#8217;s really just about expanding at some bleeding edge. And I think sometimes we just have to recontextualize that that bleeding edge is sometimes the group of people that we haven&#8217;t been necessarily paying attention to.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. Love it. Muhammad, do you have a quick example … or, I mean, you don&#8217;t have to, but I just was curious.&nbsp;</p>



<p><strong>IDRIS:</strong> Yeah, maybe I&#8217;ll just give one quick example that I think keeps me excited, actually has to do with the idea of kind of small language models, right? And so, you know, I gave the example of GPT-3 and how it&#8217;s trained on the entirety of the internet and with that is kind of baked in some unfortunate biases, right? And so we asked ourselves the flip side of that question. Well, how is it that we can go about actually baking in some of the good bias, right? The cultural context that&#8217;s important to train these models on. And the reality is that we started off by saying, let&#8217;s just have focus groups. Let&#8217;s talk to people. But of course that takes time, it takes money, it takes effort. And what we quickly realized actually is there are literally generations of people who have done these focus groups specifically on breast and cervical cancer screening. And so what we actually have since done is leverage that real world data in order to actually start developing synthetic data sets that are …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Ahhhh.&nbsp;&nbsp;</p>



<p><strong>IDRIS:</strong> … small enough but are of higher quality enough that allow us to address the specific concerns around bias that might not exist. And so for me, that&#8217;s a really like awesome thing that we came across that I think in trying to solve a problem for our kind of specific use case, I think this could actually be a method for developing more representative, context-aware, culturally sensitive models and I think overall this contributes to the overall safety and reliability of these large language models and hopefully can create a method for people to be able to do it as well.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, I see why it&#8217;s so cool for you to be sitting at Microsoft Research and working with these guys … It&#8217;s about now that I pose the “what could possibly go wrong if you got everything right?” question on this podcast. And I&#8217;m really interested in how researchers are thinking about the potential downsides and consequences of their work. So, Evelyne, do you have any insights on things that you&#8217;ve discovered along the path that might make you take preemptive steps to mitigate?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, I think it&#8217;s coming back to actually what Muhammed was just talking about, I think Cesar, too, around data, the importance of data and the cultural value and the local value. I think an important piece of continuing to be positive for me [LAUGHTER] is to make sure that we fully understand that at the end of the day, data, which is so important to build those foundation models is, especially language models in particular, are just proxies to human beings. And I feel that it’s uh … we need to remember that it&#8217;s a proxy to humans and that we all have some different beliefs, values, goals, preferences. And so how do we take all that into account? And I think that beyond the data safety, provenance, I think there&#8217;s an aspect of “data caring.” I don&#8217;t know how to say it differently, [LAUGHTER] but it&#8217;s kind of in the same way that we care for people, how do we care for the data as a proxy to humans? And I&#8217;m thinking of, you know, when we talk about like in, especially in cases where there is no economic value, right? [LAUGHTER] And so, but there is local value for those communities. And I think actually there is cultural value across countries. So just wanted to say that there is also an aspect, I think we need to do more research on, as data as proxies to humans. And as complex humans we are, right?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Right. Well, one of the other questions I like to ask on these Ideas episodes is, is about the idea of “blue sky” or “moonshot” research, kind of outrageous ideas. And sometimes they&#8217;re not so much outrageous as they are just living outside the box of traditional research, kind of the “what if” questions that make us excited. So just briefly, is there anything on your horizon, specifically Cesar and Muhammed, that you would say, in light of this program, AFMR, that you&#8217;ve had access to things that you think, boy, this now would enable me to ask those bigger questions or that bigger question. I don&#8217;t know what it is. Can you share anything on that line?&nbsp;</p>



<p><strong>TORRES:</strong> I guess from my end, one of the things that the AFMR program has allowed me to see is this kind of ability to better visualize the terrain of creativity. And it&#8217;s a little bit of a double-edged sword because when we talk about disrupting creativity and we think about tools, it&#8217;s typically the case that the tool is making something easier for us. But at the same time, if something&#8217;s easier, then some other thing is harder. And then we run into this really strange case where if everything is easy, then we are faced with the “blank canvas syndrome,” right? Like what do you even do if everything is just equally weighted with ease? And so my big idea is to actually think about tools that are purposely making us slower …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Mmmmm …&nbsp;</p>



<p><strong>TORRES:</strong> … that have friction, that have errors, that have failures and really design how those moments can change our attitudes towards how we move around in space. To say that maybe the easiest path is not the most advantageous, but the one that you can feel the most fulfillment or agency towards. And so I really do think that this is hidden in the latent space of the data that we collect. And so we just need to be immersed in that data. We need to traverse it and really it becomes an infrastructure problem. And so the more that we expose people to these foundational models, the more that we&#8217;re going to be able to see how we can enable these new ways of walking through and exploring our environment.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. I love this so much because I&#8217;ve actually been thinking some of the best experiences in our lives haven&#8217;t seemed like the best experiences when we went through them, right? The tough times are what make us grow. And this idea that AI makes everything accessible and easy and frictionless is what you&#8217;ve said. I&#8217;ve used that term too. I think of the people floating around in that movie WALL-E and all they have to do is pick whether I&#8217;m wearing red or blue today and which drink I want. I love this, Cesar. That&#8217;s something I hadn&#8217;t even expected you might say and boom, out of the park. Muhammad, do you have any sort of outrageous …? That was flipping it back!&nbsp;</p>



<p><strong>IDRIS:</strong> I was going to say, yeah, no, I listen, I don&#8217;t know how I could top that. But no, I mean, so it&#8217;s funny, Cesar, as you were mentioning that I was thinking about grad school, how at the time, it was the most, you know, friction-filled life experience. But in hindsight, I wouldn&#8217;t trade it in for the world. For me, you know, one of the things I&#8217;m often thinking about in my job is that, you know, what if we lived in a world where everyone had all the information that they needed, access to all the care they need? What would happen then? Would we magically all be the healthiest version of ourselves? I&#8217;m a little bit skeptical. I&#8217;m not going to lie, right? [LAUGHTER] But that&#8217;s something that I&#8217;m often thinking about. Now, bringing that back down to our project, one of the things that I find a little bit amusing is that I tend to ping-pong between, this is amazing, the capabilities are just, the possibilities are endless; and then there will be kind of one or two small things where it&#8217;s pretty obvious that there&#8217;s still a lot of research that needs to be done, right? So my whole, my big “what if” actually, I want to bring that back down to a kind of a technical thing which is, what if AI can truly understand culture, not just language, right? And so right now, right, an AI model can translate a public health message. It&#8217;s pretty straightforward from English to Spanish, right? But it doesn&#8217;t inherently understand why some Spanish speaking countries may be more hesitant about certain medical interventions. It doesn&#8217;t inherently appreciate the historical context that shapes that hesitancy or what kinds of messaging would build trust rather than skepticism, right? So there’s literal like cultural nuances. That to me is what, when I say culturally congruent or cultural context, what it is that I mean. And I think for me, I think what programs like AFMR have enabled us to do is really start thinking outside the box as to how will these, or how can these, emerging technologies revolutionize public health? What truly would it take for an LLM to understand context? And really, I think for the first time, we can truly, truly achieve personalized, if you want to use that term, health communication. And so that&#8217;s what I would say for me is like, what would that world look like?&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah, the big animating “what if?” I love this. Go ahead, Evelyne, you had something. Please.&nbsp;</p>



<p><strong>VIEGAS:</strong> Can I expand? I cannot talk. I&#8217;m going to do like Muhammed, I cannot talk! Like that friction and the cultural aspect, but can I expand? And as I was listening to Cesar on the education, I think I heard you talk about the educational rite of passage at some point, and Muhammed on those cultural nuances. So first, before talking about “what if?” I want to say that there is some work, again, when we talk about AFMR, is the technology is all the brain power of people thinking, having crazy ideas, very creative in the research being done. And there is some research where people are looking at what it means, actually, when you build those language models and how you can take into account different language and different culture or different languages within the same culture or between different cultures speaking the same language, or … So there is very interesting research. And so it made me think, expanding on what Muhammed and Cesar were talking about, so this educational rite of passage, I don&#8217;t know if you&#8217;re aware, so in Europe in the 17th, 18th century, there was this grand tour of Europe and that was reserved to just some people who had the funds to do that grand tour of Europe, [LAUGHTER] let&#8217;s be clear! But it was this educational rite of passage where actually they had to physically go to different countries to actually get familiar and experience, experiment, philosophy and different types of politics, and … So that was kind of this “passage obligé” we say in French. I don&#8217;t know if there is a translation in English, but kind of this rite of passage basically. And so I am like, wow, what if actually we could have, thanks to the AI looking at different nuances of cultures, of languages … not just language, but in a multimodal point of viewpoint, what if we could have this “citizen of the world” rite of passage, where we … before we are really citizens of the world, we need to understand other cultures, at least be exposed to them. So that would be my “what if?” How do we make AI do that? And so without … and for anyone, right, not just people who can afford it.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, I don&#8217;t even want to close, but we have to. And I&#8217;d like each of you to reflect a bit. I think I want to frame this in a way you can sort of pick what you&#8217;d like to talk about. But I often have a little bit of vision casting in this section. But there are some specific things I&#8217;d like you to talk about. What learnings can you share from your experience with AFMR? Or/and what&#8217;s something that strikes you as important now that may not have seemed that way when you started? And you can also, I&#8217;m anticipating you people are going to flip that and say, what wasn&#8217;t important that is now? And also, how do see yourself moving forward in light of this experience that you&#8217;ve had? So Muhammed, let&#8217;s go first with you, then Cesar, and then Evelyne, you can close the show.&nbsp;</p>



<p><strong>IDRIS:</strong> Awesome. One of the things that, that I&#8217;m often thinking about and one of the concepts I&#8217;m often reminded of, given the significance of the work that institutions like a Morehouse School of Medicine and UT Arlington and kind of Minority Serving Institutions, right, it almost feels like there is an onslaught of pushback to addressing some of these more systemic issues that we all struggle with, is what does it mean to strive for excellence, right? So in our tradition there&#8217;s a concept called Ihsan. Ihsan … you know there&#8217;s a lot of definitions of it but essentially to do more than just the bare minimum to truly strive for excellence and I think it was interesting, having spent time at Microsoft Research in Redmond as part of the AFMR program, meeting other folks who also participated in the program that, that I started to appreciate for myself the importance of this idea of the responsible design, development, and deployment of technologies if we truly are going to achieve the potential benefits. And I think this is one of the things that I could kind of throw out there as something to take away from this podcast, it&#8217;s really, don&#8217;t just think of what we&#8217;re developing as tools, but also think of them as how will they be applied in the real world? And when you&#8217;re thinking about the context within which something is going to be deployed, that brings up a lot of interesting constraints, opportunities, and just context that I think is important, again, to not just work on an interesting technology for the sake of an interesting technology, but to truly achieve that benefit for society.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Hmm. Cesar.&nbsp;</p>



<p><strong>TORRES:</strong> I mean, echoing Muhammad, I think the community is really at the center of how we can move forward. I would say the one element that really struck a chord with me, and something that I very much undervalued, was the power of infrastructure and spending time laying down the proper scaffolds and steppingstones, not just for you to do what you&#8217;re trying to do, but to allow others to also find their own path. I was setting up Azure from one of my classes and it took time, it took effort, but the payoff has been incredible in … in so much the impact that I see now of students from my class sharing with their peers. And I think this culture of entrepreneurship really comes from taking ownership of where you&#8217;ve been and where you can go. But it really just, it all comes down to infrastructure. And so AFMR for me has been that infrastructure to kind of get my foot out the door and also have the ability to bring some folks along the journey with me, so …&nbsp;</p>



<p><strong>HUIZINGA:</strong> Yeah. Evelyne, how blessed are you to be working with people like this? Again, my face hurts from grinning so hard. Bring us home. What are your thoughts on this?&nbsp;</p>



<p><strong>VIEGAS:</strong> Yeah, so first of all, I mean, it&#8217;s so wonderful just here live, like listening to the feedback from Muhammed and Cesar of what AFMR brings and has the potential to bring. And first, let me acknowledge that to put a program like AFMR, it takes a village. So I&#8217;m here, the face here, or well, not the face, the voice rather! [LAUGHTER] But it&#8217;s so many people who have, at Microsoft on the engineering side, we&#8217;re just talking about infrastructure, Cesar was talking about, you know, the pain and gain of leveraging an industry-grade infrastructure like Azure and Azure AI services. So, also our policy teams, of course, our researchers. But above all, the external research community … so grateful to see. It’s, as you said, I feel super blessed and fortunate to be working on this program and really listening what we need to do next. How can we together do better? There is one thing for me, I want to end on the community, right? Muhammed talked about this, Cesar too, the human aspect, right? The technology is super important but also understanding the human aspect. And I will say, actually, my “curb cut moment” for me [LAUGHTER] was really working with the MSIs and the cohort, including Muhammed and Cesar, when they came to Redmond, and really understanding some of the needs which were going beyond the infrastructure, beyond you know a small network, how we can put it bigger and deployments ideas too, coming from the community and that&#8217;s something which actually we also try to bring to the whole of AFMR moving forward. And I will finish on one note, which for me is really important moving forward. We heard from Muhammed talking about the really importance of interdisciplinarity, right, and let us not work in silo. And so, and I want to see AFMR go more international, internationality if the word exists … [LAUGHTER]&nbsp;</p>



<p><strong>HUIZINGA:</strong> It does now!&nbsp;</p>



<p><strong>VIEGAS:</strong> It does now! But it&#8217;s just making sure that when we have those collaborations, it&#8217;s really hard actually, time zones, you know, practically it&#8217;s a nightmare! But I think there is definitely an opportunity here for all of us.&nbsp;</p>



<p><strong>HUIZINGA:</strong> Well, Cesar Torres, Muhammed Idris, Evelyne Viegas. This has been so fantastic. Thank you so much for coming on the show to share your insights on AFMR today.&nbsp;</p>



<p>[MUSIC PLAYS]&nbsp;</p>



<p><strong>TORRES:</strong> It was a pleasure.&nbsp;&nbsp;</p>



<p><strong>IDRIS:</strong> Thank you so much.&nbsp;</p>



<p><strong>VIEGAS:</strong> Pleasure.&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-20"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/ideas-accelerating-foundation-models-research-ai-for-all/">Ideas: Accelerating Foundation Models Research: AI for all</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Research Focus: Week of March 24, 2025</title>
		<link>https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/</link>
		
		<dc:creator><![CDATA[Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, Lili Qiu, Jianfeng Gao, Yangyu Huang, Tengchao Lv, Lei Cui, Scarlett Li, Furu Wei, Xenofon Foukas, Bozidar Radunovic, Margus Veanes, Sarah Alamdari, Carles Domingo-Enrich, Ava Amini, Kevin Kaichuang Yang, Peter Lee, Ade Famoti, Christopher Bishop]]></dc:creator>
		<pubDate>Wed, 26 Mar 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1135033</guid>

					<description><![CDATA[<p>In this issue, we examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/">Research Focus: Week of March 24, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<p class="has-text-align-center"><strong>In this issue:</strong></p>



<p class="has-text-align-left">We examine a new conversation segmentation method that delivers more coherent and personalized agent conversation, and we review efforts to improve MLLMs’ understanding of geologic maps. Check out the latest research and other updates.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1401" height="789" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1.jpg" alt="Research Focus -- Week of March 24" class="wp-image-1135034" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1.jpg 1401w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1024x577.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-768x433.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/03/RF58-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1401px) 100vw, 1401px" /></figure>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-e734c6e9609233ab051742bb3beeed63" id="new-research">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="secom-on-memory-construction-and-retrieval-for-personalized-conversational-agents">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents</h3>



<p>Researchers from Microsoft and Tsinghua University propose a new method to help conversational AI agents deliver more coherent and personalized responses during complex long-term dialogue.</p>



<p>Large language models (LLMs) are widely used to enable more complicated discussions across a broader range of topics than traditional dialogue systems. However, managing excessively long context that contains irrelevant information is a major challenge. Existing solutions typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization.</p>



<p>The proposed new approach, <a href="https://www.microsoft.com/en-us/research/project/secom/" target="_blank" rel="noreferrer noopener">SeCom</a>, constructs the memory bank at segment level by introducing a conversation <strong>Se</strong>gmentation model that partitions long-term conversations into topically coherent segments, while applying <strong>Com</strong>pression based denoising on memory units to enhance memory retrieval. Experimental results show that <strong>SeCom</strong> exhibits a significant performance advantage over baselines on long-term conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-17 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--21"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/secom-on-memory-construction-and-retrieval-for-personalized-conversational-agents/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-9a2357e04d6b68359937ec2fcc67b1a5" id="new-research-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="peace-empowering-geologic-map-holistic-understanding-with-mllms">PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</h3>



<p>Microsoft Researchers and external colleagues introduce <a href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">GeoMap-Agent</a>, an AI system specifically designed for geologic map understanding and analysis. In the lab, they measure its effectiveness using a new benchmark called GeoMap-Bench, a novel gauge for evaluating multimodal large language models (MLLMs) in geologic map understanding. Geologic maps provide critical insights into the structure and composition of Earth&#8217;s surface and subsurface. They are indispensable in fields including disaster detection, resource exploration, and civil engineering.</p>



<p>Current MLLMs often fall short in understanding geologic maps, largely due to the challenging nature of cartographic generalization, which involves handling high-resolution maps, managing multiple associated components, and requiring domain-specific knowledge.</p>



<p>This paper presents results of experiments in which GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming the 0.369 score of GPT-4o. The researchers intend to enable advanced AI applications in geology, powering more efficient and accurate geological investigations.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-18 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--22"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/peace-empowering-geologic-map-holistic-understanding-with-mllms/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-8580525ca5a22a10ee7a4694b8f59445" id="new-research-2">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="the-future-of-the-industrial-ai-edge-is-cellular">The future of the industrial AI edge is cellular</h3>



<p>Reliable, high-bandwidth wireless connectivity and local processing at the edge are crucial enablers for emerging industrial AI applications. This work proposes that cellular networking is the ideal connectivity solution for these applications, due to its virtualization and support for open APIs. The researchers project the emergence of a converged industrial AI edge encompassing both computing and connectivity, in which application developers leverage the API to implement advanced functionalities. They present a case study showing evidence of the effectiveness of this approach, evaluated on an enterprise-grade 5G testbed.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-19 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--23"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/the-future-of-the-industrial-ai-edge-is-cellular/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-fd412f1d3373c86bbb4f4db1930ad0fa" id="new-research-3">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds">RE#: High Performance Derivative-Based Regex Matching with Intersection, Complement, and Restricted Lookarounds</h3>



<p>A regular expression (regex or RE) is a sequence of characters used to match, search, and manipulate strings in text based on specific criteria. REs are used in programming languages for data validation, text parsing, and search operations.</p>



<p><a href="https://www.microsoft.com/en-us/research/publication/re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds/">This paper</a> presents a tool and theory built on symbolic derivatives that does not use backtracking, while supporting both classical operators and complement, intersection, and restricted lookarounds. The researchers show that the main matching algorithm has input-linear complexity both in theory as well as experimentally. They apply thorough evaluation on popular benchmarks that show that RE# is over 71% faster than the next fastest regex engine in Rust on the baseline, and outperforms all state-of-the-art engines on extensions of the benchmarks, often by several orders of magnitude.&nbsp;</p>



<p>This work could potentially enable new applications in LLM prompt engineering frameworks, new applications in medical research and bioinformatics, and new opportunities in access and resource policy language design by web service providers.</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-20 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--24"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/re-high-performance-derivative-based-regex-matching-with-intersection-complement-and-restricted-lookarounds/">Read the paper</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-1c1c9f6b894c49f1e811e2f568b9620a" id="research-areas-1">NEW RESEARCH</h2>



<h3 class="wp-block-heading h2" id="toward-deep-learning-sequence-structure-co-generation-for-protein-design">Toward deep learning sequence–structure co-generation for protein design</h3>



<p>Researchers review recent advances in deep generative models for protein design, with a focus on sequence-structure co-generation methods. They describe the key methodological and evaluation principles underlying these methods, highlight recent advances from the literature, and discuss opportunities for continued development of sequence-structure co-generation approaches.</p>



<p>Deep generative models that learn from the distribution of natural protein sequences and structures may enable the design of new proteins with valuable functions. While most of today’s models focus on generating either sequences or structures, emerging co-generation methods promise more accurate and controllable protein design, ideally achieved by modeling both modalities simultaneously.&nbsp;</p>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-21 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--25"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/publication/toward-deep-learning-sequence-structure-co-generation-for-protein-design/">Read the paper</a></div>
</div>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1116360">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Microsoft Research Blog</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300" href="https://www.microsoft.com/en-us/research/story/microsoft-research-2024-a-year-in-review/" aria-label="Research at Microsoft 2024: Meeting the challenge of a changing world" data-bi-cN="Research at Microsoft 2024: Meeting the challenge of a changing world" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2024/12/Year-in-review-2024_Stories_Hero_Feature-1400x788-1.jpg" alt="Research at Microsoft 2024 - Year in Review" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Research at Microsoft 2024: Meeting the challenge of a changing world</h2>
				
								<p class="large">In this new AI era, technology is changing even faster than before, and the transition from research to reality, from concept to solution, now takes days or weeks rather than months or years.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/microsoft-research-2024-a-year-in-review/" class="btn btn-brand glyph-append glyph-append-chevron-right" aria-label="Read more" data-bi-cN="Research at Microsoft 2024: Meeting the challenge of a changing world" target="_blank">
							Read more						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-fcf31b9f10037de288b1b6ba58a7098f" id="podcast-1">PODCAST</h2>



<h3 class="wp-block-heading h2" id="new-series-the-ai-revolution-in-medicine-revisited">New Series: The AI Revolution in Medicine, Revisited</h3>



<p>Two years ago, OpenAI’s GPT-4 kick-started a new era in AI. In the months leading up to its public release, <a href="https://www.microsoft.com/en-us/research/people/petelee/">Peter Lee</a>, president of Microsoft Research, cowrote <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pearson.com/en-us/subject-catalog/p/the-ai-revolution-in-medicine-gpt-4-and-beyond/P200000011399/9780138279516"><em>The AI Revolution in Medicine: GPT-4 and Beyond</em></a>, a book full of optimism for the potential of advanced AI models to transform the world of healthcare. In this special <a href="https://www.microsoft.com/en-us/research/podcast/">Microsoft Research Podcast</a> series, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="The AI Revolution in Medicine, Revisited: An Introduction" width="500" height="281" src="https://www.youtube-nocookie.com/embed/3qOWQpnZDmI?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<div class="wp-block-buttons is-content-justification-center is-content-justification-center is-layout-flex wp-container-core-buttons-is-layout-22 wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--26"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">Watch the series</a></div>
</div>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-61c604b63ea2c27eb637663a9f89e42c" id="podcast">PODCAST</h2>



<h3 class="wp-block-heading h2" id="the-future-of-generative-ai-for-scientific-discovery">The future of generative AI for scientific discovery</h3>



<p>Most of us think of generative AI in the context of text or image generation, but it’s also a powerful tool for scientific discovery. In this episode of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=d3V0caeGIFU&feature=youtu.be">Leading the Shift podcast<span class="sr-only"> (opens in new tab)</span></a>, host Susan Etlinger speaks with Ade Famoti, a senior leader on the Microsoft Research Accelerator team. Ade discusses what he calls “AI’s physics moment,” and why he believes generative AI feels fundamentally different from past platform shifts. Ade shares examples of the work Microsoft Research is doing to uncover the opportunities of generative AI for materials discovery—to improve energy efficiency and carbon capture, and for drug discovery, to fight disease. Ade also highlights the role of culture in building trust, informing priorities and driving adoption of emerging technologies.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="The future of generative AI for scientific discovery | Microsoft Research" width="500" height="281" src="https://www.youtube-nocookie.com/embed/d3V0caeGIFU?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div class="wp-block-group is-layout-constrained wp-block-group-is-layout-constrained">
<h2 class="wp-block-heading h6 has-blue-color has-text-color has-link-color wp-elements-ad7541ccbcbf3a60c1bd4dd58805b339" id="video">VIDEO</h2>



<h3 class="wp-block-heading h2" id="microsoft-research-s-chris-bishop-talks-ai-for-science-what-it-really-means">Microsoft Research’s Chris Bishop talks AI for Science (what it really means)</h3>



<p>In this interview, the director of Microsoft Research AI for Science, Chris Bishop, discusses how AI is unlocking new scientific outcomes, from drug creation to materials generation to improved climate modeling.</p>



<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-16-9 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="Director of Microsoft Research talks AI for science (what it really means)" width="500" height="281" src="https://www.youtube-nocookie.com/embed/rfyS_rLOKUc?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<hr class="wp-block-separator has-alpha-channel-opacity is-style-dots"/>
</div>



<div style="padding-bottom:64px; padding-top:64px" class="wp-block-msr-immersive-section alignfull row has-background has-lighter-gray-background-color has-text-color has-black-color wp-block-msr-immersive-section">
	
	<div class="container">
		<div class="wp-block-msr-immersive-section__inner">
			<div class="wp-block-msr-cards msr-cards msr-cards--default mt-4 has-text-align-left" data-bi-aN="microsoft-research-in-case-you-missed-it">
	<div class="msr-cards__inner">
					<div class="heading-wrapper">
				<h2 class="mb-5 ">Microsoft Research | In case you missed it</h2>
			</div>
		
		<div class="row row-cols-1 row-cols-sm-2 row-cols-lg-3">
	<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Tech Life - The doctor will see you now"
						href="https://www.bbc.co.uk/sounds/play/w3ct5wnm"
					>
						<span>Tech Life &#8211; The doctor will see you now</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>BBC Sounds | March 4, 2025</p><p>An update on live trials in Ghana of 3D telemedicine technology, developed by Microsoft Research and external collaborators. Using portable equipment and holoportation technology, patients in remote locations can connect with a doctor many miles away. The BBC speaks to Spencer Fowers, who is the lead engineer on the project, as well as a patient and a doctor benefiting from the program.</p>				</div>
			
					</div>
	</div>
</div>
<div class="msr-cards__card msr-cards__card--default col">
	<div class="card has-spectrum-border-top__hover material-card h-100 p-0" data-mount="click-group">

		
		<div class="card-body bg-white p-4 pt-3">
							<h3 class="h5">
					<a
						class="text-decoration-none text-black"
						data-bi-cN="Katja Hofmann: Why we're training AI on video games"
						href="https://www.ted.com/talks/katja_hofmann_why_we_re_training_ai_on_video_games"
					>
						<span>Katja Hofmann: Why we're training AI on video games</span>&nbsp;<span class="glyph-prepend glyph-prepend-small glyph-prepend-chevron-right" aria-hidden="true"></span>
					</a>
				</h3>
										<div class="card__description card__citation small">
					<p>TED Talk | October 2024</p><p>In a recent TED Talk: <a href="https://www.ted.com/talks/katja_hofmann_why_we_re_training_ai_on_video_games" target="_blank" rel="noreferrer noopener">Why we&#8217;re training AI on video games,</a> Microsoft researcher Katja Hofmann discusses the work the Game Intelligence team at Microsoft Research is doing to develop AI that can transform video games. Using AI trained on years of human gameplay data, the team built World and Human Action Model, which can learn to think, play and innovate alongside humans, enabling video game creators to build more robust games. Hoffmann was also interviewed in a related article: <a href="https://spectrum.ieee.org/ai-video-games" target="_blank" rel="noreferrer noopener">Microsoft’s Muse AI Edits Video Games on the Fly</a>.</p>				</div>
			
					</div>
	</div>
</div>
</div>

					<div class="justify-content-center text-center mb-4">
				<a
					href="https://www.microsoft.com/en-us/research/news-and-awards/"
					class="btn btn-outline-primary glyph-append glyph-append-small glyph-append-chevron-right msr-cards__cta"
					data-bi-cN="View more news and awards"
					data-bi-type="button"
				>
					View more news and awards				</a>
			</div>
			</div>
</div>		</div>
	</div>

	</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/research-focus-week-of-march-24-2025/">Research Focus: Week of March 24, 2025</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
