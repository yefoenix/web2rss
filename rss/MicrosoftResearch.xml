<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Microsoft Research</title>
	<atom:link href="https://www.microsoft.com/en-us/research/feed/" rel="self" type="application/rss+xml" />
	<link>https://www.microsoft.com/en-us/research/</link>
	<description></description>
	<lastBuildDate>Mon, 22 Sep 2025 14:47:57 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.8.2</generator>
	<item>
		<title>Using AI to assist in rare disease diagnosis</title>
		<link>https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/</link>
		
		<dc:creator><![CDATA[Mandi Hall, Ashley Conard]]></dc:creator>
		<pubDate>Mon, 22 Sep 2025 14:17:03 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1142402</guid>

					<description><![CDATA[<p>New research from Microsoft, Drexel, and the Broad explores how generative AI could support genetic professionals in rare disease diagnosis.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img fetchpriority="high" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg" alt="Icons representing individual and group connections to a central computer monitor with a globe, symbolizing online connectivity, set against a gradient background transitioning from blue to pink." class="wp-image-1143080" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/AItoAssistDiagnosis-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="(max-width: 1400px) 100vw, 1400px" /></figure>



<p>In the promising and rapidly evolving field of genetic analysis, the ability to accurately interpret whole genome sequencing data is crucial for diagnosing and improving outcomes for people with rare genetic diseases. Yet despite technological advancements, genetic professionals face steep challenges in managing and synthesizing the vast amounts of data required for these analyses. Fewer than 50% of&nbsp;initial&nbsp;cases yield a diagnosis, and while reanalysis can lead to new findings, the process remains&nbsp;time-consuming and complex.&nbsp;</p>



<p>To better understand and address these challenges, Microsoft Research—in collaboration with Drexel University and the Broad Institute​​—conducted a comprehensive study titled <em><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">AI-Enhanced Sensemaking: Exploring the Design of a Generative AI-Based Assistant to Support Genetic Professionals<span class="sr-only"> (opens in new tab)</span></a>.</em> The study was recently published in a special edition of <em>ACM Transactions on Interactive Intelligent Systems</em> journal focused on generative AI.  </p>



<p>The study focused on integrating generative AI to support the complex, time-intensive, and information-dense sensemaking tasks inherent in whole genome sequencing analysis. Through detailed empirical research and collaborative design sessions with experts in the field, we identified key obstacles genetic professionals face and proposed AI-driven solutions to enhance their workflows.&nbsp;​&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;​We&nbsp;developed strategies for how generative AI can help synthesize biomedical data, enabling AI-expert collaboration to increase the diagnoses of previously unsolved rare diseases—ultimately aiming to improve patients’ quality of life and life expectancy.</p>



<h2 class="wp-block-heading" id="whole-genome-sequencing-in-rare-disease-diagnosis">Whole genome sequencing in rare disease diagnosis</h2>



<p>Rare diseases affect up to half a billion people globally and obtaining a diagnosis can take multiple years. These diagnoses often involve specialist consultations, laboratory tests, imaging studies, and invasive procedures. Whole genome sequencing is used to identify genetic variants responsible for these diseases by comparing a patient’s DNA sequence to reference genomes.&nbsp;​​Genetic professionals use bioinformatics tools such as&nbsp;<em>seqr,&nbsp;</em>an open-source, web-based tool for rare disease case analysis and project management to assist them in filtering and prioritizing&nbsp; > 1 million variants to determine their potential role in disease.&nbsp;A critical component of&nbsp;their&nbsp;work is sensemaking: the process of searching, filtering, and synthesizing data to build, refine, and present models from complex sets of gene and variant information.&nbsp;&nbsp;</p>



<p>​​The multi-step sequencing process​​​&nbsp;typically takes three to 12 weeks and requires extensive amounts of evidence and time to synthesize and aggregate information&nbsp;​​to understand the gene and variant effects for the patient.&nbsp;If a patient&#8217;s case goes unsolved, their whole genome sequencing data is set aside until enough time has passed to warrant a reanalysis​​. This creates a backlog of patient cases​​. The ability to easily&nbsp;identify&nbsp;when new scientific evidence&nbsp;emerges&nbsp;and when to reanalyze an unsolved patient case is key to shortening the time patients suffer with an unknown rare disease diagnosis.&nbsp;</p>



<h2 class="wp-block-heading" id="the-promise-of-ai-systems-to-assist-with-complex-human-tasks">The promise of AI systems to assist with complex human tasks</h2>



<p>Approximately 87% of AI systems never reach deployment&nbsp;​simply because they solve​​​&nbsp;the wrong problems.&nbsp;​​Understanding the AI support desired by different types of professionals, their current workflows, and AI capabilities is critical to successful AI system deployment and use. Matching technology capabilities with user tasks is particularly challenging in AI design because AI models can generate numerous outputs, and their capabilities can be unclear.&nbsp;​To design an effective​​​&nbsp;AI-based system​, one needs to identify​&nbsp;​​tasks AI can support,&nbsp;​​determine​​​​​​&nbsp;the appropriate level of AI involvement, and&nbsp;​​design​​​​​​&nbsp;user-AI interactions. This necessitates considering how humans interact with technology and how&nbsp;​​AI&nbsp;can best be incorporated into workflows and tools.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1141385">
		

	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://ai.azure.com/labs" aria-label="Azure AI Foundry Labs" data-bi-cN="Azure AI Foundry Labs" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Azure-AI-Foundry_1600x900.jpg" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Azure AI Foundry Labs</h2>
				
								<p id="azure-ai-foundry-labs" class="large">Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://ai.azure.com/labs" aria-describedby="azure-ai-foundry-labs" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Azure AI Foundry Labs" target="_blank">
							Azure AI Foundry						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="study-objectives-and-co-designing-a-genetic-ai-assistant">Study objectives and co-designing a genetic AI assistant</h2>



<p>Our study aimed to understand the current challenges and needs of genetic professionals performing whole genome sequencing analyses and explore the tasks where they want an AI assistant to support them in their work. The first phase of our study involved interviews with 17 genetics professionals to better understand their workflows, tools, and challenges. They included genetic analysts directly involved in interpreting data, as well as other roles participating in whole genome sequencing. In the second phase of our study, we conducted co-design sessions with study participants on how an AI assistant could support their workflows. We then developed a prototype of an AI assistant, which was further tested and refined with study participants in follow-up design walk-through sessions.</p>



<h2 class="wp-block-heading" id="identifying-challenges-in-whole-genome-sequencing-analysis">Identifying challenges in whole genome sequencing analysis</h2>



<p>Through our in-depth interviews with genetic professionals, our study uncovered three critical challenges in whole genome sequencing analysis:</p>



<ol class="wp-block-list">
<li><em>Information Overload</em>: Genetic analysts need to gather and synthesize vast amounts of data from multiple sources. This task is incredibly time-consuming and prone to human error.</li>



<li><em>Collaborative Sharing</em>: Sharing findings with others in the field can be cumbersome and inefficient, often relying on outdated methods that slow the collaborative analysis process.</li>



<li><em>Prioritizing Reanalysis</em>: Given the continuous influx of new scientific discoveries, prioritizing unsolved cases to reanalyze is a daunting challenge. Analysts need a systematic approach to identify cases that might benefit most from reanalysis.</li>
</ol>



<p>Genetic professionals highlighted the time-consuming nature of gathering and synthesizing information about genes and variants from different data sources. Other genetic professionals may have insights into certain genes and variants, but sharing and interpreting information with others for collaborative sensemaking requires significant time and effort. Although new scientific findings could affect unsolved cases through reanalysis, prioritizing cases based on new findings was challenging given the number of unsolved cases and limited time of genetic professionals.</p>



<h2 class="wp-block-heading" id="co-designing-with-experts-and-ai-human-sensemaking-tasks">Co-designing with experts and AI-human sensemaking tasks</h2>



<p>Our study participants prioritized two potential tasks of an AI assistant. The first task was flagging cases for reanalysis based on new scientific findings. The assistant would alert analysts to unsolved cases that could benefit from new research, providing relevant updates drawn from recent publications. The second task focused on aggregating and synthesizing information about genes and variants from the scientific literature. This feature would compile essential information from numerous scientific papers about genes and variants, presenting it in a user-friendly format and saving analysts significant time and effort. Participants emphasized the need to balance selectivity with comprehensiveness in the evidence they review. They also envisioned collaborating with other genetic professionals to interpret, edit, and verify artifacts generated by the AI assistant.</p>



<p>Genetic professionals require both broad and focused evidence at different stages of their workflow. The AI assistant prototypes were designed to allow flexible filtering and thorough evidence aggregation, ensuring users can delve into comprehensive data or selectively focus on pertinent details. The prototypes included features for collaborative sensemaking, enabling users to interpret, edit, and verify AI-generated information collectively. This&nbsp;​​approach not only&nbsp;​underscores​​​&nbsp;the trustworthiness of AI outputs, but also facilitates shared understanding and decision-making among genetic professionals.</p>



<h2 class="wp-block-heading" id="design-implications-for-expert-ai-sensemaking">Design implications for expert-AI sensemaking</h2>



<p>In the&nbsp;shifting frontiers of genome sequence analysis,&nbsp;leveraging generative AI to enhance sensemaking offers intriguing possibilities​​. The task of staying&nbsp;​​current​​​​​​, synthesizing information from diverse sources, and making informed decisions&nbsp;​​is challenging​​​​​​.&nbsp;&nbsp;</p>



<p>Our study participants emphasized the hurdles in integrating data from multiple sources without losing critical components, documenting decision rationales, and fostering collaborative environments. Generative AI models, with their advanced capabilities, have started to address these challenges by automatically generating interactive artifacts to support sensemaking. However, the effectiveness of such systems hinges on careful design considerations,&nbsp;​​particularly in how they facilitate distributed sensemaking, support both initial and ongoing sensemaking, and combine evidence from multiple modalities. We next discuss three design considerations for using generative AI models to support sensemaking.</p>



<h2 class="wp-block-heading" id="distributed-expert-ai-sensemaking-design">Distributed expert-AI sensemaking design</h2>



<p>Generative AI models can create artifacts that aid an individual user&#8217;s sensemaking process; however, the true potential lies in sharing these artifacts among users to foster collective understanding and efficiency. Participants in our study emphasized the importance of explainability, feedback, and trust when interacting with AI-generated content.&nbsp;​​​​​​​​​​Trust is gained by​​​​​​&nbsp;viewing portions of artifacts marked as correct by other users, or observing edits made to AI-generated information​​.&nbsp;​​Some​​​​​​&nbsp;users​, however,​&nbsp;cautioned against over-reliance on AI, which could obscure underlying inaccuracies. Thus, design strategies should ensure that any corrections are clearly marked&nbsp;​​and annotated​​​​​​. Furthermore, to enhance distributed sensemaking, visibility of others&#8217; notes and context-specific synthesis through AI can streamline the process​​.&nbsp;</p>



<h2 class="wp-block-heading" id="initial-expert-ai-sensemaking-and-re-sensemaking-design">Initial expert-AI sensemaking and re-sensemaking design</h2>



<p>In our fast-paced, information-driven world,&nbsp;​​it is essential to understand a situation both&nbsp;initially&nbsp;and again when new information arises.​​&nbsp;​​Sensemaking is inherently temporal, reflecting and shaping our understanding of time as we revisit tasks to reevaluate past decisions or incorporate new information. Generative AI plays a pivotal role here by transforming static data into dynamic artifacts that evolve, offering a comprehensive view of past rationales. Such AI-generated artifacts provide continuity, allowing users—both&nbsp;original decision-makers or new individuals—to access the rationale behind decisions made in earlier task instances. By continuously editing and updating these artifacts, generative AI highlights new information since the last review, supporting ongoing understanding and decision-making.&nbsp;Moreover, AI systems enhance&nbsp;​​transparency​​​​​​&nbsp;by summarizing previous notes and questions, offering insights into earlier thought processes and facilitating a deeper understanding of how conclusions were drawn. This reflective capability not only can reinforce initial sensemaking efforts but also equips users with the clarity needed for informed re-sensemaking as new data emerges.&nbsp;</p>



<h2 class="wp-block-heading" id="combining-evidence-from-multiple-modalities-to-enhance-ai-expert-sensemaking">Combining evidence from multiple modalities to enhance AI-expert sensemaking</h2>



<p>​​​The​​​​​​&nbsp;ability to combine evidence from multiple modalities is essential for effective sensemaking. Users often need to integrate diverse types of data—text, images, spatial coordinates, and more—into a coherent narrative to make informed decisions. Consider the case of search and rescue operations, where workers must rapidly synthesize information from texts, photographs, and GPS data to strategize their efforts. Recent advancements in multimodal generative AI models have empowered users by incorporating and synthesizing these varied inputs into a unified, comprehensive view. For instance, a participant in our study illustrated this capability by using a generative AI model to merge text from scientific publications with a visual gene structure depiction. This integration&nbsp;​​could create​​​​​​&nbsp;an image that contextualizes an individual&#8217;s genetic variant within the&nbsp;​​context​​​​​​&nbsp;of documented variants. Such advanced synthesis enables users to capture complex relationships and insights briefly, streamlining decision-making and expanding the potential for innovative solutions across diverse fields.&nbsp;</p>



<h2 class="wp-block-heading" id="sensemaking-process-with-ai-assistant">Sensemaking Process with AI Assistant</h2>



<figure class="wp-block-image aligncenter size-full"><img decoding="async" width="952" height="481" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png" alt="Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines." class="wp-image-1142535" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall.png 952w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-300x152.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-768x388.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/FIG1_Mandi-Hall-240x121.png 240w" sizes="(max-width: 952px) 100vw, 952px" /><figcaption class="wp-element-caption">Figure: Sensemaking process when interpreting variants with the introduction of prototype AI assistant. Gray boxes represent sensemaking activities which are currently performed by an analyst but are human-in-the-loop processes with involvement of our prototype AI assistant. Non-gray boxes represent activities reserved for analyst completion without assistance by our AI assistant prototype. Within the foraging searching and synthesizing processes, examples of data sources and data types for each, respectively, are connected by dotted lines.</figcaption></figure>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>We explored the potential of generative AI&nbsp;to support​​ genetic professionals​&nbsp;​in diagnosing rare diseases​​. By designing an AI-based assistant, we aim to streamline whole genome sequencing analysis, helping professionals diagnose rare genetic diseases more efficiently. Our study unfolded in two key phases:&nbsp;​pinpointing​​​&nbsp;existing challenges in analysis, and design ideation, where we crafted a prototype AI assistant. This tool is designed to boost diagnostic yield and cut down diagnosis time by flagging cases for reanalysis and synthesizing crucial gene and variant data. Despite valuable findings, more research is needed​​. Future research will involve testing the AI assistant in real-time, task-based user testing with genetic professionals to assess the AI&#8217;s impact on their workflow. The promise of AI advancements lies in solving the right user problems and building the appropriate solutions, achieved through collaboration among model developers, domain experts, system designers, and HCI researchers. By fostering these collaborations, we aim to develop robust, personalized AI assistants tailored to specific domains.&nbsp;</p>



<h2 class="wp-block-heading" id="join-the-conversation">Join the conversation</h2>



<p>Join us as we continue to explore the transformative potential of generative AI in genetic analysis, and please read the full text publication&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dl.acm.org/doi/10.1145/3756326" target="_blank" rel="noopener noreferrer">here<span class="sr-only"> (opens in new tab)</span></a>. Follow us on social media, share this post with your network, and let us know your thoughts on how AI can transform genetic research. If interested in our other related research work, check out&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.biorxiv.org/content/10.1101/2025.03.10.642480v1" target="_blank" rel="noopener noreferrer">Evidence Aggregator: AI reasoning applied to rare disease diagnosis.<span class="sr-only"> (opens in new tab)</span></a>&nbsp;&nbsp;</p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/using-ai-to-assist-in-rare-disease-diagnosis/">Using AI to assist in rare disease diagnosis</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tool-space interference in the MCP era: Designing for agent compatibility at scale</title>
		<link>https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/</link>
		
		<dc:creator><![CDATA[Adam Fourney, Tyler Payne, Maya Murad, Saleema Amershi]]></dc:creator>
		<pubDate>Thu, 11 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149210</guid>

					<description><![CDATA[<p>As agentic AI ushers in a new era marked by tool expansion, systems are converging, and complexity is rising. Microsoft Research explores the Model Context Protocol (MCP) as a new standard for agent collaboration across fragmented tool ecosystems.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-large"><img decoding="async" width="1024" height="576" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg" alt="Three white icons on a gradient background transitioning from blue to purple to pink. From left to right: a globe with a magnifying glass representing internet search, a central circle connected to smaller circles symbolizing network connectivity, and a checklist with two checkmarks and one empty box indicating task management." class="wp-image-1149369" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/ToolSpaceInterference-BlogHeroFeature-1400x788-1.jpg 1400w" sizes="(max-width: 1024px) 100vw, 1024px" /></figure>



<p>This year&nbsp;we’ve&nbsp;seen&nbsp;remarkable&nbsp;advances in agentic AI, including&nbsp;systems that conduct deep research,&nbsp;operate&nbsp;computers, complete substantial software engineering tasks, and tackle a range of other complex,&nbsp;multi-step goals. In each case,&nbsp;the industry relied&nbsp;on careful vertical integration: tools and agents were co-designed, co-trained, and tested together&nbsp;for peak&nbsp;performance. For example,&nbsp;OpenAI&#8217;s&nbsp;recent models&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/openai/gpt-oss?tab=readme-ov-file#tools" target="_blank" rel="noopener noreferrer">presume&nbsp;the&nbsp;availability&nbsp;of web search and document retrieval&nbsp;tools<span class="sr-only"> (opens in new tab)</span></a>. Likewise,&nbsp;the prompts and actions&nbsp;of&nbsp;<a href="https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/" target="_blank" rel="noreferrer noopener">Magentic-One</a>&nbsp;are&nbsp;set up to make hand-offs easy—for example, allowing the WebSurfer agent to pass downloaded files to the Coder agent. &nbsp;But as agents proliferate, we anticipate strategies relying heavily on vertical integration will not age well.&nbsp;Agents&nbsp;from&nbsp;different&nbsp;developers&nbsp;or companies will&nbsp;increasingly&nbsp;encounter&nbsp;each other and&nbsp;must&nbsp;work together to complete tasks, in what we refer to as a&nbsp;<em>society of agents</em>.&nbsp;These systems can vary in how coordinated they are, how aligned their goals are, and how much information they share. Can heterogenous agents and tools cooperate&nbsp;in this&nbsp;setting, or will they hinder one another and slow progress?</p>



<p>Early clues have&nbsp;emerged&nbsp;from an&nbsp;unexpected&nbsp;source:&nbsp;namely,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://modelcontextprotocol.io/" target="_blank" rel="noopener noreferrer">Model Context Protocol<span class="sr-only"> (opens in new tab)</span></a>&nbsp;(MCP). Since January 2025, MCP has grown from a&nbsp;promising spec to a&nbsp;thriving&nbsp;market&nbsp;of&nbsp;tool&nbsp;servers.&nbsp;As an example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://docs.zapier.com/mcp/home" target="_blank" rel="noopener noreferrer">Zapier boasts a catalog of 30,000 tools<span class="sr-only"> (opens in new tab)</span></a>&nbsp;across 7,000 services.&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://mcp.composio.dev/" target="_blank" rel="noopener noreferrer">Composio&nbsp;provide over 100 managed MCP servers<span class="sr-only"> (opens in new tab)</span></a>, surfacing hundreds of tools. Hugging&nbsp;Face is now serving&nbsp;many&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://huggingface.co/spaces?filter=mcp-server" target="_blank" rel="noopener noreferrer">Spaces&nbsp;apps over MCP<span class="sr-only"> (opens in new tab)</span></a>, and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://shopify.dev/docs/apps/build/storefront-mcp/servers/storefront" target="_blank" rel="noopener noreferrer">Shopify has enabled MCP for millions of storefronts<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;A&nbsp;society of&nbsp;<em>tools</em>&nbsp;is already here, and it promises to&nbsp;extend&nbsp;agent capabilities through&nbsp;cross-provider&nbsp;horizontal integration.&nbsp;</p>



<p>So,&nbsp;what does MCP have to say about&nbsp;horizontal integration? As catalogs grow,&nbsp;we expect some new failure modes to surface.&nbsp;This&nbsp;blog&nbsp;post introduces&nbsp;these&nbsp;as <em>tool-space interference</em>, and sketches both early observations and some pragmatic interventions to keep the society&nbsp;we’re&nbsp;building&nbsp;from stepping on its own feet.&nbsp;</p>



<p>Tool-space interference describes situations where otherwise reasonable tools or agents, when co-present, reduce end-to-end effectiveness. This can look like longer action sequences, higher token cost, brittle recovery from errors, or, in some cases, task failure.</p>



<h2 class="wp-block-heading" id="a-framing-example">A framing example</h2>



<p>Consider MCP as a means for extending <a href="https://www.microsoft.com/en-us/research/publication/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/">Magentic-One</a>, a generalist multi-agent system we released last year, to cover more software engineering tasks. Magentic-One ships with agents to write code, interact with the computer terminal, browse the web, and access local files. To help Magentic-One navigate version control, find issues to solve, and make pull requests, we could add an agent equipped with the GitHub MCP Server. However, now each time the team encounters a task involving GitHub, it must choose whether to visit github.com in the browser, execute a git command at the command line, or engage the GitHub MCP server. As the task progresses, agent understanding of state can also diverge: changing the branch in the browser won’t change the branch in the terminal, and an authorized MCP tool does not imply authorization in the browser.&nbsp;Thus, while any single agent might complete the task efficiently, the larger set of agents might misunderstand or interfere with one another, leading to additional rounds of debugging, or even complete task failure.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1021" height="410" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png" alt="Diagram depicting Magentic-One's multi-agentic architecture. An Orchestrator agent has access to 4 specialized sub-agents: a Coder agent that can write code and reason to sol solve tasks, a Computer Terminal Agent that can execute code written by the Coder agent, a WebSurfer agent that browse the internet (navigate pages, fill forms, etc), and a FileSurfer agent that can navigate files (e.g. PDFs, PPTx, etc). The diagram is annotated to show that for any incoming git-related task, the Orchestrator agent has to decide at evert orchestration step whether to access Git CLI via ComputerTerminal, visit Github site via WebSurfer, or directly access Github’s MCP server." class="wp-image-1149211" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image.png 1021w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-300x120.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-768x308.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-240x96.png 240w" sizes="auto, (max-width: 1021px) 100vw, 1021px" /><figcaption class="wp-element-caption">Figure 1: We can extend&nbsp;Magentic-One by adding an agent that equips the GitHub MCP server. However, on every turn involving a git-related task, the orchestrator will need to decide between messaging the Computer Terminal agent (with access to the git command line interface), WebSurfer agent (with access to github.com), and the agent with the GitHub MCP server. This overlap raises the possibility that they will interfere with one another.&nbsp;&nbsp;</figcaption></figure>



<h2 class="wp-block-heading" id="tool-space-interference-through-the-lens-of-mcp">Tool-space interference, through the lens of MCP</h2>



<p>To better understand the potential interference patterns and the current state of the MCP ecosystem, we conducted a survey of MCP servers listed on two registries: <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://smithery.ai/">smithery.ai<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://hub.docker.com/mcp">Docker MCP Hub<span class="sr-only"> (opens in new tab)</span></a>. Smithery is an MCP Server registry with over 7,000 first-party and community-contributed servers, which we sampled from the Smithery API. Likewise, Docker MCP Hub is a registry that distributes MCP servers as Docker images, and we manually collected popular entries. We then launched each server for inspection. After excluding servers that were empty or failed to launch, and deduplicating servers with identical features, 1,470 servers remained in our catalog.</p>



<p>To&nbsp;automate the&nbsp;inspection&nbsp;of&nbsp;running MCP servers,&nbsp;we developed an&nbsp;MCP&nbsp;Interviewer&nbsp;tool.&nbsp;The MCP&nbsp;Interviewer&nbsp;begins by cataloging the server’s tools, prompts, resources, resource templates, and capabilities.&nbsp;From&nbsp;this catalog we can compute&nbsp;descriptive statistics&nbsp;such as the number of tools, or the depth of the parameter&nbsp;schemas.&nbsp;&nbsp;Then, given the list of available tools, the interviewer uses&nbsp;an LLM (in our case,&nbsp;OpenAI&#8217;s GPT-4.1)&nbsp;to construct a functional testing&nbsp;plan&nbsp;that&nbsp;calls each tool at least once, collecting outputs, errors, and statistics along the way. Finally,&nbsp;the&nbsp;interviewer&nbsp;can&nbsp;also&nbsp;grade&nbsp;more qualitative&nbsp;criteria&nbsp;by&nbsp;using&nbsp;an LLM&nbsp;to&nbsp;apply purpose-built rubrics&nbsp;to&nbsp;tool&nbsp;schemas&nbsp;and&nbsp;tool call outputs.&nbsp;&nbsp;We are excited to&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">release the MCP Interviewer&nbsp;as an open-source CLI&nbsp;tool<span class="sr-only"> (opens in new tab)</span></a>, so server developers can automatically evaluate their MCP servers with agent usability in mind,&nbsp;and users can&nbsp;validate&nbsp;new servers.&nbsp;</p>



<p>While our survey provides informative initial results, it also faces significant limitations, the most obvious of which is authorization: many of the most popular MCP servers provide access to services that require authorization to use, hindering automated analysis. We are often still able to collect static features from these servers but are limited in the functional testing that can be done.</p>



<h3 class="wp-block-heading" id="one-size-fits-all-but-some-more-than-others">One-size fits all (but some more than others)</h3>



<p>So, what does our survey of MCP servers tell us about the MCP ecosystem? We will get into the numbers in a moment, but as we contemplate the statistics, there is one overarching theme to keep in mind: MCP servers do not know which clients or models they are working with, and present one common set of tools, prompts, and resources to everyone. However, some models handle long contexts and large tool spaces better than others (with diverging hard limits), and respond quite differently to common prompting patterns. For example, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">OpenAI’s guide on function calling<span class="sr-only"> (opens in new tab)</span></a> advises developers to:</p>



<p>“<em>Include examples and edge cases, especially to rectify any recurring failures. (Note: Adding examples may hurt performance for reasoning models).”</em></p>



<p>So already, this places MCP at a disadvantage over vertical integrations that optimize to the operating environment. And with that, let’s dive into more numbers.</p>



<h3 class="wp-block-heading" id="tool-count">Tool count</h3>



<p>While models generally vary in their proficiency for tool calling, the general trend has been that performance drops as the number of tools increases. For example, OpenAI limits developers to 128 tools, but <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://platform.openai.com/docs/guides/function-calling#best-practices-for-defining-functions">recommends<span class="sr-only"> (opens in new tab)</span></a> that developers:</p>



<p>“<em>Keep the number of functions small for higher accuracy. Evaluate your performance with different numbers of functions. Aim for fewer than 20 functions at any one time, though this is just a soft suggestion.</em>”</p>



<p>While we expect this to improve with each new model generation, at present, large tool spaces can <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">lower performance by up to 85% for some models<span class="sr-only"> (opens in new tab)</span></a>. Thankfully, the majority of servers in our survey contain four or fewer tools. But there are outliers: the largest MCP server we cataloged adds 256 distinct tools, while the 10 next-largest servers add more than 100 tools each. Further down the list we find popular servers like <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/microsoft/playwright-mcp">Playwright-MCP<span class="sr-only"> (opens in new tab)</span></a> (29 tools, at the time of this writing), and GitHub MCP (91 tools, with subsets available at alternative endpoint URLs), which might be too large for some models.</p>



<figure class="wp-block-image size-large"><img loading="lazy" decoding="async" width="1024" height="1024" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png" alt="chart" class="wp-image-1149361" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/tool-counts-per-server-360x360.png 360w" sizes="auto, (max-width: 1024px) 100vw, 1024px" /><figcaption class="wp-element-caption">Figure 2: The number of tools listed by each catalogued server directly after initialization. Note: servers can change the tools they list at any time, but only 226 servers in our catalog declare this capability.</figcaption></figure>



<h3 class="wp-block-heading" id="response-length">Response length</h3>



<p>Tools are generally called in agentic loops, where the output is then fed back into the model as input context. Models have hard limits on input context, but even within these limits, large contexts can drive costs up and performance down, so <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://research.trychroma.com/context-rot">practical limits can be much lower<span class="sr-only"> (opens in new tab)</span></a>. MCP offers no guidance on how many tokens a tool call can produce, and the size of some responses can come as a surprise. In our analysis, we consider the 2,443 tool calls across 1,312 unique tools that the MCP Interviewer was able to call successfully during the active testing phase of server inspection. While a majority of tools produced 98 or fewer tokens <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/tiktoken"><span class="sr-only"> (opens in new tab)</span></a>, some tools are extraordinarily heavyweight: the top tool returned an average of 557,766 tokens, which is enough to swamp the context windows of many popular models like GPT-5. Further down the list, we find that 16 tools produce more than 128,000 tokens, swamping GPT-4o and other popular models. Even when responses fit into the context window length, overly long responses can significantly degrade performance (<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://arxiv.org/abs/2505.10570v1">up to 91% in one study<span class="sr-only"> (opens in new tab)</span></a>), and limit the number of future calls that can be made. Of course, agents are free to implement their own context management strategies, but this behavior is left undefined in the MCP specification and server developers cannot count on any particular client behavior or strategy.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td></td><td></td><td colspan="4"><strong># of tools that would overflow context in</strong></td></tr><tr><td><strong>Model</strong></td><td><strong>Context Window</strong></td><td><strong>1 call</strong></td><td><strong>2 calls</strong></td><td><strong>3-5 calls</strong></td><td><strong>6-10 calls</strong></td></tr><tr><td>GPT 4.1</td><td>1,000,000</td><td>0</td><td>1</td><td>7</td><td>11</td></tr><tr><td>GPT 5</td><td>400,000</td><td>1</td><td>7</td><td>15</td><td>25</td></tr><tr><td>GPT-4o, Llama 3.1,</td><td>128,000</td><td>16</td><td>15</td><td>33</td><td>40</td></tr><tr><td>Qwen 3</td><td>32,000</td><td>56</td><td>37</td><td>86</td><td>90</td></tr><tr><td>Phi-4</td><td>16,000</td><td>93</td><td>60</td><td>116</td><td>109</td></tr></tbody></table></figure>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="936" height="935" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png" alt="Chart showing the average tool call output lengths (in tokens) for 1,312 tools, as observed by the MCP Interviewer’s functional test plan. The x-axis represents individual tools (sorted by index), and the y-axis displays the average output length on a logarithmic scale. Horizontal dashed lines indicate context window limits for GPT-4o (128k tokens) and GPT-5 (400k tokens). A pink annotation box summarizes statistics: total tools (1,312), mean (4,431 tokens), median (98 tokens), minimum (0 tokens), and maximum (557,766 tokens)." class="wp-image-1149213" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1.png 936w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-768x767.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/image-1-360x360.png 360w" sizes="auto, (max-width: 936px) 100vw, 936px" /><figcaption class="wp-element-caption">Figure 3: Tool call response length averages, in tokens, as&nbsp;observed&nbsp;by the MCP Interviewer’s functional test plan. Only successful tool calls are considered. Horizontal lines&nbsp;indicate&nbsp;context window limits for GPT-4o and GPT-5.</figcaption></figure>



<h3 class="wp-block-heading" id="tool-parameter-complexity">Tool parameter complexity</h3>



<p>Mirroring the challenges from increasing&nbsp;the&nbsp;number of tools,&nbsp;increasing the complexity of a tool’s parameter space can also lead to degradation.&nbsp;For example, while MCP tools can take complex object types and structures as parameters,&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://composio.dev/blog/gpt-4-function-calling-example" target="_blank" rel="noopener noreferrer">composio<span class="sr-only"> (opens in new tab)</span></a>&nbsp;found that&nbsp;flattening the parameter space could improve tool-calling performance&nbsp;by 47%&nbsp;compared to baseline performance.&nbsp;&nbsp;In our analysis, we&nbsp;find&nbsp;numerous examples of deeply nested structure—in&nbsp;one&nbsp;case, going&nbsp;20&nbsp;levels deep.</p>



<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="2560" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png" alt="Chart showing the maximum depth of each tool’s input properties schema. The x-axis represents individual tools (sorted by index), and the y-axis shows the maximum property schema depth. Most tools have a depth  of 2 (named and annotated properties). A pink annotation box summarizes statistics: total tools (12,643), mean (2.24), median (2.00), standard deviation (1.38), minimum (0.00), and maximum (20.00). " class="wp-image-1149365" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-scaled.png 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-300x300.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1024x1024.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-150x150.png 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-768x768.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-1536x1536.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-2048x2048.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-180x180.png 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/input_schema_depth-360x360.png 360w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /><figcaption class="wp-element-caption">Figure 4: The maximum depth of each tool’s input properties schema. A depth of 0&nbsp;indicates&nbsp;a tool with no properties. A depth of 1&nbsp;indicates&nbsp;a tool with named properties but no annotations (e.g., no description or type). A depth of 2&nbsp;indicates&nbsp;a tool with named and annotated properties.&nbsp;&nbsp;A depth of 3+&nbsp;indicates&nbsp;a tool with structured properties that have&nbsp;additional&nbsp;nested annotations.&nbsp;</figcaption></figure>



<h3 class="wp-block-heading" id="namespacing-issues-and-naming-ambiguity">Namespacing issues and naming ambiguity</h3>



<p>Another often-cited issue with the current MCP specification is the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/modelcontextprotocol/modelcontextprotocol/discussions/128">lack of a formal namespace mechanism<span class="sr-only"> (opens in new tab)</span></a>. If two servers are registered to the same agent or application, and the servers have tool names in common, then disambiguation becomes impossible. Libraries like the OpenAI Agents SDK <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://github.com/openai/openai-agents-python/issues/464">raise an error<span class="sr-only"> (opens in new tab)</span></a> under this circumstance. Clients, like Claude Code, prefix tool names with unique identifiers to work around this issue. In our analysis of MCP servers, we found name collisions between 775 tools. The most common collision was “search”, which appears across 32 distinct MCP servers. The following table lists the top 10 collisions.</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td><strong>Tool Name</strong></td><td><strong>Number of Instances</strong></td></tr><tr><td><strong>search</strong></td><td>32</td></tr><tr><td><strong>get_user</strong></td><td>11</td></tr><tr><td><strong>execute_query</strong></td><td>11</td></tr><tr><td><strong>list_tables</strong></td><td>10</td></tr><tr><td><strong>update_task</strong></td><td>9</td></tr><tr><td><strong>generate_image</strong></td><td>9</td></tr><tr><td><strong>send_message</strong></td><td>9</td></tr><tr><td><strong>execute_command</strong></td><td>8</td></tr><tr><td><strong>list_tasks</strong></td><td>8</td></tr><tr><td><strong>search_files</strong></td><td>8</td></tr></tbody></table></figure>



<p>Even when names are unique, they can be semantically similar. If these tools behave similarly, then the redundancy may not be immediately problematic, but if you are expecting to call a particular tool then the name similarities raise the potential for confusion. The following table lists some examples of semantically similar tool names relating to web search:</p>



<figure class="wp-block-table"><table class="has-fixed-layout"><tbody><tr><td>websearch</td><td>brave_web_search</td></tr><tr><td>search-web</td><td>tavily_web_search</td></tr><tr><td>web_search</td><td>google_news_search</td></tr><tr><td>search_web</td><td>google-play-search</td></tr><tr><td>search_webkr</td><td>google_search_parsed</td></tr><tr><td>google_search</td><td>search_google_images</td></tr><tr><td>search_google</td><td>get_webset_search_exa</td></tr><tr><td>ai_web_search</td><td>search_google_scholar</td></tr><tr><td>web_search_exa</td><td>duckduckgo_web_search</td></tr><tr><td>search_web_tool</td><td>google_search_scraper</td></tr><tr><td>web_search_agent</td><td>answer_query_websearch</td></tr><tr><td>batch-web-search</td><td>&nbsp;</td></tr></tbody></table></figure>



<h3 class="wp-block-heading" id="errors-and-error-messages">Errors and error messages</h3>



<p>Like all software libraries, MCP will occasionally encounter error conditions. In these cases, it is important to provide sufficient information for the agent to handle the error and plan next steps. In our analysis, we found this was not always the case. While MCP provides an “IsError” flag to signal errors, we found that it was common for servers to handle errors by returning strings while leaving this flag set to false, signaling a normal exit. Out of 5,983 tool call results with no error flag, GPT-4.1 judged that 3,536 indicated errors in their content. More worrisome: the error messages were often of low quality. For instance, one tool providing web search capabilities failed with the string “error: job,” while another tool providing academic search returned “Please retry with 0 or fewer IDs.”</p>



<h3 class="wp-block-heading" id="resource-sharing-conventions">Resource sharing conventions</h3>



<p>Finally, in addition to tools, MCP allows servers to share resources and resource templates with clients. In our survey, only 112 (7.6%) servers reported any resources, while 74 (5%) provided templates. One potential reason for low adoption is that the current MCP specification provides limited guidance for when resources are retrieved, or how they are incorporated into context. One clearcut situation where a client might retrieve a resource is in response to a tool returning a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://modelcontextprotocol.io/specification/2025-06-18/server/tools#resource-links">resource_link<span class="sr-only"> (opens in new tab)</span></a> as a result &#8212; but only 4 tools exhibited this behavior in our survey (arguably, this would be the ideal behavior for tools that return very long, document-like responses, as outlined earlier).</p>



<p>Conversely, a whole different set of issues arises when there is a need to share resources from the client to the server. Consider for example a tool that provides some analysis of a <em>local</em> PDF file. In the case of a local MCP server utilizing STDIO transport, a local file path can be provided as an argument to the tool, but no similar conventions exist for delivering a local file to a remote MCP server. These issues are challenging enough when implementing a single server. When multiple tools or servers need to interact within the same system, the risk of interoperability errors compounds.</p>



<h2 class="wp-block-heading" id="recommendations">Recommendations</h2>



<p>On balance, along any given dimension, the average MCP server is quite reasonable—but, as we have seen, outliers and diverging assumptions can introduce trouble. While we expect many of these challenges to improve with time, we are comfortable making small recommendations that we feel are evergreen. We organize them below by audience.</p>



<h3 class="wp-block-heading" id="protocol-developers">Protocol developers</h3>



<p>We recognize the advantages of keeping MCP relatively lightweight, avoiding being overly prescriptive in an environment where AI models and use cases are rapidly changing. However, a few small recommendations are warranted. First, we believe MCP should be extended to include a specification for client-provided resources so that tools on remote servers have a mechanism for operating on specified local files or documents. This would more effectively position MCP as a clearinghouse for resources passed between steps of agentic workflows. The MCP specification would also benefit from taking a more opinionated stance on when resources are retrieved and used overall.</p>



<p>Likewise, we believe&nbsp;MCP should&nbsp;quickly move to&nbsp;provide formal namespaces&nbsp;to eliminate tool name collisions.&nbsp;If namespaces&nbsp;are hierarchical, then this also provides a way of organizing large catalogs&nbsp;of functions&nbsp;into thematically&nbsp;related tool&nbsp;sets.&nbsp;Tool sets, as an organizing principle,&nbsp;are already showing some promise&nbsp;in&nbsp;GitHub MCP Server’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/github/github-mcp-server?tab=readme-ov-file#dynamic-tool-discovery" target="_blank" rel="noopener noreferrer">dynamic tool discovery,<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and VS Code’s&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://code.visualstudio.com/updates/v1_103#_tool-grouping-experimental" target="_blank" rel="noopener noreferrer">tool grouping (with virtual tools)<span class="sr-only"> (opens in new tab)</span></a>,&nbsp;where agents or users&nbsp;can&nbsp;enable and disable tools&nbsp;as needed.&nbsp;&nbsp;In the future,&nbsp;a standardized mechanism for grouping tools would allow&nbsp;<em>clients</em>&nbsp;to engage in hierarchical tool-calling,&nbsp;where they first select a category, then select a tool, without needing to keep all possible&nbsp;tools in context.</p>



<h3 class="wp-block-heading" id="server-developers">Server developers</h3>



<p>While our MCP Interviewer tool can catalog many outward-facing properties of MCP servers, developers are often in a much better position to characterize the nature of their tools. To this end, we believe developers should publish an MCP Server card alongside their servers or services, clearly outlining the runtime characteristics of the tools (e.g., the expected number of tokens generated, or expected latency of a tool call). Ideally developers should also indicate which models, agents and clients the server was tested with, how the tools were tested (e.g., provide sample tasks), list any known incompatibilities, and be mindful of limitations of various models throughout development.</p>



<h3 class="wp-block-heading" id="client-developers">Client developers</h3>



<p>Client developers have the opportunity to experiment with various mitigations or optimizations that might help the average MCP server work better for a given system or environment. For example, clients could cache tool schemas, serving them as targets for prompt optimizations, or as an index for RAG-like tool selection approaches. To this end, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://www.anthropic.com/engineering/multi-agent-research-system">Anthropic recently reported using a tool testing agent<span class="sr-only"> (opens in new tab)</span></a> to rewrite the prompts of defective MCP servers, improving task completion time by 40%. Likewise, rather than waiting for the protocol to evolve, clients could take proactive steps to resolve name collisions— for example, generating namespaces from server names—and could reduce token outputs by summarizing or paginating long tool results.</p>



<h3 class="wp-block-heading" id="market-developers">Market developers</h3>



<p>Finally, we see an opportunity for marketplaces to codify best-practices, spot compatibility issues at a global level, and perhaps centralize the generation and serving of model or agent-specific optimizations. Mirroring how a market like PyPI <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://packaging.python.org/en/latest/specifications/platform-compatibility-tags/">distributes Python wheels matched to a developer’s operating system or processor<span class="sr-only"> (opens in new tab)</span></a>, an MCP marketplace could serve tool schemas optimized for a developer’s chosen LLM, agent or client library. We are already seeing small steps in this direction, with registries like Smithery providing customized launch configurations to match users’ clients.</p>



<h2 class="wp-block-heading" id="conclusion">Conclusion</h2>



<p>In summary, the MCP&nbsp;ecosystem offers significant value for AI agent development,&nbsp;despite&nbsp;some&nbsp;early&nbsp;growing pains.&nbsp;Grounded in insights from the&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/mcp-interviewer" target="_blank" rel="noopener noreferrer">MCP Interviewer<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and our survey of live servers, the evidence is clear: horizontal integration is expanding capability, yet it also exposes forms of toolspace interference that can erode end to end effectiveness. Anticipating rapid advances in model capability and growing architectural diversity, the recommendations provided here aim to ensure that protocol, server, client, and marketplace developers are&nbsp;well positioned&nbsp;to adapt and thrive. Key steps include implementing formal namespaces to&nbsp;eliminate&nbsp;collisions, enhancing protocol support for&nbsp;client provided&nbsp;resources, and encouraging transparent server documentation to foster interoperability and robust development practices across the ecosystem.&nbsp;</p>



<p>By embracing these evergreen recommendations and proactively addressing compatibility, usability, and optimization issues, the AI agent community can create a more reliable, scalable, and efficient infrastructure that benefits both developers and end users. The future of MCP is bright, with ample opportunities for experimentation, standardization, and collective progress.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/tool-space-interference-in-the-mcp-era-designing-for-agent-compatibility-at-scale/">Tool-space interference in the MCP era: Designing for agent compatibility at scale</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>RenderFormer: How neural networks are reshaping 3D rendering</title>
		<link>https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/</link>
		
		<dc:creator><![CDATA[Yue Dong]]></dc:creator>
		<pubDate>Wed, 10 Sep 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1149051</guid>

					<description><![CDATA[<p> RenderFormer, from Microsoft Research, is the first model to show that a neural network can learn a complete graphics rendering pipeline. It’s designed to support full-featured 3D rendering using only machine learning—no traditional graphics computation required. </p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: network node icon, lightbulb-shaped icon with a path tool icon in the center; a monitor icon showing a web browser icon" class="wp-image-1149127" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>3D rendering—the process of converting three-dimensional models into two-dimensional images—is a foundational technology in computer graphics, widely used across gaming, film, virtual reality, and architectural visualization. Traditionally, this process has depended on physics-based techniques like ray tracing and rasterization, which simulate light behavior through mathematical formulas and expert-designed models.</p>



<p>Now, thanks to advances in AI, especially neural networks, researchers are beginning to replace these conventional approaches with machine learning (ML). This shift is giving rise to a new field known as neural rendering.</p>



<p>Neural rendering combines deep learning with traditional graphics techniques, allowing models to simulate complex light transport without explicitly modeling physical optics. This approach offers significant advantages: it eliminates the need for handcrafted rules, supports end-to-end training, and can be optimized for specific tasks. Yet, most current neural rendering methods rely on 2D image inputs, lack support for raw 3D geometry and material data, and often require retraining for each new scene—limiting their generalizability.</p>



<h2 class="wp-block-heading" id="renderformer-toward-a-general-purpose-neural-rendering-model">RenderFormer: Toward a general-purpose neural rendering model</h2>



<p>To overcome these limitations, researchers at Microsoft Research have developed RenderFormer, a new neural architecture designed to support full-featured 3D rendering using only ML—no traditional graphics computation required. RenderFormer is the first model to demonstrate that a neural network can learn a complete graphics rendering pipeline, including support for arbitrary 3D scenes and global illumination, without relying on ray tracing or rasterization. <a href="https://www.microsoft.com/en-us/research/publication/renderformer-transformer-based-neural-rendering-of-triangle-meshes-with-global-illumination/">This work</a> has been accepted at SIGGRAPH 2025 and is <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://microsoft.github.io/renderformer" target="_blank" rel="noopener noreferrer">open-sourced on GitHub<span class="sr-only"> (opens in new tab)</span></a>.</p>



<h2 class="wp-block-heading" id="architecture-overview">Architecture overview</h2>



<p>As shown in Figure 1, RenderFormer represents the entire 3D scene using triangle tokens—each one encoding spatial position, surface normal, and physical material properties such as diffuse color, specular color, and roughness. Lighting is also modeled as triangle tokens, with emission values indicating intensity.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="2419" height="1008" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png" alt="Figure 1: The figure illustrates the architecture of RenderFormer. It includes a Triangle Mesh Scene with a 3D rabbit model inside a colored cube, a Camera Ray Map grid, a View Independent Transformer (12 layers of Self-Attention and Feed Forward Network), a View Dependent Transformer (6 layers with Cross-Attention and Self-Attention), and a DPT Decoder. Scene attributes—Vertex Normal, Reflectance (Diffuse, Specular, Roughness), Emission, and Position—are embedded into Triangle Tokens via Linear + Norm operations. These tokens and Ray Bundle Tokens (from the Camera Ray Map) are processed by the respective transformers and decoded to produce a rendered image of a glossy rabbit in a colored room." class="wp-image-1149133" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1.png 2419w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-300x125.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1024x427.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-768x320.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-1536x640.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-2048x853.png 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig1-240x100.png 240w" sizes="auto, (max-width: 2419px) 100vw, 2419px" /><figcaption class="wp-element-caption">Figure 1. Architecture of RenderFormer</figcaption></figure>



<p>To describe the viewing direction, the model uses ray bundle tokens derived from a ray map—each pixel in the output image corresponds to one of these rays. To improve computational efficiency, pixels are grouped into rectangular blocks, with all rays in a block processed together.</p>



<p>The model outputs a set of tokens that are decoded into image pixels, completing the rendering process entirely within the neural network.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144028">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-label="The AI Revolution in Medicine, Revisited" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Episode7-PeterBillSebastien-AIRevolution_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshot of Bill Gates, Peter Lee, and Sébastien Bubeck" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">The AI Revolution in Medicine, Revisited</h2>
				
								<p id="the-ai-revolution-in-medicine-revisited" class="large">Join Microsoft’s Peter Lee on a journey to discover how AI is impacting healthcare and what it means for the future of medicine.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/" aria-describedby="the-ai-revolution-in-medicine-revisited" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="The AI Revolution in Medicine, Revisited" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="dual-branch-design-for-view-independent-and-view-dependent-effects">Dual-branch design for view-independent and view-dependent effects</h2>



<p>The RenderFormer architecture is built around two transformers: one for view-independent features and another for view-dependent ones.</p>



<ul class="wp-block-list">
<li>The <strong>view-independent transformer</strong> captures scene information unrelated to viewpoint, such as shadowing and diffuse light transport, using self-attention between triangle tokens.</li>



<li>The <strong>view-dependent transformer</strong> models effects like visibility, reflections, and specular highlights through cross-attention between triangle and ray bundle tokens.</li>
</ul>



<p>Additional image-space effects, such as anti-aliasing and screen-space reflections, are handled via self-attention among ray bundle tokens.</p>



<p>To validate the architecture, the team conducted ablation studies and visual analyses, confirming the importance of each component in the rendering pipeline.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="963" height="509" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png" alt="Table 1: A table comparing the performance of different network variants in an ablation study. The columns are labeled Variant, PSNR (↑), SSIM (↑), LPIPS (↓), and FLIP (↓). Variants include configurations such as "full view-dependent stage," "w/o DPT," "w/o self-attention," and "w/o DPT & w/o self-attention." Each variant is associated with numerical values for the four metrics, showing how removing or altering components affects performance. The full view-dependent stage achieves the highest PSNR and SSIM and lowest LPIPS and FLIP, indicating optimal performance. Additional rows explore configurations involving camera space and world space view-dependent stages with various token and layer setups." class="wp-image-1149129" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1.png 963w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-300x159.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-768x406.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_table-1-240x127.png 240w" sizes="auto, (max-width: 963px) 100vw, 963px" /><figcaption class="wp-element-caption">Table 1. Ablation study analyzing the impact of different components and attention mechanisms on the final performance of the trained network. </figcaption></figure>



<p>To test the capabilities of the view-independent transformer, researchers trained a decoder to produce diffuse-only renderings. The results, shown in Figure 2, demonstrate that the model can accurately simulate shadows and other indirect lighting effects.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="943" height="240" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png" alt="Figure 2: The figure displays four 3D-rendered objects showcasing view-independent rendering effects. From left to right: a purple teapot on a green surface, a blue rectangular object on a red surface, an upside-down table casting shadows on a green surface, and a green apple-like object on a blue surface. Each object features diffuse lighting and coarse shadow effects, with distinct highlights and shadows produced by directional light sources." class="wp-image-1149132" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2.png 943w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-300x76.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-768x195.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig2-240x61.png 240w" sizes="auto, (max-width: 943px) 100vw, 943px" /><figcaption class="wp-element-caption">Figure 2. View-independent rendering effects decoded directly from the view-independent transformer, including diffuse lighting and coarse shadow effects. </figcaption></figure>



<p>The view-dependent transformer was evaluated through attention visualizations. For example, in Figure 3, the attention map reveals a pixel on a teapot attending to its surface triangle and to a nearby wall—capturing the effect of specular reflection. These visualizations also show how material changes influence the sharpness and intensity of reflections.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="947" height="620" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png" alt="Figure 3: The figure contains six panels arranged in two rows and three columns. The top row displays a teapot in a room with red and green walls under three different roughness values: 0.3, 0.7, and 0.99 (left to right). The bottom row shows the corresponding attention outputs for each roughness setting, featuring the teapot silhouette against a dark background with distinct light patterns that vary with roughness." class="wp-image-1149131" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3.png 947w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-300x196.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-768x503.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig3-240x157.png 240w" sizes="auto, (max-width: 947px) 100vw, 947px" /><figcaption class="wp-element-caption">Figure 3. Visualization of attention outputs</figcaption></figure>



<h2 class="wp-block-heading" id="training-methodology-and-dataset-design">Training methodology and dataset design</h2>



<p>RenderFormer was trained using the Objaverse dataset, a collection of more than 800,000 annotated 3D objects that is designed to advance research in 3D modeling, computer vision, and related fields. The researchers designed four scene templates, populating each with 1–3 randomly selected objects and materials. Scenes were rendered in high dynamic range (HDR) using Blender’s Cycles renderer, under varied lighting conditions and camera angles.</p>



<p>The base model, consisting of 205 million parameters, was trained in two phases using the AdamW optimizer:</p>



<ul class="wp-block-list">
<li>500,000 steps at 256×256 resolution with up to 1,536 triangles</li>



<li>100,000 steps at 512×512 resolution with up to 4,096 triangles</li>
</ul>



<p>The model supports arbitrary triangle-based input and generalizes well to complex real-world scenes. As shown in Figure 4, it accurately reproduces shadows, diffuse shading, and specular highlights.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="805" height="805" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg" alt="Figure 4: The figure presents a 3×3 grid of diverse 3D scenes rendered by RenderFormer. In the top row, the first scene shows a room with red, green, and white walls containing two rectangular prisms; the second features a metallic tree-like structure in a blue-walled room with a reflective floor; and the third depicts a red animal figure, a black abstract shape, and a multi-faceted sphere in a purple container on a yellow surface. The middle row includes three constant width bodies (black, red, and blue) floating above a colorful checkered floor; a green shader ball with a square cavity inside a gray-walled room; and crystal-like structures in green, purple, and red on a reflective surface. The bottom row showcases a low-poly fox near a pink tree emitting particles on grassy terrain; a golden horse statue beside a heart-shaped object split into red and grey halves on a reflective surface; and a wicker basket, a banana and a bottle placed on a white platform." class="wp-image-1149130" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4.jpg 805w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-300x300.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-150x150.jpg 150w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-768x768.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-180x180.jpg 180w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_fig4-360x360.jpg 360w" sizes="auto, (max-width: 805px) 100vw, 805px" /><figcaption class="wp-element-caption">Figure 4. Rendered results of different 3D scenes generated by RenderFormer </figcaption></figure>



<p>RenderFormer can also generate continuous video by rendering individual frames, thanks to its ability to model viewpoint changes and scene dynamics.</p>



<figure class="wp-block-video aligncenter"><video controls src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4"></video><figcaption class="wp-element-caption">3D animation sequence rendered by RenderFormer </figcaption></figure>



<h2 class="wp-block-heading" id="looking-ahead-opportunities-and-challenges">Looking ahead: Opportunities and challenges</h2>



<p>RenderFormer represents a significant step forward for neural rendering. It demonstrates that deep learning can replicate and potentially replace the traditional rendering pipeline, supporting arbitrary 3D inputs and realistic global illumination—all without any hand-coded graphics computations.</p>



<p>However, key challenges remain. Scaling to larger and more complex scenes with intricate geometry, advanced materials, and diverse lighting conditions will require further research. Still, the transformer-based architecture provides a solid foundation for future integration with broader AI systems, including video generation, image synthesis, robotics, and embodied AI. </p>



<p>Researchers hope that RenderFormer will serve as a building block for future breakthroughs in both graphics and AI, opening new possibilities for visual computing and intelligent environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/renderformer-how-neural-networks-are-reshaping-3d-rendering/">RenderFormer: How neural networks are reshaping 3D rendering</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		<enclosure url="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/09/RenderFormer_animate.mp4" length="58187117" type="video/mp4" />

			</item>
		<item>
		<title>Breaking the networking wall in AI infrastructure </title>
		<link>https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/</link>
		
		<dc:creator><![CDATA[Paolo Costa]]></dc:creator>
		<pubDate>Tue, 09 Sep 2025 14:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false"></guid>

					<description><![CDATA[<p>Datacenter memory and network limits are restraining AI system performance. MOSAIC uses microLEDs and a wide-and-slow optical architecture to deliver faster, longer, more reliable, and energy efficient connections that could transform AI cluster designs.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg" alt="Two white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a set of gears; an icon representing three connected nodes each containing a user icon." class="wp-image-1148762" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MicroLED-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Memory and network bottlenecks are increasingly limiting AI system performance by reducing GPU&nbsp;utilization&nbsp;and overall efficiency,&nbsp;ultimately preventing&nbsp;infrastructure from reaching its full potential&nbsp;despite enormous investments.&nbsp;At the&nbsp;core&nbsp;of this challenge is a fundamental trade-off in the communication technologies used for memory and network interconnects.</p>



<p>Datacenters typically deploy two types of physical cables&nbsp;for&nbsp;communication between&nbsp;GPUs.&nbsp;Traditional copper links&nbsp;are power-efficient and&nbsp;reliable,&nbsp;but&nbsp;limited to&nbsp;very short&nbsp;distances&nbsp;(< 2 meters)&nbsp;that&nbsp;restrict their use&nbsp;to within a single&nbsp;GPU&nbsp;rack. Optical&nbsp;fiber&nbsp;links&nbsp;can&nbsp;reach&nbsp;tens of meters,&nbsp;but&nbsp;they&nbsp;consume far more&nbsp;power&nbsp;and fail up to 100 times&nbsp;as often as&nbsp;copper. A&nbsp;team working across&nbsp;Microsoft&nbsp;aims&nbsp;to&nbsp;resolve&nbsp;this trade-off&nbsp;by&nbsp;developing&nbsp;MOSAIC,&nbsp;a novel optical link technology&nbsp;that&nbsp;can provide&nbsp;low power and cost, high reliability, and long reach (up to 50 meters)&nbsp;<em>simultaneously</em>.&nbsp;This approach leverages a hardware-system co-design and adopts&nbsp;a wide-and-slow design with hundreds of parallel low-speed channels using&nbsp;microLEDs.&nbsp;</p>



<p>The fundamental trade-off&nbsp;among&nbsp;power, reliability, and reach&nbsp;stems from&nbsp;the&nbsp;<em>narrow-and-fast</em>&nbsp;architecture&nbsp;deployed&nbsp;in&nbsp;today&#8217;s copper and optical links,&nbsp;comprising&nbsp;a few channels&nbsp;operating&nbsp;at&nbsp;very high&nbsp;data rates. For example,&nbsp;an&nbsp;800 Gbps link&nbsp;consists of eight 100 Gbps channels.&nbsp;With&nbsp;copper links, higher channel speeds lead to greater signal integrity challenges, which limits their reach.&nbsp;With optical&nbsp;links,&nbsp;high-speed transmission is inherently inefficient, requiring power-hungry laser drivers and&nbsp;complex electronics&nbsp;to compensate for transmission impairments. These challenges&nbsp;grow&nbsp;as speeds increase&nbsp;with&nbsp;every&nbsp;generation&nbsp;of networks.&nbsp;Transmitting at high speeds also pushes the limits of optical components, reducing&nbsp;systems&nbsp;margins&nbsp;and increasing failure rates.&nbsp;</p>



<p>These limitations force systems designers to make unpleasant&nbsp;choices,&nbsp;limiting the scalability of AI infrastructure.&nbsp;For example,&nbsp;scale-up networks connecting AI accelerators at&nbsp;multi-Tbps&nbsp;bandwidth&nbsp;typically&nbsp;must&nbsp;rely on&nbsp;copper links&nbsp;to meet&nbsp;the&nbsp;power budget,&nbsp;requiring&nbsp;ultra-dense racks that&nbsp;consume&nbsp;hundreds of kilowatts&nbsp;<em>per rack</em>. This creates significant challenges in cooling&nbsp;and&nbsp;mechanical design,&nbsp;which constrain&nbsp;the practical scale of these networks and end-to-end performance. This imbalance&nbsp;ultimately&nbsp;erects&nbsp;a&nbsp;<em>networking wall</em>&nbsp;akin&nbsp;to the&nbsp;<em>memory wall</em>, in&nbsp;which CPU speeds have outstripped memory speeds, creating performance bottlenecks.</p>



<p class="has-text-align-left">A technology offering copper-like power efficiency and reliability over long distances can overcome this networking wall, enabling multi-rack scale-up domains and unlocking new architectures. This is a highly active R&D area, with many candidate technologies currently being developed across the industry. In our recent paper, <em>“<a href="https://www.microsoft.com/en-us/research/publication/mosaic-breaking-the-optics-versus-copper-trade-off-with-a-wide-and-slow-architecture-and-microleds/">MOSAIC: Breaking the Optics versus Copper Trade-off with a Wide-and-Slow Architecture and MicroLEDs</a>”</em>, which received the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://conferences.sigcomm.org/sigcomm/2025/program/papers-info/" target="_blank" rel="noopener noreferrer">Best Paper award at ACM SIGCOMM<span class="sr-only"> (opens in new tab)</span></a>, we present one such promising approach that is the result of a multi-year collaboration between Microsoft Research, Azure, and M365. This work is centered around an optical wide-and-slow architecture, shifting from a small number of high-speed serial channels towards hundreds of parallel low-speed channels. This would be impractical to realize with today’s copper and optical technologies because of i) electromagnetic interference challenges in high-density copper cables and ii) the high cost and power consumption of lasers in optical links, as well as the increase in packaging complexity. MOSAIC overcomes these issues by leveraging directly modulated microLEDs, a technology originally developed for screen displays. </p>



<p>MicroLEDs&nbsp;are significantly smaller than traditional LEDs (ranging from a few to tens of&nbsp;microns) and, due to their&nbsp;small size,&nbsp;they&nbsp;can be modulated at several Gbps.&nbsp;They&nbsp;are manufactured in large arrays,&nbsp;with over half a million&nbsp;in a small physical footprint for high-resolution displays&nbsp;like&nbsp;head-mounted devices or smartwatches. For example, assuming 2 Gbps per&nbsp;microLED&nbsp;channel, an 800 Gbps MOSAIC link can be realized by using a 20×20&nbsp;microLED&nbsp;array, which can fit in less than 1 mm×1 mm&nbsp;silicon&nbsp;die.&nbsp;</p>



<p>MOSAIC’s&nbsp;wide-and-slow&nbsp;design&nbsp;provides four core benefits.</p>



<ul class="wp-block-list">
<li>Operating&nbsp;at low speed improves power efficiency&nbsp;by&nbsp;eliminating&nbsp;the need for&nbsp;complex&nbsp;electronics&nbsp;and&nbsp;reducing optical power requirements.</li>



<li>By&nbsp;leveraging&nbsp;optical transmission (via&nbsp;microLEDs),&nbsp;MOSAIC&nbsp;sidesteps&nbsp;copper’s reach issues, supporting distances up to 50 meters,&nbsp;or&nbsp;> 10x&nbsp;further&nbsp;than copper.</li>



<li>MicroLEDs’&nbsp;simpler structure&nbsp;and temperature insensitivity&nbsp;make them more reliable than lasers. The parallel nature of&nbsp;wide-and-slow&nbsp;also&nbsp;makes it easy to add redundant channels, further increasing reliability, up to two orders of magnitude higher than optical links.&nbsp;</li>



<li>The&nbsp;approach is also scalable, as higher aggregate speeds (e.g.,&nbsp;1.6&nbsp;Tbps&nbsp;or 3.2&nbsp;Tbps) can be achieved by increasing the number of&nbsp;channels and/or raising per-channel speed&nbsp;(e.g., to 4-8 Gbps).&nbsp;</li>
</ul>



<p>Further,&nbsp;MOSAIC is fully compatible with today’s pluggable transceivers’ form&nbsp;factor&nbsp;and it provides a drop-in replacement for today’s copper and optical cables, without requiring any changes to existing server and network infrastructure.&nbsp;MOSAIC is protocol-agnostic, as it simply relays bits from one endpoint to another without&nbsp;terminating&nbsp;or inspecting the connection&nbsp;and, hence,&nbsp;it’s&nbsp;fully compatible with today’s protocols (e.g.,&nbsp;Ethernet, PCIe, CXL).&nbsp;We are currently working with our suppliers to&nbsp;productize&nbsp;this technology and&nbsp;scale&nbsp;to mass production.&nbsp;</p>



<p>While&nbsp;conceptually simple, realizing this architecture posed a few key challenges&nbsp;across the stack, which&nbsp;required&nbsp;a multi-disciplinary team with&nbsp;expertise&nbsp;spanning across integrated photonics, lens design, optical transmission, and&nbsp;analog&nbsp;and digital design.&nbsp;For example, using individual&nbsp;fibers&nbsp;per channel would be prohibitively complex and costly due to the&nbsp;large number&nbsp;of channels. We addressed this by employing imaging&nbsp;fibers,&nbsp;which are typically used for medical applications (e.g., endoscopy).&nbsp;They&nbsp;can support thousands of cores&nbsp;per&nbsp;fiber, enabling multiplexing&nbsp;of&nbsp;many channels within a single&nbsp;fiber.&nbsp;Also,&nbsp;microLEDs&nbsp;are a less pure light source&nbsp;than lasers,&nbsp;with&nbsp;a larger beam shape (which complicates&nbsp;fiber&nbsp;coupling) and&nbsp;a broader spectrum (which&nbsp;degrades&nbsp;fiber&nbsp;transmission due to chromatic dispersion).&nbsp;We tackled these issues through&nbsp;a novel&nbsp;microLED and&nbsp;optical lens design,&nbsp;and&nbsp;a power-efficient&nbsp;analog-only electronic back&nbsp;end, which does not require any expensive digital signal processing.&nbsp;&nbsp;</p>



<p>Based on our current estimates, this approach can save&nbsp;up to 68% of power, i.e., more&nbsp;than 10W per cable while reducing failure rates by up to 100x. With global annual shipments of optical cables&nbsp;reaching into&nbsp;the tens of millions, this translates to over 100MW of power savings per year,&nbsp;enough to power more than 300,000 homes. While these immediate gains are already significant, the unique combination of low power consumption, reduced cost, high reliability, and long reach opens up exciting new opportunities&nbsp;to rethink&nbsp;AI&nbsp;infrastructure from network and cluster architectures to compute and memory designs.</p>



<p>For example,&nbsp;by&nbsp;supporting&nbsp;low-power,&nbsp;high-bandwidth connectivity at long reach,&nbsp;MOSAIC&nbsp;removes the need for ultra-dense racks and&nbsp;enables&nbsp;novel network topologies, which would be impractical today. The resulting redesign could&nbsp;reduce&nbsp;resource fragmentation and&nbsp;simplify&nbsp;collective optimization.&nbsp;Similarly,&nbsp;on the&nbsp;compute&nbsp;front,&nbsp;the ability&nbsp;to&nbsp;connect&nbsp;silicon&nbsp;dies at low power over long distances&nbsp;could&nbsp;enable&nbsp;resource&nbsp;disaggregation, shifting from today’s&nbsp;large,&nbsp;multi-die packages to&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/good-things-come-in-small-packages-should-we-adopt-lite-gpus-in-ai-infrastructure/">smaller, more cost-effective, ones</a>.&nbsp;Bypassing packaging area constraints would also make it possible to drastically increase&nbsp;GPU&nbsp;memory&nbsp;capacity and bandwidth,&nbsp;while&nbsp;facilitating&nbsp;adoption of&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/storage-class-memory-is-dead-all-hail-managed-retention-memory-rethinking-memory-for-the-ai-era/">novel memory technologies</a>.&nbsp;</p>



<p>Historically, step changes in network technology have unlocked entirely new classes of applications and workloads. While our SIGCOMM paper provides&nbsp;possible future&nbsp;directions, we hope this work sparks broader discussion and collaboration across the research and industry communities.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/breaking-the-networking-wall-in-ai-infrastructure/">Breaking the networking wall in AI infrastructure </a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Crescent library brings privacy to digital identity systems</title>
		<link>https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/</link>
		
		<dc:creator><![CDATA[Christian Paquin, Greg Zaverucha]]></dc:creator>
		<pubDate>Tue, 26 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148317</guid>

					<description><![CDATA[<p>Crescent helps make digital IDs private by preventing tracking across uses while letting users only disclose what’s necessary from their credentials.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/">Crescent library brings privacy to digital identity systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: icon representing a computer chip, padlock icon, an avatar icon" class="wp-image-1148394" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Digital identities, the electronic credentials embedded in phone wallets, workplace logins, and other apps, are becoming ubiquitous. While they offer unprecedented convenience, they also create new privacy risks, particularly around tracking and surveillance.&nbsp;</p>



<p>One of these risks is <em>linkability, </em>the ability to associate one or more uses of a credential to a specific person. Currently, when people use their mobile driver&#8217;s license or log into various apps, hidden identifiers can link these separate activities together, building detailed profiles of user behavior.&nbsp;&nbsp;</p>



<p>To address this, we have released <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://eprint.iacr.org/2024/2013" target="_blank" rel="noopener noreferrer">Crescent<span class="sr-only"> (opens in new tab)</span></a>, a cryptographic library that adds <em>unlinkability </em>to widely used identity formats, protecting privacy. These include JSON Web Tokens (the authentication standard behind many app logins) and mobile driver&#8217;s licenses. Crescent also works without requiring the organizations that issue these credentials to update their systems. &nbsp;</p>



<p>The protection goes beyond existing privacy features. Some digital identity systems already offer <em>selective disclosure</em>, allowing users to share only specific pieces of information in each interaction. &nbsp;</p>



<p>But even with selective disclosure, credentials can still be linked through serial numbers, cryptographic signatures, or embedded identifiers. Crescent&#8217;s unlinkability feature is designed to prevent anything in the credential, beyond what a user explicitly chooses to reveal, from being used to connect their separate digital interactions.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="400" height="242" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability.png" alt="Figure 1: Unlinkability between a credential issuance and presentation" class="wp-image-1148323" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability.png 400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability-300x182.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig1_unlinkability-240x145.png 240w" sizes="auto, (max-width: 400px) 100vw, 400px" /><figcaption class="wp-element-caption">Figure 1: Unlinkability between a credential issuance and presentation</figcaption></figure>



<h2 class="wp-block-heading" id="two-paths-to-unlinkability">Two paths to unlinkability&nbsp;</h2>



<p>To understand how Crescent works, it helps to examine the two main approaches researchers have developed for adding unlinkability to identity systems:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Specialized cryptographic signature schemes</strong>. These schemes can provide unlinkability but require extensive changes to existing infrastructure. New algorithms must be standardized, implemented, and integrated into software and hardware platforms. For example, the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://identity.foundation/bbs-signature/draft-irtf-cfrg-bbs-signatures.html" target="_blank" rel="noopener noreferrer">BBS<span class="sr-only"> (opens in new tab)</span></a> signature scheme is currently being standardized by the Internet Engineering Task Force (IETF), but even after completion, adoption may be slow.&nbsp;&nbsp;&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li><strong>Zero-knowledge proofs with existing credentials</strong>. This approach, used by <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/crescent-credentials" target="_blank" rel="noopener noreferrer">Crescent<span class="sr-only"> (opens in new tab)</span></a>, allows users to prove specific facts about their credentials without revealing the underlying data that could enable tracking. For example, someone could prove they hold a valid driver&#8217;s license and live in a particular ZIP code without exposing any other personal information or identifiers that could link this interaction to future ones.&nbsp;</li>
</ol>



<p>Zero-knowledge proofs have become more practical since they were first developed 40 years ago but they are not as efficient as the cryptographic algorithms used in today’s credentials. Crescent addresses this computational challenge through preprocessing, performing the most complex calculations once in advance so that later proof generation is quick and efficient for mobile devices.&nbsp;</p>



<p>Beyond unlinkability, Crescent supports selective disclosure, allowing users to prove specific facts without revealing unnecessary details. For example, it can confirm that a credential is valid and unexpired without disclosing the exact expiration date, which might otherwise serve as a unique identifier. These privacy protections work even when credentials are stored in a phone&#8217;s secure hardware, which keeps them tied to the device and prevents unauthorized access.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1141385">
		

	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://ai.azure.com/labs" aria-label="Azure AI Foundry Labs" data-bi-cN="Azure AI Foundry Labs" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Azure-AI-Foundry_1600x900.jpg" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Azure AI Foundry Labs</h2>
				
								<p id="azure-ai-foundry-labs" class="large">Get a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://ai.azure.com/labs" aria-describedby="azure-ai-foundry-labs" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Azure AI Foundry Labs" target="_blank">
							Azure AI Foundry						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="behind-the-cryptographic-curtain">Behind the cryptographic curtain&nbsp;</h2>



<p>At its core, Crescent uses a sophisticated form of cryptographic proof called a zero-knowledge SNARK (Zero-Knowledge Succinct Noninteractive Argument of Knowledge). This method allows one party to prove possession of information or credentials without revealing the underlying data itself.&nbsp;</p>



<p>Crescent specifically uses the Groth16 proof system, one of the first practical implementations of this technology. What makes Groth16 particularly useful is that its proofs are small in size, quick to verify, and can be shared in a single step without back-and-forth communication between the user and verifier.&nbsp;</p>



<p>The system works by first establishing shared cryptographic parameters based on a credential template. Multiple organizations issuing similar credentials, such as different state motor vehicle departments issuing mobile driver&#8217;s licenses, can use the same parameters as long as they follow compatible data formats and security standards.&nbsp;</p>



<p>The mathematical rules that define what each proof will verify are written using specialized programming tools that convert them into a Rank-1 Constraint System (R1CS), a mathematical framework that describes exactly what needs to be proven about a credential.&nbsp;</p>



<p>To make the system fast enough for real-world use, Crescent splits the proof generation into two distinct stages:&nbsp;</p>



<ol start="1" class="wp-block-list">
<li><strong>Prepare stage</strong>. This step runs once and generates cryptographic values that can be stored on the user&#8217;s device for repeated use.&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li><strong>Show stage</strong>. When a user needs to present their credential, this quicker step takes the stored values and randomizes them to prevent any connection to previous presentations. It also creates a compact cryptographic summary that reveals only the specific information needed for that particular interaction.&nbsp;</li>
</ol>



<p>Figures 2 and 3 illustrate this credential-proving workflow and the division between the prepare and show steps.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="453" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline.png" alt="Figure 2: Crescent’s credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier." class="wp-image-1148322" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-300x97.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-1024x331.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-768x249.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig2_crescent_pipeline-240x78.png 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2: Crescent’s credential-proving workflow includes a compilation of a circuit to R1CS, followed by the prepare and show steps. The output zero-knowledge proof is sent to the verifier. </figcaption></figure>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="443" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview.png" alt="Figure 3: The Crescent presentation steps show the division between prepare and show steps." class="wp-image-1148321" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview.png 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-300x95.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-1024x324.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-768x243.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig3_proof_overview-240x76.png 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 3: The Crescent presentation steps show the division between prepare and show steps.</figcaption></figure>



<h2 class="wp-block-heading" id="a-sample-application">A sample application&nbsp;</h2>



<p>To demonstrate how Crescent works, we created a sample application covering two real-world scenarios: verifying employment and proving age for online access. The application includes sample code for setting up fictional issuers and verifiers as Rust servers, along with a browser-extension wallet for the user. The step numbers correspond to the steps in Figure 4.&nbsp;</p>



<h3 class="wp-block-heading" id="setup">Setup&nbsp;</h3>



<ol start="1" class="wp-block-list">
<li>A Crescent service pre-generates the zero-knowledge parameters for creating and verifying proofs from JSON Web Tokens and mobile driver’s licenses.&nbsp;</li>
</ol>



<ol start="2" class="wp-block-list">
<li>The user obtains a mobile driver’s license from their Department of Motor Vehicles.&nbsp;</li>
</ol>



<ol start="3" class="wp-block-list">
<li>The user obtains a proof-of-employment JSON Web Token from their employer, Contoso.&nbsp;</li>
</ol>



<ol start="4" class="wp-block-list">
<li>These credentials and their private keys are stored in the Crescent wallet.&nbsp;</li>
</ol>



<h3 class="wp-block-heading" id="scenarios">Scenarios&nbsp;</h3>



<ol start="5" class="wp-block-list">
<li><strong>Employment verification</strong>: The user presents their JSON Web Token to Fabrikam, an online health clinic, to prove they are employed at Contoso and eligible for workplace benefits. Fabrikam learns that the user works at Contoso but not the user&#8217;s identity, while Contoso remains unaware of the interaction.&nbsp;</li>
</ol>



<ol start="6" class="wp-block-list">
<li><strong>Age verification</strong>:<strong> </strong>The user presents their mobile driver’s license to a social network, proving they are over 18. The proof confirms eligibility without revealing their age or identity.&nbsp;</li>
</ol>



<p>Across both scenarios, Crescent ensures that credential presentations remain unlinkable, preventing any party from connecting them to the user.&nbsp;</p>



<p>For simplicity, the sample defines its own issuance and presentation protocol, but it could be integrated into higher-level identity frameworks such as OpenID/OAuth, Verifiable Credentials, or the mobile driver’s license ecosystem.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="800" height="502" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch.png" alt="Figure 4. The sample architecture, from credential issuance to presentation." class="wp-image-1148404" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch.png 800w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-300x188.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-768x482.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Crescent_fig4_sample-arch-240x151.png 240w" sizes="auto, (max-width: 800px) 100vw, 800px" /><figcaption class="wp-element-caption">Figure 4. The sample architecture, from credential issuance to presentation.</figcaption></figure>



<p>To learn more about the project, visit the Crescent project <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/microsoft/crescent-credentials/" target="_blank" rel="noopener noreferrer">GitHub<span class="sr-only"> (opens in new tab)</span></a> page, or check out our recent presentations given at the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/live/gnB76DQI1GE?t=3475s" target="_blank" rel="noopener noreferrer">Real-Word Crypto 2025<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.youtube.com/watch?v=9IT659uUXfs&t=13361s" target="_blank" rel="noopener noreferrer">North Sec 2025<span class="sr-only"> (opens in new tab)</span></a> conferences. </p>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/crescent-library-brings-privacy-to-digital-identity-systems/">Crescent library brings privacy to digital identity systems</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Applicability vs. job displacement: further notes on our recent research on AI and occupations</title>
		<link>https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/</link>
		
		<dc:creator><![CDATA[Kiran Tomlinson, Sonia Jaffe, Will Wang, Scott Counts, Siddharth Suri]]></dc:creator>
		<pubDate>Thu, 21 Aug 2025 17:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148193</guid>

					<description><![CDATA[<p>Recently, we released a paper Working with AI: Measuring the Occupational Implications of Generative AI that studied what occupations might find AI chatbots useful, and to what degree. The paper sparked significant discussion, which is no surprise since people care deeply about the future of AI and jobs--that’s part of why we think it’s important to study these topics.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: a network structure with connected circles, an upward-trending line graph with bars and an arrow, and a checklist with horizontal lines and checkmarks." class="wp-image-1148296" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/WhatOurPaperSays-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>Recently, we released a paper&nbsp;(<em><a href="https://www.microsoft.com/en-us/research/publication/working-with-ai-measuring-the-occupational-implications-of-generative-ai/">Working with AI: Measuring the Occupational Implications of Generative AI</a></em>)&nbsp;that studied what occupations might&nbsp;find&nbsp;AI chatbots&nbsp;useful, and to what degree.&nbsp;The paper sparked significant discussion,&nbsp;which is no&nbsp;surprise&nbsp;since&nbsp;people care&nbsp;deeply&nbsp;about&nbsp;the future of AI and&nbsp;jobs&#8211;that’s part of why we think&nbsp;it’s&nbsp;important to study these&nbsp;topics.</p>



<p>Unfortunately, not all the&nbsp;discussion&nbsp;was&nbsp;accurate&nbsp;in its portrayal of the&nbsp;study’s scope or conclusions.&nbsp;Specifically, our&nbsp;study&nbsp;does not&nbsp;draw any conclusions about jobs being eliminated; in the paper,&nbsp;we&nbsp;explicitly&nbsp;cautioned&nbsp;against using our findings to make that conclusion.&nbsp;</p>



<p>Given the importance&nbsp;of this&nbsp;topic, we&nbsp;want&nbsp;to&nbsp;clarify any misunderstandings and&nbsp;provide&nbsp;a more digestible summary of the paper,&nbsp;our&nbsp;methodology,&nbsp;and its limitations.&nbsp;</p>



<h2 class="wp-block-heading" id="what-did-our-research-find">What&nbsp;did our research find?</h2>



<p>We set out to better understand how people are using AI, <strong>highlighting where AI might be useful in different occupations</strong>. To do this, we analyzed how people currently use generative AI—specifically Microsoft Bing Copilot (now Microsoft Copilot)—to assist with tasks. We then compared these sets of tasks against the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.onetcenter.org/overview.html" target="_blank" rel="noopener noreferrer">O*NET database<span class="sr-only"> (opens in new tab)</span></a>, a widely used occupational classification system, to understand potential applicability to various occupations.</p>



<p>We found&nbsp;that AI&nbsp;is most&nbsp;useful&nbsp;for&nbsp;tasks related to knowledge work and communication, particularly tasks such as writing, gathering information, and learning.</p>



<p>Those in occupations with these tasks&nbsp;may benefit by&nbsp;considering&nbsp;how AI&nbsp;can be used&nbsp;as a tool to help improve their workflows. On the&nbsp;flip side,&nbsp;it’s&nbsp;not surprising that physical tasks like performing surgeries or moving objects had less&nbsp;direct&nbsp;AI&nbsp;chatbot applicability.</p>



<p>So, to summarize, our paper is about&nbsp;identifying&nbsp;the occupations where&nbsp;AI may be most useful,&nbsp;by&nbsp;assisting&nbsp;or performing subtasks.&nbsp;&nbsp;Our data do&nbsp;not&nbsp;indicate, nor&nbsp;did&nbsp;we&nbsp;suggest, that certain jobs will be replaced by AI.</p>



<h2 class="wp-block-heading" id="methodological-limitations-are-acknowledged-and-important">Methodological limitations are acknowledged—and important</h2>



<p>The paper is transparent about the limitations of our approach.&nbsp;&nbsp;</p>



<p>We analyzed&nbsp;anonymized&nbsp;Bing Copilot conversations to see what&nbsp;activities&nbsp;users are seeking AI&nbsp;assistance&nbsp;with and what activities AI can perform when mapped to the O*NET database.&nbsp;While O*NET provides a structured list of&nbsp;activities&nbsp;associated with various occupations, it does&nbsp;<strong>not</strong>&nbsp;capture the full spectrum of skills, context, and nuance&nbsp;required&nbsp;in the real&nbsp;world.&nbsp;&nbsp;<strong>A job is far more than the collection of tasks that make&nbsp;it up.</strong></p>



<p>For example, a task might involve “writing reports,” but O*NET&nbsp;won’t&nbsp;reflect the interpersonal judgment, domain&nbsp;expertise, or ethical considerations that go into doing that well. The paper acknowledges this gap and warns against over-interpreting the AI applicability scores as measures of AI’s ability to perform an occupation.</p>



<p>Additionally, the dataset is based on user queries from Bing Copilot (from January – September 2024), which may be influenced by factors like awareness, access, or comfort with AI tools.&nbsp;&nbsp;Different people use different LLMs for different purposes and it also is&nbsp;very difficult&nbsp;(or&nbsp;nearly impossible) to&nbsp;determine&nbsp;what conversations are performed in a work context or for leisure.&nbsp;</p>



<p>Finally, we only evaluated AI chatbot usage, so this study does not evaluate the impact or applicability of other forms of AI.</p>



<h2 class="wp-block-heading" id="where-do-we-go-from-here-1">Where do we go from here?</h2>



<p>Given the intense interest in how AI will shape our collective future,&nbsp;it&#8217;s&nbsp;important we continue to study and better understand its societal and economic impact. As with&nbsp;all&nbsp;research on this topic,&nbsp;the findings&nbsp;are&nbsp;nuanced, and&nbsp;it’s&nbsp;important to pay attention to this nuance.&nbsp;</p>



<p>The public interest in our research is based, in large part, on the&nbsp;topic&nbsp;of AI&nbsp;and job displacement.&nbsp;However,&nbsp;our current&nbsp;methodology&nbsp;for this study&nbsp;is unlikely to lead to firm conclusions about this.&nbsp;&nbsp;AI may prove to be a useful tool for many occupations, and we believe the right balance lies in finding how to use the technology in a way that&nbsp;leverages&nbsp;its abilities while complementing human strengths and accounting for people&#8217;s preferences.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>For more information from Microsoft on the future of work and AI skilling, check out Microsoft’s Annual&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://news.microsoft.com/annual-work-trend-index-2025/" target="_blank" rel="noopener noreferrer">Work Trend Index<span class="sr-only"> (opens in new tab)</span></a>&nbsp;and&nbsp;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://blogs.microsoft.com/on-the-issues/2025/07/09/elevate/" target="_blank" rel="noopener noreferrer">Microsoft Elevate<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/applicability-vs-job-displacement-further-notes-on-our-recent-research-on-ai-and-occupations/">Applicability vs. job displacement: further notes on our recent research on AI and occupations</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</title>
		<link>https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/</link>
		
		<dc:creator><![CDATA[Peter Lee, Carey Goldberg, Dr. Isaac Kohane]]></dc:creator>
		<pubDate>Thu, 21 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1148218</guid>

					<description><![CDATA[<p>For the series finale, Peter Lee, Carey Goldberg, and Dr. Zak Kohane compare their predictions to insights from the series’ most recent guests, including experts on AI’s economic and societal impact, leaders in AI-driven medicine, and doctors in training.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/">Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated headshots of Carey Goldberg, Peter Lee, and Dr. Isaac Kohane." class="wp-image-1148279" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode11-Peter-Carey-Isaac_AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=147919496&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this series finale, Lee welcomes back coauthors <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.careygoldberg.net/" target="_blank" rel="noopener noreferrer">Carey Goldberg<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dbmi.hms.harvard.edu/people/isaac-kohane" target="_blank" rel="noopener noreferrer">Dr. Zak Kohane<span class="sr-only"> (opens in new tab)</span></a> to discuss how their predictions stack up against key takeaways from guests in the second half of the series: experts on AI’s economic and societal impact; technologists on the cutting edge; leaders in AI-driven medicine; next-generation physicians; and heads of healthcare organizations. Lee, Goldberg, and Kohane explore thinking innovatively about existing healthcare processes, including the structure of care teams and the role of specialties, to take advantage of AI opportunities and consider what clinicians and patients might need these new AI tools to be to feel empowered when it comes to giving and receiving the best healthcare. They close the episode with their hopes for the future of AI in health.</p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a href="https://www.microsoft.com/en-us/research/publication/scalable-emulation-of-protein-equilibrium-ensembles-with-generative-deep-learning/">Scalable emulation of protein equilibrium ensembles with generative deep learning</a>&nbsp;<br>Publication | July 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/sequential-diagnosis-with-language-models/">Sequential Diagnosis with Language Models</a>&nbsp;<br>Publication | July 2025</li>



<li><a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/05/19/developing-next-generation-cancer-care-management-with-multi-agent-orchestration/" target="_blank" rel="noreferrer noopener">Developing next-generation cancer care management with multi-agent orchestration</a>&nbsp;<br>Microsoft Industry Blogs | May 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/">The AI Revolution in Medicine: GPT-4 and Beyond</a><br>Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;</li>
</ul>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript</h2>



<p>[MUSIC] </p>



<p>[BOOK PASSAGE]&nbsp;</p>



<p><strong>PETER LEE: </strong>“As a society—indeed, as a species—we have a choice to make. Do we constrain or even kill artificial intelligence out of fear of its risks and obvious ability to create new harms? Do we submit ourselves to Al and allow it to freely replace us, make us less useful and less needed? Or do we start, today, shaping our Al future together, with the aspiration to accomplish things that humans alone, and Al alone, can&#8217;t do but that humans+Al can? The choice is in our hands … .”&nbsp;</p>



<p>[END OF BOOK PASSAGE]</p>



<p>[THEME MUSIC]</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. </p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? </p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here. </p>



				</span>
				<span id="show-more-show-less-toggle-1" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The book passage I read at the top is from the epilogue, and I think it’s a truly fitting closing sentiment for the conclusion of this podcast series—because it calls back to the very beginning.</p>



<p>As I’ve mentioned before, Carey, Zak, and I wrote <em>The AI Revolution in Medicine</em> as a guide to help answer these big questions, particularly as they pertain to medicine. You know, we wrote the book to empower people to make a choice about AI’s development and use. Well, have they? Have <em>we</em>?</p>



<p>Perhaps we’ll need more time to tell. But over the course of this podcast series, I’ve had the honor of speaking with folks from across the healthcare ecosystem. And my takeaway? They’re all committed to shaping AI into a tool that can improve the industry for practitioners and patients alike.</p>



<p>In this final episode, I’m thrilled to welcome back my coauthors, Carey Goldberg and Dr. Zak Kohane. We’ll examine the insights from the second half of the season.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Carey, Zak—it’s really great to have you here again!&nbsp;</p>



<p><strong>CAREY</strong> <strong>GOLDBERG: </strong>Hey, Peter!&nbsp;</p>



<p><strong>ZAK</strong> <strong>KOHANE:</strong> Hi, Peter.&nbsp;</p>



<p><strong>LEE:</strong> So this is the second roundtable. And just to recap, you know, we had several early episodes of the podcast where we talked to some doctors, some technology developers, some people who think about regulation and public policy, patient advocates, a venture capitalist who invests in, kind of, consumer and patient-facing medical ventures, and some bioethicists.&nbsp;</p>



<p>And I think we had a great conversation there. I think, you know, it felt mostly validating. A lot of the things that we predicted might happen happened, and then we learned a lot of new things. But now we have five more episodes, and the mix of kinds of people that we talk to here is different than the original.&nbsp;</p>



<p>And so I thought it would be great for us to have a conversation and recap what we think we heard from all of them. So let&#8217;s just start at the top.&nbsp;</p>



<p>So in this first episode in the second half of this podcast series, we talked to economists Azeem Azhar and Ethan Mollick. And I thought those conversations were really interesting. Maybe there were, kind of, two things, two main topics. One was just the broader impact on the economy, on the cost of healthcare, on overall workforce issues.&nbsp;</p>



<p>One of the things that I thought was really interesting was something that Ethan Mollick brought up. And maybe just to refresh our memories, let&#8217;s play this little clip from Ethan.<strong>&nbsp;</strong></p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-ab782358e1fff73901dd881b85e04b33"><strong><em>ETHAN MOLLICK: </em></strong><em>So</em><strong><em> </em></strong><em>we’re in this really interesting period where there’s incredible amounts of individual innovation in productivity and performance improvements in this field, like very high levels of it. …</em><em> </em><em>We’re seeing that in nonmedical problems, the same kind of thing, which is, you know, we’ve got research showing 20 and 40% performance improvements. … But then the organization doesn’t capture it; the system doesn’t capture it. Because the individuals are doing their own work, and the systems don’t have the ability to, kind of, learn or adapt as a result.</em>&nbsp;</p>



<p><strong>LEE:</strong> So let me start with you, Zak. Does that make sense to you? Are you seeing something similar?&nbsp;</p>



<p><strong>KOHANE:</strong> I thought it was incredibly insightful because we discussed on our earlier podcast how a chief AI officer in one of the healthcare hospitals, in one of the healthcare systems, was highly regulating the use of AI, but yet in her own practice on her smartphone was using all these AI technologies.&nbsp;</p>



<p>And so it&#8217;s insightful that on the one hand, she is increasing her personal productivity, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> … and perhaps she&#8217;s increasing her quality of her care. But it&#8217;s very hard for the healthcare system to actually realize any gains. It&#8217;s unlikely … let&#8217;s put it this way. It would be for her a defeat if they said, “Now you should see <em>more</em> patients.”&nbsp;</p>



<p><strong>LEE:</strong> Yes. [LAUGHS]&nbsp;</p>



<p><strong>KOHANE:</strong> Now, I&#8217;m not saying that won&#8217;t happen. It could happen. But, you know, gains of productivity are really at the individual level of the doctors. And that&#8217;s why they&#8217;re adopting it. That&#8217;s why the ambient dictation tools are so successful. But really turning it into things that matter in terms of productivity for healthcare, namely making sure that patients are getting healthy, requires that every piece of the puzzle works well together. You know, it&#8217;s well-tread ground to talk about how patients get very expensive procedures, like a cardiac transplant, and then go home, and they’re not put on blood thinners …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> … and then they get a stroke. You know, the chain is as strong as the weakest link. And just having AI in one part of it is not going to do it. And so hospitals, I think, are doubly burdened by the fact that, (A) they tend to not like innovation because they are high-revenue, low-margin companies. But if they want it implemented effectively, they have to do it across the entire processes of healthcare, which are vast and not completely under their control.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Yep. You know, that was Sara Murray, who&#8217;s the chief health AI officer at UC San Francisco.&nbsp;</p>



<p>And then, you know, Carey, remember, we were puzzled by Chris Longhurst&#8217;s finding in a controlled study that the, you know, having an AI respond to patient emails didn&#8217;t seem to lead to any, I guess you would call it, <em>productivity benefits</em>. I remember we were both kind of puzzled by that. I wonder if that&#8217;s related to what Ethan is saying here.&nbsp;</p>



<p><strong>GOLDBERG:</strong> I mean, possibly, but I think we&#8217;ve seen since then that there have been multiple studies showing that in fact using AI can be extremely effective or helpful, even, for example, for diagnosis.&nbsp;</p>



<p>And so I find just from the patient point of view, it kind of drives me crazy that you have individual physicians using AI because they know that it will improve the care that they&#8217;re offering. And yet you don&#8217;t have their institutions kind of stepping up and saying, “OK, these are the new norms.”&nbsp;</p>



<p>By the way, Ethan Mollick is a national treasure, right. Like, he is the classic example of someone who just stepped up at this moment …&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… when we saw this extraordinary technological advance. And he&#8217;s not only stepping up for himself. He&#8217;s spreading the word to the masses that this is what these things can do.&nbsp;</p>



<p>And so it&#8217;s frustrating to see the institutions not stepping up and instead the individual doctors having to do it.&nbsp;</p>



<p><strong>KOHANE:</strong> But he made another very interesting point, which was that the reason that <em>he</em> could be so informative to not only the public but practitioners of AI is these things would emerge out of the shop, and they would not be aged too long, like a fine wine, before they were just released to the public.&nbsp;</p>



<p>And so he was getting exposure to these models just weeks after some of the progenitors had first seen it. And therefore, because he&#8217;s actually a really creative person in terms of how he exercises models, he sees uses and problems very early on. But the point is institutions, think about how much they are disadvantaged. They&#8217;re not Ethan Mollick. They&#8217;re not the progenitors. So they&#8217;re even further behind. So it&#8217;s very hard. If you talk to most of the C-suite of hospitals, they&#8217;d be delighted to know as much about the impact as Ethan Mollick.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. By the way, you know, I picked out this quote because within Microsoft, and I suspect every other software company, we&#8217;re seeing something very similar, where individual programmers are 20 to 30% more productive just in the number of lines of code they write per day or the number of pull requests per week. Any way you measure it, it&#8217;s very consistent. And yet by the time you get to, say, a 25-person software engineering team, the productivity of that whole team isn&#8217;t 25% more productive.&nbsp;</p>



<p>Now, that <em>is</em> starting to change because we&#8217;re starting to figure out that, well, maybe we should reshape how the team operates. And there&#8217;s more of an orientation towards having, you know, smaller teams of full-stack developers. And then you start to see the gains. But if you just keep the team organized in the usual way, there seems to be a loss. So there&#8217;s something about what Ethan was saying that resonated very strongly with me.&nbsp;</p>



<p><strong>GOLDBERG: </strong>But I would argue that it&#8217;s not just productivity we&#8217;re talking about. There&#8217;s a moral imperative to improve the care. And if you have tools that will do that, you should be using them or trying harder to.&nbsp;</p>



<p><strong>LEE:</strong> Right. Yep.&nbsp;</p>



<p><strong>KOHANE:</strong> I think, yes, first of all, absolutely you would. Unfortunately, most of the short-term productivity measures will not measure improvements in the quality of care because it takes a long time to die even with bad care.&nbsp;</p>



<p>And so that doesn&#8217;t show up right away. But I think what Peter just said actually came across in several of the podcasts, which is that it&#8217;s very tricky trying to shoehorn these things into making what we&#8217;re already doing more productive.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. Existing structures.&nbsp;</p>



<p><strong>KOHANE:</strong> Yeah. And I know, Carey, that you&#8217;ve raised this issue many times. But it really calls into question, what should we be doing with our time with doctors? And they are a scarce resource. And what is the most efficient way to use them?&nbsp;</p>



<p>You know, I remember we [<em>The New England Journal of Medicine AI</em>] published a paper of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIoa2400296" target="_blank" rel="noopener noreferrer">someone who was able to use AI to increase the throughput of their emergency room<span class="sr-only"> (opens in new tab)</span></a> by actually more appropriately having the truly sick people in the sick queue, in the triage queue, for urgent care.&nbsp;</p>



<p>And so I think we&#8217;re going to have to think that way more broadly, about we don&#8217;t have to now look at every patient as an unknown with maybe a few pointers on diagnosis. We can have a fairly extensive profiling.&nbsp;</p>



<p>And I know that colleagues in Clalit [Health Services] in Israel, for example, are using the overall trajectory of the patient and some considerations about utilities to actually figure out who to see next week.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, you know, what you said brings up another maybe connection to one thing that we see also in software development. And it relates to also what we were discussing earlier: about the last thing a doctor wants is to have a tool that allows them to see even yet more patients per day.&nbsp;</p>



<p>So in software development, there&#8217;s always this tension. Like, how many lines of code can you write per day? That&#8217;s one productivity measure.&nbsp;</p>



<p>But sometimes we&#8217;re taught, well, don&#8217;t write more lines of code per day, but make sure that your code is well structured. Take the time to document it. Make sure it&#8217;s fully commented. Take the time to talk to your fellow software engineering team members to make sure that it&#8217;s well coordinated. And in the long run, even if you&#8217;re writing half the number of lines of code per day, the software process will be far more efficient.</p>



<p>And so I&#8217;ve wondered whether there&#8217;s a similar thing where doctors could see 20% fewer patients in a day, but if they take the time and also had AI help to coordinate, maybe a patient&#8217;s journey might be half as long. And therefore, the health system would be able to see twice as many patients in a year&#8217;s period or something like that.&nbsp;</p>



<p><strong>KOHANE:</strong> So I think you&#8217;ve “nerd sniped” me because you [LAUGHTER]—which is all too easy—but I think there&#8217;s a central issue here. And I think this is the stumbling block between what Ethan&#8217;s telling us about between the individual productivity and the larger productivity, is the <em>team&#8217;s</em> productivity.&nbsp;</p>



<p>And there is actually a good analogy in computer science and that&#8217;s, uh, Brooks’s “mythical man-month,” &#8230;&nbsp;</p>



<p><strong>LEE:</strong> Yes, exactly.&nbsp;</p>



<p><strong>KOHANE:</strong> … where he shows how you can have more and more resources, but when the coordination starts failing, because you have so many, uh, individuals on the team, you start falling apart. And so even if the, uh, individual doctors get that much better, yeah, they take better care of patients, make less stupid things.&nbsp;</p>



<p>But in terms of giving the “I get you into the emergency room, and I get you out of a hospital as fast as possible, as safely as possible, as effectively as possible,” that&#8217;s teamwork. And we don&#8217;t do it. And we&#8217;re not really optimizing our tools for that.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And just to throw in a little reality check, I&#8217;m not aware of <em>any</em> indication yet that AI is in any way shortening medical journeys or making physicians more efficient. Yet …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>GOLDBERG:</strong> …<strong> </strong>at least. Yeah.&nbsp;</p>



<p><strong>LEE:</strong> Yes. So I think, you know, with respect to our book, critiquing our book, you know, I think it&#8217;s fair to say we were fairly focused or maybe even fixated on the individual doctor or nurse or patient, and we didn&#8217;t really, at least I never had a time where I stepped back to think about the whole care coordination team or the whole health system.&nbsp;</p>



<p><strong>KOHANE: </strong>And I think that&#8217;s right. It&#8217;s because, first of all, <em>you</em> weren’t thinking about it? It&#8217;s not what we&#8217;re taught in medical school. We&#8217;re not taught to talk about team communication excellence. And I think it&#8217;s absolutely essential.&nbsp;</p>



<p>There’s a … what’s the … there was an early … [Terry] Winograd. And he was trying to capture what are the different kinds of actions related to pronouncements that you could expect and how could AI use that. And that was beginning to get at it.&nbsp;</p>



<p>But I actually think this is dark matter of human organizational technology that is not well understood. And our products don&#8217;t do well. You know, we can talk about all the groupware things that are out there. But they all don&#8217;t quite get to that thing.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> And I can imagine an AI serving as a team leader, a really active team leader, a real quarterback of, let&#8217;s say, a care team.&nbsp;</p>



<p><strong>LEE:</strong> Well, in fact, you know, we have been trying to experiment with this. My colleague, Matt Lungren, who was also one of the interviewees early on, has been <a href="https://www.microsoft.com/en-us/industry/blog/healthcare/2025/05/19/developing-next-generation-cancer-care-management-with-multi-agent-orchestration/" target="_blank" rel="noreferrer noopener">working with Stanford Medicine on a tumor board AI agent</a>—something that would facilitate tumor board meetings.&nbsp;</p>



<p>And the early experiences are pretty interesting. Whether it relates to efficiency or productivity I think remains to be seen, but it does seem pretty interesting.&nbsp;</p>



<p>But let&#8217;s move on.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Well, actually, Peter, …&nbsp;</p>



<p><strong>LEE: </strong>Oh, go ahead.&nbsp;</p>



<p><strong>GOLDBERG: </strong>…<strong> </strong>if you&#8217;re willing to not quite move on yet …&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] All right.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… this kind of segues into one of, I think, the most provocative questions that arose in the course of these episodes and that I&#8217;d love to have you answer, which was, remember, it was a question at a gathering that you were at, and you were asked, “Well, you&#8217;re focusing a lot on potential AI effects on individual patient and physician experiences. But what about the revolution, right? What about, like, can you be more big-picture and envision how generative AI could actually, kind of, overturn or fix the broken system, right?”&nbsp;</p>



<p>I&#8217;m sure you&#8217;ve thought about that a lot. Like, what&#8217;s your answer?&nbsp;</p>



<p><strong>LEE:</strong> You know, I think ultimately, it will have to. For it to really make a difference, I think that the normal processes, our normal concept of how healthcare is delivered—how new medical discoveries are made and brought into practice—I think those things are going to have to change a lot.&nbsp;</p>



<p>You know, one of the things I think about a lot right at the moment is, you know, we tend to think about, let&#8217;s say, medical diagnosis as a problem-solving exercise. And I think, at least at the Kaiser Permanente School of Medicine, the instruction really treats it as a kind of detective thing based on a lot of knowledge about biology and biomedicine and human condition, and so on.&nbsp;</p>



<p>But there&#8217;s another way to think about it, given AI, which is when you see a patient and you develop some data, maybe through a physical exam, labs, and so on, you can just simply ask, “You know, what did the 500 other people who are most similar to this experience, how were they diagnosed? How were they treated? What were their outcomes? What were their experiences?”&nbsp;</p>



<p>And that&#8217;s really a fundamentally different paradigm. And it just seems like at least the technical means will be there. And by the way, that also then relates to [the questions]: “And what was most efficacious cost-wise? What was most efficient in terms of the total length of the patient journey? How does this relate to my quality scores so I can get more money from Medicare and Medicaid?”&nbsp;</p>



<p>All of those things, I think, you know, we&#8217;re starting to confront.&nbsp;</p>



<p>One of the other episodes that we&#8217;re going to talk about, was my interview with two medical students. Actually, thinking of a Morgan Cheatham as just a medical student or medical resident [LAUGHTER] is a little strange. But he is.&nbsp;</p>



<p>One of the things he talks about is the importance that he placed in his medical training about adopting AI. So, Zak, I assume you see this also with some students at Harvard Medical School. And the other medical student we interviewed, Daniel Chen, seemed to indicate this, too, where it seems like it&#8217;s the students who are bringing AI into the medical education ahead of the faculty. Does that resonate with you?&nbsp;</p>



<p><strong>KOHANE:</strong> It absolutely resonates with me. There are students I run into who, honestly, my first thought when I&#8217;m talking to them is, why am I teaching you [LAUGHTER], and why are you not starting a big AI company, AI medicine company, now and really change healthcare instead of going through the rest of the rigmarole? And I think broadly, higher education has a problem there, which is we have not embraced, again, going back to Ethan, a lot of the tools that can be used. And it&#8217;s because we don&#8217;t know necessarily the right way to teach them. And so far, the only lasting heuristic seems to be: use them and use them often.&nbsp;</p>



<p>And so it&#8217;s an awkward thing, where the person who knows how to use the AI tools now in the first-year medical school can teach themselves better and faster than anybody else in their class who is just relying on the medical school curriculum.&nbsp;</p>



<p><strong>LEE: </strong>Now, the reason I brought up Morgan now after our discussion with Ethan Mollick is Morgan also talked about AI collapsing medical specialties.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes.&nbsp;</p>



<p><strong>LEE:</strong> And so let&#8217;s hear this snippet from him.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-df1ec79bf2184e924d149dd5a8bfd566"><strong><em>MORGAN CHEATHAM:</em></strong><em> AI collapses medical specialties onto themselves, right. You have the canonical example of the cardiologist, you know, arguing that we should diuresis and maybe the nephrologist arguing that we should, you know, protect the kidneys. And how do two disciplines disagree on what is right for the patient when in theory, there is an objective best answer given that patient’s clinical status? … So I’m interested in this question of whether medical specialties themselves need to evolve. And if we look back in the history of medical technology, there are many times where a new technology forced a medical specialty to evolve.</em></p>



<p><strong>LEE:</strong> So on the specific question about specialties, Zak, do you have a point of view? And let me admit, first of all, for us, all three of us, we didn&#8217;t have any clue about this in our book. I don&#8217;t think.&nbsp;</p>



<p><strong>KOHANE:</strong> Not much. Not much of a clue.&nbsp;</p>



<p>So I&#8217;m reminded of a <em>New Yorker</em> cartoon where you see a bunch of surgeons around the patient, and someone says, “Is that a spleen?” And it says, “I don&#8217;t know. I slept during the spleen lecture,” [LAUGHTER] and &#8230; or “I didn&#8217;t take the spleen course.”&nbsp;</p>



<p>And yet when we measure things, we measure things much more than we think we are doing. So for example, we [<em>NEJM AI</em>] just published a paper where echocardiograms were being done. And it turns out those ultrasound waves just happen to also permeate the liver. And <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://ai.nejm.org/doi/full/10.1056/AIoa2400948" target="_blank" rel="noopener noreferrer">you can actually diagnose on the way with AI all the liver disease<span class="sr-only"> (opens in new tab)</span></a> that is in—and treatable liver disease—that&#8217;s in those patients.&nbsp;</p>



<p>But if you&#8217;re a cardiologist, “Liver? You know, I slept through liver lecture.” [LAUGHTER] And so I do think that, (A) the natural, often guild/dollar-driven silos in medicine are less obvious to AI, despite the fact that they do exist in departments and often in chapters.&nbsp;</p>



<p>But Morgan&#8217;s absolutely right. I can tell you as an endocrinologist, if I have a child in the ICU, the endocrinologist, the nephrologist, and the neurosurgeon will argue about the right thing to do.&nbsp;</p>



<p>And so in my mind, the truly revolutionary thing to do is to go back to 1994 with Pete Szolovits, the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://dspace.mit.edu/bitstream/handle/1721.1/149765/MIT-LCS-TR-604.pdf?sequence=1" target="_blank" rel="noopener noreferrer">Guardian Angel Project<span class="sr-only"> (opens in new tab)</span></a>. What I think you need is a process. And the process is the quarterback. And the quarterback has only one job: take care of the patient.&nbsp;</p>



<p>And it should be thinking all the time about the patient. What&#8217;s the right thing? And can be as school-marmish or not about, “Zak, you&#8217;re eating this or that or exercise or sleep,” but also, “Hey, surgeons and endocrinologists, you&#8217;re talking about my host, Zak. This is the right way because this problem and this problem and our best evidence is this is the right way to get rid of the fluid. The other ways will kill him.”</p>



<p>And I think you need an authoritative quarterback that has the view of the others but then makes the calls.&nbsp;</p>



<p><strong>LEE:</strong> Is that quarterback going to be AI or human?&nbsp;</p>



<p><strong>KOHANE:</strong> Well, for the very lucky people, it&#8217;ll be a human augmented by AI, <em>super concierge</em>.&nbsp;</p>



<p>But I think we&#8217;re running out of doctors. And so realistically, it&#8217;s going to be an AI that will have to be certified in very different ways, along the ways Dave Blumenthal says, essentially, trial by fire. Like putting residents into clinics, we&#8217;re going to be putting AIs into clinics.&nbsp;</p>



<p>But what&#8217;s worse, by the way, than the three doctors arguing about care in front of the patient is, what happens so frequently, is then you see them outpatient, and each one of them gives you a different set of decisions to make. Sometimes that actually interact pathologically, unhealthily with each other. And only the very smart nurses or primary care physicians will actually notice that and call, quote, a “family meeting,” or bring everybody in the same room to align them.&nbsp;</p>



<p><strong>LEE: </strong>Yeah, I think this idea of quarterback is really very, very topical right now because there&#8217;s so much intensity in the AI space around agents. And in fact, you know, the Microsoft AI team under Mustafa Suleyman and Dominic King, Harsha Nori, and team just recently posted <a href="https://www.microsoft.com/en-us/research/publication/sequential-diagnosis-with-language-models/" target="_blank" rel="noreferrer noopener">a paper on something called sequential diagnosis</a>, which is basically an AI quarterback that is supposed to smartly consult with other AI specialties. And interestingly, one of the AI agents is sort of the devil&#8217;s advocate that&#8217;s always criticizing and questioning things. </p>



<p><strong>GOLDBERG: </strong>That’s interesting.&nbsp;</p>



<p><strong>LEE:</strong> And at least on very, very hard, rare cases, it can develop some impressive results. There&#8217;s something to this that I think is emerging.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And, Peter, Morgan said something that blew me away even more, which was, well, why do we even need specialists if the reason for a specialist is because there&#8217;s so much medical knowledge that no single physician can know all of it, and therefore we create specialists, but that limitation does not exist for AI.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>And so there he was kind of undermining this whole elaborate structure that has grown up because of human limitations that may not ultimately need to be there.&nbsp;</p>



<p><strong>LEE: </strong>Right. So now that gives me a good segue to get back to our economist and get to something that Azeem Azhar said. And so there&#8217;s a clip here from Azeem.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-6ff46028707e465b3cd8582210840bd0"><strong><em>AZEEM AZHAR: </em></strong><em>We didn’t talk about, you know, AI in its ability to potentially do this, which is to extend the clinician’s presence throughout the week. <em>You know, t</em>he idea that maybe some part of what the clinician would do if you could talk to them on Wednesday, Thursday, and Friday could be delivered through an app or a chatbot just as a way of encouraging the compliance, which is often, especially with older patients, one reason why conditions, you know, linger on for longer.</em>&nbsp;</p>



<p><strong>LEE:</strong> And, you know, in the same conversation, he also talked about his own management of asthma and the fact that he&#8217;s been managing this for several decades and knows more than any other human being, no matter how well medically trained, could possibly know. And it&#8217;s also very highly personalized. And it&#8217;s not a big leap to imagine AI having that sort of lifelong understanding.&nbsp;</p>



<p><strong>KOHANE:</strong> So in fact, I want to give credit back to our book since you insulted us. [LAUGHTER] You challenged us. You doubted us. We do have at the end of the book a AI which is helping this woman manage her way through life. It&#8217;s quarterbacking for the woman all these different services.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE:</strong> So there.&nbsp;</p>



<p><strong>LEE: </strong>Ah, you&#8217;re right. Yes. In fact, it&#8217;s very much, I think, along the lines of the vision that Azeem laid out in our conversation.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. It also reminded me of the piece <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.wbur.org/news/2017/06/16/managing-mom-weight-algorithm" target="_blank" rel="noopener noreferrer">Zak wrote about his mother<span class="sr-only"> (opens in new tab)</span></a> at one point when she was managing congestive heart failure and she needed to watch her weight very carefully to see her fluid status. And absolutely, there&#8217;s no … I see no reason whatsoever why that couldn&#8217;t be done with AI right now. Actually, although back then, Zak, you were writing that it takes much more than an AI [LAUGHS] to manage such a thing, right?&nbsp;</p>



<p><strong>KOHANE:</strong> You need an AI that you can trust. Now, my mother was born in 1927, and she&#8217;d learned through the school of hard knocks that you can&#8217;t trust too many people, maybe even not your son, <em>MD</em>, <em>PhD</em> [LAUGHTER].&nbsp;</p>



<p>But what I&#8217;ve been surprised [by] is how, for example, how many people are willing to trust and actually see effective use of AI as mental health counselors, for example.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah&nbsp;</p>



<p><strong>KOHANE:</strong> So it may in fact be that there&#8217;s a generational thing going on, and at least there&#8217;ll be some very large subset of patients which will be completely comfortable in ways that my mother would have never tolerated.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. Now, I think we&#8217;re starting to veer into some of the core AI.&nbsp;</p>



<p>And so I think maybe one of the most fun conversations I had was in the episode with both Sébastien Bubeck, my former colleague at Microsoft Research, and now he&#8217;s at OpenAI, and Bill Gates. And there was so much that was, I thought, interesting there. And there was one point, I think that sort of touches tangentially on what we were just conversing about, that Sébastien said. So let&#8217;s hear this snippet.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-19a223099867a12ed2599ae811939b43"><strong><em>SÉBASTIEN BUBECK: </em></strong><em>And one example that I really like, a study that recently appeared where … they were comparing doctors without and with ChatGPT. … So this was a set of cases where the accuracy of the doctors alone was around 75%. ChatGPT alone was 90%. … But then the kicker is that doctors with ChatGPT was 80%. Intelligence alone is not enough. It’s also how it’s presented, how you interact with it. And ChatGPT, it’s an amazing tool. Obviously, I absolutely love it. But it’s not … you don’t want a doctor to have to type in, you know, prompts and use it that way. It should be, as Bill was saying, kind of running continuously in the background, sending you notifications.</em></p>



<p><strong>LEE:</strong> So I thought Sébastien was saying something really profound, but I haven&#8217;t been able to quite decide or settle in my mind what it is. What do you make of what Seb just said?&nbsp;</p>



<p><strong>KOHANE:</strong> I think it&#8217;s context. I think that it requires an enormous amount of energy, brain energy, to actually correctly provide the context that you want this thing to work on. And it&#8217;s only going to really feel like we&#8217;re in a different playing field when it&#8217;s listening all the time, and it just steps right in.&nbsp;</p>



<p>There is an advantage that, for example, a good programmer can have in prompting Cursor or any of these tools to do so. But it takes effort. And I think being in the conversation all the time so that you understand the context in the widest possible way is incredibly important. And I think that&#8217;s what Seb is getting at, which is if we spoon feed these machines, yes, 90%.&nbsp;</p>



<p>But then, talking to a human being who then has to interact and gets distracted from whatever flow they&#8217;re in and maybe even makes them feel like an early bicycle rider who all of a sudden realizes, “I&#8217;m balancing on two wheels—oh no!” And they fall over. You know, there&#8217;s that interaction which is negatively synergistic.&nbsp;</p>



<p>And so I do think it&#8217;s a very hard human-computer engineering problem. How do we make these two agents, human and computational, work in an ongoing way in the flow? I don&#8217;t think I&#8217;m seeing anything that&#8217;s particularly new. And the things that you&#8217;re beginning to hint about, Peter, in terms of agentic coordination, I think we&#8217;ll get to some of that. </p>



<p><strong>LEE:</strong> Yeah. Carey, does this give you any pause? The kind of results that … they&#8217;re puzzling results. I mean, the idea of doctors with AI seeming at least in this one test—it&#8217;s just one test—but it&#8217;s odd that it does worse than the AI alone.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes. I would want to understand more about the actual conditions of that study.&nbsp;</p>



<p>From what Bill Gates said, I was most struck by the question of resource-poor environments. That even though this was absolutely one of the most promising, brightest perspectives that we highlighted in the book, we still don&#8217;t seem to be seeing a lot of use among the one half of humanity that lacks decent access to healthcare.&nbsp;</p>



<p>I mean, there are access problems everywhere, including here in the United States. And it is one of the most potentially promising uses of AI. And I thought if anyone would know about it, he would with the work that the Gates Foundation does.&nbsp;</p>



<p><strong>LEE: </strong>You know, I think both you and Bill, I felt, are really simpatico. You know, Bill expressed genuine surprise that more isn&#8217;t happening yet. And it really echoed, in fact, maybe even using some of the exact same words that you&#8217;ve used. And so two years on, you&#8217;ve expressed repeatedly expecting to have seen more out in the field by now. And then I thought Bill was saying something in our conversation very similar.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p><strong>LEE: </strong>You know, for me, I see it both ways. I see the world of medicine really moving fast in confronting the reality of AI in such a serious way. But at the same time, it&#8217;s also hard to escape the feeling that somehow, we should be seeing even more.&nbsp;</p>



<p>So it&#8217;s an odd thing, a little bit paradoxical.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I think one thing that we didn&#8217;t focus on hardly at all in the book but that we are seeing is these companies rising up, stepping up to the challenge, Abridge and OpenEvidence, and what Morgan describes as a new stack, right.&nbsp;</p>



<p>So there is that on the flip side.&nbsp;</p>



<p><strong>LEE: </strong>Now, I want to get back to this thing that Seb was saying. And, you know, I had to bring up the issue of sycophancy, which we discussed at our last roundtable also. But it was particularly … at the time that Seb, Bill, and I had our conversation, OpenAI had just gone through having to retract a fresh update of GPT-4o because it had become too sycophantic.&nbsp;</p>



<p>So I can&#8217;t escape the feeling that some of these human-computer interaction issues are related to this tension between you want AI to follow your directions and be faithful to you, but at the same time not agree with you so often that it becomes a fault.&nbsp;</p>



<p><strong>KOHANE:</strong> I think it&#8217;s asking the AI to enter into a fundamental human conundrum, which is there are extreme versions of doublethink, and there&#8217;s everyday things, everyday asks of doublethink, which is how to be an effective citizen.&nbsp;</p>



<p>And even if you&#8217;re thinking, “Hmm. I&#8217;m thinking this. I&#8217;m just not going to say it because that would be rude or counterproductive.” Or some of the official doublethinks, where you&#8217;re actually told you must say this, even if you think something else. And I think we&#8217;re giving a very tough mission for these things: be nice to the user and be useful.&nbsp;</p>



<p>And, in education, where the thing is not always one in the same. Sometimes you have to give a little tough love to educate someone, and doing that well is both an art and it&#8217;s also very difficult. And so, you know, I&#8217;m willing to believe that the latest frontier models that have made the news in the last month are very high-performing, but they&#8217;re also all highlighting that tension …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>… that tension between behaving like a good citizen and being helpful. And this gets back to what are the fundamental values that we hope these things are following.&nbsp;</p>



<p>It&#8217;s not, you know, “Are these things going to develop us into the paperclip factory?” It&#8217;s more of, “Which of our values are going to be elevated, and which one will be suppressed?”&nbsp;</p>



<p><strong>LEE: </strong>Well, since I criticized our book before, let me pat ourselves on the back this time because, I think, pervasive throughout our book, we were touching on some of these issues.&nbsp;</p>



<p>In fact, we started the book, you know, with GPT-4 scolding me for wanting it to impersonate Zak. And there was the whole example of asking it to rewrite a poem in a certain way, and it kind of silently just tried to slide, you know, without me knowing, slide by without following through on the whole thing.&nbsp;</p>



<p>And so that early version of GPT-4 was definitely not sycophantic at all. In fact, it was just as prone to call you an idiot if it thought you were wrong. [LAUGHTER]&nbsp;</p>



<p><strong>KOHANE:</strong> I had some very testy conversations around my endocrine diagnosis with it. [LAUGHTER]&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. Well then, Peter, I would ask you, I mean last time I asked you about, <em>well, hallucinations, aren&#8217;t those solvable?</em> And this time I would ask you, well, sycophancy, isn&#8217;t that kind of like a dial you can turn? Like, is that not solvable?&nbsp;</p>



<p><strong>LEE: </strong>You know, I think there are several interlocking problems. But if we assume superintelligence, even with superintelligence, medicine is such an inexact science that there will always be situations that are guesses that take into account other factors of a person&#8217;s life, other value judgments, exactly as Zak had pointed out in our previous roundtable conversation.&nbsp;</p>



<p>And so I think there&#8217;s always going to be an opening for either differences of opinion or agreeing with you too much. And there are dangers in both cases. And I think they&#8217;ll always be present. I don&#8217;t know that, at least in something as inexact as medical science, I don&#8217;t know that it&#8217;ll ever be completely eliminated.&nbsp;</p>



<p><strong>KOHANE: </strong>And it&#8217;s interesting because I was trying to think what&#8217;s the right balance, but there are patients who want to be told this is what you do. Whereas there&#8217;s other patients who want to go through every detail of the reasoning.&nbsp;</p>



<p>And it&#8217;s not a matter of education. It&#8217;s really a temperamental, personality issue. And so we&#8217;re going to <em>have to</em>, I think, develop personalities …&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>… that are most effective for those different kinds of individuals. And so I think that is going to be the real frontier. Having human values and behaving in ways that are recognizable and yet effective for certain groups of patients.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>And lots of deep questions, including how paternalistic do we want to be?&nbsp;</p>



<p><strong>LEE: </strong>All right, so we&#8217;re getting into medical science and hallucination. So that gives me a great segue to the conversations in the episode on biomedical research. And one of the people that I interviewed was Noubar Afeyan from Moderna and Flagship Pioneering. So let&#8217;s listen to this snippet.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-574184a033e6f29ff2ba40f7e611b28a"><strong><em>NOUBAR AFEYAN:</em></strong><em> We, some hundred or so times a year, ask “what if” questions that lead us to totally weird places of thought. We then try to iterate, iterate, iterate to come up with something that’s testable. Then we go into a lab, and we test it. So in that world, right, sitting there going, like, “How do I know this transformer is going to work?” The answer is, “For what?” Like, it’s going to work to make something up … well, guess what? We knew early on with LLMs that hallucination was a feature, not a bug for what we wanted to do.</em></p>



<p><strong>LEE: </strong>[LAUGHS] So I think that really touches on just the fact that there&#8217;s so many unknowns and such lack of precision and exactness in our understanding of human biology and of medicine. Carey, what do you think?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, I just have this emotional reaction, which is that I love the idea of AI marching into biomedical science and everything from getting to the virtual cell eventually to, Zak, I think it was a colleague of yours who recently published about &#8230; it was a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.nature.com/articles/s41591-025-03832-2" target="_blank" rel="noopener noreferrer">new medication that had been sort of discovered by AI<span class="sr-only"> (opens in new tab)</span></a>, and it was actually testing out up to the phase II level or something, right?</p>



<p><strong>KOHANE:</strong> Oh, this is Marinka’s work.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah, Marinka, Marinka Zitnik. And … yeah. So, I mean, I think it avoids a lot of the, sort of, dilemmas that are involved with safety and so on with AI coming into medicine. And it&#8217;s just the discovery process, which we all want to advance as quickly as possible. And it seems like it actually has a great deal of potential that&#8217;s already starting to be realized.&nbsp;</p>



<p><strong>LEE:</strong> Oh, absolutely.&nbsp;</p>



<p><strong>KOHANE:</strong> I love this topic. First of all, I thought, actually, I think Bill and Seb, actually, had interesting things to say on that very topic, rationales which I had not really considered why, in fact, things might progress faster in the discovery space than in the clinical delivery space, just because we don&#8217;t know in clinical medicine what we&#8217;re trying to maximize precisely. Whereas for a drug effect, we do know what we&#8217;re trying to maximize.&nbsp;</p>



<p><strong>LEE: </strong>Well, in fact, I happened to save that snippet from Bill Gates saying that. So let&#8217;s cue that up.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-6ca7a5edfbf41028162616b68d8380d1"><strong><em>BILL GATES: </em></strong><em>I think it’s very much within the realm of possibility that the AI is not only accelerating healthcare discovery but substituting for a lot of the roles of, you know, “I’m an organic chemist,” or “I run various types of assays.” I can see those, which are, you know, testable-output-type jobs but with still very high value, I can see, you know, some replacement in those areas before the doctor.</em>&nbsp;</p>



<p><strong>LEE: </strong>So, Zak, isn&#8217;t that Bill saying exactly what you’re saying?&nbsp;</p>



<p><strong>KOHANE:</strong> That is my point. I have to say that this is another great bet, that either we&#8217;re all going to be surprised or a large group of people will be surprised or disappointed.&nbsp;</p>



<p>There&#8217;s still a lot of people in the sort of medicinal chemist, trialist space who are still extremely skeptical that this is going to work. And we haven&#8217;t quite shown them yet that it is. Why have we not shown them? Because we haven&#8217;t gone all the way to a phase III study, which showed that the drug behaves as expected to, is effective, and basically doesn&#8217;t hurt people. That turns out to require a lot of knowledge. I actually think we&#8217;re getting there, but I understand the skepticism.&nbsp;</p>



<p><strong>LEE: </strong>Carey, what are your thoughts?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I mean, there will be no way around going through full-on clinical trials for anything to ever reach the market. But at the same time, you know, it&#8217;s clearly very promising. And just to throw out something for the pure fun of it, Peter, I saw &#8230; one of my favorite tweets recently was somebody saying, you know, isn&#8217;t it funny how computer science is actually becoming a lot more like biology in that it&#8217;s just becoming empirical.&nbsp;</p>



<p>It&#8217;s like you just throw stuff at the AI and see what it does. [LAUGHTER] And I was like,<em> oh, yeah, that&#8217;s what Peter was doing when we wrote the book.</em> I mean, he understood as many innards as anybody can. But at the same time, it was a totally empirical exercise in seeing what this thing would do when you threw things at it.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>GOLDBERG: </strong>So it&#8217;s the new biology.&nbsp;</p>



<p><strong>LEE:</strong> Well, yeah. So I think we talked in our book about accelerating, you know, biomedical knowledge and medical science. And that actually seems to be happening. And I really had fun talking to Daphne Koller about some of the accomplishments that she&#8217;s made. And so here&#8217;s a little snippet from Daphne.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-b519fe67233d03b2946f95df076d6451"><strong><em>DAPHNE KOLLER: </em></strong><em>This will impact not only the early stages of which hypotheses we interrogate, which molecules we move forward, but also hopefully at the end of the day, which molecule we prescribe to which patient. And I think there’s been obviously so much narrative over the years about precision medicine, personalized medicine, and very little of that has come to fruition, with the exception of, you know, certain islands in oncology, primarily on genetically driven cancers.</em>&nbsp;</p>



<p><strong>LEE:</strong> So, Zak, when I was listening to that, I was reminded of one of the very first examples that you had where, you know, you had a very rare case of a patient, and you&#8217;re having to narrow down some pretty complex and very rare genetic conditions. This thing that Daphne says, that seems to be the logical conclusion that everyone who&#8217;s thinking hard about AI and biology is coming to. Does it seem more real now two years on?&nbsp;</p>



<p><strong>KOHANE:</strong> It absolutely seems more real. Here&#8217;s some sad facts. If you are at a cancer center, you will get targeted therapies if you qualify for it. Outside cancer centers, you won&#8217;t. And it&#8217;s not that the therapies aren&#8217;t available. It&#8217;s just that you won&#8217;t have people thinking about it in that way. And especially if you have some of the rare and more aggressive cancers, if you&#8217;re outside one of those cancer centers, you&#8217;re at a significant disadvantage for survival for that reason. And so anything that provides just the “simple,” in quotes, dogged investigation of the targeted therapies for patients, it&#8217;s a home run.&nbsp;</p>



<p>So my late graduate student, Atul Butte, died recently at UCSF, where he was both a professor and the leader of the Bakar Institute, and he was a Zuckerberg Chan Professor of Pediatrics.&nbsp;</p>



<p>He was diagnosed with a rare tumor two years ago. His wife is a PhD biologist, and when he was first diagnosed, she sent me the diagnosis and the mutations. And I don&#8217;t know if you know this, Peter, but this was still when we were writing the book and people didn&#8217;t know about GPT-4.&nbsp;</p>



<p>I put in those mutations into GPT-4 and the diagnosis. And I said, “I&#8217;d like to help treat my friend. What&#8217;s the right treatment?” And GPT, to paraphrase, GPT-4 said, “Before we start talking about treatment, are you sure this is the right diagnosis? Those mutations are not characteristic for that tumor.&#8221; And he had been misdiagnosed. And then they changed the diagnosis therapy and some personnel. </p>



<p>So I don&#8217;t have to hallucinate this. It&#8217;s already happened, and we&#8217;re going to need this. And so I think targeted therapy for cancers is the most obvious use. And if God forbid one of you has a family member who has cancer, it&#8217;s moral malpractice not to look at the genetics and run it past GPT-4 and say, “What are the available therapies?”&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> I really deeply believe that.&nbsp;</p>



<p><strong>LEE:</strong> Carey, I think one thing you&#8217;ve always said is that you&#8217;re surprised that we don&#8217;t hear more stories along these lines. And I think you threw a quote from Mustafa Suleyman back at me. Do you want to share that?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yes. Recently, I believe it was a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.bigtechnology.com/p/microsoft-ai-ceo-mustafa-suleyman-c45" target="_blank" rel="noopener noreferrer">Big Technology interview<span class="sr-only"> (opens in new tab)</span></a>, and the reporter asked Mustafa Suleyman, “So you guys are seeing 50 million queries, medical queries, a day [to Copilot and Bing]. You know, how&#8217;s that going?” And I think I am a bit surprised that we&#8217;re not seeing more stories of <em>all</em> types. Both here&#8217;s how it helped me and also here was maybe, you know, a suggestion that was not optimal.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. I do think in our book, we did predict both positive and negative outcomes of this. And it is odd. Atul was very open with his story. And of course, he is such … he was such a prominent leader in the world of medicine.&nbsp;</p>



<p>But I think I share your surprise, Carey. I expected by now that a lot more public stories would be out. Maybe there is someone writing a book collecting these things, I don&#8217;t know.&nbsp;</p>



<p><strong>KOHANE:</strong> Maybe someone called Carey Goldberg should write that book. [LAUGHTER]&nbsp;</p>



<p><strong>GOLDBERG: </strong>Write a book, maybe. I mean, we have <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://patientsuseai.substack.com/" target="_blank" rel="noopener noreferrer">Patients Use AI<span class="sr-only"> (opens in new tab)</span></a>, which is a wonderful blog by Dave deBronkart, the patient advocate.&nbsp;</p>



<p>But I wonder if it&#8217;s also something structural, like who would be or what would be the institution that would be gathering these stories? I don’t know.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>KOHANE:</strong> And that&#8217;s the problem. You see, this goes back to the same problem that [Ethan] Mollick was talking about. Individual doctors are using them. The hospital as a whole is not doing that. So it&#8217;s not judging the quality, as part of its quality metrics, of how good the AI is performing and what new has happened. And the other audience, namely the patients, have no mechanism. There is no mechanism to go to Better Business Bureau and say, “They screwed up,” or “This was great.”&nbsp;</p>



<p><strong>LEE: </strong>So now I want to get a little more futuristic. And this gets into whether AI is really going to get almost to the <em>ab initio</em> understanding of human biology. And so Eric Topol, who is one of the guests, spoke to this a bit. So let&#8217;s hear this.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-a8cd8394bbdf383bb04d1e5902cb8b97"><strong><em>LEE: </em></strong><em>So you talk about a virtual cell. Is that achievable within 10 years, or is that still too far out?</em><br><br><strong><em>ERIC TOPOL:</em></strong><em> No, I think within 10 years for sure. You know, the group that got assembled, that Steve Quake pulled together, I think has 42 authors in a paper in Cell. The fact that he could get these 42 experts in life science and some in computer science to come together and all agree that not only is this a worthy goal, but it’s actually going to be realized, that was impressive.</em> </p>



<p><strong>LEE:</strong> You know, I have to say Eric&#8217;s optimism took me aback. Just speaking as a techie, I think I started off being optimistic: as soon as we can figure out molecular dynamics, biology can be solved. And then you start to learn more about biochemistry, about the human cell, and then you realize, oh, my God, this is just so vast and unknowable. And now you have Eric Topol saying, “Well, in less than 10 years.”&nbsp;</p>



<p><strong>KOHANE: </strong>So what&#8217;s delightful about this period is that those of us who are cautious were so incredibly wrong about AI two years ago. [LAUGHTER] That&#8217;s a true joy &#8230; I mean, absolute joy. It&#8217;s great to have your futurism made much more positive.&nbsp;</p>



<p>But I think that we&#8217;re going from, you know, for example, AlphaFold has had tremendous impact. But remember, that was built on years of acquisition of crystallography data that was annotated. And of course, the annotation process becomes less relevant as you go down the pipe, but it started from that.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE:</strong> And there&#8217;s lots of parts of the cell. So when people talk about virtual cells—I don&#8217;t mean to get too technical—mostly they&#8217;re talking about perturbation of gene expression. They&#8217;re not talking about, “Oh, this is how the liposome and the centrosome interact, and notice how the Golgi bodies bump into each other.”&nbsp;</p>



<p>There&#8217;s a whole bunch of other levels of abstraction we know nothing about. This is a complex factory. And right now, we&#8217;re sort of the level from code into loading code into memory. We&#8217;re not talking about how the rest of the robots work in that cell, and how the rest of those robots work in the cell turns out to be pretty important to functioning.&nbsp;</p>



<p>So I&#8217;d love to be wrong again. And in 10 years, oh yeah, not only, you know, our first in-human study will be you, Dr. Zak. We&#8217;re going put the drug because we fully simulated you. That&#8217;d be great.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>And, by the way, just to give people their due, there probably was a lot of animal research that could be done <em>in silico </em>and that for various political reasons we&#8217;re now seeing happen. That&#8217;s a good thing. But I think that sometimes it takes a lot of hubris to get us where we need to get, but my horizon is not the same as his.&nbsp;</p>



<p><strong>LEE: </strong>So I guess I have to take this time to brag. Just recently out of our AI for Science team did publish in <em>Science</em> a <a href="https://www.microsoft.com/en-us/research/publication/scalable-emulation-of-protein-equilibrium-ensembles-with-generative-deep-learning/" target="_blank" rel="noreferrer noopener">biological emulator that does pretty long timespan, very, very precise, and very efficient molecular dynamics</a>, biomolecular dynamics emulation. We call it <em>emulation</em> because it&#8217;s not simulating every single time step but giving you the final confirmations.&nbsp;</p>



<p><strong>KOHANE: </strong>That&#8217;s an amazing result.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> But … that is an amazing result. And you&#8217;re doing it in some very important interactions. But there&#8217;s so much more to do.&nbsp;</p>



<p><strong>LEE:</strong> I know, and it&#8217;s single molecules; it&#8217;s not even two molecules. There&#8217;s so much more to go for here. But on the other hand, Eric is right, you know, 42 experts writing for <em>Cell</em>, you know, that&#8217;s not a small matter.&nbsp;</p>



<p><strong>KOHANE:</strong> So I think sometimes you really need to drink your own hallucinogens to actually succeed. Because remember, when the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.genome.gov/human-genome-project" target="_blank" rel="noopener noreferrer">Human Genome Project<span class="sr-only"> (opens in new tab)</span></a> was launched, we didn&#8217;t know how to sequence at scale.&nbsp;</p>



<p>We said maybe we would get there. And then in order to get the right funding and excitement and, I think, focus, we predicted that by early 2000s we&#8217;d be transforming medicine. Has not happened yet. Things have happened, but at a much slower pace. And we&#8217;re 25 years out. In fact, we&#8217;re 35 years out from the launch.&nbsp;</p>



<p>But again, things are getting faster and faster. Maybe the singularity is going to make a whole bunch of things easier. And GPT-6 will just say, “Zak, you are such a pessimist. Let me show you how it&#8217;s done.”&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p>It really is a pessimism versus optimism. Like is it, I mean, biology is such a bitch, right. [LAUGHTER] Can we actually get there?&nbsp;</p>



<p>At the same time, everyone was surprised and blown away by the, you know, the quantum leap of GPT-4. Who knows when enough data gets in there if we might not have a similar leap.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. All right.&nbsp;</p>



<p>So let&#8217;s get back to healthcare delivery. Besides Morgan Cheatham, we talked to [a] more junior medical student who&#8217;s at the Kaiser Permanente School of Medicine, Daniel Chen. And, you know, I asked him about this question of patients who come in armed [LAUGHS] with a lot of their own information. Let&#8217;s hear what he said about this.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-b854f48f19d32b116ea80de7d13c8a99"><strong><em>DANIEL CHEN: </em></strong><em>But for those that come in with a list, I sometimes sit down with them, and we’ll have a discussion, honestly. … “I don’t think you have meningitis because, you know, you’re not having a fever. Some of the physical exam maneuvers we did were also negative. So I don’t think you have anything to worry about that,” you know. So I think it’s having that very candid conversation with the patient that helps build that initial trust.</em>&nbsp;</p>



<p><strong>LEE:</strong> So, Zak, as far as I can tell, Daniel and Morgan are figuring this out on their own as medical students. I don&#8217;t think this is part of the curriculum. Does it need to be?&nbsp;</p>



<p><strong>KOHANE:</strong> It&#8217;s missing the bigger point. The incentives and economic forces are such that even if you were Daniel, and things have not changed in terms of incentives, and it&#8217;s 2030, he still has to see this many patients in an hour.&nbsp;</p>



<p>And sitting down, going over that with a patient, let&#8217;s say some might need more &#8230; in fact, I think computer scientists are enriched for these sort of neurotic “explain [to] me why this works,” when often the answer is, “I have no idea; empirically it does.”&nbsp;</p>



<p>And patients in some sense deserve that conversation, and we&#8217;re taught about joint decision making, but in practice, there&#8217;s a lot of skills that are deployed to actually deflect so that you can get through the appointment and see enough patients per hour.&nbsp;</p>



<p>And that&#8217;s why I think that one of the central … another task for AI is how to engage with patients to actually explain to them why their doctor is doing what he&#8217;s doing and perhaps ask the one or two questions that you should be asking the doctor in order to reassure you that they&#8217;re doing the right thing.</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>KOHANE: </strong>I<strong> </strong>just …<strong> </strong>right now, we are going to have less doctor time, not more doctor time.&nbsp;</p>



<p>And so I&#8217;ve always been struck by the divide between medicine that we&#8217;re taught as it should be practiced as a gentle person&#8217;s vocation or sport as opposed to assembly line, heads down “you&#8217;ve got to see those patients by the end of the day” because, otherwise, you haven&#8217;t seen all the patients at the end of the day.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Carey, I&#8217;ve been dying to ask you this, and I have not asked you this before. When you go see a doctor, are you coming in armed with ChatGPT information?&nbsp;</p>



<p><strong>GOLDBERG: </strong>I haven&#8217;t needed to yet, but I certainly would. And also my reaction to the medical student description was, I think we need to distinguish between the last 20 years, when patients would come in armed with Google, and what they&#8217;re coming in with now because at least the experiences that I&#8217;ve witnessed, it is miles better to have gone back and forth with GPT-4 than with, you know, dredging what you can from Google. And so I think we should make that distinction.&nbsp;</p>



<p>And also, the other thing that most interested me was this question for medical students of whether they should not use AI for a while so that they can learn …&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>GOLDBERG: </strong>… how to think and similarly maybe don&#8217;t use the automated scribes for a while so they can learn how to do a note. And at what point should they then start being able to use AI? And I suspect it&#8217;s fairly early on that, in fact, they&#8217;re going be using it so consistently that there&#8217;s not that much they need to learn before they start using the tools.&nbsp;</p>



<p><strong>LEE:</strong> These two students were incredibly impressive. And so I have wondered, you know, if we got a skewed view of things. I mean, Morgan is, of course, a very, very impressive person. And Daniel was handpicked by the dean of the medical school to be a subject of this interview.&nbsp;</p>



<p><strong>KOHANE: </strong>You know, we filter our students, by and large, I mean, there&#8217;s exceptions, but students in medical school are so starry eyed. And they are really &#8230; they got into medical school—I mean, some of them may have faked it—but a lot of them because they really wanted to do good.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>KOHANE: </strong>And they really wanted to help. And so this is very constant with them. And it&#8217;s only when they&#8217;re in the machine, past medical school, that they realize, oh my God, this is a very, very different story.&nbsp;</p>



<p>And I can tell you, because I teach a course in computational-enabled medicine, so I get a lot of these nerd medical students, and I&#8217;m telling them, “You&#8217;re going to experience this. And you&#8217;re going to say, ‘I&#8217;m not going to able to change medicine until I get enough cred 10, 15 years from now, whereas I could start my own company and immediately change medicine.’”&nbsp;</p>



<p>And increasingly I&#8217;m getting calls in like residency and saying, “Zak, help me. How do I get out of this?”&nbsp;</p>



<p><strong>GOLDBERG: </strong>Wow.&nbsp;</p>



<p><strong>KOHANE:</strong> And so I think there&#8217;s a real disillusionment of, like, between what we&#8217;re asking for people coming to medical school—we&#8217;re looking for a phenotype—and then we&#8217;re disappointing them massively, not everywhere, but massively.&nbsp;</p>



<p>And for me, it&#8217;s very sad because among our best and brightest, and then because of economics and expectations and the nature of the beast, they&#8217;re not getting to enjoy the most precious part of being a doctor, which is that real human connection, and longitudinality, you know, the connection between the same doctor visit after visit, is more and more of a luxury.&nbsp;</p>



<p><strong>LEE:</strong> Well, maybe this gets us to the last episode, you know, where I talk to a former, you know, state director of public health, Umair Shah, and with Gianrico Farrugia, who&#8217;s the CEO of Mayo Clinic. And I think if there&#8217;s one theme that I took away from those conversations is that we&#8217;re not thinking broadly enough nor big enough.&nbsp;</p>



<p>And so here&#8217;s a little quote of exchange that Umair Shah, who was the former head of public health in the State of Washington and prior to that in Harris County, Texas, and we had a conversation about what techies tend to focus on when they&#8217;re thinking about AI and medicine.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-0a05c3ff13da1926875eed47432d27b2"><strong><em>UMAIR SHAH: </em></strong><em>I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.</em><br><br><strong><em>LEE: </em></strong><em>Yes.</em><strong><em> </em></strong><em>And in fact, it&#8217;s not even delivery. I think techies—I did this, too—tend to gravitate specifically to diagnosis.</em></p>



<p><strong>LEE:</strong> I have been definitely guilty. I think Umair, of course, was speaking as a former frustrated public health official in just thinking about all the other things that are important to maintain a healthy population.&nbsp;</p>



<p>Is there some lesson that we should take away? I think our book also focused a lot on things like diagnosis.&nbsp;</p>



<p><strong>KOHANE:</strong> Yeah. Well, first of all, I think we just have to have humility. And I think it&#8217;s a really important ingredient. I found myself staring at the increase in lifespan in human beings over the last two centuries and looking for bumps that were attributable.&nbsp;</p>



<p>I&#8217;m in medical school. I&#8217;ve already made this major commitment. What are the bumps that are attributable to medicine? And there was one bump that was due to vaccines, a small bump. Another small bump that was due to antibiotics. And the rest of it is nutrition, sanitation, yeah, nutrition and sanitation.&nbsp;</p>



<p>And so I think doctors can be incredibly valuable, but not all the time. And we&#8217;re spending now one-sixth of our GDP on it. The majority of it is not effectively prolonging life. And so the humility has to be the right medicine at the right time.&nbsp;</p>



<p>But that runs, (A) against a bunch of business models. It runs against the primacy of doctors in healthcare. It was one thing when there were no textbooks; there was no PubMed. You know, the doctor was the repository of all the probably knowledge that we have. But I think your guests were right. We have to think more broadly in the public health way. How do we make knowledge pervasive like sanitation?&nbsp;</p>



<p><strong>GOLDBERG: </strong>Although I would add that since what we&#8217;re talking about is AI, it&#8217;s harder to see if &#8230; and if what you&#8217;re talking about is public health, I mean, it was certainly very important to have good data during the pandemic, for example.&nbsp;</p>



<p>But most of the ways to improve public health, like getting people to stop smoking and eat better and sleep better and exercise more, are not things that AI can help with that much. Whereas diagnosis or trying to improve treatment are places that it could tackle.&nbsp;</p>



<p>And in fact, Peter, I wanted to put you—oh, wait, Zak&#8217;s going to say something—but, Peter, I wanted to put you on the spot.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>GOLDBERG: </strong>I mean, if you had a medical issue now, and you went to a physician, would you be OK with them not using generative AI?&nbsp;</p>



<p><strong>LEE:</strong> I think if it&#8217;s a complex or a mysterious case, I would want them to use generative AI. I would want that second opinion on things. And I would personally be using it. If for no other reason than just to understand what the chart is saying.&nbsp;</p>



<p>I don&#8217;t see, you know, how or why one wouldn&#8217;t do that now.&nbsp;</p>



<p><strong>KOHANE:</strong> It&#8217;s such a cheap second opinion, and people are making mistakes. And even if there are mistakes on the part of AI, if there&#8217;s a collision, discrepancy, that&#8217;s worth having a discussion. And again, this is something that we used to do more of when we had more time with the patients; we&#8217;d have clinic conferences.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>KOHANE:</strong> And we don&#8217;t have that now. So I do think that there is a role for AI. But I think again, it&#8217;s much more of a continual presence, being part of a continued conversation rather than an oracle.&nbsp;</p>



<p>And I think that&#8217;s when you&#8217;ll start seeing, when the AI is truly a colleague, and saying, “You know, Zak, that&#8217;s the second time you made that mistake. You know, that&#8217;s not obesity. That&#8217;s the effect of your drugs that you&#8217;re giving her. You better back off of it.” And that&#8217;s what we need to see happen.&nbsp;</p>



<p><strong>LEE:</strong> Well, and for the business of healthcare, that also relates directly to quality scores, which translates into money for healthcare providers.&nbsp;</p>



<p>So the last person that we interviewed was Gianrico Farrugia. And, you know, I was sort of wondering, I was expecting to get a story from a CEO saying, “Oh, my God, this has been so disruptive, incredibly important, meaningful, but wow, what a headache.”&nbsp;</p>



<p>At least Gianrico didn&#8217;t expose any of that. Here&#8217;s one of the snippets to give you a sense.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-e494c3919056a8b99abdfa66f7e6fae6"><strong><em>GIANRICO</em></strong><em> </em><strong><em>FARRUGIA:</em></strong><em> When generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, because something as disruptive as that instantly became enabling at Mayo Clinic.</em>&nbsp;</p>



<p><strong>LEE:</strong> So I tried pretty hard in that interview to get Gianrico to admit that there was a period of headache and disruption here. And he never, ever gave me that. And so I take him at his word.&nbsp;</p>



<p>Zak, maybe I should ask you, what about Harvard and the whole Harvard medical ecosystem?&nbsp;</p>



<p><strong>KOHANE:</strong> I would be surprised if there are system-wide measurable gains in health quality right now from AI. And I do have to say that Mayo is one of the most marvelous organizations in terms of team behavior. So if there&#8217;s someone who&#8217;s gotten the team part of it right, they&#8217;ve come the closest, which relates to our prior conversation. They have the quarterback idea …&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>KOHANE: </strong>… pretty well down compared to others.&nbsp;</p>



<p>Nonetheless, I take him at his word, that it hasn&#8217;t disrupted them. But I&#8217;m also, I have yet to see the evidence that there&#8217;s been a quantum leap in quality or efficacy. And I do believe that it&#8217;s possible to have a quantum leap in efficacy in the right system.&nbsp;</p>



<p>So if they haven&#8217;t been disrupted, I would venture that they&#8217;ve absorbed it, but they haven&#8217;t used it to its fullest potential. And the way I could be proven wrong is next year, also the metrics showing that over the last year, they&#8217;ve had, you know, decreased readmissions, decreased complications, decreased errors and all that. And if so, God bless them. And we should all be more like Mayo.&nbsp;</p>



<p><strong>LEE:</strong> So I thought a little bit about two other quotes from the interviews that sort of maybe would send us off with some more inspirational kind of view of the future. And so there&#8217;s one from Bill Gates and one from Gianrico Farrugia. So what I&#8217;d like to do is to play both of those and then maybe we can have our last comments.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-34fa8f74d3c5dd1ddf63456b250b860b"><strong><em>BILL GATES</em></strong><em>: You know, I’ve gone so far as to tell politicians with national health systems that if they deploy AI appropriately, that the quality of care, the overload of the doctors, the improvement in the economics will be enough that their voters will be stunned because they just don’t expect this, and, you know, they could be reelected just on this one thing of fixing what is a very overloaded and economically challenged health system in these rich countries.</em>&nbsp;</p>



<p>And now Gianrico.&nbsp;</p>



<p class="has-white-color has-gray-background-color has-text-color has-background has-link-color wp-elements-daa9fe33118b2710490d542ab9959977"><strong><em>GIANRICO FARRUGIA: </em></strong><em>And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. … And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, no, is that let&#8217;s start with that aim, the last aim …</em><strong><em> </em></strong><em>because the others will come automatically if you&#8217;re working on that harder problem. Because one, to get to that harder problem, you&#8217;ll find all the other solutions.</em>&nbsp;</p>



<p>All right. I think these are both kind of calls to be more assertive about this and more forward leaning. I think two years into the GPT-4 era, those are pretty significant and pretty optimistic calls to action. So maybe just to give you both one last word. What would be one hope that you would have for the world of healthcare and medicine two years from now?&nbsp;</p>



<p><strong>KOHANE:</strong> I would hope for businesses that whoever actually owns them at some holding company level, regardless of who owns them, are truly patient-focused companies, companies where the whole AI is about improving your care, and it&#8217;s only trying to maximize your care and it doesn&#8217;t care about resource limitations.&nbsp;</p>



<p>And as I was listening to Bill, and the problem with what he was saying about saving dollars for governments is for many things, we have some very expensive things that work. And if the AI says, “This is the best thing,” it&#8217;s going to break your bank. And instead, because of research limitations, we play a human-based fancy footwork to get out of it.&nbsp;</p>



<p>That&#8217;s a hard game to play, and I leave it to the politicians and the public health officials who have to do those trades of utilities.&nbsp;</p>



<p>In my role as doctor and patient, I&#8217;d like to see very informed, authoritative agents acting only on our behalf so that when we go and we seek to have our maladies addressed, the only issue is, what&#8217;s the best and right thing for me now? And I think that is both technically realizable. And even in our weird system, there are business plans that will work that can achieve that. That&#8217;s my hope for two years from now.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, fantastic. Carey.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah. I second that so enthusiastically. And I think, you know, we have this very glass half full/glass half empty phenomenon two years after the book came out.&nbsp;</p>



<p>And it&#8217;s certainly very nice to see, you know, new approaches to administrative complexity and to prior authorization and all kinds of ways to make physicians&#8217; lives easier. But really what we all care about is our own health and that we would like to be able to optimize the use of this truly glorious technological achievement to be able to live longer and better lives. And I think what Zak just described is the most logical way to do that.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p><strong>LEE:</strong> Yeah, I think for me, two years from now, I would like to see all of this digital data that&#8217;s been so painful, such a burden on every doctor and nurse to record, actually amount to something meaningful in the care of patients. And I think it&#8217;s possible.&nbsp;</p>



<p><strong>KOHANE:</strong> Amen.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Yeah.&nbsp;</p>



<p><strong>LEE:</strong> All right, so it&#8217;s been quite a journey. We were joking before we&#8217;re still on speaking terms after having written a book. [LAUGHS]&nbsp;</p>



<p>And then, um, I think listeners might enjoy knowing that we debated amongst ourselves what to do about a second edition, which seemed too painful to me, and so I suggested the podcast, which seemed too painful to the two of you [LAUGHTER]. And in the end, I don&#8217;t know what would have been easier, writing a book or doing this podcast series, but I do think that we learned a lot.&nbsp;</p>



<p>Now, last bit of business here.<strong> </strong>To avoid having the three of us try to write a book again and do this podcast, I leaned on the production team in Microsoft Research and the Microsoft Research Podcast. And I thought it would be good to give an explicit acknowledgment to all the people who&#8217;ve contributed to this.&nbsp;</p>



<p>So it&#8217;s a long list of names. I&#8217;m going to read through them all. And then I suggest that we all give an applaud [LAUGHTER] to them. And so here we go.&nbsp;</p>



<p>There’s Neeltje Berger, Tetiana Bukhinska, David Celis Garcia, Matt Corwine, Jeremy Crawford, Kristina Dodge, Chris Duryee, Ben Ericson, Kate Forster, Katy Halliday, Alyssa Hughes, Jake Knapp, Weishung Liu, Matt McGinley, Jeremy Mashburn, Amanda Melfi, Wil Morrill, Joe Plummer, Brenda Potts, Lindsay Shanahan, Sarah Sobolewski, David Sullivan,&nbsp;Stephen Sullivan, Amber Tingle, Caitlyn Treanor, Craig Tuschhoff, Sarah Wang, and Katie Zoller.&nbsp;</p>



<p>Really a great team effort, and they made it super easy for us.&nbsp;</p>



<p><strong>GOLDBERG: </strong>Thank you. Thank you. Thank you.&nbsp;</p>



<p><strong>KOHANE:</strong> Thank you. Thank you.</p>



<p><strong>GOLDBERG: </strong>Thank you.&nbsp;</p>



<p>[THEME MUSIC]&nbsp;</p>



<p><strong>LEE: </strong>A big thank you again to all of our guests for the work they do and the time and expertise they shared with us.&nbsp;</p>



<p>And, last but not least, to our listeners, thank you for joining us. We hope you enjoyed it and learned as much as we did. If you want to go back and catch up on any episodes you may have missed or to listen to any again, you can visit <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://aka.ms/airevolutionpodcast">aka.ms/AIrevolutionPodcast<span class="sr-only"> (opens in new tab)</span></a>.</p>



<p>Until next time.</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-1"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--2"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/coauthor-roundtable-reflecting-on-healthcare-economics-biomedical-research-and-medical-education/">Coauthor roundtable: Reflecting on healthcare economics, biomedical research, and medical education</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</title>
		<link>https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/</link>
		
		<dc:creator><![CDATA[Yuncong Yang, Reuben Tan, Swadheen Shukla, Jianfeng Gao]]></dc:creator>
		<pubDate>Wed, 20 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1147961</guid>

					<description><![CDATA[<p>MindJourney can enable AI to navigate and interpret 3D environments from limited visual input, potentially improving performance in navigation, planning, and safety-critical tasks.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg" alt="Three white line icons on a gradient background transitioning from blue to pink. From left to right: a network or molecule structure with a central circle and six surrounding nodes, a 3D cube, and an open laptop with an eye symbol above it." class="wp-image-1147994" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/ImprovingImagination-BlogHeroFeature-1400x788-1-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>



<p>A new research framework helps AI agents explore three-dimensional spaces they can’t directly detect. Called <a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">MindJourney</a>, the approach addresses a key limitation in vision-language models (VLMs), which give AI agents their ability to interpret and describe visual scenes.&nbsp;&nbsp;</p>



<p>While VLMs&nbsp;are strong&nbsp;at identifying objects in&nbsp;static&nbsp;images,&nbsp;they struggle to&nbsp;interpret&nbsp;the interactive 3D world behind 2D images.&nbsp;This&nbsp;gap shows up&nbsp;in spatial&nbsp;questions&nbsp;like&nbsp;“If I sit on the couch&nbsp;that is on my right&nbsp;and face the chairs, will the kitchen be to my right or left?”—tasks that require an agent to&nbsp;interpret&nbsp;its&nbsp;position and movement through space.&nbsp;</p>



<p>People&nbsp;overcome this challenge by mentally exploring a space,&nbsp;imagining moving through it and combining those mental snapshots to work out where objects are.&nbsp;MindJourney&nbsp;applies the same process&nbsp;to&nbsp;AI agents,&nbsp;letting&nbsp;them roam a virtual&nbsp;space before answering spatial questions.&nbsp;</p>



<h2 class="wp-block-heading" id="how-mindjourney-navigates-3d-space">How&nbsp;MindJourney&nbsp;navigates 3D space</h2>



<p>To perform this type of spatial navigation, MindJourney uses a <em>world model</em>—in this case, a video generation system trained on a large collection of videos captured from a single moving viewpoint, showing actions such as going forward and turning left or right, much like a 3D cinematographer. From this, it learns to predict how a new scene would appear from different perspectives.</p>



<p>At inference time, the model can generate photo-realistic images of a scene based on possible movements from the agent’s current position. It generates multiple possible views of a scene while the VLM acts as a filter, selecting the constructed perspectives that are most likely to answer the user&#8217;s question.</p>



<p>These are kept and expanded in the next iteration, while less promising paths are discarded. This process, shown in Figure 1, avoids the need to generate and evaluate thousands of possible movement sequences by focusing only on the most informative perspectives.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="854" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg" alt="Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM's spatial interpretation through generated observations when encountering a new  challenges. " class="wp-image-1147968" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-300x183.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-1024x625.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-768x468.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney-fig1-240x146.jpg 240w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 1. Given a spatial reasoning query, MindJourney searches through the imagined 3D space using a world model and improves the VLM&#8217;s spatial interpretation through generated observations when encountering new challenges.<em>&nbsp;</em></figcaption></figure>



<p>&nbsp;</p>



<p>To make its search through&nbsp;a simulated&nbsp;space both effective and efficient,&nbsp;MindJourney&nbsp;uses a <em>spatial beam search</em>—an&nbsp;algorithm that prioritizes the most promising paths. It works within a fixed number of steps, each representing a movement. By balancing breadth with depth, spatial beam search enables&nbsp;MindJourney&nbsp;to gather strong supporting evidence.&nbsp;This process is illustrated in Figure 2.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg" alt="MindJourney pipeline diagram" class="wp-image-1147897" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/MindJourney_pipeline_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /><figcaption class="wp-element-caption">Figure 2. The MindJourney workflow starts with a spatial beam search for a set number of steps before answering the query. The world model interactively generates new observations, while a VLM interprets the generated images, guiding the search throughout the process.</figcaption></figure>



<p class="has-text-align-left">By iterating through&nbsp;simulation,&nbsp;evaluation, and integration,&nbsp;MindJourney&nbsp;can reason about spatial relationships far beyond what any single 2D image can convey, all without the need for additional training.&nbsp;On&nbsp;the&nbsp;Spatial Aptitude Training (SAT)&nbsp;benchmark,&nbsp;it improved the accuracy of&nbsp;VLMs&nbsp;by&nbsp;8%&nbsp;over&nbsp;their&nbsp;baseline&nbsp;performance.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="670821">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">Spotlight: Microsoft research newsletter</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-label="Microsoft Research Newsletter" data-bi-cN="Microsoft Research Newsletter" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2019/09/Newsletter_Banner_08_2019_v1_1920x1080.png" alt="" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">Microsoft Research Newsletter</h2>
				
								<p id="microsoft-research-newsletter" class="large">Stay connected to the research community at Microsoft.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button is-style-fill-chevron">
						<a href="https://info.microsoft.com/ww-landing-microsoft-research-newsletter.html" aria-describedby="microsoft-research-newsletter" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="Microsoft Research Newsletter" target="_blank">
							Subscribe today						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<figure class="wp-block-embed is-type-video is-provider-youtube wp-block-embed-youtube wp-embed-aspect-4-3 wp-has-aspect-ratio"><div class="wp-block-embed__wrapper">
<iframe loading="lazy" title="MindJourney: Test-Time Scaling with World Models for Spatial Reasoning" width="500" height="375" src="https://www.youtube-nocookie.com/embed/Z4-5NZmdV44?feature=oembed&rel=0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
</div></figure>



<h2 class="wp-block-heading" id="building-smarter-agents">Building&nbsp;smarter agents&nbsp;&nbsp;</h2>



<p>MindJourney&nbsp;showed&nbsp;<a href="https://www.microsoft.com/en-us/research/publication/mindjourney-test-time-scaling-with-world-models-for-spatial-reasoning/" target="_blank" rel="noreferrer noopener">strong performance</a>&nbsp;on multiple 3D spatial-reasoning benchmarks, and even advanced VLMs&nbsp;improved&nbsp;when paired with its imagination loop. This suggests that the spatial patterns that world models learn from raw images, combined with the symbolic&nbsp;capabilities&nbsp;of VLMs, create a more complete spatial capability&nbsp;for agents. Together, they enable agents to infer what lies beyond the visible frame and&nbsp;interpret&nbsp;the physical world&nbsp;more accurately.&nbsp;</p>



<p>It also demonstrates that pretrained VLMs and trainable world models can work together in 3D without retraining either one—pointing toward general-purpose agents capable of&nbsp;interpreting&nbsp;and acting in real-world environments. This opens the way to&nbsp;possible&nbsp;applications in autonomous robotics, smart home technologies, and accessibility tools for people with visual impairments.&nbsp;</p>



<p>By converting systems that simply describe static images into active agents that continually evaluate where to look next,&nbsp;MindJourney&nbsp;connects computer vision with planning. Because exploration occurs entirely within the model’s latent space—its internal representation of the scene—robots would be able to test multiple viewpoints before determining their next move,&nbsp;potentially&nbsp;reducing wear, energy use, and collision risk.&nbsp;</p>



<p>Looking ahead, we plan to extend the framework to&nbsp;use&nbsp;world models that&nbsp;not only&nbsp;predict&nbsp;new viewpoints&nbsp;but also forecast&nbsp;how the scene might change over time.&nbsp;We envision&nbsp;MindJourney&nbsp;working&nbsp;alongside VLMs that interpret&nbsp;those predictions&nbsp;and use&nbsp;them to&nbsp;plan&nbsp;what to do&nbsp;next. This&nbsp;enhancement could enable&nbsp;agents&nbsp;more accurately&nbsp;interpret&nbsp;spatial relationships and physical dynamics, helping them to operate effectively&nbsp;in changing environments.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/mindjourney-enables-ai-to-explore-simulated-3d-worlds-to-improve-spatial-interpretation/">MindJourney enables AI to explore simulated 3D worlds to improve spatial interpretation</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Dion: the distributed orthonormal update revolution is here</title>
		<link>https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/</link>
		
		<dc:creator><![CDATA[Kwangjun Ahn, John Langford]]></dc:creator>
		<pubDate>Tue, 12 Aug 2025 20:09:21 +0000</pubDate>
				<category><![CDATA[Research Blog]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/?p=1146334</guid>

					<description><![CDATA[<p>Dion is a new AI model optimization method that boosts scalability and performance over existing leading methods by orthonormalizing only a top rank subset of singular vectors, enabling more efficient training of large models such as LLaMA-3 with reduced overhead.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/">Dion: the distributed orthonormal update revolution is here</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="2560" height="1441" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-scaled.jpg" alt="Three white icons on a gradient background transitioning from blue to green. From left to right: a network of interconnected nodes, a speedometer with the needle pointing right, and a flowchart with squares and a diamond shape." class="wp-image-1147793" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-scaled.jpg 2560w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1536x865.jpg 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-2048x1153.jpg 2048w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1280x720.jpg 1280w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-1920x1080.jpg 1920w" sizes="auto, (max-width: 2560px) 100vw, 2560px" /></figure>



<p>Training AI models requires choosing an optimizer and for nearly a decade, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1412.6980" target="_blank" rel="noopener noreferrer">Adam(<span class="sr-only"> (opens in new tab)</span></a>&#8211;<a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1711.05101" target="_blank" rel="noopener noreferrer">W)<span class="sr-only"> (opens in new tab)</span></a> has been the optimizer of choice. Given that durability and success, it was fair to doubt that any further improvement was possible. And yet, last December, a new optimizer called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://kellerjordan.github.io/posts/muon/" target="_blank" rel="noopener noreferrer">Muon<span class="sr-only"> (opens in new tab)</span></a> showed serious promise by powering a <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://github.com/KellerJordan/modded-nanogpt/tree/master" target="_blank" rel="noopener noreferrer">nanoGPT speedrun<span class="sr-only"> (opens in new tab)</span></a>. This proved out, with multiple AI labs (e.g., <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2502.16982" target="_blank" rel="noopener noreferrer">Kimi-AI<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2505.02222" target="_blank" rel="noopener noreferrer">Essential-AI<span class="sr-only"> (opens in new tab)</span></a>) reporting 2x scale improvements and the release of the 1T parameter <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://moonshotai.github.io/Kimi-K2/" target="_blank" rel="noopener noreferrer">Kimi K2<span class="sr-only"> (opens in new tab)</span></a> model.&nbsp;Restated: you can train a model to similar performance with half as many GPUs.</p>



<p>There’s one fly in the ointment: Muon requires large matrix multiplications in the optimizer, which requires heavy communication in large models at the scale where <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2304.11277" target="_blank" rel="noopener noreferrer">FSDP<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener noreferrer">TP<span class="sr-only"> (opens in new tab)</span></a> parallelization becomes desirable.&nbsp;Going back to the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://jeremybernste.in/writing/deriving-muon" target="_blank" rel="noopener noreferrer">inspiration for Muon,<span class="sr-only"> (opens in new tab)</span></a> the key idea is an orthonormal update, which sparked the search&nbsp;for more scalable alternative linear algebras realizing the same goal. That’s exactly what <a href="https://www.microsoft.com/en-us/research/publication/dion-distributed-orthonormalized-updates/" target="_blank" rel="noreferrer noopener">Dion</a> is. We have open-sourced this new optimizer to enable anyone to train large models more efficiently at scale. &nbsp;</p>



<h2 class="wp-block-heading" id="what-s-an-orthonormal-update">What’s an orthonormal update?</h2>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1422" height="515" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion.png" alt="Illustration of matrix parameters" class="wp-image-1146808" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion.png 1422w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-300x109.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-1024x371.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-768x278.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion-240x87.png 240w" sizes="auto, (max-width: 1422px) 100vw, 1422px" /><figcaption class="wp-element-caption">Figure1. Illustration of matrix parameters</figcaption></figure>



<p>At the core of Transformers, a set of input activations is multiplied by a learned weight matrix to produce a new set of output activations. When the weight matrix is updated during training, the resulting change in the output activations generally depends on the direction of the input activations. As a result, the learning rate must be chosen conservatively to accommodate the input direction that induces the largest change. Orthonormalized updates alter this behavior by (approximately) making the change in output activations invariant to the direction of the input. This is achieved by enforcing <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://en.wikipedia.org/wiki/Orthonormality" target="_blank" rel="noopener noreferrer">orthonormality<span class="sr-only"> (opens in new tab)</span></a> on the update matrix, thereby equalizing its effect across all input directions.</p>



<h2 class="wp-block-heading" id="what-is-dion">What is Dion?</h2>



<p>While Muon has shown strong empirical results, scaling it to very large models poses challenges. As reported by <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.essential.ai/blog/infra" target="_blank" rel="noopener noreferrer">Essential AI<span class="sr-only"> (opens in new tab)</span></a>, applying Muon to large architectures like LLaMA-3 becomes <em>compute-bound</em>—and potentially <em>communication-bound</em>—due to the cost of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://docs.modula.systems/algorithms/newton-schulz/" target="_blank" rel="noopener noreferrer">Newton–Schulz orthonormalization steps<span class="sr-only"> (opens in new tab)</span></a>.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1685" height="857" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion.png" alt="Pseudocode of the centralized version of Dion" class="wp-image-1146810" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion.png 1685w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-300x153.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-1024x521.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-768x391.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-1536x781.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion-240x122.png 240w" sizes="auto, (max-width: 1685px) 100vw, 1685px" /><figcaption class="wp-element-caption">Figure 2. Pseudocode of the centralized version of Dion</figcaption></figure>



<p>This is where <strong>Dion</strong> enters. At a high level, Dion introduces a new axis for scalability: the <strong>rank</strong>. Specifically, for a given rank r, Dion orthonormalizes only the top r of the singular vector space, reducing communication and compute overhead while preserving performance.&nbsp;Empirically, we observe that the necessary rank for good performance grows much more slowly than the number of parameters in larger models.</p>



<div class="annotations " data-bi-aN="margin-callout">
	<article class="annotations__list card depth-16 bg-body p-4 annotations__list--right">
		<div class="annotations__list-item">
						<span class="annotations__type d-block text-uppercase font-weight-semibold text-neutral-300 small">Download</span>
			<a href="https://github.com/microsoft/dion/" target="_blank" class="annotations__link font-weight-semibold text-decoration-none" data-bi-type="annotated-link" aria-label="Dion optimizer" data-bi-aN="margin-callout" data-bi-cN="Dion optimizer">
				Dion optimizer&nbsp;<span class="glyph-append glyph-append-share glyph-append-xsmall"></span>
			</a>
					</div>
	</article>
</div>



<p>Dion implements orthonormalization using <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/pdf/1905.13727" target="_blank" rel="noopener noreferrer"><em>amortized power iteration</em><span class="sr-only"> (opens in new tab)</span></a><em>.&nbsp;</em>Power iteration typically pulls out the largest singular value by repeated matrix multiplication.&nbsp;By amortizing this process over optimization steps—applied to the slowly-evolving momentum matrix—we reduce the cost to just two matrix multiplications per step. Incorporating a QR decomposition allows us to extract an approximate orthonormal basis spanning the top singular directions, rather than just the leading one.&nbsp;This amortized power iteration is fully compatible with standard distributed training techniques such as <strong>FSDP</strong> and <strong>tensor parallelism</strong>.&nbsp;Here, we show a simple centralized version, but the technique works for more complex forms of parallelization as presented in the paper. In other words, we can orthogonalize a matrix <em>without ever seeing a full row or column of it</em>.&nbsp;</p>



<p>Low-rank approximation would ordinarily introduce error, but Dion overcomes this through an error feedback mechanism. This keeps the residual of low rank approximation in the momentum matrix so that any systematic gradient structure not initially captured accumulates to eventually be applied in a future update.</p>



	<div class="border-bottom border-top border-gray-300 mt-5 mb-5 msr-promo text-center text-md-left alignwide" data-bi-aN="promo" data-bi-id="1144027">
		

		<p class="msr-promo__label text-gray-800 text-center text-uppercase">
		<span class="px-4 bg-white display-inline-block font-weight-semibold small">PODCAST SERIES</span>
	</p>
	
	<div class="row pt-3 pb-4 align-items-center">
						<div class="msr-promo__media col-12 col-md-5">
				<a class="bg-gray-300 display-block" href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-label="AI Testing and Evaluation: Learnings from Science and Industry" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
					<img decoding="async" class="w-100 display-block" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/EP2-AI-TE_Hero_Feature_River_No_Text_1400x788.jpg" alt="Illustrated headshots of Daniel Carpenter, Timo Minssen, Chad Atalla, and Kathleen Sullivan for the Microsoft Research Podcast" />
				</a>
			</div>
			
			<div class="msr-promo__content p-3 px-5 col-12 col-md">

									<h2 class="h4">AI Testing and Evaluation: Learnings from Science and Industry</h2>
				
								<p id="ai-testing-and-evaluation-learnings-from-science-and-industry" class="large">Discover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.</p>
				
								<div class="wp-block-buttons justify-content-center justify-content-md-start">
					<div class="wp-block-button">
						<a href="https://www.microsoft.com/en-us/research/story/ai-testing-and-evaluation-learnings-from-science-and-industry/" aria-describedby="ai-testing-and-evaluation-learnings-from-science-and-industry" class="btn btn-brand glyph-append glyph-append-chevron-right" data-bi-cN="AI Testing and Evaluation: Learnings from Science and Industry" target="_blank">
							Listen now						</a>
					</div>
				</div>
							</div><!--/.msr-promo__content-->
	</div><!--/.msr-promo__inner-wrap-->
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span>	</div><!--/.msr-promo-->
	


<h2 class="wp-block-heading" id="how-does-it-work">How does it work?</h2>



<p>Something very strange happened in our experiments. Usually, adding an extra constraint on the way an algorithm works can be expected to <em>decrease</em> overall performance. And indeed, at the 120M parameter scale of the speedrun, we see Dion’s update taking more time than Muon, while not yielding any significant gains. But at larger scales, we observed a different trend: Dion began to outperform Muon.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="699" height="414" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion.png" alt="Wall-clock time speedup of Dion for 3B model training" class="wp-image-1146815" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion.png 699w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion-300x178.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion-240x142.png 240w" sizes="auto, (max-width: 699px) 100vw, 699px" /><figcaption class="wp-element-caption">Figure 3. Wall-clock time speedup of Dion for 3B model training</figcaption></figure>



<p>Why would adding a constraint <em>improve</em> the update rule? The answer lies in what the constraint enforces. Dion achieves a much closer approximation to true orthonormalization than Muon. This precision, initially subtle, becomes increasingly important as the number of singular vectors grows. Over increasing model scale and training steps, this small advantage accumulates—leading to a measurable improvement in performance.</p>



<p>This edge further grows with batch size—with larger batches the update quality tends to degrade, but notably more slowly with Dion than Muon (and Muon is already a significant improvement over AdamW).</p>



<figure class="wp-block-image aligncenter size-full is-resized"><img loading="lazy" decoding="async" width="786" height="637" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion.png" alt="Scaling of Dion across different batch sizes" class="wp-image-1146818" style="width:596px;height:auto" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion.png 786w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-300x243.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-768x622.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion-222x180.png 222w" sizes="auto, (max-width: 786px) 100vw, 786px" /><figcaption class="wp-element-caption">Figure 4. Scaling of Dion across different batch sizes</figcaption></figure>



<p>Here you can see how the number of steps to reach a pretraining loss compared to AdamW varies as batch size grows with full rank and ¼ rank Dion (in orange) and Muon (in blue).&nbsp;&nbsp;&nbsp;</p>



<p>In our experiments, these benefits extend to various post-training regimes as well.</p>



<p>We also experimented with rank, discovering empirically that larger models tolerate smaller rank well.</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1893" height="511" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion.png" alt="Low-rank Dion across different model sizes" class="wp-image-1146821" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion.png 1893w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-300x81.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-1024x276.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-768x207.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-1536x415.png 1536w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion-240x65.png 240w" sizes="auto, (max-width: 1893px) 100vw, 1893px" /><figcaption class="wp-element-caption">Figure 5. Low-rank Dion across different model sizes</figcaption></figure>



<p>Projecting this trend out to the scale of the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2407.21783" target="_blank" rel="noopener noreferrer">LLaMA-3<span class="sr-only"> (opens in new tab)</span></a> 405B parameter models suggests that Dion is fully effective even with <strong>rank fractions as low as 1/16 or 1/64</strong> for large dense models like LLaMA-3.&nbsp;&nbsp;&nbsp;&nbsp;</p>



<p>Using hardware timings of the individual update steps suggests a story that looks this:</p>



<figure class="wp-block-image aligncenter size-full"><img loading="lazy" decoding="async" width="1075" height="645" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6.png" alt="Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon." class="wp-image-1147684" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6.png 1075w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-300x180.png 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-1024x614.png 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-768x461.png 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6-240x144.png 240w" sizes="auto, (max-width: 1075px) 100vw, 1075px" /><figcaption class="wp-element-caption">Figure 6. Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.</figcaption></figure>



<p>We’ve open-sourced a PyTorch FSDP2 + Tensor Parallel (TP) implementation of <strong>Dion</strong>, available via a simple pip install. Our goal is to make faster training with Dion accessible to everyone. As a bonus, the repository also includes a PyTorch FSDP2 implementation of <strong>Muon.</strong></p>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-fill-github"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://github.com/microsoft/dion/">Dion optimizer</a></div>
</div>



<h2 class="wp-block-heading" id="acknowledgements">Acknowledgements</h2>



<p>We thank Riashat Islam and Pratyusha Sharma for their helpful feedback on the writing and presentation.</p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/">Dion: the distributed orthonormal update revolution is here</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Reimagining healthcare delivery and public health with AI</title>
		<link>https://www.microsoft.com/en-us/research/podcast/reimagining-healthcare-delivery-and-public-health-with-ai/</link>
		
		<dc:creator><![CDATA[Peter Lee, Dr. Umair Shah, Dr. Gianrico Farrugia]]></dc:creator>
		<pubDate>Thu, 07 Aug 2025 16:00:00 +0000</pubDate>
				<category><![CDATA[Microsoft Research Podcast]]></category>
		<guid isPermaLink="false">https://www.microsoft.com/en-us/research/podcast/reimagining-healthcare-delivery-and-public-health-with-ai/</guid>

					<description><![CDATA[<p>Former Washington State Secretary of Health Dr. Umair Shah and Mayo Clinic CEO Dr. Gianrico Farrugia explore how healthcare leaders are approaching AI when it comes to public health, care delivery, the healthcare-research connection, and the patient experience.</p>
<p>The post <a href="https://www.microsoft.com/en-us/research/podcast/reimagining-healthcare-delivery-and-public-health-with-ai/">Reimagining healthcare delivery and public health with AI</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full"><img loading="lazy" decoding="async" width="1400" height="788" src="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788.jpg" alt="Illustrated headshots of Peter Lee, Umair Shah, Gianrico Farrugia" class="wp-image-1147485" srcset="https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788.jpg 1400w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-300x169.jpg 300w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-1024x576.jpg 1024w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-768x432.jpg 768w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-1066x600.jpg 1066w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-655x368.jpg 655w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-240x135.jpg 240w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-640x360.jpg 640w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-960x540.jpg 960w, https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Episode10-PeterUmairGianrico-AIRevolution_Hero_Feature_No_Text_1400x788-1280x720.jpg 1280w" sizes="auto, (max-width: 1400px) 100vw, 1400px" /></figure>


<div class="wp-block-msr-podcast-container my-4">
	<iframe loading="lazy" src="https://player.blubrry.com/?podcast_id=147585250&modern=1" class="podcast-player" frameborder="0" height="164px" width="100%" scrolling="no" title="Podcast Player"></iframe>
</div>



<p>In November 2022, OpenAI’s ChatGPT kick-started a new era in AI. This was followed less than a half year later by the release of GPT-4. In the months leading up to GPT-4’s public release, Peter Lee, president of Microsoft Research, cowrote a book full of optimism for the potential of advanced AI models to transform the world of healthcare. What has happened since? In this special podcast series, <em>The AI Revolution in Medicine, Revisited</em>, Lee revisits the book, exploring how patients, providers, and other medical professionals are experiencing and using generative AI today while examining what he and his coauthors got right—and what they didn’t foresee.&nbsp;</p>



<p>In this episode, healthcare leaders <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.linkedin.com/in/umair-a-shah-md-mph/" target="_blank" rel="noopener noreferrer">Dr. Umair Shah<span class="sr-only"> (opens in new tab)</span></a> and <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://www.mayoclinic.org/content/dam/media/global/documents/trustees/bios/gianrico-farrugia.pdf">Dr. Gianrico Farrugia<span class="sr-only"> (opens in new tab)</span></a> join Lee to discuss AI’s impact on the business of public health and healthcare delivery, the healthcare-research connection, and the patient experience. Shah, a healthcare strategic consultant and former state secretary of health, explores the role of public health in the larger ecosystem and why it might not get the attention it needs or deserves and how AI could be leveraged to assist in data analysis, to help better engage with people on matters of public health, and to help narrow gaps between care delivery and public health responses during health emergencies. Farrugia, president and CEO of Mayo Clinic, traces AI’s path from predictive to generative and discusses how that progress has helped usher in a new healthcare architecture for Mayo Clinic and its partners, one powered by the goal of longer, healthier lives for patients, and how AI is also changing Mayo Clinic’s research and the education it provides, including the offering of masters and PhDs in AI and other emerging technologies.&nbsp;</p>



<div class="wp-block-group msr-pattern-link-list is-layout-flow wp-block-group-is-layout-flow">
<hr class="wp-block-separator has-alpha-channel-opacity"/>



<h2 class="wp-block-heading h5" id="learn-more-1">Learn more:</h2>



<ul class="wp-block-list list-unstyled">
<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://rickshawhealth.com/" target="_blank" rel="noopener noreferrer">Rickshaw Health<span class="sr-only"> (opens in new tab)</span></a> (Shah)&nbsp;<br>Homepage</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://doh.wa.gov/emergencies/covid-19/covid-19-after-action-report" target="_blank" rel="noopener noreferrer">COVID-19 After-Action Report<span class="sr-only"> (opens in new tab)</span></a> (Shah)&nbsp;<br>Washington State Department of Health | March 2024</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.mayoclinicplatform.org/" target="_blank" rel="noopener noreferrer">Mayo Clinic Platform<span class="sr-only"> (opens in new tab)</span></a> (Farrugia)&nbsp;<br>Homepage</li>



<li><a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2501.05409" target="_blank" rel="noopener noreferrer">Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charité, and Aignostics<span class="sr-only"> (opens in new tab)</span></a> (Farrugia)&nbsp;<br>Publication | January 2025</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/" target="_blank" rel="noreferrer noopener">MAIRA-2: Grounded Radiology Report Generation</a> (Farrugia)<br>Publication | June 2024</li>



<li><a href="https://www.microsoft.com/en-us/research/publication/the-ai-revolution-in-medicine-gpt-4-and-beyond/" target="_blank" rel="noreferrer noopener">The AI Revolution in Medicine: GPT-4 and Beyond</a>&nbsp;<br>Book | Peter Lee, Carey Goldberg, Isaac Kohane | April 2023&nbsp;</li>
</ul>



<div style="height:25px" aria-hidden="true" class="wp-block-spacer"></div>
</div>



<section class="wp-block-msr-subscribe-to-podcast subscribe-to-podcast">
	<div class="subscribe-to-podcast__inner border-top border-bottom border-width-2">
		<h2 class="h5 subscribe-to-podcast__heading">
			Subscribe to the <a href="https://www.microsoft.com/en-us/research/podcast">Microsoft Research Podcast</a>:		</h2>
		<ul class="subscribe-to-podcast__list list-unstyled">
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://itunes.apple.com/us/podcast/microsoft-research-a-podcast/id1318021537?mt=2" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="black" viewBox="0 0 32 32">  <path d="M7.12 0c-3.937-0.011-7.131 3.183-7.12 7.12v17.76c-0.011 3.937 3.183 7.131 7.12 7.12h17.76c3.937 0.011 7.131-3.183 7.12-7.12v-17.76c0.011-3.937-3.183-7.131-7.12-7.12zM15.817 3.421c3.115 0 5.932 1.204 8.079 3.453 1.631 1.693 2.547 3.489 3.016 5.855 0.161 0.787 0.161 2.932 0.009 3.817-0.5 2.817-2.041 5.339-4.317 7.063-0.812 0.615-2.797 1.683-3.115 1.683-0.12 0-0.129-0.12-0.077-0.615 0.099-0.792 0.192-0.953 0.64-1.141 0.713-0.296 1.932-1.167 2.677-1.911 1.301-1.303 2.229-2.932 2.677-4.719 0.281-1.1 0.244-3.543-0.063-4.672-0.969-3.595-3.907-6.385-7.5-7.136-1.041-0.213-2.943-0.213-4 0-3.636 0.751-6.647 3.683-7.563 7.371-0.245 1.004-0.245 3.448 0 4.448 0.609 2.443 2.188 4.681 4.255 6.015 0.407 0.271 0.896 0.547 1.1 0.631 0.447 0.192 0.547 0.355 0.629 1.14 0.052 0.485 0.041 0.62-0.072 0.62-0.073 0-0.62-0.235-1.199-0.511l-0.052-0.041c-3.297-1.62-5.407-4.364-6.177-8.016-0.187-0.943-0.224-3.187-0.036-4.052 0.479-2.323 1.396-4.135 2.921-5.739 2.199-2.319 5.027-3.543 8.172-3.543zM16 7.172c0.541 0.005 1.068 0.052 1.473 0.14 3.715 0.828 6.344 4.543 5.833 8.229-0.203 1.489-0.713 2.709-1.619 3.844-0.448 0.573-1.537 1.532-1.729 1.532-0.032 0-0.063-0.365-0.063-0.803v-0.808l0.552-0.661c2.093-2.505 1.943-6.005-0.339-8.296-0.885-0.896-1.912-1.423-3.235-1.661-0.853-0.161-1.031-0.161-1.927-0.011-1.364 0.219-2.417 0.744-3.355 1.672-2.291 2.271-2.443 5.791-0.348 8.296l0.552 0.661v0.813c0 0.448-0.037 0.807-0.084 0.807-0.036 0-0.349-0.213-0.683-0.479l-0.047-0.016c-1.109-0.885-2.088-2.453-2.495-3.995-0.244-0.932-0.244-2.697 0.011-3.625 0.672-2.505 2.521-4.448 5.079-5.359 0.547-0.193 1.509-0.297 2.416-0.281zM15.823 11.156c0.417 0 0.828 0.084 1.131 0.24 0.645 0.339 1.183 0.989 1.385 1.677 0.62 2.104-1.609 3.948-3.631 3.005h-0.015c-0.953-0.443-1.464-1.276-1.475-2.36 0-0.979 0.541-1.828 1.484-2.328 0.297-0.156 0.709-0.235 1.125-0.235zM15.812 17.464c1.319-0.005 2.271 0.463 2.625 1.291 0.265 0.62 0.167 2.573-0.292 5.735-0.307 2.208-0.479 2.765-0.905 3.141-0.589 0.52-1.417 0.667-2.209 0.385h-0.004c-0.953-0.344-1.157-0.808-1.553-3.527-0.452-3.161-0.552-5.115-0.285-5.735 0.348-0.823 1.296-1.285 2.624-1.291z"/></svg>
						<span class="subscribe-to-podcast__link-text">Apple Podcasts</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribebyemail.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.4 6a2.392 2.392 0 00-2.372 2.119L16 15.6l11.972-7.481A2.392 2.392 0 0025.6 6H6.4zM4 10.502V22.8a2.4 2.4 0 002.4 2.4h19.2a2.4 2.4 0 002.4-2.4V10.502l-11.365 7.102a1.2 1.2 0 01-1.27 0L4 10.502z"/></svg>
						<span class="subscribe-to-podcast__link-text">Email</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://subscribeonandroid.com/www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M12.414 4.02c-.062.012-.126.023-.18.06a.489.489 0 00-.12.675L13.149 6.3c-1.6.847-2.792 2.255-3.18 3.944h13.257c-.388-1.69-1.58-3.097-3.179-3.944l1.035-1.545a.489.489 0 00-.12-.675.492.492 0 00-.675.135l-1.14 1.68a7.423 7.423 0 00-2.55-.45c-.899 0-1.758.161-2.549.45l-1.14-1.68a.482.482 0 00-.494-.195zm1.545 3.824a.72.72 0 110 1.44.72.72 0 010-1.44zm5.278 0a.719.719 0 110 1.44.719.719 0 110-1.44zM8.44 11.204A1.44 1.44 0 007 12.644v6.718c0 .795.645 1.44 1.44 1.44.168 0 .33-.036.48-.09v-9.418a1.406 1.406 0 00-.48-.09zm1.44 0V21.76c0 .793.646 1.44 1.44 1.44h10.557c.793 0 1.44-.647 1.44-1.44V11.204H9.878zm14.876 0c-.169 0-.33.035-.48.09v9.418c.15.052.311.09.48.09a1.44 1.44 0 001.44-1.44v-6.719a1.44 1.44 0 00-1.44-1.44zM11.8 24.16v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84zm5.759 0v1.92a1.92 1.92 0 003.84 0v-1.92h-3.84z"/></svg>
						<span class="subscribe-to-podcast__link-text">Android</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://open.spotify.com/show/4ndjUXyL0hH1FXHgwIiTWU" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M16 4C9.383 4 4 9.383 4 16s5.383 12 12 12 12-5.383 12-12S22.617 4 16 4zm5.08 17.394a.781.781 0 01-1.086.217c-1.29-.86-3.477-1.434-5.303-1.434-1.937.002-3.389.477-3.403.482a.782.782 0 11-.494-1.484c.068-.023 1.71-.56 3.897-.562 1.826 0 4.365.492 6.171 1.696.36.24.457.725.217 1.085zm1.56-3.202a.895.895 0 01-1.234.286c-2.338-1.457-4.742-1.766-6.812-1.747-2.338.02-4.207.466-4.239.476a.895.895 0 11-.488-1.723c.145-.041 2.01-.5 4.564-.521 2.329-.02 5.23.318 7.923 1.995.419.26.547.814.286 1.234zm1.556-3.745a1.043 1.043 0 01-1.428.371c-2.725-1.6-6.039-1.94-8.339-1.942h-.033c-2.781 0-4.923.489-4.944.494a1.044 1.044 0 01-.474-2.031c.096-.023 2.385-.55 5.418-.55h.036c2.558.004 6.264.393 9.393 2.23.497.292.663.931.371 1.428z"/></svg>
						<span class="subscribe-to-podcast__link-text">Spotify</span>
					</a>
				</li>
			
							<li class="subscribe-to-podcast__list-item">
					<a class="subscribe-to-podcast__link" href="https://www.blubrry.com/feeds/microsoftresearch.xml" target="_blank" rel="noreferrer noopener">
						<svg class="subscribe-to-podcast__svg" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 32 32"><path fill="currentColor" d="M6.667 4a2.676 2.676 0 00-2.612 2.13v.003c-.036.172-.055.35-.055.534v18.666c0 .183.019.362.055.534v.003a2.676 2.676 0 002.076 2.075h.002c.172.036.35.055.534.055h18.666A2.676 2.676 0 0028 25.333V6.667a2.676 2.676 0 00-2.13-2.612h-.003A2.623 2.623 0 0025.333 4H6.667zM8 8h1.333C17.42 8 24 14.58 24 22.667V24h-2.667v-1.333c0-6.618-5.382-12-12-12H8V8zm0 5.333h1.333c5.146 0 9.334 4.188 9.334 9.334V24H16v-1.333A6.674 6.674 0 009.333 16H8v-2.667zM10 20a2 2 0 11-.001 4.001A2 2 0 0110 20z"/></svg>
						<span class="subscribe-to-podcast__link-text">RSS Feed</span>
					</a>
				</li>
					</ul>
	</div>
</section>


<div class="wp-block-msr-show-more">
	<div class="bg-neutral-100 p-5">
		<div class="show-more-show-less" data-mount="show-more-show-less">
			<div>
				<span>
					

<h2 class="wp-block-heading" id="transcript">Transcript&nbsp;</h2>



<p>[MUSIC] </p>



<p>[BOOK PASSAGE]&nbsp;</p>



<p><strong>PETER LEE:</strong> “In US healthcare, quality ratings are increasingly used to tie the improvement in patient health outcomes to the reimbursement rates that healthcare providers can receive. The ability of GPT-4 to understand these systems and give concrete advice … has a chance to make it easier for providers to achieve success in both dimensions.”&nbsp;</p>



<p>[END OF BOOK PASSAGE]</p>



<p>[THEME MUSIC]</p>



<p>This is <em>The AI Revolution in Medicine, Revisited</em>. I’m your host, Peter Lee. </p>



<p>Shortly after OpenAI&#8217;s GPT-4 was publicly released, Carey Goldberg, Dr. Zak Kohane, and I published <em>The AI Revolution in Medicine </em>to help educate the world of healthcare and medical research about the transformative impact this new generative AI technology could have. But because we wrote the book when GPT-4 was still a secret, we had to speculate. Now, two years later, what did we get right, and what did we get wrong? </p>



<p>In this series, we’ll talk to clinicians, patients, hospital administrators, and others to understand the reality of AI in the field and where we go from here.</p>



				</span>
				<span id="show-more-show-less-toggle-3" class="show-more-show-less-toggleable-content">
					



<p>[THEME MUSIC FADES]&nbsp;</p>



<p>The book passage I read at the top is from Chapter 7, “The Ultimate Paperwork Shredder.” </p>



<p>Public health officials and healthcare system leaders influence the well-being and health of people at the population level. They help shape people’s perceptions and responses to public health emergencies, as well as to chronic disease. They help determine the type, quality, and availability of treatment. All this is critical for maintaining good public health, as well as aligning better health and financial outcomes. That, of course, is the main goal of the concept of value-based care. AI can definitely have significant ramifications for achieving this.&nbsp;</p>



<p>Joining us today to talk about how leaders in public health and healthcare systems are thinking about and acting on this new generation of AI is Dr. Umair Shah and Dr. Gianrico Farrugia.&nbsp;</p>



<p>Dr. Umair Shah is a nationally recognized health leader and innovator. He led one of America’s top-rated pandemic responses as Washington State’s secretary of health, a position he held from 2020 to 2025. Umair previously directed Harris County Public Health in Texas, overseeing large-scale emergency response for the nation’s third-largest county, while building an emergency-care career spanning 20-plus years. He now advises organizations on health innovation and strategy as founder and principal of Rickshaw Health.&nbsp;</p>



<p>Dr. Gianrico Farrugia is the president and CEO of Mayo Clinic, the world&#8217;s top-ranked hospital for seven consecutive years, and a pioneer in technology-forward, platform-based healthcare. Under his leadership, Mayo has built and deployed the Mayo Clinic Platform. The platform enables Mayo and its partners to gain practical insights from a comprehensive repository of longitudinal de-identified clinical data spanning four continents. Gianrico is also a Mayo Clinic physician and professor and an author.&nbsp;</p>



<p>Umair and Gianrico are CEO-level leaders representing some of the best of the worlds of public health, healthcare delivery, medical research, and medical education.&nbsp;</p>



<p>[TRANSITION MUSIC]&nbsp;</p>



<p>Here is my interview with Dr. Umair Shah:&nbsp;</p>



<p><strong>LEE:</strong> Umair, it&#8217;s really great to have you here.&nbsp;</p>



<p><strong>UMAIR SHAH:</strong> Peter, it&#8217;s my pleasure. I&#8217;ve been looking forward to this conversation, and I hope you are well today.&nbsp;</p>



<p><strong>LEE: </strong>[LAUGHS] I am doing extremely well.</p>



<p>So, you know, what I&#8217;d like to do in these conversations is first just to start, a little bit about you.</p>



<p><strong>SHAH:</strong> Sure.&nbsp;</p>



<p><strong>LEE:</strong> You served actually during a really tumultuous time as the secretary of health in the State of Washington. But you recently stepped away from that and you started your own firm, Rickshaw Health. So can we start there? What&#8217;s that all about?&nbsp;</p>



<p><strong>SHAH:</strong> Yeah, no, absolutely. First of all, you know, I would say that the transition from Texas to Washington could not have been more geopolitically different, [LAUGHTER] as you can imagine.</p>



<p><strong>LEE:</strong> Sure.&nbsp;</p>



<p><strong>SHAH:</strong> You know, if you like the red-blue paradigms, you couldn’t be more, you know, red and you couldn’t be more blue, I think.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>SHAH:</strong> But what happened is, back in November this past year, as I saw some of the playout of continuation of this red-blue dynamic, I made the decision to step down. And Jan. 15, I stepped down, as you mentioned, and I spent some time really thinking about what I wanted to do next and was looking at a number of opportunities.&nbsp;</p>



<p>And then a moment in time, there were some things happening in our—my wife and our family&#8217;s personal lives that sort of made me think that I wanted to focus a little bit more on family. And I felt the universe was saying, “Stay still.” [LAUGHTER]&nbsp;</p>



<p>And I launched <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://rickshawhealth.com/" target="_blank" rel="noopener noreferrer">Rickshaw Health<span class="sr-only"> (opens in new tab)</span></a> and the notion that, as you know, Peter, rickshaws are oftentimes known across the globe as these modes of transport that reliably get you through ever-changing streets and traffic patterns and all sorts of ecosystems that are evolving at all times. And they get you to the other side and they get you also with a sense of exhilaration. Like when I took my boys to Karachi, and we were—you know, they jumped in a rickshaw and the, you know, open air [LAUGHTER] and they felt this incredible excitement.&nbsp;</p>



<p>And so Rickshaw Health was speaking to the three wheels of a rickshaw that symbolize the three children that we have and the real notion of how do we bring balance and agility and performance to the forefront and then move in an ever—just like streets—ever-changing healthcare environment that is constantly evolving, and we too must evolve with it. And that&#8217;s what Rickshaw Health is all about, is taking clients to that next level of trying to navigate, especially at this time, a very, very different landscape than even several months ago. So, excited about it.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, absolutely. You know, you made this transition from Texas to the State of Washington. And for people who listen to this podcast and don&#8217;t know, the particular part of Texas where you were—Harris County—is <em>really big</em>, very, very important in that state. That&#8217;s just not, you know, the normal county in Texas.&nbsp;</p>



<p><strong>SHAH:</strong> Yeah. [LAUGHS]&nbsp;</p>



<p><strong>LEE: </strong>It&#8217;s actually … it&#8217;s actually known as quite a forward-looking place, technologically.&nbsp;</p>



<p><strong>SHAH:</strong> That’s right.&nbsp;</p>



<p><strong>LEE: </strong>So what was, you know, the transition like, then, going from, you know, possibly the most, sort of, maybe advanced county in the State of Texas, a large place, to the State of Washington?&nbsp;</p>



<p><strong>SHAH:</strong> Yeah, you know, Harris County is the third-largest county in the US. So it had close to five million. And now it&#8217;s probably … it&#8217;s exceeded the five million people, and a very diverse, very forward-looking, as you mentioned, technologically very, very much looking at what&#8217;s the next horizon, and home to Texas Medical Center [TMC] as well, which is …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH: </strong>… the largest medical center. Of course, it had to be Texas. So it can’t be the largest in the state or the country [LAUGHTER]—the largest in the world, right.&nbsp;</p>



<p>And TMC also had a number of different initiatives related to startups and venture capital and VC. And so they had launched something called TMCX. And that was a real opportunity—and I know you&#8217;re familiar with it—an opportunity to really look at how do you incubate all sorts of different innovations and bringing private sector, public sector as well as healthcare delivery alongside these startups to really look at the landscape.&nbsp;</p>



<p>And so when I left Houston and came to Washington, I realized that obviously, I was in the backyard … I mean, you know, you all at Microsoft Research and the work that you&#8217;re all doing is part of an ecosystem of advanced innovation that&#8217;s occurring in the Pacific Northwest that, you know, when we see all the players that are here, all the, you know, ones that do so many different things, but they&#8217;re doing them with an eye towards technology, advancements, and adoptions, it&#8217;s been quite amazing.&nbsp;</p>



<p>When I made that transition, it was really about, you know, the vaccines and what was happening with, you know, with COVID and fighting the—you know, remember, this was the state that had the first case in the continental United States, had the first outbreak, and the first [lab-confirmed] death. And fast-forward a few years later, we had the fifth-lowest death rate in the US. And that was because we all came together to do so much.</p>



<p><strong>LEE: </strong>Yeah, well maybe that gets us into a question that I ask a lot of our guests, which is, you know, and maybe let&#8217;s, since we&#8217;re on your time as the secretary of health in Washington State, [start] with that job. I ask, how would you explain to your mother what you do every day?&nbsp;</p>



<p><strong>SHAH:</strong> [LAUGHS] I laugh because that&#8217;s been such a fascinating conversation in public health because we have oftentimes been—it&#8217;s been really hard to describe what that is.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> And, you know, there are so many metaphors and, you know, analogies that we&#8217;ve used. I&#8217;ve always wondered why we do not have more television shows or sitcoms or dramas that are about the public health workforce or the work that we do in the field, because you have, you know, all sorts of healthcare delivery ones, right.&nbsp;</p>



<p><strong>LEE: </strong>Yup.&nbsp;</p>



<p><strong>SHAH:</strong> As a practicing physician for 20 years, I realized that people knew what doctors did; they knew what nurses did, right. They intimately touch the healthcare system.&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>SHAH:</strong> They understood, you know, that an ambulance picks you up at your home or somewhere else, transports you … gets you to the emergency department. The emergency department, they do some things to you or within the four walls of that ER, and then you&#8217;re either admitted, sent home, and several days, weeks, whatever later, you get home if you&#8217;re admitted, and you start your, you know, post-hospital stay at home or your rehab or what have you. And that all is known to people.&nbsp;</p>



<p>But when you ask your mother, your grandmother, or your, you know, your uncle, or your brother, your neighbor, your coworker about what is public health, they have a very quizzical look on their face of what that is.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>SHAH:</strong> And so what I&#8217;ve …&nbsp;</p>



<p><strong>LEE:</strong> You know, just one thing I&#8217;ve learned is: it&#8217;s not just all the people you mentioned. Even healthcare professionals sometimes have that quizzical look.&nbsp;</p>



<p><strong>SHAH:</strong> Yeah, good point. That&#8217;s right. Good point. And a lot of it is because we don&#8217;t get exposed to it or trained in it. You know, we think about public health when we&#8217;re in our training. And, you know, I&#8217;m sure you had a very similar piece of this is that, you know, you see it as, oh, that&#8217;s the health department that takes care of, you know, STDs, or it takes care, you know, it does the immunizations, or, you know, maybe they do some water quality, or maybe they do mosquitoes [mosquito control], and things like that. But the reality is, we do <em>all</em> of those things and more.&nbsp;</p>



<p>So my metaphor has been that we are the offensive line of a football team, and the healthcare delivery is the quarterback. So everybody focuses on, you know, from a few years back, everybody knows Tom Brady, right.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. [LAUGHS]&nbsp;</p>



<p><strong>SHAH:</strong> He won the Super Bowls, everybody knows what … but if you asked people who was number 75 on the offensive line of the New England Patriots …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> … or name your favorite football team. And the answer would be: you would not be able to likely answer that question. You would know Tom Brady, the quarterback, and that&#8217;s healthcare delivery, the ER doc or the hospitalist or the nurse or the, you know, the medical assistant, or the people that are doing all the work in the field that are the ones that are more visible, but the invisible workforce of the offensive line, that&#8217;s who we don&#8217;t know. And yet these are the people that are blocking and sweating and doing all things to complement the work and make sure the quarterback is successful.&nbsp;</p>



<p>And here&#8217;s where the metaphor breaks down, that when Tom Brady wins the Super Bowl, we continue to invest in the offensive line because we recognize the value of it and we want the quarterback to be successful the next season. But in public health or in society, we do the exact opposite.&nbsp;</p>



<p>When tuberculosis rates come down, we say, well, you know what? We&#8217;ve solved the problem; we don&#8217;t need it anymore.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> Or you have another, you know, environmental issue that&#8217;s no longer there, you say, “We don&#8217;t need it anymore.” And we <em>disinvest</em> from public health or that offensive line. And then you start to see those rates go back up.&nbsp;</p>



<p>And so my answer to Mom and Grandma and Dad and Grandpa is we are <em>critical</em> to your health because we touch you every single day. And so please invest in us.&nbsp;</p>



<p><strong>LEE: </strong>Yeah. And, you know, I think I&#8217;m going to want to get a little deeper on that in just a few minutes here, because, I think especially during the pandemic, that issue of not understanding the importance of that offensive lineman actually really came to the forefront.&nbsp;</p>



<p>And so I&#8217;d like to get into that. But the, kind of, second, kind of, standard thing I&#8217;ve been probing with people is still just focusing on you and your background is what touchpoints or experiences you&#8217;ve had with AI in the past.&nbsp;</p>



<p>And not everyone has. Like, it maybe isn&#8217;t too surprising that doctors and healthcare developers, tech developers, have lots of contact with AI, but would the top dog, you know, at a public health agency ever have had significant contact with AI? What about you?&nbsp;</p>



<p><strong>SHAH:</strong> You know, it&#8217;s interesting. Several years ago, I was in the audience with the [then] FEMA director, [Rich Serino], who just did such an incredible job. And I remember he made this comment at that time. And, Peter, this may have been like … I don’t know—I&#8217;m dating myself—10, 15, maybe even 20 years ago, and he said, “Everybody in the audience, there&#8217;s this, you know, app called Twitter.” And, you know, “How many people in the audience have ever sent a tweet or know about this?” And I don&#8217;t know, maybe—it was a public health audience—maybe about 15% of the people raised their hands.&nbsp;</p>



<p>He said, “I challenge you to right now, pick up your phone, download the app, and go ahead and send a tweet right now.”&nbsp;</p>



<p>And I remember I sent my first tweet at that time. And it was so thought provoking for me was that he was saying you need to be engaged in social media, but the other 85% of the audience had not even done that or had …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> … even understood the importance of social media at that time. Or maybe they understood, but they had restrictions on how to utilize, right.&nbsp;</p>



<p>So that has stayed with me because that&#8217;s very much about this revolution of AI that I know that public health and population health practitioners like myself who have been in the trenches and understand the importance of it, they really believe in the importance or think they know the importance.&nbsp;</p>



<p>But NACCHO, the National Association of County and City Health Officials, had done a survey of local health agencies. And about two-thirds, if not three-quarters, of local health agencies reported that they had an AI capacity that was low or lower than ideal.&nbsp;</p>



<p><strong>LEE: </strong>Hmm. Yeah. Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> And that is very much where I come from. When I was in public sector and at the state health agency, our transformation was very much about how do we advance the work, and how do we utilize this in a population health standpoint?&nbsp;</p>



<p>And I was fortunate to have a chief of innovation at Washington State Department of Health, Les Becker, who understood the value of AI. And as you know, we did also hold a AI science convening that …&nbsp;</p>



<p><strong>LEE: </strong>Yep. Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> … your team was there with University of Washington. And that was really an opportunity for us to say that AI is here. It&#8217;s not tomorrow. It&#8217;s not next year. It&#8217;s not the future. It&#8217;s already here. We need to embrace it.&nbsp;</p>



<p>But here&#8217;s the problem, Peter, far too few people in our field understand just how to embrace it.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> So I have become a markedly more champion of AI. One, since I read your book. So I think there’s that. So thank you for writing it. But two, since I really recognize that when I became a solo or a primary-few practitioner in my own realm, I needed to force-amplify the work that I was doing.&nbsp;</p>



<p>And when I look back, and I continue to stay in touch with my colleagues in the field of public health, what they&#8217;re also struggling with is that you have an epidemiologist who&#8217;s got a mound of information—data, statistics, etc.—that they are going through, and they&#8217;re doing everything in their power to get that processed and analyzed.&nbsp;</p>



<p><strong>LEE: </strong>Yep. Yep.&nbsp;</p>



<p><strong>SHAH:</strong> AI can take 80% of that and do it. And that epidemiologist can now turn to more of an overseer and a gatekeeper and to really recognize the patterns …&nbsp;</p>



<p><strong>LEE: </strong>Yep.&nbsp;</p>



<p><strong>SHAH:</strong> … and let AI be able to do the, you know, grunt work. And similarly, as you know, measles—with the outbreaks that we&#8217;ve seen, especially in Texas but elsewhere—you&#8217;ve got an opportunity where our communications people who are saying, “Look, we&#8217;re about to have, or we know we&#8217;re about to announce that there&#8217;s a measles outbreak in, you know, in our community or our state or what have you—our region.”&nbsp;</p>



<p>And they can have AI go through different press briefings and/or press releases and say, “Give me the state of the art on how I should communicate this message to the community.”&nbsp;</p>



<p><strong>LEE: </strong>Hmm.&nbsp;</p>



<p><strong>SHAH:</strong> And bam! You can do that. And now you can oversee that work, as well. And then the third example is that we are always looking at how do we find ways to have a deeper connection with those who come to our, you know, our websites or come to our engagement tools—with bots and things like that. AI can really accelerate that work, as well. So there&#8217;s so many use cases that AI has for population health or public health.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> But I think the challenge is that we just don&#8217;t have enough adoption because they&#8217;re … one, we&#8217;ve had funding cuts, but two is that there is this real hesitation on, what is it that we can do? And I argue—the last thing I&#8217;ll say about this, Peter—is that I argue that AI is happening right now. The discussions, the technology advancements, the work, the policy work, all that&#8217;s happening right now. If public health practitioners are not at the table, if they&#8217;re not part of the, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>SHAH:</strong> … &#8220;What does this look like? How does it work in our field?&#8221; &#8230; guess what? It&#8217;s going to be done <em>to</em> us and <em>for</em> us rather than with us. And if we do not get with that and get to the table, then unfortunately it may not be exactly what we want it to be at the end of the day.&nbsp;</p>



<p><strong>LEE:</strong> I find it really interesting that you are using the terms “public health” and “population health” …&nbsp;</p>



<p><strong>SHAH:</strong> Yeah.&nbsp;</p>



<p><strong>LEE: </strong>… pretty much interchangeably here. And I think that that&#8217;s something that I think touches on an assumption that was both implicit and explicit in the book that we wrote, which is: we were making some predictions that our ability to extract insights and knowledge from population health data would be enhanced through the use of AI. And I think that it looks to me like that has been more challenging and has come along more slowly over the past two years. But what is your view?&nbsp;</p>



<p><strong>SHAH:</strong> Yeah, I think part of, and I think you and I have had this conversation, you know, in bits and pieces. I think one of the real challenges is that when even tech companies, and you can name all of them, when they look at what they&#8217;re doing in the AI space, they gravitate towards healthcare delivery.&nbsp;</p>



<p><strong>LEE: </strong>Yes.&nbsp;</p>



<p><strong>SHAH:</strong> Right? That&#8217;s, it&#8217;s …&nbsp;</p>



<p><strong>LEE: </strong>And in fact, it&#8217;s not even delivery. I think techies—I did this, too—tend to gravitate specifically to diagnosis.&nbsp;</p>



<p><strong>SHAH:</strong> Yes, that&#8217;s right. That&#8217;s right. You know, I think that&#8217;s a really good point. And, you know, when you look at sepsis or you look at pneumonia or try to figure out ways that, you know, radiologists or x-rays or CT scans can be read, it&#8217;s, I mean, there are so many use cases that are within the healthcare sector. And I think that gets back to this inequity that we have when we look at population health or, you know, this broad, um, swath of land that is, oftentimes, left behind or unexplored, and you have healthcare delivery. Now, healthcare delivery we know gets 95 cents or 96 cents of every dollar. So it makes sense why, right. But we also know that, at the end of the day, we&#8217;re looking at value-based outcomes, and you cannot be successful in the healthcare delivery system unless we are truly looking at prevention and what&#8217;s happening in the community and the population.&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>SHAH: </strong>And that&#8217;s why I use it interchangeably, but I know that “public health” has got a very specific term, and “population health” is a different set of ways of looking at the world. The reason that people try to shy away from pop health in essence is that you could talk about population health as being my population of patients in a clinic. It could be my health systems population. It could be an insurance company saying, these are the lives covered, right. So it becomes, what is population? When we think of public health, we think of the entirety of the population, right. In the State of Washington, eight million people. Harris County, five million people. Or in the US, 300—whatever the number of millions of people that—we think of the entire population. And what is it that actually impacts the health and well-being of that population is really what that&#8217;s about.&nbsp;</p>



<p>Yet here&#8217;s the challenge. When we then talk to those of our partners and our colleagues in the tech field, there are two things happening. One is, there&#8217;s a motivation because of the amount of dollars that are in [the] healthcare sector. And number two is, because it&#8217;s more familiar, right.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> And so there are very few practitioners similar to me that are out there, that are in the pop health who kind of know healthcare delivery because they&#8217;ve also seen patients, but they&#8217;re also—they worked at that federal, state, local level, community level—they&#8217;ve, you know, they&#8217;ve done you know various different kinds of environments.&nbsp;</p>



<p>And they say, “Look, I&#8217;ve got a perspective to really help a tech company or somebody see the rest of it,” but you have to have both partners coming together to see that. And I think that&#8217;s one of the real challenges that we have.&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p>And so now I&#8217;m going to want to go into specific problems, …&nbsp;</p>



<p><strong>SHAH:</strong> Yeah. Sure.&nbsp;</p>



<p><strong>LEE: </strong>… and maybe COVID is a good thing to focus on—the breadth of problems that had to get solved in pandemic response and where the gaps between healthcare delivery and public health were really exposed.&nbsp;</p>



<p>And so the first problem that I remember really keenly that just seemed so vexing was understanding where the PPE was, the <em>personal protective equipment</em> &#8230;&nbsp;</p>



<p><strong>SHAH:</strong> Hmm. Yeah.&nbsp;</p>



<p><strong>LEE: </strong>…<strong> </strong>and where it needed to be.&nbsp;</p>



<p><strong>SHAH:</strong> Yes.&nbsp;</p>



<p><strong>LEE:</strong> And so that turned out … you would think just getting masks and gowns and gloves to the right places at the right times or even understanding where they are so that, you know … and being able to predict, you know, what hospitals, what clinics are most likely to get a big influx of patients during the height of the pandemic would be something that would be straightforward to solve, but that turned out to be an extremely difficult problem.&nbsp;</p>



<p>But how did it look from where you were sitting? Because you were sitting at the helm having to deal with these problems.&nbsp;</p>



<p><strong>SHAH:</strong> Yeah, we were constantly chasing data and information. And oftentimes, you know, because a lot of these data systems in the public health sector have been underinvested in over the decades, then, you know, you had our biggest emergency crisis of our time, and a lot of public health agencies were either getting, you know, thrown a whole host of resources or had to create things on the fly.&nbsp;</p>



<p>And whether that was at Harris County or in the State of Washington, I will tell you that what I saw was that, you know, a lot of agencies across the country were still using fax machines, you know, to get data that were coming in.&nbsp;</p>



<p>And I remember actually—it&#8217;s kind of a funny story—there was a fax machine that was highlighted down in our agency in Texas. And we actually had this fax machine, had mounds of, you know, data … sorry, <em>papers</em> that were next to … <em>faxes</em> that were coming in and all these things.&nbsp;</p>



<p>And you would have, you know, <em>Mr. Peter Lee</em> listed as a patient. And then the next, you know, transmission would have <em>Pete Lee</em>. And then the next transmission would have <em>Peter Lee</em>, but instead of L-E-E, it was L-E-A-H or something, or L-I or something, right. And it was just …&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> &#8230; or you had a date of birth missing, or you had, you know, an address that was off. And what we realized is that over time, a lot of the data that were coming in were just incomplete data, and being able to chase that was really hard.&nbsp;</p>



<p>And so, you know, I think AI has that potential to really organize it, and to stratify it, and to especially get you to a point of at least cleaning it up. So I don&#8217;t think it&#8217;s just that AI … AI doesn&#8217;t just save <em>time</em>; it saves <em>lives</em>. Truly used …&nbsp;</p>



<p><strong>LEE: </strong>Hmm. Yeah.&nbsp;</p>



<p><strong>SHAH: </strong>… that&#8217;s, I think, where we&#8217;re talking here.&nbsp;</p>



<p>And so when you have PPE and things of that nature, as you talked about, here in the State of Washington or what we were trying to do to get vaccines out or everything we&#8217;re doing to try to get communication messages to the public. And we did a fantastic job of that, although not ideal.&nbsp;</p>



<p>I mean, there are so many things that I could point to that we could have done better—all of us in the field of public health and healthcare delivery alike.&nbsp;</p>



<p>I will tell you that the one thing that stays with me is that if we had those tools <em>then</em>, and we had them in place <em>then</em>, and we had invested in them at that time in advance of, I think there was a real opportunity for us to be able to move ahead and even be better at how we affected the health outcomes of the very populations that we were trying to get to.&nbsp;</p>



<p>And I think it&#8217;s [that] AI allows us to shift from reactive to proactive systems, catching health issues before they escalate and allow us to really communicate with empathy <em>at scale</em>.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> And when we can do those things, whether it&#8217;s opioids or whether it&#8217;s, you know, something that&#8217;s happening related to an infectious disease, or, you know, even this, the new agenda with <em>Make America Healthy Again</em>—which by the way, as you know, we had a <em>Be Well, WA</em> &#8230; <em>Be Well,</em> <em>Washington</em> …&nbsp;</p>



<p><strong>LEE: </strong>Right. Yes.&nbsp;</p>



<p><strong>SHAH:</strong> … very much that was about, you know, looking at, you know, physical health and nutritional health and emotional well-being and social connectedness—that there is a real opportunity for us to address the very drivers of ill health. And when we can do that, and AI can help us accelerate that, I think we truly have the ability to drive down costs and increase the value that&#8217;s returned to all of us.&nbsp;</p>



<p><strong>LEE: </strong>What is your assessment of public health agencies’ readiness to use technology like AI? Because if there&#8217;s one thing AI is good at, it&#8217;s predicting things. Are they [public health agencies] in a better position to predict things now?&nbsp;</p>



<p><strong>SHAH:</strong> You know, I think it&#8217;s a tale of two cities.&nbsp;</p>



<p>I think on the one hand, we&#8217;re better because we have the tools. On the other hand, we&#8217;ve lost the capacity to be able to utilize those tools. So, you know, it&#8217;s a plus and a minus.&nbsp;</p>



<p>Many, many years ago, there was the buzzword of what we called <em>syndromic surveillance</em>. And, Peter, you know this term well.&nbsp;</p>



<p>It was like you would have, you know, a whole host of accumulation of data points in, let&#8217;s say, a hospital setting or an emergency department …&nbsp;</p>



<p><strong>LEE: </strong>Yup. Yup.&nbsp;</p>



<p><strong>SHAH: </strong>… where, you know, you’d have runny nose, you&#8217;d have cough, you&#8217;d have a fever, and you would take that, what was happening and people presenting to the emergency department, with what was happening in the area pharmacies where people were going to get Kleenexes and tissues …&nbsp;</p>



<p><strong>LEE: </strong>Yep.&nbsp;</p>



<p><strong>SHAH:</strong> … and buying over-the-counter, you know, medication, and things of that nature, Tylenol, etc.&nbsp;</p>



<p>And you would say … you would put those two things together, and you would come up with a quote-unquote “syndrome,” and you would say our ability to say there was an alert to that syndrome allows us to say something<em> uh-oh</em> is going on in the community, and we got many, many advancements related to wastewater surveillance over the last several years as you know …&nbsp;</p>



<p><strong>LEE:</strong> Yep. Yep. Well, also, wasn&#8217;t patient number one in the United States discovered also because of the Seattle Flu Study, or at least that sort of syndromic surveillance.&nbsp;</p>



<p><strong>SHAH:</strong> That&#8217;s right.&nbsp;</p>



<p><strong>LEE: </strong>They weren&#8217;t even looking for COVID. They were just taking, you know, snot samples from people.&nbsp;</p>



<p><strong>SHAH:</strong> That&#8217;s right. That&#8217;s right. That&#8217;s right.&nbsp;</p>



<p>And so that&#8217;s the kind of thing that you, you know, we underappreciate. Is you have to have a smart, intelligent, agile practitioner, right.&nbsp;</p>



<p>So if I think about down in Dallas when Ebola was, you know, the gentleman who was, you know, the index case for Ebola was sent out of the emergency department and came back several days later.&nbsp;</p>



<p>And it was the nurse who picked up this time because the practitioner, the provider, the healthcare provider, the doc missed it. And I wouldn&#8217;t want to say in a negative way. It was just, like, not obvious. You aren&#8217;t thinking of Ebola in the middle of Texas. And it was the nurse who picked up: <em>there&#8217;s something wrong here</em>.&nbsp;</p>



<p>And what AI has the ability to do is to pick up those symptoms &#8230;&nbsp;</p>



<p><strong>LEE: </strong>Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> … or those patterns and be able to recognize the importance of those and be able to then alert the practitioner. So what I … we call it <em>artificial intelligence</em>—it almost becomes artificial wisdom.&nbsp;</p>



<p><strong>LEE: </strong>Hmm. Yeah, interesting. So that actually reminds me of my next question, which is another thing that I watched you and public health officials do is try to play “what if” games.&nbsp;</p>



<p>So, for example, I think one decision you were involved in had to do with, you know, what would be the impact if we put a ban on large gatherings like concerts or movie theaters or imposed an 8 PM curfew on restaurants, and you were trying to play “what if” games. Like, what would be the impact on the spread of the pandemic there?&nbsp;</p>



<p>So now, again, today with AI, would that aspect of what you did play out differently than it did during the pandemic?&nbsp;</p>



<p><strong>SHAH:</strong> As you know, COVID was the most studied condition on the planet at one point. And it was, you know, things that usually we would learn over years or months, we were learning in weeks or days or hours.&nbsp;</p>



<p>And I remember in Houston, I would say something in the morning, and I would always try to give the caveat, “This is the best information we know right now,” because it kept changing, whether it was around masks or whether it was around, you know, the way the virus was operating, whether it was around &#8230;&nbsp;</p>



<p>I remember even … I was just watching something recently where I was asked to comment about whether spiders could transmit COVID-19. You know, just questions that were just evolving, evolving, evolving. And the information was evolving. By morning, you would say something. By evening, it would change.&nbsp;</p>



<p>And why I say that is that it would have been great in the pandemic if we could have said, if you could give us all the information that&#8217;s happening across the globe, synthesize that information, and be able to help us forecast the right decisions that we should be making and help us model that information so we could decide: if you did a curfew, or if you did, you know, a mask, or if you could, you know, change something else related to policy—what are the impacts of it?&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>SHAH: </strong>What we found constantly in public health was that we were weighing decisions in incomplete data, incomplete information.&nbsp;</p>



<p>So great now that everybody can armchair quarterback looking back three, five years ago and say, “I would have done it this way,” or “I would have done it that way.” Gosh, I would have as well. But guess what—we didn&#8217;t have that information at that time. And so you had to make the best decisions you could with incomplete data.&nbsp;</p>



<p>But what AI has the potential to do is to help <em>complete</em> the incomplete data. Now, it&#8217;s not going to get 100%.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> And I think, Peter, you know, the one thing we&#8217;ve got to be really mindful [of] is phantom information, or information where it sort of makes up things, or may somehow get you incomplete information, or skews it a certain way.&nbsp;</p>



<p>This is why we can&#8217;t take the person out of it yet.&nbsp;</p>



<p><strong>LEE: </strong>Right.&nbsp;</p>



<p><strong>SHAH:</strong> Now, maybe one day we can.&nbsp;</p>



<p>I&#8217;m not one of those Pollyanna-ish that people will never be replaced. I actually believe that those people who are skilled with AI and the tools will eventually have a competitive advantage over those who are not. Just like if I had a physician who knows how to use their smartphone or knows how to use a word processor or knows how to do a PowerPoint presentation is going to replace the ones that use scantrons &#8230;&nbsp;</p>



<p><strong>LEE: </strong>Yeah. Yeah.&nbsp;</p>



<p><strong>SHAH:</strong> … or the ones that write it on pieces of paper—that eventually it makes it more efficient and effective, but we&#8217;re not there yet. But I think that the potential is <em>absolutely</em> there.&nbsp;</p>



<p><strong>LEE: </strong>So I have one more question. And you can, kind of, tell I&#8217;m trying to expand people&#8217;s understanding of just the incredible breadth of what goes on in public health, you know, all of these sorts of different issues.&nbsp;</p>



<p>And again, just sticking to COVID, but this is a much broader issue. Another thing you had to cope with were significant rise of misinformation …&nbsp;</p>



<p><strong>SHAH:</strong> Yes.&nbsp;</p>



<p><strong>LEE: </strong>… and maybe going along with that, very, very significant inequities in outcomes in the COVID response. And when you think about AI there, I think you can argue it both ways, that it both exacerbates the problem but also gives you new tools to mitigate the problems.&nbsp;</p>



<p>What is your view?&nbsp;</p>



<p><strong>SHAH:</strong> I think you … I don&#8217;t even have to say it … I think you hit on it, is that, you know, it really is two sides of one coin.&nbsp;</p>



<p>On the one hand, it has the power of really advancing and allowing us to move forward in a way that incredibly accelerates and accentuates, but on the other hand, in the case of inequities, right? So if you have inequitable information data that&#8217;s already out in the literature or already out in the, you know, media, or what have you, about a certain population or people or certain kinds of ideas or thoughts, etc., then AI will tend to accumulate that. You&#8217;re going to take that information, thinking that&#8217;s the best out there, but it may have missed out on information and now you go with it. And that&#8217;s a potential problem.&nbsp;</p>



<p>And I think it&#8217;s the same thing on information is that when we have people that are able to classify or misclassify information, I think it really becomes hard because it can accelerate the inequities of trust or inequities of trusted sources of information. It can also close the gap.&nbsp;</p>



<p>So I think, you know, it&#8217;s really up to us and this responsible AI to really think about how we can go about doing this in a way that&#8217;s going to allow us to further the advancements but also be careful of those, you know, those kind of places where we&#8217;re going to step into that are not going to be well received or successful.&nbsp;</p>



<p>You know, the one thing that&#8217;s really fascinating about this whole conversation is that this is why we&#8217;ve got to be at the table, Peter.&nbsp;</p>



<p><strong>LEE: </strong>Yep. Yep.&nbsp;</p>



<p><strong>SHAH:</strong> Because if we&#8217;re not at the table, you know, what&#8217;s the, you know, or if tech companies that are out there doing this work and aren&#8217;t even seeing a field of practitioners that are actually wrestling with the same problems but just cannot actually get to the solutions, we&#8217;re just going to continue to accentuate the problems.&nbsp;</p>



<p>And that&#8217;s why I&#8217;m a firm proponent of: we&#8217;ve got to be at the table.&nbsp;</p>



<p>And so even when we&#8217;ve seen in, and this is going to be a little controversial, but governmental spaces where, you know, policymakers have said, “Look, we are not going to let you do certain things,” or they say to public health practitioners or even healthcare delivery practitioners in certain spaces, “You cannot even play with this. You cannot have it on your phones. You can&#8217;t do any &#8230; ”&nbsp;</p>



<p>You know, what I really believe it does is that it takes [an] almost like we put our head in the sand type of approach rather than saying, “What is it that we can do to help improve AI and make it work for all of us?” What we&#8217;re doing is we&#8217;re essentially saying, “We&#8217;re going to let the tech companies and all the other developers come up with the solutions, but it&#8217;s not going to be informed by the people in the field.” And that&#8217;s dangerous. We have to do both. We have to be working together.&nbsp;</p>



<p><strong>LEE: </strong>Umair, that&#8217;s really so well said, and I think a great way to wrap things up. I&#8217;ve certainly learned a lot from this conversation. So thank you again.&nbsp;</p>



<p><strong>SHAH:</strong> It&#8217;s been a pleasure to be with you this morning. Thank you so much for the time. And I&#8217;m looking forward to further conversations.&nbsp;</p>



<p>[TRANSITION MUSIC] </p>



<p>I live in the State of Washington and because of that, I&#8217;ve been able to watch Umair in action as our state&#8217;s former secretary of health. And some of that action was pretty intense to say the least because his tenure as secretary of health spanned the period of the COVID pandemic.&nbsp;</p>



<p>Now, as a dyed-in-the-wool techie, I have to admit that at the beginning, I don&#8217;t think I really understood the scope and importance of the field of public health. But as the conversation with Umair showed, it&#8217;s really important and it is arguably both an underfunded and underappreciated part of our healthcare system.&nbsp;</p>



<p>Now, public health is also very much an area that&#8217;s ripe for advancement and transformation through AI. As Umair explained in our discussion, the core of public health is the idea of population health, the idea of extracting new health insights from signals from population-scale data. And already we&#8217;re starting to see AI making a difference.&nbsp;</p>



<p>Now here&#8217;s my interview with Dr. Gianrico Farrugia.&nbsp;</p>



<p><strong>LEE:</strong> Gianrico, it&#8217;s really great to have you here today.&nbsp;</p>



<p><strong>GIANRICO FARRUGIA:</strong> Peter, thanks for having me. Thanks for making me part of your podcast.&nbsp;</p>



<p><strong>LEE:</strong> You know, what I&#8217;d like to do in these conversations is, you know, we&#8217;ll definitely want to talk about the overall healthcare system, the state of healthcare, and what AI could or might do to help or even hurt all of that. But I always like to start with a sharper focus just on you specifically. And my first question always is, you know, I think people imagine what a hospital or a health system president and CEO does, but not really. And so how would you explain to your mother what you do every day?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So, Peter, my mother’s 88 years old. She lives in Malta, and she’s visiting at the moment, …&nbsp;</p>



<p><strong>LEE:</strong> Oh, wow.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … which is kind of nice, really.&nbsp;</p>



<p><strong>LEE: </strong>Wow, that is amazing.&nbsp;</p>



<p><strong>FARRUGIA: </strong>I&#8217;m proud that she&#8217;s still proud of me. So she does ask. I&#8217;ll tell her the scope of Mayo Clinic. We serve patients across the globe. We have about 83,000 staff members that work with us, and we&#8217;re very proud of the work we do in research, education, and the practice.&nbsp;</p>



<p>Mayo Clinic is built to serve people with serious disease. So what I tell my mother is that here we are. We&#8217;re a healthcare organization that knows what it needs to do: keep patients as the North Star. The needs of the patient come first. We have 83,000 people who want to do that, several thousand physicians and scientists. My job is to look slightly ahead and then share what I&#8217;m seeing and then, sort of, smooth the way for others to make sure Mayo remains true to its mission but also true to the fact that at the moment, we are in a category of one. We need to remain there not just from an ego standpoint, but really from a “do good to the world” standpoint.&nbsp;</p>



<p>At that point, invariably my mother will tell me that I&#8217;m working too hard. [LAUGHTER] And then of course, I change the subject, and I ask her what she cooked today because my mother, who’s 88, cooks for the whole family in Malta, and there are usually four generations eating around the table. So I tell her what she does for the family is what I do for the Mayo family.&nbsp;</p>



<p><strong>LEE:</strong> Wow, that&#8217;s a great way to put it. And it sounds like you actually have a good chance to have some good genes if she&#8217;s still that active at age 88.&nbsp;</p>



<p><strong>FARRUGIA:</strong> I think I chose a little more stressful job that may limit [that]. I will tell you very briefly is that one of the AI algorithms we have estimates biological age from an electrocardiogram. My biological age jumped by 3.7 years when I became CEO.&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] Oh no.&nbsp;</p>



<p><strong>FARRUGIA:</strong> I&#8217;m hoping it will reverse on the other side.&nbsp;</p>



<p><strong>LEE: </strong>To stick with you just for one more moment here, second question I ask is about your origin story with respect to AI. And typically, for most people, there is AI before ChatGPT and generative AI and then after the generative AI revolution. So can you share a little bit about this? Because it must be the case that you&#8217;ve been thinking about this a long time since you&#8217;ve really led Mayo Clinic to be so tech forward in this way.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Well, I&#8217;ve been, as you said, a physician for way too long. I got my MD degree in ’87. So that sort of dates me. But it also means that I saw a lot of the promise for AI that never seemed to pan out for decades and decades and decades like you did.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Around 10 years ago, Mayo could sense that there was something different, that something was changing, that we actually—at that time, predictive AI—could make a big difference. And I think that&#8217;s the moment where I and others jumped in and said Mayo Clinic needs to be involved.&nbsp;</p>



<p>And then about six years ago, when—six and a half years ago—I became CEO, it was clear that there was the right confluence of data, knowledge, tech expertise, that we could deal with what was increasingly bothering me, which is that we knew what was coming from a technology standpoint and we knew the current healthcare system could not deliver on what patients need and want within that current system. And so the answer is, how could a place like Mayo Clinic with our reputation not jump in and say there has to be a better way of doing things? I&#8217;ve always said that it is impossible for me to understand that every single government employee is incompetent. Every physician is greedy. Something&#8217;s wrong here.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> And that wrong was the architecture was wrong. And we knew that we could incorporate AI and make it better. So for me, that journey was one of <em>wait</em>, <em>wait</em>, <em>wait</em>. 10 years ago, begin to jump in. Six years ago, really jump in with our platform. And then, of course, in November 2022, things changed again.&nbsp;</p>



<p><strong>LEE:</strong> Yeah. When did this idea of a data platform, what you now call the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.mayoclinicplatform.org/" target="_blank" rel="noopener noreferrer">Mayo Clinic Platform<span class="sr-only"> (opens in new tab)</span></a>—by the way, I refer to this as <em>MCP</em>, …&nbsp;</p>



<p><strong>FARRUGIA:</strong> Yeah, I know. [LAUGHS]&nbsp;</p>



<p><strong>LEE:</strong> [LAUGHS] … which I always smirk a little bit because, of course, for those of us in computer science research, the AI research, <em>MCP</em> has also become quite a hot topic because of the model context protocol version of this. But for Mayo&#8217;s MCP, when did that become a serious, defined initiative?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So around the end of ’18, 2018, beginning of 2019. At that point, we knew that we were going to do something differently. We came up with a strategic plan, as I took on the job, that we needed to cure more patients. There’s just not enough cures in the world. There&#8217;s too much suffering. And that we had all these chronic diseases that people have accepted are chronic, but really the only reason that disease is chronic is you haven&#8217;t cured it.&nbsp;</p>



<p>And physicians have been afraid to talk about cure because, of course, eventually everybody passes away.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> But I really pushed hard to say, no, it&#8217;s OK to talk about cure. It&#8217;s OK to aspire to cure. The second was connect—connecting people with data to create new knowledge. And that&#8217;s where it became clear that data were not currently in a format that were particularly useful. By the way, you&#8217;ll hear me talk about <em>data</em> in the singular and the plural. I&#8217;m old school. I talk about <em>data</em> as plural, but I know that most younger people now use <em>data</em> singular. [LAUGHTER] And I apologize if I&#8217;ll go through that.&nbsp;</p>



<p>And then the third was transform. Let&#8217;s use Mayo&#8217;s resources to transform healthcare for ourselves and for others. And that&#8217;s the concept of, if we are able to use data in a different way, let&#8217;s create a different architecture. And that architecture had to be very closely linked to using artificial intelligence in order to create better outcomes for patients. So patients can live not only longer lives but healthier lives. And that&#8217;s the genesis of MCP, <em>Mayo Clinic Platform</em>, so I&#8217;ll timestamp that as end of 2018, beginning of 2019.&nbsp;</p>



<p><strong>LEE:</strong> So I&#8217;m really wanting to delve in in this episode, in this conversation, you know, [into the] mindset of a health system or hospital CEO. And so you&#8217;re obviously thinking about, I guess, machine learning and predictive analytics and so on. What were the, kind of, like … in 2018, what were the outcomes that you were dreaming about from this? So if you had this thing, you know, what were the things that you were hoping to be able to show or, kind of, produce as results?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So first of all, I think all of us who work at Mayo Clinic, and this tends to be a bit sugary, but it&#8217;s true, strongly feel that we have a responsibility to leave the place better than when we started. And so the Mayo brothers, when they started, did two really important things. The first was that they created the first integrated healthcare system. And the second, they created the first unified record. And that record was, of course, paper at that point.&nbsp;</p>



<p>Part of that is to say, OK, what does it look like now versus how can we improve what we have if … it&#8217;d be blasphemy to say, let&#8217;s think of ourselves as the Mayo brothers, but let&#8217;s think of ourselves as reasonably smart people at Mayo Clinic, really lucky to be surrounded by very smart people with resources. What will we do? And so we said let&#8217;s not aim for the low-hanging fruit. Let&#8217;s aim to get at whatever you want to call it, the intractable knot, the hardest problem, and that is clinical care. Let&#8217;s improve clinical care. Yes, we can deal with burnout. Yes, we can deal with administrative burden. But let&#8217;s not focus on that. Let&#8217;s really create an architecture that allows us to tackle better clinical outcomes.&nbsp;</p>



<p>And by starting there, then everything flows from that. That it&#8217;s not really worth doing unless at the end of the day, people are experiencing better health.&nbsp;</p>



<p><strong>LEE:</strong> And so I know a very good colleague and friend of mine, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.mayoclinicplatform.org/our-team/john-d-halamka-md-ms/" target="_blank" rel="noopener noreferrer">John Halamka<span class="sr-only"> (opens in new tab)</span></a>, you ended up hiring. I thought he was a very interesting choice because he is, of course, in terms of technology, quite deep and very expert, but he&#8217;s, I think, first and foremost, a doctor. And so I assume you must have had to decide what type of person you would bring in and what kinds of people you would bring in to try to create such a thing. What was your thinking around the choice of someone like John?&nbsp;</p>



<p><strong>FARRUGIA:</strong> It was one of the harder decisions. First of all, [I&#8217;m] a physician myself. We tend to want to maintain some control. And so now I am the CEO, [LAUGHTER] and I have to give this baby to somebody else. That&#8217;s very hard. Second is Mayo Clinic is really good because it is flat, and we run a lot by committee. But it also means that, therefore, you have to work really hard at change, and you cannot change by fiat. You have to change by convincing people.&nbsp;</p>



<p>So I just … I&#8217;ve always made the point that the right change agent is a servant leader because that&#8217;s how change becomes embedded. But it also means you&#8217;ve got to have that personality, the Mayo personality. And it became clear when we interviewed [that] there were some people that were really hardcore tech; others that were passionate about social issues. But John really fit that of being, as you said, deep in IT but also himself very aligned with the Mayo Clinic values. It&#8217;s as if he was a Mayo Clinic physician even though he wasn&#8217;t.&nbsp;</p>



<p>And that came together, and I felt, <em>we</em> felt, that as we were hiring, that we could do it. And then we did something interesting. We paired John with a … we created the role of a chief medical officer for the platform, which was a longstanding Mayo Clinic physician. And so we brought them together so we could get the past and the present and the future working together.&nbsp;</p>



<p><strong>LEE:</strong> So I&#8217;m going to ask you about what has come out of this. But before that, let&#8217;s get back to this origin story. So now, all of that is being set up starting around 2018. But then, you know, in 2022, there is generative AI. Now you were already experimenting with transformers, starting with BERT out of Google there. So maybe that&#8217;s a couple of years earlier. But still, there has to come a point where things are feeling very disrupted.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Yeah, so, you know, it really wasn&#8217;t. It, to me, was a relief because it gave this … we were feeling pretty good about what we&#8217;re doing. We were feeling a little impatient, but, in true Mayo fashion, were willing to, sort of, do everything, take its time, take it to the right committees, get the right approvals, and get it done.&nbsp;</p>



<p>And so when generative AI came, for us, it&#8217;s like, I wouldn&#8217;t say we told you so, but it&#8217;s like, ah, there you go. Here&#8217;s another tool. This is what we&#8217;ve been talking about. Now we can do it even better. Now we can move even faster. Now we can do more for our patients. It truly never was disruptive. It truly immediately became enabling, which is strange, right, …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … because something as disruptive as that instantly became enabling at Mayo Clinic. And I&#8217;ll take … as I think about it with you and take a moment to think and reflect on it, I think there were a couple of decisions we made earlier on that really helped us. We made the decision <em>against</em> the advice of any consulting firm to completely decentralize AI at Mayo Clinic six years ago. And we told our clinical department, you need to own this. You need to hire basic scientists in AI. We&#8217;ll help you by creating the infrastructure. We&#8217;ll help you by doing all the rest. We&#8217;ll have the compute. We&#8217;ll have the partners. You need to do this on your own. You need to treat this the same way as if a new radiological technique happened or a new surgical technique happened.&nbsp;</p>



<p>And so there was a lot of expertise already present in a very diffused way that then we were able to layer on generative AI onto that. And we found a very willingness to embrace it. In fact, I would argue initially a bit too willing because as you know, we haven&#8217;t quite figured out what&#8217;s legitimate use, what&#8217;s not use.&nbsp;We all learned together.</p>



<p><strong>LEE:</strong> Right. Yep. Yep.&nbsp;</p>



<p><strong>FARRUGIA:</strong> But it was mostly energy, which is really interesting. It was mostly energy.</p>



<p><strong>LEE:</strong> Wow. And, you know, it&#8217;s an amazing thing to hear because one common theme that we hear is that the initial reaction is oftentimes one of skepticism. In fact, I&#8217;ve been very open that even I initially had some skepticism. Was that not present in your mind or on your team&#8217;s mind at all at the beginning?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So you&#8217;re asking a physician if they are skeptical about something. [LAUGHTER] Yeah. I wonder what the answer to that is. Absolutely. The first hallucination, the first wrong reference. Can you imagine if you write the grant and the wrong reference comes. As you know, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … earlier on when some references were being made up. So massive amounts of skepticism. But the energy was such there that the people [who] were skeptical were also at the same time saying, “Let&#8217;s do a RAG [retrieval augmented generation] to clean up those references. Let&#8217;s create …” We were experimenting with discharge summaries, but let&#8217;s use AI to police AI, and let&#8217;s see what&#8217;s going on. So there was more massive skepticism, but the energy was pushing that skepticism into a positive versus into a negative frame. Now, I say that summarizing in hindsight.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Day to day, much more complicated than that. But overall, if you just … and remember, I had been at the World Economic Forum many years ago and had said, healthcare needs to run towards AI.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>FARRUGIA:</strong> If healthcare was perfect, we would wait. Healthcare is not perfect by any means, therefore let&#8217;s run and embrace AI. And, sort of, that mentality was part of who we were because at the same time, we were also saying the other thing, that we need to be the ones to lead validation. We need to be the ones that set the rules. We need to be participating in the creation of <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.chai.org/" target="_blank" rel="noopener noreferrer">CHAI [Coalition for Health AI]<span class="sr-only"> (opens in new tab)</span></a>. We need to be participating as the <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" rel="noopener noreferrer" target="_blank" href="https://nam.edu/">[National] Academy of Medicine<span class="sr-only"> (opens in new tab)</span></a>.&nbsp;</p>



<p>So people did feel that Mayo was being fairly responsible about it, but that urge to, the needs of the patient come first, was the driver that kept people wanting to say, “Not ready yet, but let&#8217;s make it ready.” And we now have 320 algorithms in the practice, and they run and we constantly are looking and seeing what else we can do to improve. But as you well know, things evolve and change. And we&#8217;re also looking and seeing which ones work and which ones don&#8217;t and which ones we have to work together on to make better.&nbsp;</p>



<p><strong>LEE:</strong> Yeah, you know, of course Mayo has such a, you know, such a reputation and is so influential, but in the world of healthcare broadly, let&#8217;s just focus on the United States to start. How common is this experience? You know, so if you are at a meeting with fellow CEOs of hospitals and health systems, what is the attitude and what is the, kind of … how common is the approach to all of this?&nbsp;</p>



<p><strong>FARRUGIA:</strong> I think it&#8217;s more common now, but going back a few years, I think it&#8217;s fair to say that it was scary for people to know how it&#8217;s going to change things. Healthcare runs on very narrow margins. It&#8217;s very expensive. So your expenses and your revenue are both massive, and they are very close to each other. So anything that changes that balance is really scary.&nbsp;</p>



<p>Because it&#8217;s not like you have the opportunity to erode into a margin or get it right the second time. So I think that is what drove a lot of the initial hesitancy. Was, one, is lack of knowledge and, two, understanding that you didn&#8217;t have a lot of room to make a mistake.&nbsp;</p>



<p><strong>LEE:</strong> On the economics of this, when you are embarking on what I suspect is a very expensive initiative like Mayo Clinic Platform, how on earth do you justify that early on?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So again, I&#8217;m trying hard to try and remember how things were versus how I think about them now. [LAUGHTER] It goes back to our history. Mayo has always invested in what it thinks is the right thing that is coming. And that&#8217;s how we&#8217;ve stayed where we are. So the investment really was having an open discussion: is this worth it for our patients? And once that discussion was over, then the board was saying, <em>go</em>, <em>go</em>, <em>go</em>.&nbsp;</p>



<p>Now we are lucky in that we have the size that we&#8217;re able to hire and absorb. We&#8217;re lucky in that the people [who] came before us have been financially astute, and one of our values is stewardship. And we&#8217;re lucky that we had a lot of patients at Mayo Clinic who were able to listen, be inspired by, and be willing to help support. And so that gave us the ability to build what we&#8217;re doing not only into the long-range plan but actually into the yearly plan. And so we built it into the yearly plan. We set up a center for digital health. We set up the platform. And then we set up the budgets to be able to do that. And the budgets came from assets we&#8217;ve had, assets that we would get as the year came by, and then from philanthropy.&nbsp;</p>



<p>We also had a really powerful calling card. And that&#8217;s one advantage I had, and that&#8217;s … and I&#8217;d been very open when I was speaking to other CEOs that would use it is that right at that very beginning, really, really in 2019, our cardiologists, both the researchers and the clinicians, had come together and had used electrocardiograms to create an AI algorithm.&nbsp;</p>



<p>The first one was for diagnosing from an electrocardiogram, which is very cheap, very easy to do, left ventricular dysfunction. That&#8217;s how hard the left part of the heart contracts. If it doesn&#8217;t do well, you get heart failure. And they were able to show that that algorithm was already making nurses better than the physician without the algorithm. And after that went on to show that you could do it from a single strip, really with an area under the curve for that single strip on a watch, that was as good as mammograms or pap smears. And so we already had that proof.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> That quickly then came into Mayo. We put it into it so that any patient now can benefit from it. And now there are, I think, 14 algorithms just from that same one.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> So we had a proof of concept thanks to those really far-seeing cardiologists that enabled things to happen a little faster and also, as I talked to other CEOs, enabled me to say, “This actually works. This is the path forward.” I have recently been vocal about also saying, we are at a point now where I believe that for some medical conditions, it is not right to not use AI to help treat them.&nbsp;</p>



<p><strong>LEE: </strong>Wow, that&#8217;s so interesting. So I think I want to get into another topic here, which is when you think about the use of AI and data, what are some of the results that maybe are top of mind for you or you think are particularly important? And if you don&#8217;t mind, I&#8217;d like to see if we can think about this not only in terms of results in terms of patient outcomes but in your other activities, core activities, like research, in the education mission, and then even in the broader impacts on the healthcare system. But maybe we start with on patient outcomes.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Yeah, they&#8217;re all linked, right.&nbsp;</p>



<p><strong>LEE:</strong> Yes.&nbsp;</p>



<p><strong>FARRUGIA:</strong> They&#8217;re part of the same ecosystem. We think of ourselves as three shields— research, education, and the practice—and that one goes into the other. So, as I said, we have about 320 AI algorithms from the practice. Some run on every patient; some run on some patients. And we have good evidence for what they do. So some specific examples, and then I&#8217;ll get into the transformer part of this.&nbsp;</p>



<p>We have a program called CEDAR [Clinical Detection and Response (tool)], and like most other people, I like acronyms for things. [LAUGHTER] But what it is, in our hospitals with patient consent, we monitor vitals. We monitor in the patient room—not in the ICU [intensive care unit], in the patient room. We monitor all sorts of things. But there&#8217;s a camera in the room, and we have a team of intensivists—nurses and physicians—who do not have any patient responsibilities but are just monitoring the algorithms, and when the algorithms are predicting decompensation, they&#8217;re able to get into the room. And what we&#8217;ve shown, for example, with that algorithm, is we&#8217;ve shown we&#8217;ve decreased length of stay in the hospital, decreased transfers into the intensive care units, and interestingly, decreased mortality and morbidity, which is not easy to show. I talked about the electrocardiogram as a good example. Of course, everybody knows about the radiology things.&nbsp;</p>



<p>We&#8217;ve created … taken part of this and said, if we can do this in the hospital, why cannot we do it in patients&#8217; homes? So being very active in looking after patients that would come to the ED, emergency room, would normally be admitted, and we say, no, here are the things we can give you. Go home if you want to, and we will safely look after you at your home. And we recently have been, looking at the last two years of data, been able to show that we&#8217;re also successfully able to give intravenous chemotherapy in patients&#8217; homes because we can monitor; we can do all the things that we can do.&nbsp;</p>



<p>Now, with generative AI, that gave us many other opportunities. One biggest opportunity for me has always been digital pathology. When we see how pathology’s currently run with a glass slide, not much has changed …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … in many, many, many years, right.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> And so really we have made a massive push to digitize pathology not just for us but for others. But talking about ourselves, we started by saying, it has to be very cheap to digitize. So we worked and created a company with partners called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.pramana.ai/" target="_blank" rel="noopener noreferrer">Pramana<span class="sr-only"> (opens in new tab)</span></a> that allows us to digitize slides relatively cheaply using AI algorithms that can take away the dirt, the fingerprint. And so we end up with 21 million of our slides digitized, and that gives you now a massive opportunity. Worked with another company called <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://www.aignostics.com/" target="_blank" rel="noopener noreferrer">Aignostics<span class="sr-only"> (opens in new tab)</span></a> to create a, what we call, <a class="msr-external-link glyph-append glyph-append-open-in-new-tab glyph-append-xsmall" href="https://arxiv.org/abs/2501.05409" target="_blank" rel="noopener noreferrer">Atlas<span class="sr-only"> (opens in new tab)</span></a>, which is an LLM that allows us to then build upon it.&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> And we, a hundred and, I think, 120 years ago, invented frozen sections at Mayo Clinic. So what that is, is that while the patient&#8217;s still on the table, you can take a piece of tissue, look at it, and tell the surgeon the margins of what you&#8217;re trying to resect are clear or not. But as a result of that, because you have to hurry, you get no information as a surgeon about, is it an invasive cancer, is it noninvasive cancer, or other things. So we&#8217;ve just found a way to digitize our frozen section practice and will completely go across the enterprise with AI-enabled digitized frozen sections, which then enables us to then do it for anybody across the globe if we need to.&nbsp;</p>



<p>And then in the genomic space, we&#8217;re working to create a true exomic transformer that is short range. And we originally started doing it to see if we can test it against the fact that 40% of people with rheumatoid arthritis don&#8217;t respond to the first-line therapy, …&nbsp;</p>



<p><strong>LEE:</strong> Right.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … but you have to wait six months to find out. And we found that we can actually do that. But it has much greater uses, of course.&nbsp;</p>



<p>And then we&#8217;re working with you—I don&#8217;t know how much you want to get into this, Peter, or [if] you want to talk about it yourself—<a href="https://www.microsoft.com/en-us/research/publication/maira-2-grounded-radiology-report-generation/" target="_blank" rel="noreferrer noopener">MAIRA-2</a>, which is really exciting, about how taking a simple problem—can you create a transformer that is able to detect if lines on the chest are in the right place, breathing tube is in the right place?—and then do it in a way that then can be used for many, many other things.&nbsp;</p>



<p>And then, Peter, because you asked about education and research, …&nbsp;</p>



<p><strong>LEE:</strong> Yeah.&nbsp;</p>



<p><strong>FARRUGIA:</strong> … imagine what this does now to the education system, right. And so we&#8217;ve got to train our physicians differently. We now have an AI curriculum for all our medical students. We offer masters and PhDs in AI. We think it&#8217;s essential for the people who want to be able to truly become experts, the same way I became an expert in my area of research.&nbsp;</p>



<p>And then from a research standpoint, when you think about all the registries that exist in people&#8217;s labs, all the spatial genomics, all the epigenomics, all the omics that exist. And if you are able to coalesce them into one big, what we call, an <em>atlas</em>, how that could really spur research at a scale that we haven&#8217;t thought of before. And so that is our aim at the moment.&nbsp;</p>



<p>From a research standpoint, we are, with Vijay Shah, who&#8217;s our dean of research, is to say, let&#8217;s make the effort of making sure all the data are available to be able to use and enable for us to take advantage of AI. And that is not easy because, of course, people have collected the data. They tend to want to embrace it.&nbsp;</p>



<p><strong>LEE: </strong>Yep.&nbsp;</p>



<p><strong>FARRUGIA: </strong>So there have to be the right incentives, the right privacy, and the right ways of doing it. And we think we&#8217;re on the way there, and we’re already seeing some advantages from doing it this way.&nbsp;</p>



<p><strong>LEE: </strong>So we&#8217;re running short on time. And so I always like to end with one or two more provocative questions. And, you know, it&#8217;s tempting to ask you the provocative question of whether you think AI will ever replace human doctors, but I don&#8217;t want to go there with you. In fact, as I thought about our discussion, I was reflecting. We were at a conference together once, and I was on stage in a fireside chat. And then, you know, after the fireside chat, there were audience questions, and I don&#8217;t remember any of the questions from the audience except <em>yours</em>.&nbsp;</p>



<p>And just to remind you, you know, I think when I was on stage, we were talking about a lot of practical uses of AI to, let&#8217;s say, reduce administrative burdens and so on in healthcare. But you got up and you, I won&#8217;t say you scolded me, but you more or less said, is it the right idea to use AI to optimize today&#8217;s somewhat broken healthcare system, or should we be thinking more boldly about, you know, a more fundamental transformation?&nbsp;</p>



<p>And so what I thought I would try to close with here is to hear what was really behind that question. You know, what were you trying to get me to think about when you asked that question?&nbsp;</p>



<p><strong>FARRUGIA:</strong> So first of all, darn your great memory [LAUGHTER]. Belated apologies … I probably should have &#8230;&nbsp;</p>



<p><strong>LEE:</strong> It was by far the best and most sophisticated and, I think, thought-provoking question of all of the ones that came out of the audience.&nbsp;</p>



<p><strong>FARRUGIA:</strong> What I was trying to get to is actually trying to clarify it in my own head and then in the head of others is that we do not need to have a linear path to get to where we want to get to. And we seemed to be on a linear path, which is, let&#8217;s try and reduce administrative burden. Let&#8217;s try and truly be a companion to a physician or other provider. Let&#8217;s make their problems better, make them feel better about providing healthcare. And then in the next step, we keep going until we get to, now we can call it agentic AI, whatever we want to talk about. And my view was, <em>no</em>, is that let&#8217;s start with that aim, the last aim, and do the others because the others will come automatically if you&#8217;re working on that harder problem.&nbsp;</p>



<p>Because one, to get to that harder problem, you&#8217;ll find all the other solutions. I was just trying to push that here&#8217;s this wonderful tool that&#8217;s been given to us. Let&#8217;s take advantage of it as quickly as we can. I think we had gotten a little too sensitized to need to say the right things. “Careful, be very careful” versus saying, “Massive opportunity. Do it right, and healthcare will be much better. Go for it.”&nbsp;</p>



<p><strong>LEE:</strong> Well, I think I understand better now where the vision, insight, and frankly, courage to take on something as ambitious and transformational as the Mayo Clinic Platform and really all of your leadership in your tenure as the president and CEO of Mayo Clinic. I think I understand it much better now.&nbsp;</p>



<p>Gianrico, it&#8217;s just always such a privilege to interact with you and now to have a chance to work with you more closely. So thank you for everything that you do and thank you for joining us today.&nbsp;</p>



<p><strong>FARRUGIA:</strong> Thank you for making it so easy, and thanks for giving us this opportunity to do good for the world.&nbsp;</p>



<p>[TRANSITION MUSIC] </p>



<p><strong>LEE: </strong>Gianrico leads what is arguably the crown jewel of the world&#8217;s healthcare systems, and so I feel it&#8217;s such a privilege to be able to talk and sometimes even brainstorm with him.&nbsp;</p>



<p>Our conversation, I think, exposed just how tech forward Gianrico is as he charts the strategies for healthcare delivery well into the future. And as I&#8217;ve interacted with many others, what I&#8217;ve learned is that this is a common trait among major health system CEOs. Roughly speaking, like we&#8217;ve seen in previous episodes where doctors and med students are polymath clinician-technologists, the same thing is true of health system CEOs and other leaders.&nbsp;</p>



<p>AI in the mind of a health system CEO today is not only a technology that can transform diagnosis and treatment, but it&#8217;s also something that can have a huge impact on the business of healthcare delivery, the connection of healthcare to medical research, and the journeys that patients go through as they seek better health.&nbsp;</p>



<p>These two conversations show that virtually all leaders in health and medicine are confronting head-on the opportunities, challenges, and the reality of AI, and they see a future that is potentially very different than what we have today.&nbsp;</p>



<p>[THEME MUSIC]&nbsp;</p>



<p>I&#8217;d like to thank Umair and Gianrico again for their time and insights. And to our listeners, thank you for joining us. We hope you&#8217;ll tune in to our final episode of the series. My coauthors, Carey and Zak, will be back to examine the takeaways from our most recent conversations.&nbsp;</p>



<p>Until next time.&nbsp;</p>



<p>[MUSIC FADES]&nbsp;</p>

				</span>
			</div>
			<button
				class="action-trigger glyph-prepend mt-2 mb-0 show-more-show-less-toggle"
				aria-expanded="false"
				data-show-less-text="Show less"
				type="button"
				aria-controls="show-more-show-less-toggle-3"
				aria-label="Show more content"
				data-alternate-aria-label="Show less content">
				Show more			</button>
		</div>
	</div>
</div>



<div class="wp-block-buttons is-layout-flex wp-block-buttons-is-layout-flex">
<div class="wp-block-button is-style-outline is-style-outline--4"><a data-bi-type="button" class="wp-block-button__link wp-element-button" href="https://www.microsoft.com/en-us/research/story/the-ai-revolution-in-medicine-revisited/">AI Revolution in Medicine podcast series</a></div>
</div>



<p></p>
<span id="label-external-link" class="sr-only" aria-hidden="true">Opens in a new tab</span><p>The post <a href="https://www.microsoft.com/en-us/research/podcast/reimagining-healthcare-delivery-and-public-health-with-ai/">Reimagining healthcare delivery and public health with AI</a> appeared first on <a href="https://www.microsoft.com/en-us/research">Microsoft Research</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
